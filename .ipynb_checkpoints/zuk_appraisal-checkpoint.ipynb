{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nurit.xlsx', 'Tzipy.xlsx', 'Anat.xlsx', 'Dan.xlsx', 'Nehoray.xlsx']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import join\n",
    "\n",
    "listdir('data/appraisal/filled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "apps = pd.concat([pd.read_excel(join('data/appraisal/filled', f), usecols=(0,1,2), names=('uri_unsh_no_clean','appraisal','comments')) for f in listdir('data/appraisal/filled')]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "apps['appraisal'] = apps['appraisal'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     0.763647\n",
       "False    0.236353\n",
       "Name: appraisal, dtype: float64"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apps.appraisal.value_counts() / apps.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "internal 404                  112\n",
       "homepage, no carbon dating     89\n",
       "Later date                     32\n",
       "irrelevant                     25\n",
       "server error                   24\n",
       "no content                     24\n",
       "generic site                   17\n",
       "Earlier date                   12\n",
       "Redirects to a dead link       11\n",
       "later date                     11\n",
       "early date                      9\n",
       "earlier date                    4\n",
       "Homepage, no carbon dating      4\n",
       "wall garden                     3\n",
       "*                               2\n",
       "Wall garden                     2\n",
       "walled garden                   1\n",
       "404                             1\n",
       "Server Error                    1\n",
       "irrelrvant                      1\n",
       "redirects to a dead link        1\n",
       "Name: comments, dtype: int64"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apps.comments.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "apps['comments'] = apps.comments.str.lower()\n",
    "apps.loc[apps.comments=='wall garden', 'comments'] = 'walled garden'\n",
    "apps.loc[apps.comments=='early date', 'comments'] = 'earlier date'\n",
    "apps.loc[apps.comments=='irrelrvant', 'comments'] = 'irrelevant'\n",
    "apps.loc[(((~apps.appraisal)) & (apps.comments.isna()) | (apps.comments=='')) , 'comments'] = 'irrelevant'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "internal 404                  112\n",
       "homepage, no carbon dating     93\n",
       "irrelevant                     56\n",
       "later date                     43\n",
       "earlier date                   25\n",
       "server error                   25\n",
       "no content                     24\n",
       "generic site                   17\n",
       "redirects to a dead link       12\n",
       "walled garden                   6\n",
       "*                               2\n",
       "Name: comments, dtype: int64"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apps.comments.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lang</th>\n",
       "      <td>zh</td>\n",
       "      <td>zh</td>\n",
       "      <td>zh</td>\n",
       "      <td>zh</td>\n",
       "      <td>zh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <td>wikipedia</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <td>2018-08-22 13:21:14</td>\n",
       "      <td>2018-06-25 15:15:42</td>\n",
       "      <td>2017-11-10 21:53:43</td>\n",
       "      <td>2017-11-10 21:53:43</td>\n",
       "      <td>2017-11-10 21:53:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uri</th>\n",
       "      <td>https://web.archive.org/web/20150626164255/htt...</td>\n",
       "      <td>https://web.archive.org/web/20140727005451/htt...</td>\n",
       "      <td>https://archive.is/20140828023420/http://www.w...</td>\n",
       "      <td>https://web.archive.org/web/20140803200140/htt...</td>\n",
       "      <td>https://web.archive.org/web/20140808044248/htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uri_cln</th>\n",
       "      <td>web.archive.org/web/20150626164255/http://www....</td>\n",
       "      <td>web.archive.org/web/20140727005451/http://trav...</td>\n",
       "      <td>archive.is/20140828023420/http://www.worldjour...</td>\n",
       "      <td>web.archive.org/web/20140803200140/http://www....</td>\n",
       "      <td>web.archive.org/web/20140808044248/http://m.al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tld</th>\n",
       "      <td>archive.org</td>\n",
       "      <td>archive.org</td>\n",
       "      <td>archive.is</td>\n",
       "      <td>archive.org</td>\n",
       "      <td>archive.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_shortened</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uri_unshrtn</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uri_cln_unshrtn</th>\n",
       "      <td>web.archive.org/web/20150626164255/http://www....</td>\n",
       "      <td>web.archive.org/web/20140727005451/http://trav...</td>\n",
       "      <td>archive.is/20140828023420/http://www.worldjour...</td>\n",
       "      <td>web.archive.org/web/20140803200140/http://www....</td>\n",
       "      <td>web.archive.org/web/20140808044248/http://m.al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uri_unshrtn_bitly</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uri_unshrtn_raw</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tld_unshrtn</th>\n",
       "      <td>archive.org</td>\n",
       "      <td>archive.org</td>\n",
       "      <td>archive.is</td>\n",
       "      <td>archive.org</td>\n",
       "      <td>archive.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uri_cln_wb_avail</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uri_cln_unshrtn_wb_avail</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uri_unsh_no_clean</th>\n",
       "      <td>https://web.archive.org/web/20150626164255/htt...</td>\n",
       "      <td>https://web.archive.org/web/20140727005451/htt...</td>\n",
       "      <td>https://archive.is/20140828023420/http://www.w...</td>\n",
       "      <td>https://web.archive.org/web/20140803200140/htt...</td>\n",
       "      <td>https://web.archive.org/web/20140808044248/htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uri_mem_avail</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uri_unsh_no_clean_mem_avail</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>status_code</th>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>content_type</th>\n",
       "      <td>text/html; charset=utf-8</td>\n",
       "      <td>text/html;charset=utf-8</td>\n",
       "      <td>text/html;charset=utf-8</td>\n",
       "      <td>text/html; charset=utf-8-sig</td>\n",
       "      <td>text/html; charset=utf-8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                             0  \\\n",
       "lang                                                                        zh   \n",
       "source                                                               wikipedia   \n",
       "timestamp                                                  2018-08-22 13:21:14   \n",
       "uri                          https://web.archive.org/web/20150626164255/htt...   \n",
       "uri_cln                      web.archive.org/web/20150626164255/http://www....   \n",
       "tld                                                                archive.org   \n",
       "is_shortened                                                             False   \n",
       "uri_unshrtn                                                                NaN   \n",
       "uri_cln_unshrtn              web.archive.org/web/20150626164255/http://www....   \n",
       "uri_unshrtn_bitly                                                          NaN   \n",
       "uri_unshrtn_raw                                                            NaN   \n",
       "tld_unshrtn                                                        archive.org   \n",
       "uri_cln_wb_avail                                                         False   \n",
       "uri_cln_unshrtn_wb_avail                                                 False   \n",
       "uri_unsh_no_clean            https://web.archive.org/web/20150626164255/htt...   \n",
       "uri_mem_avail                                                            False   \n",
       "uri_unsh_no_clean_mem_avail                                              False   \n",
       "status_code                                                                200   \n",
       "content_type                                          text/html; charset=utf-8   \n",
       "\n",
       "                                                                             1  \\\n",
       "lang                                                                        zh   \n",
       "source                                                               wikipedia   \n",
       "timestamp                                                  2018-06-25 15:15:42   \n",
       "uri                          https://web.archive.org/web/20140727005451/htt...   \n",
       "uri_cln                      web.archive.org/web/20140727005451/http://trav...   \n",
       "tld                                                                archive.org   \n",
       "is_shortened                                                             False   \n",
       "uri_unshrtn                                                                NaN   \n",
       "uri_cln_unshrtn              web.archive.org/web/20140727005451/http://trav...   \n",
       "uri_unshrtn_bitly                                                          NaN   \n",
       "uri_unshrtn_raw                                                            NaN   \n",
       "tld_unshrtn                                                        archive.org   \n",
       "uri_cln_wb_avail                                                         False   \n",
       "uri_cln_unshrtn_wb_avail                                                 False   \n",
       "uri_unsh_no_clean            https://web.archive.org/web/20140727005451/htt...   \n",
       "uri_mem_avail                                                            False   \n",
       "uri_unsh_no_clean_mem_avail                                              False   \n",
       "status_code                                                                200   \n",
       "content_type                                           text/html;charset=utf-8   \n",
       "\n",
       "                                                                             2  \\\n",
       "lang                                                                        zh   \n",
       "source                                                               wikipedia   \n",
       "timestamp                                                  2017-11-10 21:53:43   \n",
       "uri                          https://archive.is/20140828023420/http://www.w...   \n",
       "uri_cln                      archive.is/20140828023420/http://www.worldjour...   \n",
       "tld                                                                 archive.is   \n",
       "is_shortened                                                             False   \n",
       "uri_unshrtn                                                                NaN   \n",
       "uri_cln_unshrtn              archive.is/20140828023420/http://www.worldjour...   \n",
       "uri_unshrtn_bitly                                                          NaN   \n",
       "uri_unshrtn_raw                                                            NaN   \n",
       "tld_unshrtn                                                         archive.is   \n",
       "uri_cln_wb_avail                                                         False   \n",
       "uri_cln_unshrtn_wb_avail                                                 False   \n",
       "uri_unsh_no_clean            https://archive.is/20140828023420/http://www.w...   \n",
       "uri_mem_avail                                                            False   \n",
       "uri_unsh_no_clean_mem_avail                                              False   \n",
       "status_code                                                                200   \n",
       "content_type                                           text/html;charset=utf-8   \n",
       "\n",
       "                                                                             3  \\\n",
       "lang                                                                        zh   \n",
       "source                                                               wikipedia   \n",
       "timestamp                                                  2017-11-10 21:53:43   \n",
       "uri                          https://web.archive.org/web/20140803200140/htt...   \n",
       "uri_cln                      web.archive.org/web/20140803200140/http://www....   \n",
       "tld                                                                archive.org   \n",
       "is_shortened                                                             False   \n",
       "uri_unshrtn                                                                NaN   \n",
       "uri_cln_unshrtn              web.archive.org/web/20140803200140/http://www....   \n",
       "uri_unshrtn_bitly                                                          NaN   \n",
       "uri_unshrtn_raw                                                            NaN   \n",
       "tld_unshrtn                                                        archive.org   \n",
       "uri_cln_wb_avail                                                         False   \n",
       "uri_cln_unshrtn_wb_avail                                                 False   \n",
       "uri_unsh_no_clean            https://web.archive.org/web/20140803200140/htt...   \n",
       "uri_mem_avail                                                            False   \n",
       "uri_unsh_no_clean_mem_avail                                              False   \n",
       "status_code                                                                200   \n",
       "content_type                                      text/html; charset=utf-8-sig   \n",
       "\n",
       "                                                                             4  \n",
       "lang                                                                        zh  \n",
       "source                                                               wikipedia  \n",
       "timestamp                                                  2017-11-10 21:53:43  \n",
       "uri                          https://web.archive.org/web/20140808044248/htt...  \n",
       "uri_cln                      web.archive.org/web/20140808044248/http://m.al...  \n",
       "tld                                                                archive.org  \n",
       "is_shortened                                                             False  \n",
       "uri_unshrtn                                                                NaN  \n",
       "uri_cln_unshrtn              web.archive.org/web/20140808044248/http://m.al...  \n",
       "uri_unshrtn_bitly                                                          NaN  \n",
       "uri_unshrtn_raw                                                            NaN  \n",
       "tld_unshrtn                                                        archive.org  \n",
       "uri_cln_wb_avail                                                         False  \n",
       "uri_cln_unshrtn_wb_avail                                                 False  \n",
       "uri_unsh_no_clean            https://web.archive.org/web/20140808044248/htt...  \n",
       "uri_mem_avail                                                            False  \n",
       "uri_unsh_no_clean_mem_avail                                              False  \n",
       "status_code                                                                200  \n",
       "content_type                                          text/html; charset=utf-8  "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lut = pd.read_pickle('lang_uri_time_wb_avail_all.pkl.gz', compression='gzip')\n",
    "macd_all = pd.read_pickle('data/macd_all.pkl.gz', compression='gzip')\n",
    "lut['uri_unsh_no_clean'] = lut.uri_unshrtn_raw.combine_first(lut.uri_unshrtn_bitly.combine_first(lut.uri_unshrtn.combine_first(lut.uri)))\n",
    "lut['uri_mem_avail'] = lut.uri.map(~macd_all.drop_duplicates(subset='uri').set_index('uri').first_snap.isna()).fillna(False)\n",
    "lut['uri_unsh_no_clean_mem_avail'] = lut.uri_unsh_no_clean.map(~macd_all.drop_duplicates(subset='uri').set_index('uri').first_snap.isna()).fillna(False)\n",
    "curl = pd.read_csv('data/curl_results_parsed.csv.gz', compression='gzip')\n",
    "lut['status_code'] = lut.uri_unsh_no_clean.map(curl.drop_duplicates(subset='url').set_index('url').status_code)\n",
    "lut['content_type'] = lut.uri_unsh_no_clean.map(curl.drop_duplicates(subset='url').set_index('url').content_type)\n",
    "lut.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "http://www.independent.co.uk/news/world/middle-east/israelgaza-conflict-the-myth-of-hamass-human-shield-9619810.html                                                                                                                                                                                                                                                                                                                                                                                                  737\n",
       "https://www.amnesty.org/en/latest/news/2014/07/israelgaza-conflict-questions-and-answers/                                                                                                                                                                                                                                                                                                                                                                                                                             376\n",
       "https://www.independent.co.uk/news/world/middle-east/israel-gaza-conflict-social-media-becomes-the-latest-battleground-in-middle-east-aggression-but-9605952.html                                                                                                                                                                                                                                                                                                                                                     256\n",
       "https://www.independent.co.uk/news/uk/home-news/israel-gaza-conflict-thousands-protest-in-london-for-end-to-massacre-and-arms-trade-9659180.html                                                                                                                                                                                                                                                                                                                                                                      224\n",
       "https://abcnews.go.com/International/israel-gaza-conflict/story?id=24552237                                                                                                                                                                                                                                                                                                                                                                                                                                           218\n",
       "https://www.independent.co.uk/news/world/middle-east/israel-gaza-conflict-hamas-says-long-term-truce-agreed-with-israel-9691910.html                                                                                                                                                                                                                                                                                                                                                                                  218\n",
       "http://www.amnesty.org/en/news/israelgaza-conflict-questions-and-answers-2014-07-25                                                                                                                                                                                                                                                                                                                                                                                                                                   210\n",
       "https://www.independent.co.uk/news/world/middle-east/israel-gaza-conflict-80-per-cent-of-palestinians-killed-by-israeli-strikes-are-civilians-un-report-9606397.html                                                                                                                                                                                                                                                                                                                                                  202\n",
       "https://www.independent.co.uk/news/world/middle-east/revealed-britain-s-role-in-arming-israel-9643902.html                                                                                                                                                                                                                                                                                                                                                                                                            196\n",
       "https://www.independent.co.uk/news/world/middle-east/israel-gaza-conflict-turkish-campaign-group-to-challenge-gaza-blockade-by-sending-flotilla-full-of-9661748.html                                                                                                                                                                                                                                                                                                                                                  196\n",
       "https://www.independent.co.uk/news/world/middle-east/israel-gaza-conflict-israeli-military-using-flechette-rounds-in-gaza-strip-9617480.html                                                                                                                                                                                                                                                                                                                                                                          189\n",
       "https://www.independent.co.uk/news/world/middle-east/israel-gaza-conflict-human-cost-of-israel-s-onslaught-on-gaza-towns-continues-to-rise-9603400.html                                                                                                                                                                                                                                                                                                                                                               186\n",
       "https://www.nytimes.com/2014/08/02/world/europe/anger-in-europe-over-the-israeli-gaza-conflict-reverberates-as-anti-semitism.html                                                                                                                                                                                                                                                                                                                                                                                     186\n",
       "https://www.independent.co.uk/news/world/middle-east/israel-gaza-conflict-childs-death-unleashes-new-israeli-rocket-attacks-9687851.html                                                                                                                                                                                                                                                                                                                                                                              182\n",
       "https://www.independent.co.uk/voices/israel-gaza-conflict-medical-charity-official-likens-job-to-patching-up-torture-victims-in-an-open-9613296.html                                                                                                                                                                                                                                                                                                                                                                  178\n",
       "https://www.independent.co.uk/news/world/middle-east/israel-gaza-conflict-britain-warns-netanyahu-the-west-is-losing-sympathy-for-israel-9625417.html                                                                                                                                                                                                                                                                                                                                                                 174\n",
       "https://www.independent.co.uk/news/world/middle-east/israel-gaza-conflict-israeli-knock-on-roof-missile-warning-technique-revealed-in-stunning-video-9603179.html                                                                                                                                                                                                                                                                                                                                                     173\n",
       "https://www.independent.co.uk/news/world/middle-east/israel-gaza-conflict-gaza-death-toll-rises-as-the-world-protests-9617003.html                                                                                                                                                                                                                                                                                                                                                                                    170\n",
       "http://www.independent.co.uk/news/world/middle-east/israelgaza-conflict-israeli-knock-on-roof-missile-warning-technique-revealed-in-stunning-video-9603179.html                                                                                                                                                                                                                                                                                                                                                       169\n",
       "https://www.independent.co.uk/news/world/middle-east/israel-gaza-conflict-french-minister-says-solution-must-be-imposed-as-turkeys-pm-accuses-israel-of-9646491.html                                                                                                                                                                                                                                                                                                                                                  167\n",
       "https://www.independent.co.uk/news/world/middle-east/israel-gaza-conflict-sale-of-military-equipment-to-israel-linked-to-success-of-peace-talks-claims-9664962.html                                                                                                                                                                                                                                                                                                                                                   166\n",
       "https://www.independent.co.uk/news/world/middle-east/israel-gaza-conflict-israel-takes-brutal-revenge-on-rafah-for-the-loss-of-a-soldier-9648004.html                                                                                                                                                                                                                                                                                                                                                                 166\n",
       "https://www.independent.co.uk/voices/comment/israel-gaza-conflict-what-has-israel-achieved-in-26-bloody-days-9644508.html                                                                                                                                                                                                                                                                                                                                                                                             165\n",
       "https://www.telegraph.co.uk/news/worldnews/middleeast/gaza/10999571/Israel-Gaza-conflict-time-lapse-shows-air-strikes-destroying-entire-neighbourhood-in-an-hour.html                                                                                                                                                                                                                                                                                                                                                 164\n",
       "https://www.independent.co.uk/news/world/middle-east/israel-gaza-conflict-tanks-shell-gaza-hospital-killing-four-and-wounding-30-medical-staff-as-seven-9619055.html                                                                                                                                                                                                                                                                                                                                                  164\n",
       "https://en.wikipedia.org/wiki/2014_Israel%E2%80%93Gaza_conflict                                                                                                                                                                                                                                                                                                                                                                                                                                                       164\n",
       "https://www.independent.co.uk/news/world/middle-east/israel-gaza-conflict-palestinian-dead-now-number-500-but-israel-fights-on-9619901.html                                                                                                                                                                                                                                                                                                                                                                           162\n",
       "https://www.telegraph.co.uk/news/worldnews/middleeast/israel/10994385/Israel-Gaza-conflict-Obama-and-Pope-call-for-peace-as-child-death-toll-rises.html                                                                                                                                                                                                                                                                                                                                                               162\n",
       "https://www.almasryalyoum.com/news/details/486994                                                                                                                                                                                                                                                                                                                                                                                                                                                                     161\n",
       "https://www.independent.co.uk/news/world/middle-east/israel-gaza-conflict-when-genocide-is-permissible-article-removed-from-the-times-of-israel-website-9643888.html                                                                                                                                                                                                                                                                                                                                                  161\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ... \n",
       "http://www.lunion.presse.fr/france-monde/gaza-43-palestiniens-tues-depuis-le-debut-de-l-offensive-ia0b0n375613                                                                                                                                                                                                                                                                                                                                                                                                          1\n",
       "https://twib.in/l/aAgBnqb8g96                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           1\n",
       "http://www.alarabiya.net/ar/arab-and-world/2014/07/08/%D8%BA%D8%B2%D8%A9-%DB%B5%DB%B0-%D9%87%D8%AF%D9%81-%D9%82%D8%B5%D9%81%D8%AA%D9%87%D8%A7-%D8%A5%D8%B3%D8%B1%D8%A7%D8%A6%D9%8A%D9%84-%D9%88%D8%AD%D9%85%D8%A7%D8%B3-%D8%AA%D8%AA%D9%88%D8%B9%D8%AF-%D8%A8%D8%B1%D8%AF-%D9%85%D8%B2%D9%84%D8%B2%D9%84.html                                                                                                                                                                                                           1\n",
       "http://rotter.net/forum/scoops1/133394.shtml                                                                                                                                                                                                                                                                                                                                                                                                                                                                            1\n",
       "http://www.dagen.no/Nyheter/article107811.ece                                                                                                                                                                                                                                                                                                                                                                                                                                                                           1\n",
       "https://www.alquds.co.uk/%d8%a7%d8%b1%d8%aa%d9%81%d8%a7%d8%b9-%d8%b6%d8%ad%d8%a7%d9%8a%d8%a7-%d8%a7%d9%84%d8%ad%d8%b1%d8%a8-%d8%a7%d9%84%d8%a5%d8%b3%d8%b1%d8%a7%d8%a6%d9%8a%d9%84%d9%8a%d8%a9-%d8%b9%d9%84%d9%89-%d8%ba%d8%b2-4/                                                                                                                                                                                                                                                                                       1\n",
       "http://ask.fm/a/b0k1pgf0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                1\n",
       "https://goo.gl/fb/XNoyNR                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                1\n",
       "http://linkis.com/youtu.be/VvzSq                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        1\n",
       "http://nrodl.zdf.de/none/zdf/14/07/140716_sievers_ajo_2256k_p14v11.mp4                                                                                                                                                                                                                                                                                                                                                                                                                                                  1\n",
       "https://www.ynetnews.com/articles/0,7340,L-4539759,00.html                                                                                                                                                                                                                                                                                                                                                                                                                                                              1\n",
       "http://tamayouze.com/?p=32014                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           1\n",
       "http://atfp.co/1m29Bht                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  1\n",
       "http://thebea.st/1qOUEW7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                1\n",
       "https://www.elnashra.com/news/show/773321/%D8%A7%D9%84%D9%82%D8%B3%D8%A7%D9%85-%D9%85%D9%82%D8%AA%D9%84-131-%D8%B6%D8%A7%D8%A8%D8%B7%D8%A7-%D9%88%D8%AC%D9%86%D8%AF%D9%8A%D8%A7-%D8%A5%D8%B3%D8%B1%D8%A7%D8%A6%D9%8A%D9%84%D9%8A%D8%A7-%D8%AE%D9%84%D8%A7%D9%84-%D8%A7%D9%84%D8%AD%D8%B1%D8%A8-                                                                                                                                                                                                                         1\n",
       "https://arabi21.com/story/765589/%D8%A5%D8%BA%D9%86%D8%A7%D8%AA%D9%8A%D9%88%D8%B3-%D9%83%D9%8A%D8%B1%D9%8A-%D9%84%D9%8A%D8%B3-%D8%B6%D8%AF-%D8%A5%D8%B3%D8%B1%D8%A7%D8%A6%D9%8A%D9%84-%D9%84%D9%83%D9%86-%D9%85%D8%A8%D8%A7%D8%AF%D8%B1%D8%AA%D9%87-%D8%AE%D8%A7%D8%B7%D8%A6%D8%A9                                                                                                                                                                                                                                      1\n",
       "http://www.haokets.org/2014/08/14/%d7%95%d7%90%d7%97%d7%a8%d7%99-%d7%a6%d7%95%d7%a7-%d7%90%d7%99%d7%aa%d7%9f-%d7%9e%d7%94-%d7%9e%d7%97%d7%9b%d7%94-%d7%9c%d7%a0%d7%95/                                                                                                                                                                                                                                                                                                                                                  1\n",
       "https://www.thestar.com/news/world/2014/07/24/ban_kimoon_condemns_shelling_of_unrun_school_in_gaza.html                                                                                                                                                                                                                                                                                                                                                                                                                 1\n",
       "http://www.haaretz.com/opinion/1.613047                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 1\n",
       "http://www.skynewsarabia.com/web/article/674697                                                                                                                                                                                                                                                                                                                                                                                                                                                                         1\n",
       "https://www.aljazeera.net/news/reportsandinterviews/2014/8/2/%D8%A3%D8%B5%D9%88%D8%A7%D8%AA-%D8%A3%D8%AF%D8%A8%D9%8A%D8%A9-%D8%A8%D8%A5%D8%B3%D8%B1%D8%A7%D8%A6%D9%8A%D9%84-%D8%AA%D8%B9%D8%A7%D8%B1%D8%B6-%D8%A7%D9%84%D8%AD%D8%B1%D8%A8-%D8%B9%D9%84%D9%89-%D8%BA%D8%B2%D8%A9?utm_medium=twitter&utm_source=twitterfeed                                                                                                                                                                                               1\n",
       "http://www.the7eye.org.il/121530                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        1\n",
       "http://p.cipr.es/KMz                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    1\n",
       "http://www.elshaab.org/news/124540/%D8%A7%D9%84%D8%B9%D9%81%D9%88-%D8%A7%D9%84%D8%AF%D9%88%D9%84%D9%8A%D8%A9-%D8%A7%D9%84%D9%87%D8%AC%D9%85%D8%A7%D8%AA-%D8%B9%D9%84%D9%89-%D8%A7%D9%84%D9%85%D8%B1%D8%A7%D9%81%D9%82-%D8%A7%D9%84%D8%B7%D8%A8%D9%8A%D8%A9-%D9%88%D8%A7%D9%84%D9%85%D8%AF%D9%86%D9%8A%D9%8A%D9%86-%D8%AA%D8%B9%D8%B2%D8%B2-%D9%85%D8%B2%D8%A7%D8%B9%D9%85-%D8%AC%D8%B1%D8%A7%D8%A6%D9%85-%D8%A7%D9%84%D8%AD%D8%B1%D8%A8-%D9%81%D9%8A-%D8%BA%D8%B2%D8%A9?ref=home-latest-news#.U88B-u7AmWc.facebook      1\n",
       "https://is.gd/zJrz7O                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    1\n",
       "https://books.google.co.il/books?id=U8ImDwAAQBAJ&pg=PT40&lpg=PT40&dq=%22%D8%A7%D9%84%D8%AD%D8%B1%D8%A8+%D8%B9%D9%84%D9%89+%D8%BA%D8%B2%D8%A9+%22&source=bl&ots=ykNro2yt0l&sig=iVHY4y0frcYyrlMB4XTd0IJLGqI&hl=iw&sa=X&ved=2ahUKEwjr3-mc49HfAhWR6aQKHbZUA5o4ZBDoATBXegQIEBAB                                                                                                                                                                                                                                              1\n",
       "http://godssecret.wordpress.com/category/gaza/WATCHANDPRAYFORWORLDPEACE                                                                                                                                                                                                                                                                                                                                                                                                                                                 1\n",
       "http://news.walla.co.il/?w=/2689/2770163                                                                                                                                                                                                                                                                                                                                                                                                                                                                                1\n",
       "http://newsru.co.il/mideast/01aug2014/pal8015.html                                                                                                                                                                                                                                                                                                                                                                                                                                                                      1\n",
       "http://rotter.net/forum/scoops1/132752.shtml                                                                                                                                                                                                                                                                                                                                                                                                                                                                            1\n",
       "Name: uri_unsh_no_clean, Length: 1718, dtype: int64"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lap = lut.merge(apps, how='inner')\n",
    "lap.uri_unsh_no_clean.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>appraisal</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>google</th>\n",
       "      <td>134</td>\n",
       "      <td>478</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>twitter</th>\n",
       "      <td>226</td>\n",
       "      <td>1038</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wikipedia</th>\n",
       "      <td>62</td>\n",
       "      <td>431</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>youtube</th>\n",
       "      <td>42</td>\n",
       "      <td>44</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "appraisal  False  True  ratio\n",
       "source                       \n",
       "google       134   478   0.78\n",
       "twitter      226  1038   0.82\n",
       "wikipedia     62   431   0.87\n",
       "youtube       42    44   0.51"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lap.groupby(['source', 'appraisal']).uri_unsh_no_clean.nunique()\n",
    " .unstack().astype(int)\n",
    " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>appraisal</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_shortened</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>353</td>\n",
       "      <td>1126</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>65</td>\n",
       "      <td>548</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "appraisal     False  True  ratio\n",
       "is_shortened                    \n",
       "False           353  1126   0.76\n",
       "True             65   548   0.89"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lap.groupby(['is_shortened', 'appraisal']).uri_unsh_no_clean.nunique()\n",
    " .unstack().astype(int)\n",
    " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>appraisal</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_unshrtn</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>363</td>\n",
       "      <td>1126</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>55</td>\n",
       "      <td>548</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "appraisal   False  True  ratio\n",
       "is_unshrtn                    \n",
       "False         363  1126   0.76\n",
       "True           55   548   0.91"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lap['is_unshrtn'] = lap.uri_cln_unshrtn!=lap.uri_cln\n",
    "(lap.groupby(['is_unshrtn', 'appraisal']).uri_unsh_no_clean.nunique()\n",
    " .unstack().astype(int)\n",
    " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>appraisal</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <th>is_unshrtn</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">google</th>\n",
       "      <th>False</th>\n",
       "      <td>134</td>\n",
       "      <td>478</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">twitter</th>\n",
       "      <th>False</th>\n",
       "      <td>175</td>\n",
       "      <td>523</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>53</td>\n",
       "      <td>546</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wikipedia</th>\n",
       "      <th>False</th>\n",
       "      <td>62</td>\n",
       "      <td>431</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">youtube</th>\n",
       "      <th>False</th>\n",
       "      <td>40</td>\n",
       "      <td>42</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "appraisal             False  True  ratio\n",
       "source    is_unshrtn                    \n",
       "google    False         134   478   0.78\n",
       "          True            0     0    NaN\n",
       "twitter   False         175   523   0.75\n",
       "          True           53   546   0.91\n",
       "wikipedia False          62   431   0.87\n",
       "youtube   False          40    42   0.51\n",
       "          True            2     2   0.50"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lap.groupby(['source', 'is_unshrtn', 'appraisal']).uri_unsh_no_clean.nunique()\n",
    " .unstack().fillna(0).astype(int)\n",
    " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "lut['archived'] = lut[['uri_cln_wb_avail', 'uri_cln_unshrtn_wb_avail', 'uri_mem_avail', 'uri_unsh_no_clean_mem_avail']].any(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_values(g):\n",
    "    return ','.join(list(g.unique()))\n",
    "tlds_to_remove = ['fb.me', 'facebook.com', 'youtu.be', 'youtube.com']\n",
    "uris_for_appr = lut[(lut.uri_unsh_no_clean.isin(apps.uri_unsh_no_clean)) & (~lut.tld_unshrtn.isin(tlds_to_remove))].groupby(['uri_unsh_no_clean', 'tld_unshrtn', 'status_code']).agg({'archived': np.max, 'source': get_values}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "uris_for_appr = uris_for_appr.merge(apps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uri_unsh_no_clean</th>\n",
       "      <th>tld_unshrtn</th>\n",
       "      <th>status_code</th>\n",
       "      <th>archived</th>\n",
       "      <th>source</th>\n",
       "      <th>appraisal</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>feedproxy.google.com/~r/DefenseAndDiplomacy/~3...</td>\n",
       "      <td>google.com</td>\n",
       "      <td>301.0</td>\n",
       "      <td>False</td>\n",
       "      <td>twitter</td>\n",
       "      <td>False</td>\n",
       "      <td>server error</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://20to30.com/video/israeli-soldier-victim/</td>\n",
       "      <td>20to30.com</td>\n",
       "      <td>200.0</td>\n",
       "      <td>True</td>\n",
       "      <td>twitter</td>\n",
       "      <td>False</td>\n",
       "      <td>homepage, no carbon dating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://24sata.info/vijesti/vijesti/svijet/2036...</td>\n",
       "      <td>24sata.info</td>\n",
       "      <td>200.0</td>\n",
       "      <td>False</td>\n",
       "      <td>google</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://7online.com/217058/</td>\n",
       "      <td>7online.com</td>\n",
       "      <td>301.0</td>\n",
       "      <td>False</td>\n",
       "      <td>twitter</td>\n",
       "      <td>False</td>\n",
       "      <td>homepage, no carbon dating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://85fashionkitty.blogspot.co.il</td>\n",
       "      <td>blogspot.co.il</td>\n",
       "      <td>302.0</td>\n",
       "      <td>True</td>\n",
       "      <td>youtube</td>\n",
       "      <td>False</td>\n",
       "      <td>homepage, no carbon dating</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   uri_unsh_no_clean     tld_unshrtn  \\\n",
       "0  feedproxy.google.com/~r/DefenseAndDiplomacy/~3...      google.com   \n",
       "1    http://20to30.com/video/israeli-soldier-victim/      20to30.com   \n",
       "2  http://24sata.info/vijesti/vijesti/svijet/2036...     24sata.info   \n",
       "3                         http://7online.com/217058/     7online.com   \n",
       "4               http://85fashionkitty.blogspot.co.il  blogspot.co.il   \n",
       "\n",
       "   status_code  archived   source  appraisal                    comments  \n",
       "0        301.0     False  twitter      False                server error  \n",
       "1        200.0      True  twitter      False  homepage, no carbon dating  \n",
       "2        200.0     False   google       True                         NaN  \n",
       "3        301.0     False  twitter      False  homepage, no carbon dating  \n",
       "4        302.0      True  youtube      False  homepage, no carbon dating  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uris_for_appr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "uris_for_appr.to_csv('data/appraisal/appraisal_join.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>appraisal</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>google</th>\n",
       "      <td>122</td>\n",
       "      <td>129</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>twitter</th>\n",
       "      <td>174</td>\n",
       "      <td>397</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>twitter,google</th>\n",
       "      <td>10</td>\n",
       "      <td>325</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wikipedia</th>\n",
       "      <td>30</td>\n",
       "      <td>105</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wikipedia,google</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wikipedia,twitter</th>\n",
       "      <td>27</td>\n",
       "      <td>288</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wikipedia,twitter,google</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wikipedia,youtube</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wikipedia,youtube,twitter</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>youtube</th>\n",
       "      <td>26</td>\n",
       "      <td>17</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>youtube,google</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>youtube,twitter</th>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "appraisal                  False  True  ratio\n",
       "source                                       \n",
       "google                       122   129   0.51\n",
       "twitter                      174   397   0.70\n",
       "twitter,google                10   325   0.97\n",
       "wikipedia                     30   105   0.78\n",
       "wikipedia,google               2    12   0.86\n",
       "wikipedia,twitter             27   288   0.91\n",
       "wikipedia,twitter,google       0    11   1.00\n",
       "wikipedia,youtube              1     9   0.90\n",
       "wikipedia,youtube,twitter      2     6   0.75\n",
       "youtube                       26    17   0.40\n",
       "youtube,google                 0     1   1.00\n",
       "youtube,twitter               13    11   0.46"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(uris_for_appr\n",
    " .groupby(['source', 'appraisal']).size().unstack().fillna(0)\n",
    " .astype(int)\n",
    " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9233983286908078, 0.648)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared = ((uris_for_appr.source.str.contains(',')) & (uris_for_appr.appraisal)).sum() / uris_for_appr.source.str.contains(',').sum()\n",
    "not_shared = ((~uris_for_appr.source.str.contains(',')) & (uris_for_appr.appraisal)).sum() / (~uris_for_appr.source.str.contains(',')).sum()\n",
    "shared, not_shared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>appraisal</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>status_code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200.0</th>\n",
       "      <td>154</td>\n",
       "      <td>810</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301.0</th>\n",
       "      <td>211</td>\n",
       "      <td>451</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302.0</th>\n",
       "      <td>42</td>\n",
       "      <td>50</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "appraisal    False  True  ratio\n",
       "status_code                    \n",
       "200.0          154   810   0.84\n",
       "301.0          211   451   0.68\n",
       "302.0           42    50   0.54"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(uris_for_appr\n",
    " .groupby(['status_code', 'appraisal']).size().unstack().fillna(0)\n",
    " .astype(int)\n",
    " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>comments</th>\n",
       "      <th>*</th>\n",
       "      <th>earlier date</th>\n",
       "      <th>generic site</th>\n",
       "      <th>homepage, no carbon dating</th>\n",
       "      <th>internal 404</th>\n",
       "      <th>irrelevant</th>\n",
       "      <th>later date</th>\n",
       "      <th>no content</th>\n",
       "      <th>redirects to a dead link</th>\n",
       "      <th>server error</th>\n",
       "      <th>walled garden</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>google</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>33</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>twitter</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>50</td>\n",
       "      <td>53</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>twitter,google</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wikipedia</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wikipedia,google</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wikipedia,twitter</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wikipedia,youtube</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wikipedia,youtube,twitter</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>youtube</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>youtube,twitter</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "comments                   *  earlier date  generic site  \\\n",
       "source                                                     \n",
       "google                     0            13             1   \n",
       "twitter                    0             2             8   \n",
       "twitter,google             1             1             0   \n",
       "wikipedia                  0             5             0   \n",
       "wikipedia,google           0             0             0   \n",
       "wikipedia,twitter          1             3             7   \n",
       "wikipedia,youtube          0             0             0   \n",
       "wikipedia,youtube,twitter  0             0             0   \n",
       "youtube                    0             0             1   \n",
       "youtube,twitter            0             1             0   \n",
       "\n",
       "comments                   homepage, no carbon dating  internal 404  \\\n",
       "source                                                                \n",
       "google                                             20            23   \n",
       "twitter                                            50            53   \n",
       "twitter,google                                      3             2   \n",
       "wikipedia                                           5            11   \n",
       "wikipedia,google                                    0             0   \n",
       "wikipedia,twitter                                   3             9   \n",
       "wikipedia,youtube                                   0             1   \n",
       "wikipedia,youtube,twitter                           0             2   \n",
       "youtube                                            10             3   \n",
       "youtube,twitter                                     2             8   \n",
       "\n",
       "comments                   irrelevant  later date  no content  \\\n",
       "source                                                          \n",
       "google                             33          32           0   \n",
       "twitter                            11           2          18   \n",
       "twitter,google                      2           2           1   \n",
       "wikipedia                           2           3           1   \n",
       "wikipedia,google                    0           2           0   \n",
       "wikipedia,twitter                   1           1           3   \n",
       "wikipedia,youtube                   0           0           0   \n",
       "wikipedia,youtube,twitter           0           0           0   \n",
       "youtube                             7           0           0   \n",
       "youtube,twitter                     0           1           1   \n",
       "\n",
       "comments                   redirects to a dead link  server error  \\\n",
       "source                                                              \n",
       "google                                            2             0   \n",
       "twitter                                           7            20   \n",
       "twitter,google                                    0             0   \n",
       "wikipedia                                         1             3   \n",
       "wikipedia,google                                  0             0   \n",
       "wikipedia,twitter                                 0             0   \n",
       "wikipedia,youtube                                 0             0   \n",
       "wikipedia,youtube,twitter                         0             0   \n",
       "youtube                                           2             2   \n",
       "youtube,twitter                                   0             0   \n",
       "\n",
       "comments                   walled garden  \n",
       "source                                    \n",
       "google                                 0  \n",
       "twitter                                5  \n",
       "twitter,google                         0  \n",
       "wikipedia                              0  \n",
       "wikipedia,google                       0  \n",
       "wikipedia,twitter                      0  \n",
       "wikipedia,youtube                      0  \n",
       "wikipedia,youtube,twitter              0  \n",
       "youtube                                1  \n",
       "youtube,twitter                        0  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(uris_for_appr\n",
    " .groupby(['source', 'comments']).size().unstack().fillna(0).astype(int)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appraisal 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ALL_NO_DUPLICATES.csv', '2014-URLs-Responses-xlsxALL.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.listdir('data/appraisal2/filled/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>URL</th>\n",
       "      <th>Is this page active? (If the answer is NO, you may submit the  form after answering)</th>\n",
       "      <th>In general, is the content of this page relevant to  \"Operation Protective Edge\" ?</th>\n",
       "      <th>Date of page (if not mentioned - type 11/11/1111) Note American date format - M D Y</th>\n",
       "      <th>Title of page (if there isn't- type N/A), If not in English, please translate to English via Google Translate</th>\n",
       "      <th>Define this page by its content</th>\n",
       "      <th>Language (if you do not recognize, please use the google translate language detector)</th>\n",
       "      <th>Your name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sun Jul 14 11:08:30 IDT 2019</td>\n",
       "      <td>http://time.com/3057517/israel-humanitarian-ce...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Audio-visual and interactive media</td>\n",
       "      <td>en- English</td>\n",
       "      <td>Tzipy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sat Jul 13 21:19:28 IDT 2019</td>\n",
       "      <td>https://thewallwillfall.org/2014/09/29/the-mis...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Mon Sep 29 00:00:00 IDT 2014</td>\n",
       "      <td>The Missing of Gaza ~ Civilians taken prisoner...</td>\n",
       "      <td>Blog</td>\n",
       "      <td>en- English</td>\n",
       "      <td>Anat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sat Jul 13 21:17:40 IDT 2019</td>\n",
       "      <td>http://news.google.com/news/url?sa=t&amp;fd=R&amp;ct2=...</td>\n",
       "      <td>No- internal 404/page not found/content remove...</td>\n",
       "      <td>N/A- inactive</td>\n",
       "      <td>N/A- inactive</td>\n",
       "      <td>N/A- inactive</td>\n",
       "      <td>N/A- inactive</td>\n",
       "      <td>N/A- inactive</td>\n",
       "      <td>Anat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sat Jul 13 21:16:42 IDT 2019</td>\n",
       "      <td>http://nyti.ms/1tgPnI6</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sat Aug 16 00:00:00 IDT 2014</td>\n",
       "      <td>Artistsג€™ Work Rises From the Destruction of ...</td>\n",
       "      <td>News article</td>\n",
       "      <td>en- English</td>\n",
       "      <td>Anat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sat Jul 13 21:14:51 IDT 2019</td>\n",
       "      <td>https://www.nybooks.com/articles/2014/09/25/fa...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Thu Sep 25 00:00:00 IDT 2014</td>\n",
       "      <td>Failure in Gaza</td>\n",
       "      <td>News article</td>\n",
       "      <td>en- English</td>\n",
       "      <td>Anat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Timestamp  \\\n",
       "0  Sun Jul 14 11:08:30 IDT 2019   \n",
       "1  Sat Jul 13 21:19:28 IDT 2019   \n",
       "2  Sat Jul 13 21:17:40 IDT 2019   \n",
       "3  Sat Jul 13 21:16:42 IDT 2019   \n",
       "4  Sat Jul 13 21:14:51 IDT 2019   \n",
       "\n",
       "                                                 URL  \\\n",
       "0  http://time.com/3057517/israel-humanitarian-ce...   \n",
       "1  https://thewallwillfall.org/2014/09/29/the-mis...   \n",
       "2  http://news.google.com/news/url?sa=t&fd=R&ct2=...   \n",
       "3                             http://nyti.ms/1tgPnI6   \n",
       "4  https://www.nybooks.com/articles/2014/09/25/fa...   \n",
       "\n",
       "  Is this page active? (If the answer is NO, you may submit the  form after answering)  \\\n",
       "0                                                Yes                                     \n",
       "1                                                Yes                                     \n",
       "2  No- internal 404/page not found/content remove...                                     \n",
       "3                                                Yes                                     \n",
       "4                                                Yes                                     \n",
       "\n",
       "  In general, is the content of this page relevant to  \"Operation Protective Edge\" ?  \\\n",
       "0                                                 No                                   \n",
       "1                                                Yes                                   \n",
       "2                                      N/A- inactive                                   \n",
       "3                                                Yes                                   \n",
       "4                                                Yes                                   \n",
       "\n",
       "  Date of page (if not mentioned - type 11/11/1111) Note American date format - M D Y  \\\n",
       "0                                                NaN                                    \n",
       "1                       Mon Sep 29 00:00:00 IDT 2014                                    \n",
       "2                                      N/A- inactive                                    \n",
       "3                       Sat Aug 16 00:00:00 IDT 2014                                    \n",
       "4                       Thu Sep 25 00:00:00 IDT 2014                                    \n",
       "\n",
       "  Title of page (if there isn't- type N/A), If not in English, please translate to English via Google Translate  \\\n",
       "0                                                NaN                                                              \n",
       "1  The Missing of Gaza ~ Civilians taken prisoner...                                                              \n",
       "2                                      N/A- inactive                                                              \n",
       "3  Artistsג€™ Work Rises From the Destruction of ...                                                              \n",
       "4                                    Failure in Gaza                                                              \n",
       "\n",
       "      Define this page by its content  \\\n",
       "0  Audio-visual and interactive media   \n",
       "1                                Blog   \n",
       "2                       N/A- inactive   \n",
       "3                        News article   \n",
       "4                        News article   \n",
       "\n",
       "  Language (if you do not recognize, please use the google translate language detector)  \\\n",
       "0                                        en- English                                      \n",
       "1                                        en- English                                      \n",
       "2                                      N/A- inactive                                      \n",
       "3                                        en- English                                      \n",
       "4                                        en- English                                      \n",
       "\n",
       "  Your name  \n",
       "0     Tzipy  \n",
       "1      Anat  \n",
       "2      Anat  \n",
       "3      Anat  \n",
       "4      Anat  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd = pd.read_csv('data/appraisal2/filled/ALL_NO_DUPLICATES.csv')\n",
    "nd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nd['URL'] = nd.URL.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omilab/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (7,9,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "clut = pd.read_csv('data/clut.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clut['archived'] = clut[['uri_cln_wb_avail', 'uri_cln_unshrtn_wb_avail', 'uri_mem_avail', 'uri_unsh_no_clean_mem_avail']].any(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_values(g):\n",
    "    return ','.join(list(g.unique()))\n",
    "uris = clut.groupby(['uri_unsh_no_clean', 'tld_unshrtn']).agg({'archived': np.max, 'source': get_values}).reset_index().rename(columns={'uri_unsh_no_clean': 'URL'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>tld_unshrtn</th>\n",
       "      <th>archived</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>//tools.wmflabs.org/geohack/geohack.php?langua...</td>\n",
       "      <td>wmflabs.org</td>\n",
       "      <td>False</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>//tools.wmflabs.org/geohack/geohack.php?langua...</td>\n",
       "      <td>wmflabs.org</td>\n",
       "      <td>False</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>//tools.wmflabs.org/geohack/geohack.php?langua...</td>\n",
       "      <td>wmflabs.org</td>\n",
       "      <td>False</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>//tools.wmflabs.org/geohack/geohack.php?langua...</td>\n",
       "      <td>wmflabs.org</td>\n",
       "      <td>False</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>//tools.wmflabs.org/geohack/geohack.php?langua...</td>\n",
       "      <td>wmflabs.org</td>\n",
       "      <td>False</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URL  tld_unshrtn  archived  \\\n",
       "0  //tools.wmflabs.org/geohack/geohack.php?langua...  wmflabs.org     False   \n",
       "1  //tools.wmflabs.org/geohack/geohack.php?langua...  wmflabs.org     False   \n",
       "2  //tools.wmflabs.org/geohack/geohack.php?langua...  wmflabs.org     False   \n",
       "3  //tools.wmflabs.org/geohack/geohack.php?langua...  wmflabs.org     False   \n",
       "4  //tools.wmflabs.org/geohack/geohack.php?langua...  wmflabs.org     False   \n",
       "\n",
       "      source  \n",
       "0  wikipedia  \n",
       "1  wikipedia  \n",
       "2  wikipedia  \n",
       "3  wikipedia  \n",
       "4  wikipedia  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>URL</th>\n",
       "      <th>Is this page active? (If the answer is NO, you may submit the  form after answering)</th>\n",
       "      <th>In general, is the content of this page relevant to  \"Operation Protective Edge\" ?</th>\n",
       "      <th>Date of page (if not mentioned - type 11/11/1111) Note American date format - M D Y</th>\n",
       "      <th>Title of page (if there isn't- type N/A), If not in English, please translate to English via Google Translate</th>\n",
       "      <th>Define this page by its content</th>\n",
       "      <th>Language (if you do not recognize, please use the google translate language detector)</th>\n",
       "      <th>Your name</th>\n",
       "      <th>tld_unshrtn</th>\n",
       "      <th>archived</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sun Jul 14 11:08:30 IDT 2019</td>\n",
       "      <td>http://time.com/3057517/israel-humanitarian-ce...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Audio-visual and interactive media</td>\n",
       "      <td>en- English</td>\n",
       "      <td>Tzipy</td>\n",
       "      <td>time.com</td>\n",
       "      <td>True</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sat Jul 13 21:19:28 IDT 2019</td>\n",
       "      <td>https://thewallwillfall.org/2014/09/29/the-mis...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Mon Sep 29 00:00:00 IDT 2014</td>\n",
       "      <td>The Missing of Gaza ~ Civilians taken prisoner...</td>\n",
       "      <td>Blog</td>\n",
       "      <td>en- English</td>\n",
       "      <td>Anat</td>\n",
       "      <td>thewallwillfall.org</td>\n",
       "      <td>False</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sat Jul 13 21:17:40 IDT 2019</td>\n",
       "      <td>http://news.google.com/news/url?sa=t&amp;fd=R&amp;ct2=...</td>\n",
       "      <td>No- internal 404/page not found/content remove...</td>\n",
       "      <td>N/A- inactive</td>\n",
       "      <td>N/A- inactive</td>\n",
       "      <td>N/A- inactive</td>\n",
       "      <td>N/A- inactive</td>\n",
       "      <td>N/A- inactive</td>\n",
       "      <td>Anat</td>\n",
       "      <td>google.com</td>\n",
       "      <td>False</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sat Jul 13 21:16:42 IDT 2019</td>\n",
       "      <td>http://nyti.ms/1tgPnI6</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sat Aug 16 00:00:00 IDT 2014</td>\n",
       "      <td>Artistsג€™ Work Rises From the Destruction of ...</td>\n",
       "      <td>News article</td>\n",
       "      <td>en- English</td>\n",
       "      <td>Anat</td>\n",
       "      <td>nyti.ms</td>\n",
       "      <td>False</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sat Jul 13 21:14:51 IDT 2019</td>\n",
       "      <td>https://www.nybooks.com/articles/2014/09/25/fa...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Thu Sep 25 00:00:00 IDT 2014</td>\n",
       "      <td>Failure in Gaza</td>\n",
       "      <td>News article</td>\n",
       "      <td>en- English</td>\n",
       "      <td>Anat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Timestamp  \\\n",
       "0  Sun Jul 14 11:08:30 IDT 2019   \n",
       "1  Sat Jul 13 21:19:28 IDT 2019   \n",
       "2  Sat Jul 13 21:17:40 IDT 2019   \n",
       "3  Sat Jul 13 21:16:42 IDT 2019   \n",
       "4  Sat Jul 13 21:14:51 IDT 2019   \n",
       "\n",
       "                                                 URL  \\\n",
       "0  http://time.com/3057517/israel-humanitarian-ce...   \n",
       "1  https://thewallwillfall.org/2014/09/29/the-mis...   \n",
       "2  http://news.google.com/news/url?sa=t&fd=R&ct2=...   \n",
       "3                             http://nyti.ms/1tgPnI6   \n",
       "4  https://www.nybooks.com/articles/2014/09/25/fa...   \n",
       "\n",
       "  Is this page active? (If the answer is NO, you may submit the  form after answering)  \\\n",
       "0                                                Yes                                     \n",
       "1                                                Yes                                     \n",
       "2  No- internal 404/page not found/content remove...                                     \n",
       "3                                                Yes                                     \n",
       "4                                                Yes                                     \n",
       "\n",
       "  In general, is the content of this page relevant to  \"Operation Protective Edge\" ?  \\\n",
       "0                                                 No                                   \n",
       "1                                                Yes                                   \n",
       "2                                      N/A- inactive                                   \n",
       "3                                                Yes                                   \n",
       "4                                                Yes                                   \n",
       "\n",
       "  Date of page (if not mentioned - type 11/11/1111) Note American date format - M D Y  \\\n",
       "0                                                NaN                                    \n",
       "1                       Mon Sep 29 00:00:00 IDT 2014                                    \n",
       "2                                      N/A- inactive                                    \n",
       "3                       Sat Aug 16 00:00:00 IDT 2014                                    \n",
       "4                       Thu Sep 25 00:00:00 IDT 2014                                    \n",
       "\n",
       "  Title of page (if there isn't- type N/A), If not in English, please translate to English via Google Translate  \\\n",
       "0                                                NaN                                                              \n",
       "1  The Missing of Gaza ~ Civilians taken prisoner...                                                              \n",
       "2                                      N/A- inactive                                                              \n",
       "3  Artistsג€™ Work Rises From the Destruction of ...                                                              \n",
       "4                                    Failure in Gaza                                                              \n",
       "\n",
       "      Define this page by its content  \\\n",
       "0  Audio-visual and interactive media   \n",
       "1                                Blog   \n",
       "2                       N/A- inactive   \n",
       "3                        News article   \n",
       "4                        News article   \n",
       "\n",
       "  Language (if you do not recognize, please use the google translate language detector)  \\\n",
       "0                                        en- English                                      \n",
       "1                                        en- English                                      \n",
       "2                                      N/A- inactive                                      \n",
       "3                                        en- English                                      \n",
       "4                                        en- English                                      \n",
       "\n",
       "  Your name          tld_unshrtn archived     source  \n",
       "0     Tzipy             time.com     True  wikipedia  \n",
       "1      Anat  thewallwillfall.org    False    twitter  \n",
       "2      Anat           google.com    False    twitter  \n",
       "3      Anat              nyti.ms    False    twitter  \n",
       "4      Anat                  NaN      NaN        NaN  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd = nd.merge(uris, how='left')\n",
    "\n",
    "nd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(253, 1194, 0.211892797319933)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd.tld_unshrtn.isna().sum(), nd.shape[0], nd.tld_unshrtn.isna().sum()/nd.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nd.to_csv('data/appraisal2/filled/ALL_NO_DUPLICATES_with_extra_columns.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>url</th>\n",
       "      <th>is_page_active</th>\n",
       "      <th>is_relevant</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>language</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sun Jul 14 11:08:30 IDT 2019</td>\n",
       "      <td>http://time.com/3057517/israel-humanitarian-ce...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Audio-visual and interactive media</td>\n",
       "      <td>en- English</td>\n",
       "      <td>Tzipy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sat Jul 13 21:19:28 IDT 2019</td>\n",
       "      <td>https://thewallwillfall.org/2014/09/29/the-mis...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Mon Sep 29 00:00:00 IDT 2014</td>\n",
       "      <td>The Missing of Gaza ~ Civilians taken prisoner...</td>\n",
       "      <td>Blog</td>\n",
       "      <td>en- English</td>\n",
       "      <td>Anat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sat Jul 13 21:17:40 IDT 2019</td>\n",
       "      <td>http://news.google.com/news/url?sa=t&amp;fd=R&amp;ct2=...</td>\n",
       "      <td>No- internal 404/page not found/content remove...</td>\n",
       "      <td>N/A- inactive</td>\n",
       "      <td>N/A- inactive</td>\n",
       "      <td>N/A- inactive</td>\n",
       "      <td>N/A- inactive</td>\n",
       "      <td>N/A- inactive</td>\n",
       "      <td>Anat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sat Jul 13 21:16:42 IDT 2019</td>\n",
       "      <td>http://nyti.ms/1tgPnI6</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sat Aug 16 00:00:00 IDT 2014</td>\n",
       "      <td>Artists’ Work Rises From the Destruction of th...</td>\n",
       "      <td>News article</td>\n",
       "      <td>en- English</td>\n",
       "      <td>Anat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sat Jul 13 21:14:51 IDT 2019</td>\n",
       "      <td>https://www.nybooks.com/articles/2014/09/25/fa...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Thu Sep 25 00:00:00 IDT 2014</td>\n",
       "      <td>Failure in Gaza</td>\n",
       "      <td>News article</td>\n",
       "      <td>en- English</td>\n",
       "      <td>Anat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      timestamp  \\\n",
       "0  Sun Jul 14 11:08:30 IDT 2019   \n",
       "1  Sat Jul 13 21:19:28 IDT 2019   \n",
       "2  Sat Jul 13 21:17:40 IDT 2019   \n",
       "3  Sat Jul 13 21:16:42 IDT 2019   \n",
       "4  Sat Jul 13 21:14:51 IDT 2019   \n",
       "\n",
       "                                                 url  \\\n",
       "0  http://time.com/3057517/israel-humanitarian-ce...   \n",
       "1  https://thewallwillfall.org/2014/09/29/the-mis...   \n",
       "2  http://news.google.com/news/url?sa=t&fd=R&ct2=...   \n",
       "3                             http://nyti.ms/1tgPnI6   \n",
       "4  https://www.nybooks.com/articles/2014/09/25/fa...   \n",
       "\n",
       "                                      is_page_active    is_relevant  \\\n",
       "0                                                Yes             No   \n",
       "1                                                Yes            Yes   \n",
       "2  No- internal 404/page not found/content remove...  N/A- inactive   \n",
       "3                                                Yes            Yes   \n",
       "4                                                Yes            Yes   \n",
       "\n",
       "                           date  \\\n",
       "0                           NaN   \n",
       "1  Mon Sep 29 00:00:00 IDT 2014   \n",
       "2                 N/A- inactive   \n",
       "3  Sat Aug 16 00:00:00 IDT 2014   \n",
       "4  Thu Sep 25 00:00:00 IDT 2014   \n",
       "\n",
       "                                               title  \\\n",
       "0                                                NaN   \n",
       "1  The Missing of Gaza ~ Civilians taken prisoner...   \n",
       "2                                      N/A- inactive   \n",
       "3  Artists’ Work Rises From the Destruction of th...   \n",
       "4                                    Failure in Gaza   \n",
       "\n",
       "                              content       language   name  \n",
       "0  Audio-visual and interactive media    en- English  Tzipy  \n",
       "1                                Blog    en- English   Anat  \n",
       "2                       N/A- inactive  N/A- inactive   Anat  \n",
       "3                        News article    en- English   Anat  \n",
       "4                        News article    en- English   Anat  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = [\"timestamp\",\"url\",\"is_page_active\",\"is_relevant\",\"date\",\"title\",\"content\",\"language\",\"name\"]\n",
    "df = pd.read_csv('data/appraisal2/filled/2014-URLs-Responses-xlsxALL.csv', names=header, skiprows=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_counts = df.groupby('url').name.nunique()\n",
    "url_counts[url_counts==5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>name</th>\n",
       "      <th>Anat</th>\n",
       "      <th>Dan</th>\n",
       "      <th>Nehoray</th>\n",
       "      <th>Nurit</th>\n",
       "      <th>Tzipy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Anat</th>\n",
       "      <td>351</td>\n",
       "      <td>95</td>\n",
       "      <td>131</td>\n",
       "      <td>95</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dan</th>\n",
       "      <td>95</td>\n",
       "      <td>356</td>\n",
       "      <td>84</td>\n",
       "      <td>177</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nehoray</th>\n",
       "      <td>131</td>\n",
       "      <td>84</td>\n",
       "      <td>358</td>\n",
       "      <td>84</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nurit</th>\n",
       "      <td>95</td>\n",
       "      <td>177</td>\n",
       "      <td>84</td>\n",
       "      <td>359</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tzipy</th>\n",
       "      <td>93</td>\n",
       "      <td>169</td>\n",
       "      <td>88</td>\n",
       "      <td>171</td>\n",
       "      <td>364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "name     Anat  Dan  Nehoray  Nurit  Tzipy\n",
       "name                                     \n",
       "Anat      351   95      131     95     93\n",
       "Dan        95  356       84    177    169\n",
       "Nehoray   131   84      358     84     88\n",
       "Nurit      95  177       84    359    171\n",
       "Tzipy      93  169       88    171    364"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_asint = df.groupby(['url', 'name']).size().unstack().fillna(0).astype(int)\n",
    "coocc = df_asint.T.dot(df_asint)\n",
    "#coocc = coocc.where(np.triu(np.ones(coocc.shape)).astype(np.bool)).fillna(0).astype(int).replace(0, '')\n",
    "coocc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agreement_array(df, name, col):\n",
    "    url_counts = df.groupby('url').name.nunique()\n",
    "    m = url_counts.max()\n",
    "    co = url_counts[url_counts==m].index\n",
    "    return df.loc[(df.name==name) & (df.url.isin(co)), ['url', col]].sort_values('url')[col].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6551620044274502"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "def get_kappa_score(df, name1, name2, col):\n",
    "    return cohen_kappa_score(get_agreement_array(df, name1, col), get_agreement_array(df, name2, col))\n",
    "get_kappa_score(df, 'Anat', 'Tzipy', 'is_relevant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Yes              1074\n",
       "N/A- inactive     378\n",
       "No                332\n",
       "Name: is_relevant, dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.is_relevant.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Yes                                                           1406\n",
       "No- internal 404/page not found/content removed/no content     177\n",
       "No- redirect to generic site/portal                            106\n",
       "No- server error                                                62\n",
       "No- Redirects to a dead link                                    18\n",
       "No- Wall garden- Redirects to a log-in page                     15\n",
       "Name: is_page_active, dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.is_page_active.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "News article                           970\n",
       "N/A- inactive                          378\n",
       "Blog                                   108\n",
       "Wikipedia page                          77\n",
       "NGO's                                   57\n",
       "Educational / academic / references     37\n",
       "Audio-visual and interactive media      35\n",
       "Social media page/account               34\n",
       "Book content                            25\n",
       "Collection of news articles             17\n",
       "Forum                                   14\n",
       "Commercial / \"Landing Page\"             14\n",
       "Long-form essay / Op-Ed                  7\n",
       "Link farm                                4\n",
       "Git-Hub                                  3\n",
       "Petition                                 2\n",
       "Domain for sale                          1\n",
       "Obituary                                 1\n",
       "Name: content, dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.content.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def get_all_scores(df, cols):\n",
    "    res = []\n",
    "    for col in cols:\n",
    "        for name1, name2 in combinations(list(df.name.unique()), 2):\n",
    "            res.append((name1, name2, col, get_kappa_score(df, name1, name2, col)))\n",
    "    return pd.DataFrame(res, columns=['name1', 'name2', 'col', 'kappa'])\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "col\n",
       "content           0.620565\n",
       "is_page_active    0.649845\n",
       "is_relevant       0.678551\n",
       "Name: kappa, dtype: float64"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "              \n",
    "get_all_scores(df, ['is_page_active', 'is_relevant', 'content']).groupby('col').kappa.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omilab/anaconda3/lib/python3.6/site-packages/dateutil/parser/_parser.py:1204: UnknownTimezoneWarning: tzname BST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  category=UnknownTimezoneWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>url</th>\n",
       "      <th>is_page_active</th>\n",
       "      <th>is_relevant</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>language</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-07-14 11:11:06</td>\n",
       "      <td>http://www.israelnationalnews.com/News/News.as...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2014-07-09T00:00:00Z</td>\n",
       "      <td>120 Rockets Fired During Day 1 of Operation</td>\n",
       "      <td>News Article</td>\n",
       "      <td>en- English</td>\n",
       "      <td>Sveta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-07-14 11:10:16</td>\n",
       "      <td>http://takayukisko.blogspot.com/2014/07/ukrain...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No Date</td>\n",
       "      <td>ROAD TO PHARMACIST GENERAL☆</td>\n",
       "      <td>Blog</td>\n",
       "      <td>?</td>\n",
       "      <td>Sveta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-07-14 11:08:30</td>\n",
       "      <td>http://time.com/3057517/israel-humanitarian-ce...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No Date</td>\n",
       "      <td>N/A (Irrelevant Page)</td>\n",
       "      <td>Audio-Visual and Interactive Media</td>\n",
       "      <td>en- English</td>\n",
       "      <td>Tzipy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-07-14 11:07:44</td>\n",
       "      <td>https://abcnews.go.com/International/israel-ga...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2014-07-31T00:00:00Z</td>\n",
       "      <td>Everything You Need to Know About the Israel-G...</td>\n",
       "      <td>News Article</td>\n",
       "      <td>en- English</td>\n",
       "      <td>Sveta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-07-14 11:06:35</td>\n",
       "      <td>http://edition.cnn.com/2014/07/20/world/meast/...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2014-07-21T00:00:00Z</td>\n",
       "      <td>Hamas claims it captured an Israeli soldier; I...</td>\n",
       "      <td>News Article</td>\n",
       "      <td>en- English</td>\n",
       "      <td>Sveta</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp                                                url  \\\n",
       "0 2019-07-14 11:11:06  http://www.israelnationalnews.com/News/News.as...   \n",
       "1 2019-07-14 11:10:16  http://takayukisko.blogspot.com/2014/07/ukrain...   \n",
       "2 2019-07-14 11:08:30  http://time.com/3057517/israel-humanitarian-ce...   \n",
       "3 2019-07-14 11:07:44  https://abcnews.go.com/International/israel-ga...   \n",
       "4 2019-07-14 11:06:35  http://edition.cnn.com/2014/07/20/world/meast/...   \n",
       "\n",
       "  is_page_active is_relevant                  date  \\\n",
       "0            Yes         Yes  2014-07-09T00:00:00Z   \n",
       "1            Yes          No               No Date   \n",
       "2            Yes         Yes               No Date   \n",
       "3            Yes         Yes  2014-07-31T00:00:00Z   \n",
       "4            Yes         Yes  2014-07-21T00:00:00Z   \n",
       "\n",
       "                                               title  \\\n",
       "0        120 Rockets Fired During Day 1 of Operation   \n",
       "1                        ROAD TO PHARMACIST GENERAL☆   \n",
       "2                              N/A (Irrelevant Page)   \n",
       "3  Everything You Need to Know About the Israel-G...   \n",
       "4  Hamas claims it captured an Israeli soldier; I...   \n",
       "\n",
       "                              content     language   name  \n",
       "0                        News Article  en- English  Sveta  \n",
       "1                                Blog            ?  Sveta  \n",
       "2  Audio-Visual and Interactive Media  en- English  Tzipy  \n",
       "3                        News Article  en- English  Sveta  \n",
       "4                        News Article  en- English  Sveta  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = [\"timestamp\",\"url\",\"is_page_active\",\"is_relevant\",\"date\",\"title\",\"content\",\"language\",\"email\", \"name\"]\n",
    "df = (pd.read_csv('data/appraisal2/filled/2014-URLs-Responses-all.csv', names=header, skiprows=1,\n",
    "                  parse_dates=['timestamp'])\n",
    "      .drop('email', axis=1)\n",
    "      #.assign(timestamp=lambda x: pd.to_datetime)\n",
    "     )\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add tld_unshrtn archived and source columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>url</th>\n",
       "      <th>is_page_active</th>\n",
       "      <th>is_relevant</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>language</th>\n",
       "      <th>name</th>\n",
       "      <th>tld_unshrtn</th>\n",
       "      <th>archived</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-07-14 11:11:06</td>\n",
       "      <td>http://www.israelnationalnews.com/News/News.as...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2014-07-09T00:00:00Z</td>\n",
       "      <td>120 Rockets Fired During Day 1 of Operation</td>\n",
       "      <td>News Article</td>\n",
       "      <td>en- English</td>\n",
       "      <td>Sveta</td>\n",
       "      <td>israelnationalnews.com</td>\n",
       "      <td>True</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-07-14 11:10:16</td>\n",
       "      <td>http://takayukisko.blogspot.com/2014/07/ukrain...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No Date</td>\n",
       "      <td>ROAD TO PHARMACIST GENERAL☆</td>\n",
       "      <td>Blog</td>\n",
       "      <td>?</td>\n",
       "      <td>Sveta</td>\n",
       "      <td>blogspot.com</td>\n",
       "      <td>False</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-07-14 11:08:30</td>\n",
       "      <td>http://time.com/3057517/israel-humanitarian-ce...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No Date</td>\n",
       "      <td>N/A (Irrelevant Page)</td>\n",
       "      <td>Audio-Visual and Interactive Media</td>\n",
       "      <td>en- English</td>\n",
       "      <td>Tzipy</td>\n",
       "      <td>time.com</td>\n",
       "      <td>True</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-07-14 11:07:44</td>\n",
       "      <td>https://abcnews.go.com/International/israel-ga...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2014-07-31T00:00:00Z</td>\n",
       "      <td>Everything You Need to Know About the Israel-G...</td>\n",
       "      <td>News Article</td>\n",
       "      <td>en- English</td>\n",
       "      <td>Sveta</td>\n",
       "      <td>go.com</td>\n",
       "      <td>True</td>\n",
       "      <td>twitter,google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-07-14 11:06:35</td>\n",
       "      <td>http://edition.cnn.com/2014/07/20/world/meast/...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2014-07-21T00:00:00Z</td>\n",
       "      <td>Hamas claims it captured an Israeli soldier; I...</td>\n",
       "      <td>News Article</td>\n",
       "      <td>en- English</td>\n",
       "      <td>Sveta</td>\n",
       "      <td>cnn.com</td>\n",
       "      <td>True</td>\n",
       "      <td>wikipedia,twitter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp                                                url  \\\n",
       "0 2019-07-14 11:11:06  http://www.israelnationalnews.com/News/News.as...   \n",
       "1 2019-07-14 11:10:16  http://takayukisko.blogspot.com/2014/07/ukrain...   \n",
       "2 2019-07-14 11:08:30  http://time.com/3057517/israel-humanitarian-ce...   \n",
       "3 2019-07-14 11:07:44  https://abcnews.go.com/International/israel-ga...   \n",
       "4 2019-07-14 11:06:35  http://edition.cnn.com/2014/07/20/world/meast/...   \n",
       "\n",
       "  is_page_active is_relevant                  date  \\\n",
       "0            Yes         Yes  2014-07-09T00:00:00Z   \n",
       "1            Yes          No               No Date   \n",
       "2            Yes         Yes               No Date   \n",
       "3            Yes         Yes  2014-07-31T00:00:00Z   \n",
       "4            Yes         Yes  2014-07-21T00:00:00Z   \n",
       "\n",
       "                                               title  \\\n",
       "0        120 Rockets Fired During Day 1 of Operation   \n",
       "1                        ROAD TO PHARMACIST GENERAL☆   \n",
       "2                              N/A (Irrelevant Page)   \n",
       "3  Everything You Need to Know About the Israel-G...   \n",
       "4  Hamas claims it captured an Israeli soldier; I...   \n",
       "\n",
       "                              content     language   name  \\\n",
       "0                        News Article  en- English  Sveta   \n",
       "1                                Blog            ?  Sveta   \n",
       "2  Audio-Visual and Interactive Media  en- English  Tzipy   \n",
       "3                        News Article  en- English  Sveta   \n",
       "4                        News Article  en- English  Sveta   \n",
       "\n",
       "              tld_unshrtn archived             source  \n",
       "0  israelnationalnews.com     True            twitter  \n",
       "1            blogspot.com    False            twitter  \n",
       "2                time.com     True          wikipedia  \n",
       "3                  go.com     True     twitter,google  \n",
       "4                 cnn.com     True  wikipedia,twitter  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.merge(uris.rename(columns={'URL': 'url'}), how='left')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop same user duplicate URLs\n",
    "Keep only the latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2421\n",
       "2      63\n",
       "3       2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['url', 'name']).size().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values('timestamp').drop_duplicates(subset=['url', 'name'], keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2486\n",
       "dtype: int64"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['url', 'name']).size().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/appraisal2/filled/2014-URLs-Responses-all-NO-SAME-USER-DUPLICATES.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop between user duplicate URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1244\n",
       "7     149\n",
       "6      21\n",
       "2      12\n",
       "5       8\n",
       "3       2\n",
       "4       1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('url').size().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup_df = df.drop_duplicates(subset='url', keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1244\n",
       "dtype: int64"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dedup_df.groupby('url').size().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup_df.to_csv('data/appraisal2/filled/2014-URLs-Responses-all-ONLY_ONE_PER_URL.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Cohen's Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149,)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_counts = df.groupby('url').name.nunique()\n",
    "url_counts[url_counts==7].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Yes                                                           2000\n",
       "No- internal 404/page not found/content removed/no content     259\n",
       "No- redirect to generic site/portal                            108\n",
       "No- server error                                                79\n",
       "No- Redirects to a dead link                                    19\n",
       "No- Wall garden- Redirects to a log-in page                     15\n",
       "No                                                               7\n",
       "Name: is_page_active, dtype: int64"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.is_page_active.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name1</th>\n",
       "      <th>name2</th>\n",
       "      <th>col</th>\n",
       "      <th>kappa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anat</td>\n",
       "      <td>Sveta</td>\n",
       "      <td>is_page_active</td>\n",
       "      <td>0.689583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anat</td>\n",
       "      <td>Tzipy</td>\n",
       "      <td>is_page_active</td>\n",
       "      <td>0.640910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anat</td>\n",
       "      <td>Yonatan</td>\n",
       "      <td>is_page_active</td>\n",
       "      <td>0.622101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anat</td>\n",
       "      <td>Dan</td>\n",
       "      <td>is_page_active</td>\n",
       "      <td>0.673808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anat</td>\n",
       "      <td>Nurit</td>\n",
       "      <td>is_page_active</td>\n",
       "      <td>0.757855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Anat</td>\n",
       "      <td>Nehoray</td>\n",
       "      <td>is_page_active</td>\n",
       "      <td>0.624264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sveta</td>\n",
       "      <td>Tzipy</td>\n",
       "      <td>is_page_active</td>\n",
       "      <td>0.433191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sveta</td>\n",
       "      <td>Yonatan</td>\n",
       "      <td>is_page_active</td>\n",
       "      <td>0.708871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sveta</td>\n",
       "      <td>Dan</td>\n",
       "      <td>is_page_active</td>\n",
       "      <td>0.507682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sveta</td>\n",
       "      <td>Nurit</td>\n",
       "      <td>is_page_active</td>\n",
       "      <td>0.653623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name1    name2             col     kappa\n",
       "0   Anat    Sveta  is_page_active  0.689583\n",
       "1   Anat    Tzipy  is_page_active  0.640910\n",
       "2   Anat  Yonatan  is_page_active  0.622101\n",
       "3   Anat      Dan  is_page_active  0.673808\n",
       "4   Anat    Nurit  is_page_active  0.757855\n",
       "5   Anat  Nehoray  is_page_active  0.624264\n",
       "6  Sveta    Tzipy  is_page_active  0.433191\n",
       "7  Sveta  Yonatan  is_page_active  0.708871\n",
       "8  Sveta      Dan  is_page_active  0.507682\n",
       "9  Sveta    Nurit  is_page_active  0.653623"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa = get_all_scores(df, ['is_page_active', 'is_relevant', 'content'])\n",
    "cohen_kappa.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohen_kappa.groupby('col').kappa.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Krippendorff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from langdetect import detect\n",
      "import pickle\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('husl', 12)\n",
      "%matplotlib inline\n",
      " 1/2:\n",
      "import glob\n",
      "import zipfile\n",
      "import re\n",
      "import pickle\n",
      " 1/3: folder = 'turf/Facebook Data Backups/'\n",
      " 1/4: files = glob.glob(folder + '*/*.zip')\n",
      " 1/5:\n",
      "names = [re.match(folder + '([^/]+)', f).group(1) for f in files]\n",
      "names\n",
      " 1/6: len(names), len(files)\n",
      " 1/7:\n",
      "data_dfs = []\n",
      "shared_dfs = []\n",
      "for name, f in zip(names, files):\n",
      "    print(f)\n",
      "    with zipfile.ZipFile(f) as zf:\n",
      "        try:\n",
      "            data_df = (pd.read_csv(zf.open('data.csv'), sep='\\t', doublequote=False, lineterminator='\\n'))\n",
      "        except:\n",
      "            print ('******************** BAD ************************')\n",
      "        '''            zf.extract('data.csv')\n",
      "            data_df = (pd.read_csv(open('data.csv', newline=None, encoding='utf8'), sep='\\t', doublequote=False, lineterminator='\\n',\n",
      "                                  #dtype=['object', 'bool', 'datetime64', 'datetime64', 'object', 'int',\n",
      "                                  #       'object', 'int', 'int', 'int', 'int', 'object', 'object', \n",
      "                                  #       'object', 'object']\n",
      "                                  ))'''\n",
      "        data_df = (data_df.assign(name = name)\n",
      "                  .rename(columns={'elapsed_time(minutes)': 'elapsed_time'}))\n",
      "        data_dfs.append(data_df)          \n",
      "        shared_df = (pd.read_csv(zf.open('shared.csv'), sep='\\t', doublequote=False, lineterminator='\\n'))\n",
      "        shared_df = (shared_df.assign(name = name)\n",
      "                  .rename(columns={'elapsed_time(minutes)': 'elapsed_time'}))\n",
      "        shared_dfs.append(shared_df)\n",
      " 1/8:\n",
      "for df in data_dfs:\n",
      "    print (df.shape[0], df.name.unique())\n",
      " 1/9:\n",
      "for df, name in zip(data_dfs, files):\n",
      "    na = df[df.post_id.isna()].shape[0]\n",
      "    if na > 0:\n",
      "        print (na, name)\n",
      "1/10: data_dfs[0].dtypes\n",
      "1/11:\n",
      "df = (pd.concat(data_dfs)\n",
      "     .assign(name = lambda x: pd.Categorical(x.name))\n",
      "     .assign(sentiment = lambda x: pd.Categorical(x.sentiment))\n",
      "     .assign(created_time = lambda x: pd.to_datetime(x.created_time))\n",
      "     .assign(updated_time = lambda x: pd.to_datetime(x.updated_time)))\n",
      "df.head()\n",
      "1/12:\n",
      "sdf = (pd.concat(shared_dfs)\n",
      "     .assign(name = lambda x: pd.Categorical(x.name))\n",
      "     .assign(sentiment = lambda x: pd.Categorical(x.sentiment))\n",
      "     .assign(created_time = lambda x: pd.to_datetime(x.created_time))\n",
      "     .assign(updated_time = lambda x: pd.to_datetime(x.updated_time)))\n",
      "sdf.head()\n",
      "1/13: df.name.value_counts().head()\n",
      "1/14: sdf.name.value_counts().head()\n",
      "1/15: df.dtypes\n",
      "1/16: sdf.dtypes\n",
      "1/17:\n",
      "def apply_detect(text):\n",
      "    try:\n",
      "        return detect(text)\n",
      "    except:\n",
      "        return None\n",
      "1/18: #df['langdetect'] = pd.Categorical(df.text.fillna(' ').apply(apply_detect))\n",
      "1/19:\n",
      "with zipfile.ZipFile('turf/id_lang.zip') as zf: \n",
      "    df = df.merge(pickle.load(zf.open('id_lang.pkl')), how='left', on='id')\n",
      "1/20:\n",
      "with zipfile.ZipFile('turf/shared_id_lang.zip') as zf: \n",
      "    sdf = sdf.merge(pickle.load(zf.open('shared_id_lang.pkl')), how='left', on='id')\n",
      "1/21: df.langdetect.value_counts().head(20)\n",
      "1/22: df.groupby(['name', 'langdetect']).size().unstack()[['he', 'ar', 'en', 'ru', 'fr', 'de', 'es']].sort_values(by='he', ascending=False).head(20)\n",
      "1/23:\n",
      "from __future__ import absolute_import, print_function, unicode_literals\n",
      "\n",
      "import os\n",
      "import re\n",
      "\n",
      "import requests\n",
      "\n",
      "\n",
      "def get_block_for_codepoint(cp):\n",
      "    \"\"\"Return the Unicode block name for the provided numeric codepoint\"\"\"\n",
      "\n",
      "    for start, end, block_name in UNICODE_BLOCKS:\n",
      "        if start <= cp <= end:\n",
      "            return block_name\n",
      "\n",
      "    return 'No_Block'\n",
      "\n",
      "\n",
      "def load_unicode_blocks_from_file(f):\n",
      "    file_contents = f.read().decode('utf-8')\n",
      "\n",
      "    blocks = []\n",
      "    for start, end, block_name in re.findall(r'([0-9A-F]+)\\.\\.([0-9A-F]+);\\ (\\S.*\\S)', file_contents):\n",
      "        if block_name == 'No_Block':\n",
      "            continue\n",
      "\n",
      "        blocks.append((int(start, 16), int(end, 16), block_name))\n",
      "\n",
      "    return blocks\n",
      "\n",
      "\n",
      "def load_unicode_blocks(block_filename):\n",
      "    if not os.path.exists(block_filename):\n",
      "        print('Unicode block file %s does not exist. Downloading…' % block_filename)\n",
      "        r = requests.get('http://unicode.org/Public/UNIDATA/Blocks.txt')\n",
      "        r.raise_for_status()\n",
      "\n",
      "        with open(block_filename, 'wb') as f:\n",
      "            for chunk in r.iter_content():\n",
      "                f.write(chunk)\n",
      "\n",
      "    with open(block_filename, 'rb') as f:\n",
      "        blocks = load_unicode_blocks_from_file(f)\n",
      "\n",
      "    return blocks\n",
      "\n",
      "UNICODE_BLOCKS = load_unicode_blocks('UNIDATA-Blocks.txt')\n",
      "1/24:\n",
      "from collections import Counter\n",
      "def get_blocks_for_string(s):\n",
      "    s = ''.join(e for e in str(s) if e.isalnum())\n",
      "    return Counter([get_block_for_codepoint(ord(c)) for c in str(s)])\n",
      "1/25: len(UNICODE_BLOCKS)\n",
      "1/26:\n",
      "data_from = pd.Series(df['from'].unique())\n",
      "data_from.head()\n",
      "1/27:\n",
      "shared_from = pd.Series(sdf['from'].unique())\n",
      "shared_from_blocks = pd.concat([shared_from, shared_from.apply(get_blocks_for_string).apply(pd.Series)], axis = 1)\n",
      "1/28:\n",
      "from __future__ import absolute_import, print_function, unicode_literals\n",
      "\n",
      "import os\n",
      "import re\n",
      "\n",
      "import requests\n",
      "\n",
      "\n",
      "def get_block_for_codepoint(cp):\n",
      "    \"\"\"Return the Unicode block name for the provided numeric codepoint\"\"\"\n",
      "\n",
      "    for start, end, block_name in UNICODE_BLOCKS:\n",
      "        if start <= cp <= end:\n",
      "            return block_name\n",
      "\n",
      "    return 'No_Block'\n",
      "\n",
      "\n",
      "def load_unicode_blocks_from_file(f):\n",
      "    file_contents = f.read().decode('utf-8')\n",
      "\n",
      "    blocks = []\n",
      "    for start, end, block_name in re.findall(r'([0-9A-F]+)\\.\\.([0-9A-F]+);\\ (\\S.*\\S)', file_contents):\n",
      "        if block_name == 'No_Block':\n",
      "            continue\n",
      "\n",
      "        blocks.append((int(start, 16), int(end, 16), block_name))\n",
      "\n",
      "    return blocks\n",
      "\n",
      "\n",
      "def load_unicode_blocks(block_filename):\n",
      "    if not os.path.exists(block_filename):\n",
      "        print('Unicode block file %s does not exist. Downloading…' % block_filename)\n",
      "        r = requests.get('http://unicode.org/Public/UNIDATA/Blocks.txt')\n",
      "        r.raise_for_status()\n",
      "\n",
      "        with open(block_filename, 'wb') as f:\n",
      "            for chunk in r.iter_content():\n",
      "                f.write(chunk)\n",
      "\n",
      "    with open(block_filename, 'rb') as f:\n",
      "        blocks = load_unicode_blocks_from_file(f)\n",
      "\n",
      "    return blocks\n",
      "\n",
      "UNICODE_BLOCKS = load_unicode_blocks('UNIDATA-Blocks.txt')\n",
      "1/29:\n",
      "from collections import Counter\n",
      "def get_blocks_for_string(s):\n",
      "    s = ''.join(e for e in str(s) if e.isalnum())\n",
      "    return Counter([get_block_for_codepoint(ord(c)) for c in str(s)])\n",
      "1/30: len(UNICODE_BLOCKS)\n",
      "1/31:\n",
      "data_from = pd.Series(df['from'].unique())\n",
      "data_from.head()\n",
      "1/32:\n",
      "shared_from = pd.Series(sdf['from'].unique())\n",
      "shared_from_blocks = pd.concat([shared_from, shared_from.apply(get_blocks_for_string).apply(pd.Series)], axis = 1)\n",
      "1/33:\n",
      "with zipfile.ZipFile('turf/from_blocks_split.zip') as zf: \n",
      "    from_blocks_split = pickle.load(zf.open('from_blocks_split.pkl'))\n",
      "1/34:\n",
      "shared_text_blocks = shared.text.apply(get_blocks_for_string)\n",
      "shared_text_blocks.head()\n",
      "1/35:\n",
      "shared_text_blocks = sdf.text.apply(get_blocks_for_string)\n",
      "shared_text_blocks.head()\n",
      "1/36:\n",
      "shared_text_blocks = shared_text_blocks.apply(pd.Series)\n",
      "shared_text_blocks.head()\n",
      "1/37: data_text_blocks = df.text.apply(get_blocks_for_string)\n",
      "1/38:\n",
      "#pickle.dump(data_text_blocks, open('turf/id_data_text_blocks_counter.pkl', 'wb'))\n",
      "data_text_blocks = pickle.load(open('turf/id_data_text_blocks_counter.pkl', 'rb'))\n",
      "1/39: pd.concat([data_text_blocks['id'].head(10), data_text_blocks['text'].head(10).apply(pd.Series)], axis=1)\n",
      "1/40:\n",
      "import gzip\n",
      "\n",
      "def save(obj, filename):\n",
      "    \"\"\"Save an object to a compressed disk file.\n",
      "       Works well with huge objects.\n",
      "    \"\"\"\n",
      "    with gzip.GzipFile(filename, 'wb') as file:\n",
      "        pickle.dump(obj, file)\n",
      "\n",
      "def load(filename):\n",
      "    \"\"\"Loads a compressed object from disk\n",
      "    \"\"\"\n",
      "    with gzip.GzipFile(filename, 'rb') as file:\n",
      "        obj = pickle.load(file)\n",
      "\n",
      "    return obj\n",
      "1/41:\n",
      "#save(data_text_blocks_df, 'turf/data_id_text_blocks_df.pkl.gz')\n",
      "data_text_blocks_df = load('turf/data_id_text_blocks_df.pkl.gz')\n",
      "1/42: top_blocks = data_text_blocks_df.count().sort_values(ascending=False)\n",
      "1/43: list(top_blocks.tail(top_blocks.shape[0]-20).index)\n",
      "1/44: data_text_blocks_df['not_in_top20'] = data_text_blocks_df.loc[:,list(top_blocks.tail(top_blocks.shape[0]-20).index)].sum(axis=1)\n",
      "1/45: data_text_blocks_df[data_text_blocks_df.not_in_top20>0].head(30)\n",
      "1/46: df.loc[df.id=='1023071434456338_1023180081112140', 'text'].iloc[0]\n",
      "1/47: data_text_blocks_df.set_index('id').count(axis=1).sort_values(ascending=False).head(10)\n",
      "1/48: df.loc[df.id=='10153145238127076_10153145247742076', 'text'].iloc[0]\n",
      "1/49: data_text_blocks_df.head().loc[:,list(top_blocks.head(20).index)].drop('id', axis=1)\n",
      "1/50: data_text_blocks_df.shape\n",
      "1/51: df.shape\n",
      "1/52:\n",
      "# this caused a memory error\n",
      "df = df.merge(data_text_blocks_df.loc[:,list(top_blocks.head(20).index)+['not_in_top20']], on='id', how='left')\n",
      "1/53: df.head()\n",
      "1/54: df['created_month'] = df['created_time'].dt.to_period('M')\n",
      "1/55: df.head()['created_time'].dt.to_period('d')\n",
      "1/56: df['created_day'] = df['created_time'].dt.to_period('d')\n",
      "1/57: df['created_day_of_week'] = pd.Categorical(df['created_time'].dt.strftime('%A'))\n",
      "1/58: df['created_hour'] = pd.Categorical(df['created_time'].dt.strftime('%H'))\n",
      "1/59:\n",
      "stats = ['mean', 'std', 'min', 'max', 'median']\n",
      "\n",
      "st = (df\n",
      "      .dropna(subset=['post_id'])\n",
      "      .groupby(by=['from_id', 'from', 'name'])\n",
      "      .agg(\n",
      "          {'id': 'count', \n",
      "           'elapsed_time': stats, \n",
      "           'length': stats, \n",
      "           'like_count': stats, \n",
      "           'post_id': 'nunique', \n",
      "           'parent_id': 'nunique',\n",
      "           'created_month': ['nunique'],\n",
      "           'created_day': ['nunique'],\n",
      "           'created_day_of_week': ['nunique'],\n",
      "           'created_hour': ['nunique'],\n",
      "           \n",
      "          })\n",
      "     )\n",
      "1/60:\n",
      "stats = ['mean', 'std', 'min', 'max', 'median']\n",
      "\n",
      "st = (df\n",
      "      .dropna(subset=['post_id'])\n",
      "      .groupby(by=['from_id', 'from', 'name'])\n",
      "      .agg(\n",
      "          {'id': 'count', \n",
      "           'elapsed_time': stats, \n",
      "           'length': stats, \n",
      "           'like_count': stats, \n",
      "           'post_id': 'nunique', \n",
      "           'parent_id': 'nunique',\n",
      "           'created_month': ['nunique'],\n",
      "           'created_day': ['nunique'],\n",
      "           'created_day_of_week': ['nunique'],\n",
      "           'created_hour': ['nunique'],\n",
      "           \n",
      "          })\n",
      "     )\n",
      "1/61:\n",
      "stats = ['mean', 'std', 'min', 'max', 'median']\n",
      "\n",
      "st = (df\n",
      "      .dropna(subset=['post_id'])\n",
      "      .groupby(by=['from_id', 'from', 'name'])\n",
      "      .agg(\n",
      "          {'id': 'count', \n",
      "           'elapsed_time': stats, \n",
      "           'length': stats, \n",
      "           'like_count': stats, \n",
      "           'post_id': 'nunique', \n",
      "           'parent_id': 'nunique',\n",
      "           'created_month': ['nunique'],\n",
      "           'created_day': ['nunique'],\n",
      "           'created_day_of_week': ['nunique'],\n",
      "           'created_hour': ['nunique'],\n",
      "           \n",
      "          })\n",
      "     )\n",
      "1/62:\n",
      "stats = ['mean', 'std', 'min', 'max', 'median']\n",
      "\n",
      "st = (df.head(100000)\n",
      "      .dropna(subset=['post_id'])\n",
      "      .groupby(by=['from_id', 'from', 'name'])\n",
      "      .agg(\n",
      "          {'id': 'count', \n",
      "           'elapsed_time': stats, \n",
      "           'length': stats, \n",
      "           'like_count': stats, \n",
      "           'post_id': 'nunique', \n",
      "           'parent_id': 'nunique',\n",
      "           'created_month': ['nunique'],\n",
      "           'created_day': ['nunique'],\n",
      "           'created_day_of_week': ['nunique'],\n",
      "           'created_hour': ['nunique'],\n",
      "           \n",
      "          })\n",
      "     )\n",
      "1/63:\n",
      "stats = ['mean', 'std', 'min', 'max', 'median']\n",
      "\n",
      "st = (df.head(10000)\n",
      "      .dropna(subset=['post_id'])\n",
      "      .groupby(by=['from_id', 'from', 'name'])\n",
      "      .agg(\n",
      "          {'id': 'count', \n",
      "           'elapsed_time': stats, \n",
      "           'length': stats, \n",
      "           'like_count': stats, \n",
      "           'post_id': 'nunique', \n",
      "           'parent_id': 'nunique',\n",
      "           'created_month': ['nunique'],\n",
      "           'created_day': ['nunique'],\n",
      "           'created_day_of_week': ['nunique'],\n",
      "           'created_hour': ['nunique'],\n",
      "           \n",
      "          })\n",
      "     )\n",
      "1/64: st.head()\n",
      "1/65: df.shape\n",
      "1/66: df.dtypes\n",
      "1/67:\n",
      "stats = ['mean', 'std', 'min', 'max', 'median']\n",
      "\n",
      "st = (df[['id', 'elapsed_time', 'length', 'like_count', 'post_id', 'parent_id', 'created_month', 'created_day', 'created_day_of_week', 'created_hour']]\n",
      "      #.dropna(subset=['post_id'])\n",
      "      .groupby(by=['from_id', 'from', 'name'])\n",
      "      .agg(\n",
      "          {'id': 'count', \n",
      "           'elapsed_time': stats, \n",
      "           'length': stats, \n",
      "           'like_count': stats, \n",
      "           'post_id': 'nunique', \n",
      "           'parent_id': 'nunique',\n",
      "           'created_month': ['nunique'],\n",
      "           'created_day': ['nunique'],\n",
      "           'created_day_of_week': ['nunique'],\n",
      "           'created_hour': ['nunique'],\n",
      "           \n",
      "          })\n",
      "     )\n",
      "1/68:\n",
      "stats = ['mean', 'std', 'min', 'max', 'median']\n",
      "\n",
      "st = (df[['from_id', 'name', 'id', 'elapsed_time', 'length', 'like_count', 'post_id', 'parent_id', 'created_month', 'created_day', 'created_day_of_week', 'created_hour']]\n",
      "      #.dropna(subset=['post_id'])\n",
      "      .groupby(by=['from_id', 'name'])\n",
      "      .agg(\n",
      "          {'id': 'count', \n",
      "           'elapsed_time': stats, \n",
      "           'length': stats, \n",
      "           'like_count': stats, \n",
      "           'post_id': 'nunique', \n",
      "           'parent_id': 'nunique',\n",
      "           'created_month': ['nunique'],\n",
      "           'created_day': ['nunique'],\n",
      "           'created_day_of_week': ['nunique'],\n",
      "           'created_hour': ['nunique'],\n",
      "           \n",
      "          })\n",
      "     )\n",
      "1/69: st.shape\n",
      "1/70:\n",
      "from_id_lang = df.groupby(['from_id', 'langdetect']).size().unstack()[['he', 'ar', 'en', 'ru', 'fr', 'de', 'es']].sort_values(by='he', ascending=False)\n",
      "from_id_lang.head()\n",
      "1/71: list(top_blocks.head(20).index).append('from_id')\n",
      "1/72: top_blocks.head(20).index\n",
      "1/73: from_id_uniblock = df.loc[:,list(top_blocks.head(20).index)+ ['not_in_top20', 'from_id']].groupby('from_id').sum()\n",
      "1/74:\n",
      "top_uniblock = from_id_uniblock.idxmax(axis=1)\n",
      "top_uniblock.head()\n",
      "1/75: st[st['id', 'count']>10].sort_values(by=('elapsed_time', 'std')).head(10)\n",
      "1/76: #pickle.dump(st, open('turf/basic_stats_df.pkl', 'wb'))\n",
      "1/77: st.reset_index().from_id.nunique()\n",
      "1/78: st[st['id','count']>1].reset_index().from_id.nunique()\n",
      "1/79: st.reset_index().groupby('from_id').name.nunique().value_counts().head(20)/st.reset_index().from_id.nunique()\n",
      "1/80: st[st['id','count']>1].reset_index().groupby('from_id').name.nunique().value_counts().head(20)/st.reset_index().from_id.nunique()\n",
      "1/81:\n",
      "uniques = (st.reset_index()\n",
      " [st.reset_index().groupby('from_id').name.transform('nunique')==1]\n",
      " .name.value_counts())\n",
      "1/82: uniques.head(10)\n",
      "1/83:\n",
      "(pd.concat([uniques, \n",
      "            uniques/st.reset_index().from_id.nunique(), \n",
      "            uniques/st.reset_index().groupby('name').from_id.nunique()],\n",
      "           axis=1, keys=['uniques', 'ratio_from_all', 'ratio_from_own']).sort_values(by='uniques', ascending=False))\n",
      "1/84:\n",
      "counts = st.reset_index().groupby('from_id')['id', 'count'].sum()\n",
      "counts.head()\n",
      "1/85:\n",
      "counts = st.reset_index().groupby('from_id')[('id', 'count')].sum()\n",
      "counts.head()\n",
      "1/86:\n",
      "counts = st.reset_index().groupby('from_id')['id']['count'].sum()\n",
      "counts.head()\n",
      "1/87:\n",
      "counts = st.reset_index().groupby('from_id')[['id']]['count'].sum()\n",
      "counts.head()\n",
      "1/88:\n",
      "counts = st.reset_index().groupby('from_id')['id']['count'].sum()\n",
      "counts.head()\n",
      "1/89:\n",
      "counts = st.reset_index().groupby('from_id').agg({['id', 'count']: 'sum'})\n",
      "counts.head()\n",
      "1/90:\n",
      "counts = st.reset_index().groupby('from_id').agg({('id', 'count'): 'sum'})\n",
      "counts.head()\n",
      "1/91:\n",
      "counts = st.reset_index().groupby('from_id').agg({('id', 'count'): 'sum'}).drop_levels()\n",
      "counts.head()\n",
      "1/92:\n",
      "counts = st.reset_index().groupby('from_id').agg({('id', 'count'): 'sum'}).droplevels()\n",
      "counts.head()\n",
      "1/93:\n",
      "counts = st.reset_index().groupby('from_id').agg({('id', 'count'): 'sum'}).drop_level()\n",
      "counts.head()\n",
      "1/94:\n",
      "counts = st.reset_index().groupby('from_id').agg({('id', 'count'): 'sum'}).droplevel()\n",
      "counts.head()\n",
      "1/95:\n",
      "counts = st.reset_index().groupby('from_id').agg({('id', 'count'): 'sum'})['id', 'count']\n",
      "counts.head()\n",
      "1/96:\n",
      "counts = st.reset_index().groupby('from_id').agg({('id', 'count'): 'sum'})['id', 'count']\n",
      "counts[counts==1].shape\n",
      "1/97: st.reset_index().groupby('from_id').name.nunique().value_counts().head(20)/st.reset_index().from_id.nunique().plot(kind='bar')\n",
      "1/98: st.reset_index().groupby('from_id').name.nunique().value_counts().head(20)/st.reset_index().from_id.nunique()\n",
      "1/99: (st.reset_index().groupby('from_id').name.nunique().value_counts().head(20)/st.reset_index().from_id.nunique()).plot(kind='bar')\n",
      "1/100: st.reset_index().from_id.nunique()\n",
      "1/101: counts[counts==1].shape[0]/st.reset_index().from_id.nunique()\n",
      "1/102:\n",
      "(pd.concat([uniques, \n",
      "            uniques/st.reset_index().from_id.nunique(), \n",
      "            uniques/st.reset_index().groupby('name').from_id.nunique()],\n",
      "           axis=1, keys=['uniques', 'ratio_from_all', 'ratio_from_own']).sort_values(by='uniques', ascending=False)\n",
      ".sort_values(by='ratio_from_own', ascending=False))\n",
      "1/103: save(st, 'turf/total_stats.pkl.gz')\n",
      "1/104:\n",
      "stats = ['mean', 'std', 'min', 'max', 'median']\n",
      "\n",
      "st = (df.loc[df.is_post==0, ['from_id', 'name', 'id', 'elapsed_time', 'length', 'like_count', 'post_id', 'parent_id', 'created_month', 'created_day', 'created_day_of_week', 'created_hour']]\n",
      "      #.dropna(subset=['post_id'])\n",
      "      .groupby(by=['from_id', 'name'])\n",
      "      .agg(\n",
      "          {'id': 'count', \n",
      "           'elapsed_time': stats, \n",
      "           'length': stats, \n",
      "           'like_count': stats, \n",
      "           'post_id': 'nunique', \n",
      "           'parent_id': 'nunique',\n",
      "           'created_month': ['nunique'],\n",
      "           'created_day': ['nunique'],\n",
      "           'created_day_of_week': ['nunique'],\n",
      "           'created_hour': ['nunique'],\n",
      "           \n",
      "          })\n",
      "     )\n",
      "1/105: counts[counts>10].shape[0]/st.reset_index().from_id.nunique()\n",
      "1/106: st.reset_index().loc[name=='Benjamin_Netanyahu', ['id','count']==1].from_id.nunique()\n",
      "1/107: st['id','count']>1].reset_index().loc[st.name=='Benjamin_Netanyahu'].from_id.nunique()\n",
      "1/108: st[st['id','count']>1].reset_index().loc[st.name=='Benjamin_Netanyahu'].from_id.nunique()\n",
      "1/109: st[(st['id','count']>1) & (st.reset_index().name=='Benjamin_Netanyahu')].reset_index().from_id.nunique()\n",
      "1/110: (st.reset_index().name=='Benjamin_Netanyahu')\n",
      "1/111: (st['id','count']>1)\n",
      "1/112: st[(st['id','count']>1) & (st[:,'Benjamin_Netanyahu'])].reset_index().from_id.nunique()\n",
      "1/113: st[(st['id','count']>1) & (st[(:,'Benjamin_Netanyahu')])].reset_index().from_id.nunique()\n",
      "1/114:\n",
      "st.columns = st.reset_index().columns.droplevel()\n",
      "st[(st.count==1) & (st.name=='Benjamin_Netanyahu')].shape\n",
      "1/115:\n",
      "st.columns = st.columns.droplevel()\n",
      "st = st.reset_index()\n",
      "st[(st.count==1) & (st.name=='Benjamin_Netanyahu')].shape\n",
      "1/116:\n",
      "st.columns = st.columns.droplevel()\n",
      "st = st.reset_index()\n",
      "st.head()\n",
      "1/117: st.head()\n",
      "1/118: st[(st['count']==1) & (st.name=='Benjamin_Netanyahu')].shape\n",
      "1/119:\n",
      "mat = st[['from_id', 'name', 'count']].stack('name')\n",
      "mat.head()\n",
      "1/120:\n",
      "mat = st.set_index('from_id', 'name')[['count']].stack()\n",
      "mat.head()\n",
      "1/121:\n",
      "mat = st.set_index('from_id', 'name')[['count']].unstack()\n",
      "mat.head()\n",
      "1/122:\n",
      "mat = st.set_index('from_id', 'name').count\n",
      "mat.head()\n",
      "1/123:\n",
      "mat = st.set_index('from_id', 'name')['count']\n",
      "mat.head()\n",
      "1/124:\n",
      "mat = st.set_index('from_id', 'name')\n",
      "mat.head()\n",
      "1/125:\n",
      "mat = st.set_index(['from_id', 'name'])[['count']]\n",
      "mat.head()\n",
      "1/126:\n",
      "mat = st.set_index(['from_id', 'name'])[['count']].stack()\n",
      "mat.head()\n",
      "1/127:\n",
      "mat = st.set_index(['from_id', 'name'])[['count']].unstack()\n",
      "mat.head()\n",
      "1/128:\n",
      "mat = st.set_index(['from_id', 'name'])[['count']].unstack()\n",
      "mat.columns = mat.columns.droplevel()\n",
      "mat[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu.notna()]\n",
      "1/129:\n",
      "mat = st.set_index(['from_id', 'name'])[['count']].unstack()\n",
      "mat.columns = mat.columns.droplevel()\n",
      "mat[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu.notna())]\n",
      "1/130:\n",
      "mat = st.set_index(['from_id', 'name'])[['count']].unstack()\n",
      "mat.columns = mat.columns.droplevel()\n",
      "mat[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu.notna())].shape\n",
      "1/131:\n",
      "no good\n",
      "st[(st['count']==1) & (st.name=='Benjamin_Netanyahu')].shape\n",
      "1/132:\n",
      "#no good\n",
      "st[(st['count']==1) & (st.name=='Benjamin_Netanyahu')].shape\n",
      "1/133: mat.count(axis=1)==1\n",
      "1/134:\n",
      "mat = st.set_index(['from_id', 'name'])[['count']].unstack()\n",
      "mat.columns = mat.columns.droplevel()\n",
      "mat[(mat.count(axis=1)==1) & (!mat.Benjamin_Netanyahu.isna())].shape\n",
      "1/135:\n",
      "mat = st.set_index(['from_id', 'name'])[['count']].unstack()\n",
      "mat.columns = mat.columns.droplevel()\n",
      "mat[(mat.count(axis=1)==1) & !(mat.Benjamin_Netanyahu.isna())].shape\n",
      "1/136:\n",
      "mat = st.set_index(['from_id', 'name'])[['count']].unstack()\n",
      "mat.columns = mat.columns.droplevel()\n",
      "mat[(mat.count(axis=1)==1) & (~mat.Benjamin_Netanyahu.isna())].shape\n",
      "1/137:\n",
      "mat = st.set_index(['from_id', 'name'])[['count']].unstack()\n",
      "mat.columns = mat.columns.droplevel()\n",
      "mat[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu.notna())].shape\n",
      "1/138:\n",
      "mat = st.set_index(['from_id', 'name'])[['count']].unstack()\n",
      "mat.columns = mat.columns.droplevel()\n",
      "mat[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu==1)].shape\n",
      "1/139:\n",
      "mat = st.set_index(['from_id', 'name'])[['count']].unstack()\n",
      "mat.columns = mat.columns.droplevel()\n",
      "mat[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu==1)].shape[0] / counts[counts==1].shape[0]\n",
      "1/140:\n",
      "mat = st.set_index(['from_id', 'name'])[['count']].unstack()\n",
      "mat.columns = mat.columns.droplevel()\n",
      "mat[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu==1)].shape[0]\n",
      " 2/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from langdetect import detect\n",
      "import pickle\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('husl', 12)\n",
      "%matplotlib inline\n",
      " 2/2:\n",
      "import glob\n",
      "import zipfile\n",
      "import re\n",
      "import pickle\n",
      " 2/3: folder = 'turf/Facebook Data Backups/'\n",
      " 2/4: files = glob.glob(folder + '*/*.zip')\n",
      " 2/5:\n",
      "names = [re.match(folder + '([^/]+)', f).group(1) for f in files]\n",
      "names\n",
      " 2/6: len(names), len(files)\n",
      " 2/7:\n",
      "data_dfs = []\n",
      "shared_dfs = []\n",
      "for name, f in zip(names, files):\n",
      "    print(f)\n",
      "    with zipfile.ZipFile(f) as zf:\n",
      "        try:\n",
      "            data_df = (pd.read_csv(zf.open('data.csv'), sep='\\t', doublequote=False, lineterminator='\\n'))\n",
      "        except:\n",
      "            print ('******************** BAD ************************')\n",
      "        '''            zf.extract('data.csv')\n",
      "            data_df = (pd.read_csv(open('data.csv', newline=None, encoding='utf8'), sep='\\t', doublequote=False, lineterminator='\\n',\n",
      "                                  #dtype=['object', 'bool', 'datetime64', 'datetime64', 'object', 'int',\n",
      "                                  #       'object', 'int', 'int', 'int', 'int', 'object', 'object', \n",
      "                                  #       'object', 'object']\n",
      "                                  ))'''\n",
      "        data_df = (data_df.assign(name = name)\n",
      "                  .rename(columns={'elapsed_time(minutes)': 'elapsed_time'}))\n",
      "        data_dfs.append(data_df)          \n",
      "        shared_df = (pd.read_csv(zf.open('shared.csv'), sep='\\t', doublequote=False, lineterminator='\\n'))\n",
      "        shared_df = (shared_df.assign(name = name)\n",
      "                  .rename(columns={'elapsed_time(minutes)': 'elapsed_time'}))\n",
      "        shared_dfs.append(shared_df)\n",
      " 2/8:\n",
      "for df in data_dfs:\n",
      "    print (df.shape[0], df.name.unique())\n",
      " 2/9:\n",
      "for df, name in zip(data_dfs, files):\n",
      "    na = df[df.post_id.isna()].shape[0]\n",
      "    if na > 0:\n",
      "        print (na, name)\n",
      "2/10: data_dfs[0].dtypes\n",
      "2/11:\n",
      "df = (pd.concat(data_dfs)\n",
      "     .assign(name = lambda x: pd.Categorical(x.name))\n",
      "     .assign(sentiment = lambda x: pd.Categorical(x.sentiment))\n",
      "     .assign(created_time = lambda x: pd.to_datetime(x.created_time))\n",
      "     .assign(updated_time = lambda x: pd.to_datetime(x.updated_time)))\n",
      "df.head()\n",
      "2/12:\n",
      "sdf = (pd.concat(shared_dfs)\n",
      "     .assign(name = lambda x: pd.Categorical(x.name))\n",
      "     .assign(sentiment = lambda x: pd.Categorical(x.sentiment))\n",
      "     .assign(created_time = lambda x: pd.to_datetime(x.created_time))\n",
      "     .assign(updated_time = lambda x: pd.to_datetime(x.updated_time)))\n",
      "sdf.head()\n",
      "2/13: df.name.value_counts().head()\n",
      "2/14: sdf.name.value_counts().head()\n",
      "2/15: df.dtypes\n",
      "2/16: sdf.dtypes\n",
      "2/17:\n",
      "with zipfile.ZipFile('turf/id_lang.zip') as zf: \n",
      "    df = df.merge(pickle.load(zf.open('id_lang.pkl')), how='left', on='id')\n",
      "2/18:\n",
      "with zipfile.ZipFile('turf/shared_id_lang.zip') as zf: \n",
      "    sdf = sdf.merge(pickle.load(zf.open('shared_id_lang.pkl')), how='left', on='id')\n",
      "2/19: df.langdetect.value_counts().head(20)\n",
      "2/20: df.groupby(['name', 'langdetect']).size().unstack()[['he', 'ar', 'en', 'ru', 'fr', 'de', 'es']].sort_values(by='he', ascending=False).head(20)\n",
      "2/21:\n",
      "import gzip\n",
      "\n",
      "def save(obj, filename):\n",
      "    \"\"\"Save an object to a compressed disk file.\n",
      "       Works well with huge objects.\n",
      "    \"\"\"\n",
      "    with gzip.GzipFile(filename, 'wb') as file:\n",
      "        pickle.dump(obj, file)\n",
      "\n",
      "def load(filename):\n",
      "    \"\"\"Loads a compressed object from disk\n",
      "    \"\"\"\n",
      "    with gzip.GzipFile(filename, 'rb') as file:\n",
      "        obj = pickle.load(file)\n",
      "\n",
      "    return obj\n",
      "2/22:\n",
      "t = [1, 2, 3, 4]\n",
      "save(t, 'test.gz')\n",
      "load('test.gz')\n",
      "2/23: from_blocks_split.head()\n",
      "2/24:\n",
      "with zipfile.ZipFile('turf/from_blocks_split.zip') as zf: \n",
      "    from_blocks_split = pickle.load(zf.open('from_blocks_split.pkl'))\n",
      "2/25: from_blocks_split.head()\n",
      "2/26:\n",
      "shared_text_blocks = sdf.text.apply(get_blocks_for_string)\n",
      "shared_text_blocks.head()\n",
      "2/27:\n",
      "shared_text_blocks = shared_text_blocks.apply(pd.Series)\n",
      "shared_text_blocks.head()\n",
      "2/28:\n",
      "from __future__ import absolute_import, print_function, unicode_literals\n",
      "\n",
      "import os\n",
      "import re\n",
      "\n",
      "import requests\n",
      "\n",
      "\n",
      "def get_block_for_codepoint(cp):\n",
      "    \"\"\"Return the Unicode block name for the provided numeric codepoint\"\"\"\n",
      "\n",
      "    for start, end, block_name in UNICODE_BLOCKS:\n",
      "        if start <= cp <= end:\n",
      "            return block_name\n",
      "\n",
      "    return 'No_Block'\n",
      "\n",
      "\n",
      "def load_unicode_blocks_from_file(f):\n",
      "    file_contents = f.read().decode('utf-8')\n",
      "\n",
      "    blocks = []\n",
      "    for start, end, block_name in re.findall(r'([0-9A-F]+)\\.\\.([0-9A-F]+);\\ (\\S.*\\S)', file_contents):\n",
      "        if block_name == 'No_Block':\n",
      "            continue\n",
      "\n",
      "        blocks.append((int(start, 16), int(end, 16), block_name))\n",
      "\n",
      "    return blocks\n",
      "\n",
      "\n",
      "def load_unicode_blocks(block_filename):\n",
      "    if not os.path.exists(block_filename):\n",
      "        print('Unicode block file %s does not exist. Downloading…' % block_filename)\n",
      "        r = requests.get('http://unicode.org/Public/UNIDATA/Blocks.txt')\n",
      "        r.raise_for_status()\n",
      "\n",
      "        with open(block_filename, 'wb') as f:\n",
      "            for chunk in r.iter_content():\n",
      "                f.write(chunk)\n",
      "\n",
      "    with open(block_filename, 'rb') as f:\n",
      "        blocks = load_unicode_blocks_from_file(f)\n",
      "\n",
      "    return blocks\n",
      "\n",
      "UNICODE_BLOCKS = load_unicode_blocks('UNIDATA-Blocks.txt')\n",
      "2/29:\n",
      "from collections import Counter\n",
      "def get_blocks_for_string(s):\n",
      "    s = ''.join(e for e in str(s) if e.isalnum())\n",
      "    return Counter([get_block_for_codepoint(ord(c)) for c in str(s)])\n",
      "2/30: len(UNICODE_BLOCKS)\n",
      "2/31:\n",
      "shared_text_blocks = sdf.text.apply(get_blocks_for_string)\n",
      "shared_text_blocks.head()\n",
      "2/32:\n",
      "shared_text_blocks = shared_text_blocks.apply(pd.Series)\n",
      "shared_text_blocks.head()\n",
      "2/33:\n",
      "shared_from = pd.Series(sdf['from'].unique())\n",
      "shared_from_blocks = pd.concat([shared_from, shared_from.apply(get_blocks_for_string).apply(pd.Series)], axis = 1)\n",
      "2/34:\n",
      "shared_from = pd.Series(sdf['from'].unique())\n",
      "shared_from_blocks = pd.concat([shared_from, shared_from.apply(get_blocks_for_string).apply(pd.Series)], axis = 1)\n",
      "shared_from_blocks.head()\n",
      "2/35: from_blocks_split = pd.concat([from_blocks_split, shared_from_blocks]).drop_duplicates()\n",
      "2/36: from_blocks_split = pd.concat([from_blocks_split, shared_from_blocks], sort=False).drop_duplicates()\n",
      "2/37:\n",
      "from_blocks_split = pd.concat([from_blocks_split, shared_from_blocks], sort=False).drop_duplicates()\n",
      "from_blocks_split.head()\n",
      "2/38:\n",
      "with zipfile.ZipFile('turf/from_blocks_split.zip') as zf: \n",
      "    from_blocks_split = pickle.load(zf.open('from_blocks_split.pkl'))\n",
      "2/39: from_blocks_split.head()\n",
      "2/40:\n",
      "from_blocks_split = pd.concat([from_blocks_split, shared_from_blocks], sort=False).drop_duplicates()\n",
      "from_blocks_split.tail()\n",
      "2/41:\n",
      "shared_from = pd.Series(sdf['from'].unique())\n",
      "shared_from_blocks = (pd.concat([shared_from, shared_from.apply(get_blocks_for_string).apply(pd.Series)], axis = 1)\n",
      "                     .rename(columns={'0':'from'}))\n",
      "shared_from_blocks.head()\n",
      "2/42:\n",
      "shared_from = pd.Series(sdf['from'].unique())\n",
      "shared_from_blocks = (pd.concat([shared_from, shared_from.apply(get_blocks_for_string).apply(pd.Series)], axis = 1)\n",
      "                     .rename(columns={0:'from'}))\n",
      "shared_from_blocks.head()\n",
      "2/43:\n",
      "with zipfile.ZipFile('turf/from_blocks_split.zip') as zf: \n",
      "    from_blocks_split = pickle.load(zf.open('from_blocks_split.pkl'))\n",
      "2/44: from_blocks_split.head()\n",
      "2/45:\n",
      "from_blocks_split = pd.concat([from_blocks_split, shared_from_blocks], sort=False).drop_duplicates()\n",
      "from_blocks_split.tail()\n",
      "2/46: from_blocks_split.count()\n",
      "2/47: from_blocks_split.count().sort_values(ascending=False)\n",
      "2/48:\n",
      "f = from_blocks_split\n",
      "f[~['from']]\n",
      "2/49:\n",
      "f = from_blocks_split\n",
      "f[[~'from']]\n",
      "2/50:\n",
      "f = from_blocks_split\n",
      "f[(~['from'])]\n",
      "2/51:\n",
      "f = from_blocks_split\n",
      "\n",
      "f['from_he_count'] = f['Hebrew']\n",
      "f['from_latin_count'] = f['Basic Latin'] + f['Latin-1 Supplement'] + f['Latin Extended-A'] + f['Latin Extended-B'] + f['Latin Extended Additional']\n",
      "f['from_ar_count'] = f['Arabic'] + f['Arabic Presentation Forms-B'] + f['Arabic Presentation Forms-A']\n",
      "f['from_cyr_count'] = f['Cyrillic']\n",
      "f['from_other_count'] = f.loc[:, f.columns != 'from'].sum(axis=1) - (f['from_he_count'] + f['from_latin_count'] + f['from_ar_count'] + f['from_cyr_count'])\n",
      "2/52: f[:, f.columns.str.startswith('from_')]\n",
      "2/53: f.columns.str.startswith('from_')\n",
      "2/54: f[[f.columns.str.startswith('from_')]]\n",
      "2/55: f.loc[:, f.columns.str.startswith('from_')]\n",
      "2/56: f.loc[:, f.columns.str.startswith('from')]\n",
      "2/57:\n",
      "f = from_blocks_split\n",
      "\n",
      "f['from_he_count'] = f['Hebrew']\n",
      "f['from_latin_count'] = f['Basic Latin'] + f['Latin-1 Supplement'] + f['Latin Extended-A'] + f['Latin Extended-B'] + f['Latin Extended Additional']\n",
      "f['from_ar_count'] = f['Arabic'] + f['Arabic Presentation Forms-B'] + f['Arabic Presentation Forms-A']\n",
      "f['from_cyr_count'] = f['Cyrillic']\n",
      "f['from_other_count'] = f.loc[:, f.columns != 'from'].sum(axis=1) - (f['from_he_count'] + f['from_latin_count'] + f['from_ar_count'] + f['from_cyr_count'])\n",
      "2/58: f.columns.str.startswith('from_')\n",
      "2/59: f.loc[:, f.columns.str.startswith('from')]\n",
      "2/60:\n",
      "f = from_blocks_split\n",
      "\n",
      "f['from_he_count'] = f[['Hebrew']]\n",
      "f['from_latin_count'] = f[['Basic Latin']] + f[['Latin-1 Supplement']] + f[['Latin Extended-A']] + f[['Latin Extended-B']] + f[['Latin Extended Additional']]\n",
      "f['from_ar_count'] = f[['Arabic']] + f[['Arabic Presentation Forms-B']] + f[['Arabic Presentation Forms-A']]\n",
      "f['from_cyr_count'] = f[['Cyrillic']]\n",
      "f['from_other_count'] = f.loc[:, f.columns != 'from'].sum(axis=1) - (f['from_he_count'] + f['from_latin_count'] + f['from_ar_count'] + f['from_cyr_count'])\n",
      "2/61: f.columns.str.startswith('from_')\n",
      "2/62: f.loc[:, f.columns.str.startswith('from')]\n",
      "2/63:\n",
      "f = from_blocks_split.fillna(0)\n",
      "\n",
      "f['from_he_count'] = f[['Hebrew']]\n",
      "f['from_latin_count'] = f[['Basic Latin']] + f[['Latin-1 Supplement']] + f[['Latin Extended-A']] + f[['Latin Extended-B']] + f[['Latin Extended Additional']]\n",
      "f['from_ar_count'] = f[['Arabic']] + f[['Arabic Presentation Forms-B']] + f[['Arabic Presentation Forms-A']]\n",
      "f['from_cyr_count'] = f[['Cyrillic']]\n",
      "f['from_other_count'] = f.loc[:, f.columns != 'from'].sum(axis=1) - (f['from_he_count'] + f['from_latin_count'] + f['from_ar_count'] + f['from_cyr_count'])\n",
      "2/64: f.columns.str.startswith('from_')\n",
      "2/65: f.loc[:, f.columns.str.startswith('from')]\n",
      "2/66: from_blocks_split.count().sort_values(ascending=False).index\n",
      "2/67:\n",
      "f = from_blocks_split.fillna(0)\n",
      "\n",
      "f['from_he_count'] = f[['Hebrew']]\n",
      "f['from_latin_count'] = f.loc[:, f.columns.str.contains('Latin')].sum(axis=1)\n",
      "f['from_ar_count'] = f.loc[:, f.columns.str.contains('Arabic')].sum(axis=1)\n",
      "f['from_cyr_count'] = f.loc[:, f.columns.str.contains('Cyrillic')].sum(axis=1)\n",
      "f['from_other_count'] = f.loc[:, f.columns != 'from'].sum(axis=1) - (f['from_he_count'] + f['from_latin_count'] + f['from_ar_count'] + f['from_cyr_count'])\n",
      "2/68: f.columns.str.startswith('from_')\n",
      "2/69: f.loc[:, f.columns.str.startswith('from')]\n",
      "2/70:\n",
      "with zipfile.ZipFile('turf/from_blocks_split.zip') as zf: \n",
      "    from_blocks_split = pickle.load(zf.open('from_blocks_split.pkl'))\n",
      "2/71: from_blocks_split.head()\n",
      "2/72:\n",
      "from_blocks_split = pd.concat([from_blocks_split, shared_from_blocks], sort=False).drop_duplicates()\n",
      "from_blocks_split.tail()\n",
      "2/73: from_blocks_split.count().sort_values(ascending=False).index\n",
      "2/74:\n",
      "f = from_blocks_split.fillna(0)\n",
      "\n",
      "f['from_he_count'] = f[['Hebrew']]\n",
      "f['from_latin_count'] = f.loc[:, f.columns.str.contains('Latin')].sum(axis=1)\n",
      "f['from_ar_count'] = f.loc[:, f.columns.str.contains('Arabic')].sum(axis=1)\n",
      "f['from_cyr_count'] = f.loc[:, f.columns.str.contains('Cyrillic')].sum(axis=1)\n",
      "f['from_other_count'] = f.loc[:, f.columns.str.startswith('from')].sum(axis=1) - (f['from_he_count'] + f['from_latin_count'] + f['from_ar_count'] + f['from_cyr_count'])\n",
      "2/75: f.columns.str.startswith('from_')\n",
      "2/76: f.loc[:, f.columns.str.startswith('from')]\n",
      "2/77: f.loc[:, f.columns.str.startswith('from')].head()\n",
      "2/78: f[f.from_other_count>0]\n",
      "2/79:\n",
      "f = from_blocks_split.fillna(0)\n",
      "\n",
      "f['from_he_count'] = f[['Hebrew']]\n",
      "f['from_latin_count'] = f.loc[:, f.columns.str.contains('Latin')].sum(axis=1)\n",
      "f['from_ar_count'] = f.loc[:, f.columns.str.contains('Arabic')].sum(axis=1)\n",
      "f['from_cyr_count'] = f.loc[:, f.columns.str.contains('Cyrillic')].sum(axis=1)\n",
      "f['from_other_count'] = f.loc[:, ~f.columns.str.startswith('from')].sum(axis=1) - f.loc[:, f.columns.str.startswith('from_')].sum(axis=1)\n",
      "2/80: f.columns.str.startswith('from_')\n",
      "2/81: f.loc[:, f.columns.str.startswith('from')].head()\n",
      "2/82: f[f.from_other_count>0]\n",
      "2/83: f[f.from_other_count>0, f.columns.str.startswith('from')].head()\n",
      "2/84: f.loc[f.from_other_count>0, f.columns.str.startswith('from')].head()\n",
      "2/85: top_blocks = data_text_blocks_df.count().sort_values(ascending=False)\n",
      "2/86:\n",
      "#save(data_text_blocks_df, 'turf/data_id_text_blocks_df.pkl.gz')\n",
      "data_text_blocks_df = load('turf/data_id_text_blocks_df.pkl.gz')\n",
      "2/87: top_blocks = data_text_blocks_df.count().sort_values(ascending=False)\n",
      "2/88: list(top_blocks.tail(top_blocks.shape[0]-20).index)\n",
      "2/89: data_text_blocks_df['not_in_top20'] = data_text_blocks_df.loc[:,list(top_blocks.tail(top_blocks.shape[0]-20).index)].sum(axis=1)\n",
      "2/90: data_text_blocks_df[data_text_blocks_df.not_in_top20>0].head(30)\n",
      "2/91: df.loc[df.id=='1023071434456338_1023180081112140', 'text'].iloc[0]\n",
      "2/92: data_text_blocks_df.set_index('id').count(axis=1).sort_values(ascending=False).head(10)\n",
      "2/93: df.loc[df.id=='10153145238127076_10153145247742076', 'text'].iloc[0]\n",
      "2/94: data_text_blocks_df.head().loc[:,list(top_blocks.head(20).index)].drop('id', axis=1)\n",
      "2/95: data_text_blocks_df.shape\n",
      "2/96: df.shape\n",
      "2/97: df.id.nunique()\n",
      "2/98:\n",
      "def clean_dataframe_column_names(df):\n",
      "    cols = df.columns\n",
      "    new_column_names = []\n",
      "\n",
      "    for col in cols:\n",
      "        new_col = col.lstrip().rstrip().lower().replace (\" \", \"_\").replace (\"-\", \"_\") #strip beginning spaces, makes lowercase, add underscpre\n",
      "        new_column_names.append(new_col)\n",
      "\n",
      "    df.columns = new_column_names\n",
      "\n",
      "data_text_blocks_df.head()\n",
      "2/99:\n",
      "def clean_dataframe_column_names(df):\n",
      "    cols = df.columns\n",
      "    new_column_names = []\n",
      "\n",
      "    for col in cols:\n",
      "        new_col = col.lstrip().rstrip().lower().replace (\" \", \"_\").replace (\"-\", \"_\") #strip beginning spaces, makes lowercase, add underscpre\n",
      "        new_column_names.append(new_col)\n",
      "\n",
      "    df.columns = new_column_names\n",
      "\n",
      "clean_dataframe_column_names(data_text_blocks_df).head()\n",
      "2/100:\n",
      "def clean_dataframe_column_names(df):\n",
      "    cols = df.columns\n",
      "    new_column_names = []\n",
      "\n",
      "    for col in cols:\n",
      "        new_col = col.lstrip().rstrip().lower().replace (\" \", \"_\").replace (\"-\", \"_\") #strip beginning spaces, makes lowercase, add underscpre\n",
      "        new_column_names.append(new_col)\n",
      "\n",
      "    df.columns = new_column_names\n",
      "\n",
      "clean_dataframe_column_names(data_text_blocks_df)\n",
      "data_text_blocks_df\n",
      "2/101:\n",
      "def clean_dataframe_column_names(df):\n",
      "    cols = df.columns\n",
      "    new_column_names = []\n",
      "\n",
      "    for col in cols:\n",
      "        new_col = col.lstrip().rstrip().lower().replace (\" \", \"_\").replace (\"-\", \"_\") #strip beginning spaces, makes lowercase, add underscpre\n",
      "        new_column_names.append(new_col)\n",
      "\n",
      "    df.columns = new_column_names\n",
      "\n",
      "clean_dataframe_column_names(data_text_blocks_df)\n",
      "data_text_blocks_df.head()\n",
      "2/102:\n",
      "#save(data_text_blocks_df, 'turf/data_id_text_blocks_df.pkl.gz')\n",
      "data_text_blocks_df = load('turf/data_id_text_blocks_df.pkl.gz')\n",
      "2/103:\n",
      "def clean_dataframe_column_names(df, prefix='', suffix=''):\n",
      "    cols = df.columns\n",
      "    new_column_names = []\n",
      "\n",
      "    for col in cols:\n",
      "        new_col = col.lstrip().rstrip().lower().replace (\" \", \"_\").replace (\"-\", \"_\") #strip beginning spaces, makes lowercase, add underscpre\n",
      "        new_column_names.append(new_col)\n",
      "\n",
      "    df.columns = new_column_names\n",
      "\n",
      "clean_dataframe_column_names(data_text_blocks_df)\n",
      "data_text_blocks_df.head()\n",
      "2/104:\n",
      "def clean_dataframe_column_names(df, prefix='', suffix=''):\n",
      "    cols = df.columns\n",
      "    new_column_names = []\n",
      "\n",
      "    for col in cols:\n",
      "        new_col = prefix+col.lstrip().rstrip().lower().replace (\" \", \"_\").replace (\"-\", \"_\")+suffix #strip beginning spaces, makes lowercase, add underscpre\n",
      "        new_column_names.append(new_col)\n",
      "\n",
      "    df.columns = new_column_names\n",
      "\n",
      "clean_dataframe_column_names(data_text_blocks_df, 'text_')\n",
      "data_text_blocks_df.head()\n",
      "2/105:\n",
      "f = from_blocks_split.fillna(0)\n",
      "clean_dataframe_column_names(f, '_from_')\n",
      "f['from_he_count'] = f.loc[:, f.columns.str.contains('hebrew')].sum(axis=1)\n",
      "f['from_latin_count'] = f.loc[:, f.columns.str.contains('latin')].sum(axis=1)\n",
      "f['from_ar_count'] = f.loc[:, f.columns.str.contains('arabic')].sum(axis=1)\n",
      "f['from_cyr_count'] = f.loc[:, f.columns.str.contains('ayrillic')].sum(axis=1)\n",
      "f['from_other_count'] = f.loc[:, ~f.columns.str.startswith('from')].sum(axis=1) - f.loc[:, f.columns.str.startswith('_from_')].sum(axis=1)\n",
      "2/106:\n",
      "with zipfile.ZipFile('turf/from_blocks_split.zip') as zf: \n",
      "    from_blocks_split = pickle.load(zf.open('from_blocks_split.pkl'))\n",
      "2/107: from_blocks_split.head()\n",
      "2/108:\n",
      "from_blocks_split = pd.concat([from_blocks_split, shared_from_blocks], sort=False).drop_duplicates()\n",
      "from_blocks_split.tail()\n",
      "2/109: from_blocks_split.count().sort_values(ascending=False).index\n",
      "2/110:\n",
      "def clean_dataframe_column_names(df, prefix='', suffix=''):\n",
      "    cols = df.columns\n",
      "    new_column_names = []\n",
      "\n",
      "    for col in cols:\n",
      "        new_col = prefix+col.lstrip().rstrip().lower().replace (\" \", \"_\").replace (\"-\", \"_\")+suffix #strip beginning spaces, makes lowercase, add underscpre\n",
      "        new_column_names.append(new_col)\n",
      "\n",
      "    df.columns = new_column_names\n",
      "2/111:\n",
      "f = from_blocks_split.fillna(0)\n",
      "clean_dataframe_column_names(f, '_from_')\n",
      "f['from_he_count'] = f.loc[:, f.columns.str.contains('hebrew')].sum(axis=1)\n",
      "f['from_latin_count'] = f.loc[:, f.columns.str.contains('latin')].sum(axis=1)\n",
      "f['from_ar_count'] = f.loc[:, f.columns.str.contains('arabic')].sum(axis=1)\n",
      "f['from_cyr_count'] = f.loc[:, f.columns.str.contains('ayrillic')].sum(axis=1)\n",
      "f['from_other_count'] = f.loc[:, ~f.columns.str.startswith('from')].sum(axis=1) - f.loc[:, f.columns.str.startswith('_from_')].sum(axis=1)\n",
      "2/112: f.columns.str.startswith('from_')\n",
      "2/113: f.loc[:, f.columns.str.startswith('from')].head()\n",
      "2/114: f.loc[f.from_other_count>0, f.columns.str.startswith('from')].head()\n",
      "2/115:\n",
      "with zipfile.ZipFile('turf/from_blocks_split.zip') as zf: \n",
      "    from_blocks_split = pickle.load(zf.open('from_blocks_split.pkl'))\n",
      "2/116: from_blocks_split.head()\n",
      "2/117:\n",
      "from_blocks_split = pd.concat([from_blocks_split, shared_from_blocks], sort=False).drop_duplicates()\n",
      "from_blocks_split.tail()\n",
      "2/118: from_blocks_split.count().sort_values(ascending=False).index\n",
      "2/119:\n",
      "def clean_dataframe_column_names(df, prefix='', suffix='', exclude=None):\n",
      "    cols = df.columns\n",
      "    new_column_names = []\n",
      "\n",
      "    for col in cols:\n",
      "        if not col in exclude:\n",
      "            new_col = prefix+col.lstrip().rstrip().lower().replace (\" \", \"_\").replace (\"-\", \"_\")+suffix #strip beginning spaces, makes lowercase, add underscpre\n",
      "        new_column_names.append(new_col)\n",
      "\n",
      "    df.columns = new_column_names\n",
      "2/120:\n",
      "f = from_blocks_split.fillna(0)\n",
      "clean_dataframe_column_names(f, '_from_')\n",
      "f['from_he_count'] = f.loc[:, f.columns.str.contains('hebrew')].sum(axis=1)\n",
      "f['from_latin_count'] = f.loc[:, f.columns.str.contains('latin')].sum(axis=1)\n",
      "f['from_ar_count'] = f.loc[:, f.columns.str.contains('arabic')].sum(axis=1)\n",
      "f['from_cyr_count'] = f.loc[:, f.columns.str.contains('ayrillic')].sum(axis=1)\n",
      "f['from_other_count'] = f.loc[:, ~f.columns.str.startswith('from')].sum(axis=1) - f.loc[:, f.columns.str.startswith('_from_')].sum(axis=1)\n",
      "2/121:\n",
      "def clean_dataframe_column_names(df, prefix='', suffix='', exclude=[]):\n",
      "    cols = df.columns\n",
      "    new_column_names = []\n",
      "\n",
      "    for col in cols:\n",
      "        if not col in exclude:\n",
      "            new_col = prefix+col.lstrip().rstrip().lower().replace (\" \", \"_\").replace (\"-\", \"_\")+suffix #strip beginning spaces, makes lowercase, add underscpre\n",
      "        new_column_names.append(new_col)\n",
      "\n",
      "    df.columns = new_column_names\n",
      "2/122:\n",
      "f = from_blocks_split.fillna(0)\n",
      "clean_dataframe_column_names(f, '_from_')\n",
      "f['from_he_count'] = f.loc[:, f.columns.str.contains('hebrew')].sum(axis=1)\n",
      "f['from_latin_count'] = f.loc[:, f.columns.str.contains('latin')].sum(axis=1)\n",
      "f['from_ar_count'] = f.loc[:, f.columns.str.contains('arabic')].sum(axis=1)\n",
      "f['from_cyr_count'] = f.loc[:, f.columns.str.contains('ayrillic')].sum(axis=1)\n",
      "f['from_other_count'] = f.loc[:, ~f.columns.str.startswith('from')].sum(axis=1) - f.loc[:, f.columns.str.startswith('_from_')].sum(axis=1)\n",
      "2/123: f.columns.str.startswith('from_')\n",
      "2/124: f.loc[:, f.columns.str.startswith('from')].head()\n",
      "2/125: f.loc[f.from_other_count>0, f.columns.str.startswith('from')].head()\n",
      "2/126:\n",
      "with zipfile.ZipFile('turf/from_blocks_split.zip') as zf: \n",
      "    from_blocks_split = pickle.load(zf.open('from_blocks_split.pkl'))\n",
      "2/127: from_blocks_split.head()\n",
      "2/128:\n",
      "from_blocks_split = pd.concat([from_blocks_split, shared_from_blocks], sort=False).drop_duplicates()\n",
      "from_blocks_split.tail()\n",
      "2/129: from_blocks_split.count().sort_values(ascending=False).index\n",
      "2/130:\n",
      "def clean_dataframe_column_names(df, prefix='', suffix='', exclude=[]):\n",
      "    cols = df.columns\n",
      "    new_column_names = []\n",
      "\n",
      "    for col in cols:\n",
      "        if not col in exclude:\n",
      "            new_col = prefix+col.lstrip().rstrip().lower().replace (\" \", \"_\").replace (\"-\", \"_\")+suffix #strip beginning spaces, makes lowercase, add underscpre\n",
      "        new_column_names.append(new_col)\n",
      "\n",
      "    df.columns = new_column_names\n",
      "2/131:\n",
      "f = from_blocks_split.fillna(0)\n",
      "clean_dataframe_column_names(f, '_from_', exclude=['from'])\n",
      "f['from_he_count'] = f.loc[:, f.columns.str.contains('hebrew')].sum(axis=1)\n",
      "f['from_latin_count'] = f.loc[:, f.columns.str.contains('latin')].sum(axis=1)\n",
      "f['from_ar_count'] = f.loc[:, f.columns.str.contains('arabic')].sum(axis=1)\n",
      "f['from_cyr_count'] = f.loc[:, f.columns.str.contains('ayrillic')].sum(axis=1)\n",
      "f['from_other_count'] = f.loc[:, ~f.columns.str.startswith('from')].sum(axis=1) - f.loc[:, f.columns.str.startswith('_from_')].sum(axis=1)\n",
      "2/132:\n",
      "def clean_dataframe_column_names(df, prefix='', suffix='', exclude=[]):\n",
      "    cols = df.columns\n",
      "    new_column_names = []\n",
      "\n",
      "    for col in cols:\n",
      "        if not col in exclude:\n",
      "            new_col = prefix+col.lstrip().rstrip().lower().replace (\" \", \"_\").replace (\"-\", \"_\")+suffix #strip beginning spaces, makes lowercase, add underscpre\n",
      "        else:\n",
      "            new_col = col\n",
      "        new_column_names.append(new_col)\n",
      "\n",
      "    df.columns = new_column_names\n",
      "2/133:\n",
      "f = from_blocks_split.fillna(0)\n",
      "clean_dataframe_column_names(f, '_from_', exclude=['from'])\n",
      "f['from_he_count'] = f.loc[:, f.columns.str.contains('hebrew')].sum(axis=1)\n",
      "f['from_latin_count'] = f.loc[:, f.columns.str.contains('latin')].sum(axis=1)\n",
      "f['from_ar_count'] = f.loc[:, f.columns.str.contains('arabic')].sum(axis=1)\n",
      "f['from_cyr_count'] = f.loc[:, f.columns.str.contains('ayrillic')].sum(axis=1)\n",
      "f['from_other_count'] = f.loc[:, ~f.columns.str.startswith('from')].sum(axis=1) - f.loc[:, f.columns.str.startswith('_from_')].sum(axis=1)\n",
      "2/134: f.columns.str.startswith('from_')\n",
      "2/135: f.loc[:, f.columns.str.startswith('from')].head()\n",
      "2/136: f.loc[f.from_other_count>0, f.columns.str.startswith('from')].head()\n",
      "2/137: f.head()\n",
      "2/138:\n",
      "f = from_blocks_split.fillna(0)\n",
      "clean_dataframe_column_names(f, '_from_', exclude=['from'])\n",
      "f['from_he_count'] = f.loc[:, f.columns.str.contains('hebrew')].sum(axis=1)\n",
      "f['from_latin_count'] = f.loc[:, f.columns.str.contains('latin')].sum(axis=1)\n",
      "f['from_ar_count'] = f.loc[:, f.columns.str.contains('arabic')].sum(axis=1)\n",
      "f['from_cyr_count'] = f.loc[:, f.columns.str.contains('ayrillic')].sum(axis=1)\n",
      "f['from_other_count'] = f.loc[:, ~f.columns.str.startswith('from')].sum(axis=1) - f.loc[:, f.columns.str.startswith('from_')].sum(axis=1)\n",
      "2/139: f.columns.str.startswith('from_')\n",
      "2/140: f.head()\n",
      "2/141: f.loc[:, f.columns.str.startswith('from')].head()\n",
      "2/142: f.loc[:, f.columns.str.startswith('from')].head()\n",
      "2/143: f.loc[f.from_other_count>0, f.columns.str.startswith('from')].head()\n",
      "2/144: f.loc[f.from_other_count>0, f.columns.str.startswith('from')].head(10)\n",
      "2/145:\n",
      "#save(data_text_blocks_df, 'turf/data_id_text_blocks_df.pkl.gz')\n",
      "data_text_blocks_df = load('turf/data_id_text_blocks_df.pkl.gz')\n",
      "2/146:\n",
      "clean_dataframe_column_names(data_text_blocks_df, 'text_', exclude='id')\n",
      "data_text_blocks_df.head()\n",
      "2/147:\n",
      "#save(data_text_blocks_df, 'turf/data_id_text_blocks_df.pkl.gz')\n",
      "data_text_blocks_df = load('turf/data_id_text_blocks_df.pkl.gz')\n",
      "2/148:\n",
      "clean_dataframe_column_names(data_text_blocks_df, '_text_', exclude='id')\n",
      "data_text_blocks_df.head()\n",
      "2/149:\n",
      "\n",
      "t['text_he_count'] =    t.loc[:,  t.columns.str.contains('hebrew')].sum(axis=1)\n",
      "t['text_latin_count'] = t.loc[:,  t.columns.str.contains('latin')].sum(axis=1)\n",
      "t['text_ar_count'] =    t.loc[:,  t.columns.str.contains('arabic')].sum(axis=1)\n",
      "t['text_cyr_count'] =   t.loc[:,  t.columns.str.contains('ayrillic')].sum(axis=1)\n",
      "t['from_other_count'] = t.loc[:, ~((t.columns.str.startswith('text'))|(t.columns=='id'))].sum(axis=1) - f.loc[:, f.columns.str.startswith('text_')].sum(axis=1)\n",
      "2/150:\n",
      "t = data_text_blocks_df\n",
      "t['text_he_count'] =    t.loc[:,  t.columns.str.contains('hebrew')].sum(axis=1)\n",
      "t['text_latin_count'] = t.loc[:,  t.columns.str.contains('latin')].sum(axis=1)\n",
      "t['text_ar_count'] =    t.loc[:,  t.columns.str.contains('arabic')].sum(axis=1)\n",
      "t['text_cyr_count'] =   t.loc[:,  t.columns.str.contains('ayrillic')].sum(axis=1)\n",
      "t['from_other_count'] = t.loc[:, ~((t.columns.str.startswith('text'))|(t.columns=='id'))].sum(axis=1) - f.loc[:, f.columns.str.startswith('text_')].sum(axis=1)\n",
      "2/151:\n",
      "t = data_text_blocks_df\n",
      "t['text_he_count'] =    t.loc[:,  t.columns.str.contains('hebrew')].sum(axis=1)\n",
      "t['text_latin_count'] = t.loc[:,  t.columns.str.contains('latin')].sum(axis=1)\n",
      "t['text_ar_count'] =    t.loc[:,  t.columns.str.contains('arabic')].sum(axis=1)\n",
      "t['text_cyr_count'] =   t.loc[:,  t.columns.str.contains('cyrillic')].sum(axis=1)\n",
      "t['from_other_count'] = t.loc[:, ~((t.columns.str.startswith('text'))|(t.columns=='id'))].sum(axis=1) - f.loc[:, f.columns.str.startswith('text_')].sum(axis=1)\n",
      "2/152:\n",
      "#save(data_text_blocks_df, 'turf/data_id_text_blocks_df.pkl.gz')\n",
      "data_text_blocks_df = load('turf/data_id_text_blocks_df.pkl.gz')\n",
      "2/153:\n",
      "clean_dataframe_column_names(data_text_blocks_df, '_text_', exclude='id')\n",
      "data_text_blocks_df.head()\n",
      "2/154:\n",
      "t = data_text_blocks_df\n",
      "t['text_he_count'] =    t.loc[:,  t.columns.str.contains('hebrew')].sum(axis=1)\n",
      "t['text_latin_count'] = t.loc[:,  t.columns.str.contains('latin')].sum(axis=1)\n",
      "t['text_ar_count'] =    t.loc[:,  t.columns.str.contains('arabic')].sum(axis=1)\n",
      "t['text_cyr_count'] =   t.loc[:,  t.columns.str.contains('cyrillic')].sum(axis=1)\n",
      "t['from_other_count'] = t.loc[:, ~((t.columns.str.startswith('text'))|(t.columns=='id'))].sum(axis=1) - t.loc[:, t.columns.str.startswith('text_')].sum(axis=1)\n",
      "2/155: t['text_unicode_blocks'] =   t.loc[:,  t.columns.str.startswith('_text')].count(axis=1)\n",
      "2/156:\n",
      "with zipfile.ZipFile('turf/from_blocks_split.zip') as zf: \n",
      "    from_blocks_split = pickle.load(zf.open('from_blocks_split.pkl'))\n",
      "2/157: from_blocks_split.head()\n",
      "2/158:\n",
      "from_blocks_split = pd.concat([from_blocks_split, shared_from_blocks], sort=False).drop_duplicates()\n",
      "from_blocks_split.tail()\n",
      "2/159: from_blocks_split.count().sort_values(ascending=False).index\n",
      "2/160:\n",
      "def clean_dataframe_column_names(df, prefix='', suffix='', exclude=[]):\n",
      "    cols = df.columns\n",
      "    new_column_names = []\n",
      "\n",
      "    for col in cols:\n",
      "        if not col in exclude:\n",
      "            new_col = prefix+col.lstrip().rstrip().lower().replace (\" \", \"_\").replace (\"-\", \"_\")+suffix #strip beginning spaces, makes lowercase, add underscpre\n",
      "        else:\n",
      "            new_col = col\n",
      "        new_column_names.append(new_col)\n",
      "\n",
      "    df.columns = new_column_names\n",
      "2/161:\n",
      "f = from_blocks_split.fillna(0)\n",
      "clean_dataframe_column_names(f, '_from_', exclude=['from'])\n",
      "f['from_he_count'] = f.loc[:, f.columns.str.contains('hebrew')].sum(axis=1)\n",
      "f['from_latin_count'] = f.loc[:, f.columns.str.contains('latin')].sum(axis=1)\n",
      "f['from_ar_count'] = f.loc[:, f.columns.str.contains('arabic')].sum(axis=1)\n",
      "f['from_cyr_count'] = f.loc[:, f.columns.str.contains('cyrillic')].sum(axis=1)\n",
      "f['from_other_count'] = f.loc[:, ~f.columns.str.startswith('from')].sum(axis=1) - f.loc[:, f.columns.str.startswith('from_')].sum(axis=1) \n",
      "f['text_unicode_blocks'] =   f.loc[:,  f.columns.str.startswith('_from')].count(axis=1)\n",
      "2/162: f.columns.str.startswith('from_')\n",
      "2/163: f.loc[:, f.columns.str.startswith('from')].head()\n",
      "2/164:\n",
      "with zipfile.ZipFile('turf/from_blocks_split.zip') as zf: \n",
      "    from_blocks_split = pickle.load(zf.open('from_blocks_split.pkl'))\n",
      "2/165: from_blocks_split.head()\n",
      "2/166:\n",
      "from_blocks_split = pd.concat([from_blocks_split, shared_from_blocks], sort=False).drop_duplicates()\n",
      "from_blocks_split.tail()\n",
      "2/167: from_blocks_split.count().sort_values(ascending=False).index\n",
      "2/168:\n",
      "def clean_dataframe_column_names(df, prefix='', suffix='', exclude=[]):\n",
      "    cols = df.columns\n",
      "    new_column_names = []\n",
      "\n",
      "    for col in cols:\n",
      "        if not col in exclude:\n",
      "            new_col = prefix+col.lstrip().rstrip().lower().replace (\" \", \"_\").replace (\"-\", \"_\")+suffix #strip beginning spaces, makes lowercase, add underscpre\n",
      "        else:\n",
      "            new_col = col\n",
      "        new_column_names.append(new_col)\n",
      "\n",
      "    df.columns = new_column_names\n",
      "2/169:\n",
      "f = from_blocks_split\n",
      "clean_dataframe_column_names(f, '_from_', exclude=['from'])\n",
      "f['from_he_count'] = f.loc[:, f.columns.str.contains('hebrew')].sum(axis=1)\n",
      "f['from_latin_count'] = f.loc[:, f.columns.str.contains('latin')].sum(axis=1)\n",
      "f['from_ar_count'] = f.loc[:, f.columns.str.contains('arabic')].sum(axis=1)\n",
      "f['from_cyr_count'] = f.loc[:, f.columns.str.contains('cyrillic')].sum(axis=1)\n",
      "f['from_other_count'] = f.loc[:, ~f.columns.str.startswith('from')].sum(axis=1) - f.loc[:, f.columns.str.startswith('from_')].sum(axis=1) \n",
      "f['from_unicode_blocks'] =   f.loc[:,  f.columns.str.startswith('_from')].count(axis=1)\n",
      "2/170: f.columns.str.startswith('from_')\n",
      "2/171: f.loc[:, f.columns.str.startswith('from')].head()\n",
      "2/172: f.loc[f.from_other_count>0, f.columns.str.startswith('from')].head(10)\n",
      " 3/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from langdetect import detect\n",
      "import pickle\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('husl', 12)\n",
      "%matplotlib inline\n",
      " 3/2:\n",
      "import glob\n",
      "import zipfile\n",
      "import re\n",
      "import pickle\n",
      " 3/3: folder = 'turf/Facebook Data Backups/'\n",
      " 3/4: files = glob.glob(folder + '*/*.zip')\n",
      " 3/5:\n",
      "names = [re.match(folder + '([^/]+)', f).group(1) for f in files]\n",
      "names\n",
      " 3/6: len(names), len(files)\n",
      " 3/7:\n",
      "data_dfs = []\n",
      "shared_dfs = []\n",
      "for name, f in zip(names, files):\n",
      "    print(f)\n",
      "    with zipfile.ZipFile(f) as zf:\n",
      "        try:\n",
      "            data_df = (pd.read_csv(zf.open('data.csv'), sep='\\t', doublequote=False, lineterminator='\\n'))\n",
      "        except:\n",
      "            print ('******************** BAD ************************')\n",
      "        '''            zf.extract('data.csv')\n",
      "            data_df = (pd.read_csv(open('data.csv', newline=None, encoding='utf8'), sep='\\t', doublequote=False, lineterminator='\\n',\n",
      "                                  #dtype=['object', 'bool', 'datetime64', 'datetime64', 'object', 'int',\n",
      "                                  #       'object', 'int', 'int', 'int', 'int', 'object', 'object', \n",
      "                                  #       'object', 'object']\n",
      "                                  ))'''\n",
      "        data_df = (data_df.assign(name = name)\n",
      "                  .rename(columns={'elapsed_time(minutes)': 'elapsed_time'}))\n",
      "        data_dfs.append(data_df)          \n",
      "        shared_df = (pd.read_csv(zf.open('shared.csv'), sep='\\t', doublequote=False, lineterminator='\\n'))\n",
      "        shared_df = (shared_df.assign(name = name)\n",
      "                  .rename(columns={'elapsed_time(minutes)': 'elapsed_time'}))\n",
      "        shared_dfs.append(shared_df)\n",
      " 3/8:\n",
      "for df in data_dfs:\n",
      "    print (df.shape[0], df.name.unique())\n",
      " 3/9:\n",
      "for df, name in zip(data_dfs, files):\n",
      "    na = df[df.post_id.isna()].shape[0]\n",
      "    if na > 0:\n",
      "        print (na, name)\n",
      "3/10: data_dfs[0].dtypes\n",
      "3/11:\n",
      "df = (pd.concat(data_dfs)\n",
      "     .assign(name = lambda x: pd.Categorical(x.name))\n",
      "     .assign(sentiment = lambda x: pd.Categorical(x.sentiment))\n",
      "     .assign(created_time = lambda x: pd.to_datetime(x.created_time))\n",
      "     .assign(updated_time = lambda x: pd.to_datetime(x.updated_time)))\n",
      "df.head()\n",
      "3/12:\n",
      "sdf = (pd.concat(shared_dfs)\n",
      "     .assign(name = lambda x: pd.Categorical(x.name))\n",
      "     .assign(sentiment = lambda x: pd.Categorical(x.sentiment))\n",
      "     .assign(created_time = lambda x: pd.to_datetime(x.created_time))\n",
      "     .assign(updated_time = lambda x: pd.to_datetime(x.updated_time)))\n",
      "sdf.head()\n",
      "3/13: df.name.value_counts().head()\n",
      "3/14: sdf.name.value_counts().head()\n",
      "3/15: df.dtypes\n",
      "3/16: sdf.dtypes\n",
      "3/17:\n",
      "with zipfile.ZipFile('turf/id_lang.zip') as zf: \n",
      "    df = df.merge(pickle.load(zf.open('id_lang.pkl')), how='left', on='id')\n",
      "3/18:\n",
      "with zipfile.ZipFile('turf/shared_id_lang.zip') as zf: \n",
      "    sdf = sdf.merge(pickle.load(zf.open('shared_id_lang.pkl')), how='left', on='id')\n",
      "3/19: df.langdetect.value_counts().head(20)\n",
      "3/20: df.groupby(['name', 'langdetect']).size().unstack()[['he', 'ar', 'en', 'ru', 'fr', 'de', 'es']].sort_values(by='he', ascending=False).head(20)\n",
      "3/21:\n",
      "shared_from = pd.Series(sdf['from'].unique())\n",
      "shared_from_blocks = (pd.concat([shared_from, shared_from.apply(get_blocks_for_string).apply(pd.Series)], axis = 1)\n",
      "                     .rename(columns={0:'from'}))\n",
      "shared_from_blocks.head()\n",
      "3/22:\n",
      "#save(data_text_blocks_df, 'turf/data_id_text_blocks_df.pkl.gz')\n",
      "data_text_blocks_df = load('turf/data_id_text_blocks_df.pkl.gz')\n",
      "3/23:\n",
      "clean_dataframe_column_names(data_text_blocks_df, '_text_', exclude='id')\n",
      "data_text_blocks_df.head()\n",
      "3/24:\n",
      "import gzip\n",
      "\n",
      "def save(obj, filename):\n",
      "    \"\"\"Save an object to a compressed disk file.\n",
      "       Works well with huge objects.\n",
      "    \"\"\"\n",
      "    with gzip.GzipFile(filename, 'wb') as file:\n",
      "        pickle.dump(obj, file)\n",
      "\n",
      "def load(filename):\n",
      "    \"\"\"Loads a compressed object from disk\n",
      "    \"\"\"\n",
      "    with gzip.GzipFile(filename, 'rb') as file:\n",
      "        obj = pickle.load(file)\n",
      "\n",
      "    return obj\n",
      "3/25:\n",
      "#save(data_text_blocks_df, 'turf/data_id_text_blocks_df.pkl.gz')\n",
      "data_text_blocks_df = load('turf/data_id_text_blocks_df.pkl.gz')\n",
      "3/26:\n",
      "clean_dataframe_column_names(data_text_blocks_df, '_text_', exclude='id')\n",
      "data_text_blocks_df.head()\n",
      "3/27:\n",
      "from __future__ import absolute_import, print_function, unicode_literals\n",
      "\n",
      "import os\n",
      "import re\n",
      "\n",
      "import requests\n",
      "\n",
      "\n",
      "def get_block_for_codepoint(cp):\n",
      "    \"\"\"Return the Unicode block name for the provided numeric codepoint\"\"\"\n",
      "\n",
      "    for start, end, block_name in UNICODE_BLOCKS:\n",
      "        if start <= cp <= end:\n",
      "            return block_name\n",
      "\n",
      "    return 'No_Block'\n",
      "\n",
      "\n",
      "def load_unicode_blocks_from_file(f):\n",
      "    file_contents = f.read().decode('utf-8')\n",
      "\n",
      "    blocks = []\n",
      "    for start, end, block_name in re.findall(r'([0-9A-F]+)\\.\\.([0-9A-F]+);\\ (\\S.*\\S)', file_contents):\n",
      "        if block_name == 'No_Block':\n",
      "            continue\n",
      "\n",
      "        blocks.append((int(start, 16), int(end, 16), block_name))\n",
      "\n",
      "    return blocks\n",
      "\n",
      "\n",
      "def load_unicode_blocks(block_filename):\n",
      "    if not os.path.exists(block_filename):\n",
      "        print('Unicode block file %s does not exist. Downloading…' % block_filename)\n",
      "        r = requests.get('http://unicode.org/Public/UNIDATA/Blocks.txt')\n",
      "        r.raise_for_status()\n",
      "\n",
      "        with open(block_filename, 'wb') as f:\n",
      "            for chunk in r.iter_content():\n",
      "                f.write(chunk)\n",
      "\n",
      "    with open(block_filename, 'rb') as f:\n",
      "        blocks = load_unicode_blocks_from_file(f)\n",
      "\n",
      "    return blocks\n",
      "\n",
      "UNICODE_BLOCKS = load_unicode_blocks('UNIDATA-Blocks.txt')\n",
      "3/28:\n",
      "from collections import Counter\n",
      "def get_blocks_for_string(s):\n",
      "    s = ''.join(e for e in str(s) if e.isalnum())\n",
      "    return Counter([get_block_for_codepoint(ord(c)) for c in str(s)])\n",
      "3/29: len(UNICODE_BLOCKS)\n",
      "3/30:\n",
      "data_from = pd.Series(df['from'].unique())\n",
      "data_from.head()\n",
      "3/31:\n",
      "shared_from = pd.Series(sdf['from'].unique())\n",
      "shared_from_blocks = (pd.concat([shared_from, shared_from.apply(get_blocks_for_string).apply(pd.Series)], axis = 1)\n",
      "                     .rename(columns={0:'from'}))\n",
      "shared_from_blocks.head()\n",
      "3/32:\n",
      "with zipfile.ZipFile('turf/from_blocks_split.zip') as zf: \n",
      "    from_blocks_split = pickle.load(zf.open('from_blocks_split.pkl'))\n",
      "3/33: from_blocks_split.head()\n",
      "3/34:\n",
      "from_blocks_split = pd.concat([from_blocks_split, shared_from_blocks], sort=False).drop_duplicates()\n",
      "from_blocks_split.tail()\n",
      "3/35: from_blocks_split.count().sort_values(ascending=False).index\n",
      "3/36:\n",
      "def clean_dataframe_column_names(df, prefix='', suffix='', exclude=[]):\n",
      "    cols = df.columns\n",
      "    new_column_names = []\n",
      "\n",
      "    for col in cols:\n",
      "        if not col in exclude:\n",
      "            new_col = prefix+col.lstrip().rstrip().lower().replace (\" \", \"_\").replace (\"-\", \"_\")+suffix #strip beginning spaces, makes lowercase, add underscpre\n",
      "        else:\n",
      "            new_col = col\n",
      "        new_column_names.append(new_col)\n",
      "\n",
      "    df.columns = new_column_names\n",
      "3/37:\n",
      "f = from_blocks_split\n",
      "clean_dataframe_column_names(f, '_from_', exclude=['from'])\n",
      "f['from_he_count'] = f.loc[:, f.columns.str.contains('hebrew')].sum(axis=1)\n",
      "f['from_latin_count'] = f.loc[:, f.columns.str.contains('latin')].sum(axis=1)\n",
      "f['from_ar_count'] = f.loc[:, f.columns.str.contains('arabic')].sum(axis=1)\n",
      "f['from_cyr_count'] = f.loc[:, f.columns.str.contains('cyrillic')].sum(axis=1)\n",
      "f['from_other_count'] = f.loc[:, ~f.columns.str.startswith('from')].sum(axis=1) - f.loc[:, f.columns.str.startswith('from_')].sum(axis=1) \n",
      "f['from_unicode_blocks'] =   f.loc[:,  f.columns.str.startswith('_from')].count(axis=1)\n",
      "3/38: f.columns.str.startswith('from_')\n",
      "3/39: f.loc[:, f.columns.str.startswith('from')].head()\n",
      "3/40: f.loc[f.from_other_count>0, f.columns.str.startswith('from')].head(10)\n",
      "3/41:\n",
      "shared_text_blocks = sdf.text.apply(get_blocks_for_string)\n",
      "shared_text_blocks = shared_text_blocks.apply(pd.Series)\n",
      "shared_text_blocks.head()\n",
      "3/42:\n",
      "import gzip\n",
      "\n",
      "def save(obj, filename):\n",
      "    \"\"\"Save an object to a compressed disk file.\n",
      "       Works well with huge objects.\n",
      "    \"\"\"\n",
      "    with gzip.GzipFile(filename, 'wb') as file:\n",
      "        pickle.dump(obj, file)\n",
      "\n",
      "def load(filename):\n",
      "    \"\"\"Loads a compressed object from disk\n",
      "    \"\"\"\n",
      "    with gzip.GzipFile(filename, 'rb') as file:\n",
      "        obj = pickle.load(file)\n",
      "\n",
      "    return obj\n",
      "3/43:\n",
      "with zipfile.ZipFile('turf/from_blocks_split.zip') as zf: \n",
      "    from_blocks_split = pickle.load(zf.open('from_blocks_split.pkl'))\n",
      "3/44: from_blocks_split.head()\n",
      "3/45:\n",
      "from_blocks_split = pd.concat([from_blocks_split, shared_from_blocks], sort=False).drop_duplicates()\n",
      "from_blocks_split.tail()\n",
      "3/46: from_blocks_split.count().sort_values(ascending=False).index\n",
      "3/47:\n",
      "def clean_dataframe_column_names(df, prefix='', suffix='', exclude=[]):\n",
      "    cols = df.columns\n",
      "    new_column_names = []\n",
      "\n",
      "    for col in cols:\n",
      "        if not col in exclude:\n",
      "            new_col = prefix+col.lstrip().rstrip().lower().replace (\" \", \"_\").replace (\"-\", \"_\")+suffix #strip beginning spaces, makes lowercase, add underscpre\n",
      "        else:\n",
      "            new_col = col\n",
      "        new_column_names.append(new_col)\n",
      "\n",
      "    df.columns = new_column_names\n",
      "3/48:\n",
      "f = from_blocks_split\n",
      "clean_dataframe_column_names(f, '_from_', exclude=['from'])\n",
      "f['from_he_count'] = f.loc[:, f.columns.str.contains('hebrew')].sum(axis=1)\n",
      "f['from_latin_count'] = f.loc[:, f.columns.str.contains('latin')].sum(axis=1)\n",
      "f['from_ar_count'] = f.loc[:, f.columns.str.contains('arabic')].sum(axis=1)\n",
      "f['from_cyr_count'] = f.loc[:, f.columns.str.contains('cyrillic')].sum(axis=1)\n",
      "f['from_other_count'] = f.loc[:, ~f.columns.str.startswith('from')].sum(axis=1) - f.loc[:, f.columns.str.startswith('from_')].sum(axis=1).fillna(0) \n",
      "f['from_unicode_blocks'] =   f.loc[:,  f.columns.str.startswith('_from')].count(axis=1)\n",
      "3/49: f.columns.str.startswith('from_')\n",
      "3/50: f.loc[:, f.columns.str.startswith('from')].head()\n",
      "3/51: f.loc[f.from_other_count>0, f.columns.str.startswith('from')].head(10)\n",
      "3/52:\n",
      "shared_text_blocks = sdf.text.apply(get_blocks_for_string)\n",
      "shared_text_blocks = shared_text_blocks.apply(pd.Series)\n",
      "shared_text_blocks.head()\n",
      "3/53:\n",
      "import gzip\n",
      "\n",
      "def save(obj, filename):\n",
      "    \"\"\"Save an object to a compressed disk file.\n",
      "       Works well with huge objects.\n",
      "    \"\"\"\n",
      "    with gzip.GzipFile(filename, 'wb') as file:\n",
      "        pickle.dump(obj, file)\n",
      "\n",
      "def load(filename):\n",
      "    \"\"\"Loads a compressed object from disk\n",
      "    \"\"\"\n",
      "    with gzip.GzipFile(filename, 'rb') as file:\n",
      "        obj = pickle.load(file)\n",
      "\n",
      "    return obj\n",
      "3/54:\n",
      "t = [1, 2, 3, 4]\n",
      "save(t, 'test.gz')\n",
      "load('test.gz')\n",
      "3/55:\n",
      "#save(data_text_blocks_df, 'turf/data_id_text_blocks_df.pkl.gz')\n",
      "data_text_blocks_df = load('turf/data_id_text_blocks_df.pkl.gz')\n",
      "3/56:\n",
      "clean_dataframe_column_names(data_text_blocks_df, '_text_', exclude='id')\n",
      "data_text_blocks_df.head()\n",
      "3/57:\n",
      "t = data_text_blocks_df\n",
      "t['text_he_count'] =    t.loc[:,  t.columns.str.contains('hebrew')].sum(axis=1)\n",
      "t['text_latin_count'] = t.loc[:,  t.columns.str.contains('latin')].sum(axis=1)\n",
      "t['text_ar_count'] =    t.loc[:,  t.columns.str.contains('arabic')].sum(axis=1)\n",
      "t['text_cyr_count'] =   t.loc[:,  t.columns.str.contains('cyrillic')].sum(axis=1)\n",
      "t['text_other_count'] = t.loc[:, ~((t.columns.str.startswith('text'))|(t.columns=='id'))].sum(axis=1) - t.loc[:, t.columns.str.startswith('text_')].sum(axis=1).fillna(0) \n",
      "t['text_unicode_blocks'] =   t.loc[:,  t.columns.str.startswith('_text')].count(axis=1)\n",
      "3/58: t.loc[f.from_other_count>0, f.columns.str.startswith('text')].head(10)\n",
      "3/59: t.loc[:, t.columns.str.startswith('text')].head()\n",
      "3/60: t.loc[t.from_other_count>0, t.columns.str.startswith('text')].head(10)\n",
      "3/61: t.loc[:, t.columns.str.startswith('text')].head()\n",
      "3/62: t.loc[t.text_other_count>0, t.columns.str.startswith('text')].head(10)\n",
      " 4/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from langdetect import detect\n",
      "import pickle\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('husl', 12)\n",
      "%matplotlib inline\n",
      " 4/2:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from langdetect import detect\n",
      "import pickle\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('husl', 12)\n",
      "%matplotlib inline\n",
      " 5/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from langdetect import detect\n",
      "import pickle\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('husl', 12)\n",
      "%matplotlib inline\n",
      " 6/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from langdetect import detect\n",
      "import pickle\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('husl', 12)\n",
      "%matplotlib inline\n",
      " 6/2:\n",
      "import glob\n",
      "import zipfile\n",
      "import re\n",
      "import pickle\n",
      " 6/3: folder = 'turf/Facebook Data Backups/'\n",
      " 6/4: files = glob.glob(folder + '*/*.zip')\n",
      " 6/5:\n",
      "names = [re.match(folder + '([^/]+)', f).group(1) for f in files]\n",
      "names\n",
      " 6/6: len(names), len(files)\n",
      " 6/7:\n",
      "data_dfs = []\n",
      "shared_dfs = []\n",
      "for name, f in zip(names, files):\n",
      "    print(f)\n",
      "    with zipfile.ZipFile(f) as zf:\n",
      "        try:\n",
      "            data_df = (pd.read_csv(zf.open('data.csv'), sep='\\t', doublequote=False, lineterminator='\\n'))\n",
      "        except:\n",
      "            print ('******************** BAD ************************')\n",
      "        '''            zf.extract('data.csv')\n",
      "            data_df = (pd.read_csv(open('data.csv', newline=None, encoding='utf8'), sep='\\t', doublequote=False, lineterminator='\\n',\n",
      "                                  #dtype=['object', 'bool', 'datetime64', 'datetime64', 'object', 'int',\n",
      "                                  #       'object', 'int', 'int', 'int', 'int', 'object', 'object', \n",
      "                                  #       'object', 'object']\n",
      "                                  ))'''\n",
      "        data_df = (data_df.assign(name = name)\n",
      "                  .rename(columns={'elapsed_time(minutes)': 'elapsed_time'}))\n",
      "        data_dfs.append(data_df)          \n",
      "        shared_df = (pd.read_csv(zf.open('shared.csv'), sep='\\t', doublequote=False, lineterminator='\\n'))\n",
      "        shared_df = (shared_df.assign(name = name)\n",
      "                  .rename(columns={'elapsed_time(minutes)': 'elapsed_time'}))\n",
      "        shared_dfs.append(shared_df)\n",
      " 6/8:\n",
      "for df in data_dfs:\n",
      "    print (df.shape[0], df.name.unique())\n",
      " 6/9:\n",
      "for df, name in zip(data_dfs, files):\n",
      "    na = df[df.post_id.isna()].shape[0]\n",
      "    if na > 0:\n",
      "        print (na, name)\n",
      "6/10: data_dfs[0].dtypes\n",
      "6/11:\n",
      "df = (pd.concat(data_dfs)\n",
      "     .assign(name = lambda x: pd.Categorical(x.name))\n",
      "     .assign(sentiment = lambda x: pd.Categorical(x.sentiment))\n",
      "     .assign(created_time = lambda x: pd.to_datetime(x.created_time))\n",
      "     .assign(updated_time = lambda x: pd.to_datetime(x.updated_time)))\n",
      "df.head()\n",
      "6/12:\n",
      "sdf = (pd.concat(shared_dfs)\n",
      "     .assign(name = lambda x: pd.Categorical(x.name))\n",
      "     .assign(sentiment = lambda x: pd.Categorical(x.sentiment))\n",
      "     .assign(created_time = lambda x: pd.to_datetime(x.created_time))\n",
      "     .assign(updated_time = lambda x: pd.to_datetime(x.updated_time)))\n",
      "sdf.head()\n",
      "6/13: data_dfs[0].head()\n",
      "6/14:\n",
      "df = (pd.concat(data_dfs)\n",
      "     .assign(name = lambda x: pd.Categorical(x.name))\n",
      "     .assign(sentiment = lambda x: pd.Categorical(x.sentiment))\n",
      "     .assign(created_time = lambda x: pd.to_datetime(x.created_time, format='%Y-%m-%d %H:%M:%S'))\n",
      "     .assign(updated_time = lambda x: pd.to_datetime(x.updated_time, format='%Y-%m-%d %H:%M:%S')))\n",
      "df.head()\n",
      "6/15:\n",
      "sdf = (pd.concat(shared_dfs)\n",
      "     .assign(name = lambda x: pd.Categorical(x.name))\n",
      "     .assign(sentiment = lambda x: pd.Categorical(x.sentiment))\n",
      "     .assign(created_time = lambda x: pd.to_datetime(x.created_time))\n",
      "     .assign(updated_time = lambda x: pd.to_datetime(x.updated_time)))\n",
      "sdf.head()\n",
      "6/16:\n",
      "sdf = (pd.concat(shared_dfs)\n",
      "     .assign(name = lambda x: pd.Categorical(x.name))\n",
      "     .assign(sentiment = lambda x: pd.Categorical(x.sentiment))\n",
      "     .assign(created_time = lambda x: pd.to_datetime(x.created_time, format='%Y-%m-%d %H:%M:%S'))\n",
      "     .assign(updated_time = lambda x: pd.to_datetime(x.updated_time, format='%Y-%m-%d %H:%M:%S')))\n",
      "sdf.head()\n",
      "6/17: df.name.value_counts().head()\n",
      "6/18: sdf.name.value_counts().head()\n",
      "6/19: df.dtypes\n",
      "6/20: sdf.dtypes\n",
      "6/21:\n",
      "%% time\n",
      "sdf = (pd.concat(shared_dfs)\n",
      "     .assign(name = lambda x: pd.Categorical(x.name))\n",
      "     .assign(sentiment = lambda x: pd.Categorical(x.sentiment))\n",
      "     .assign(created_time = lambda x: pd.to_datetime(x.created_time))\n",
      "     .assign(updated_time = lambda x: pd.to_datetime(x.updated_time)))\n",
      "sdf.head()\n",
      "6/22:\n",
      "%%time\n",
      "sdf = (pd.concat(shared_dfs)\n",
      "     .assign(name = lambda x: pd.Categorical(x.name))\n",
      "     .assign(sentiment = lambda x: pd.Categorical(x.sentiment))\n",
      "     .assign(created_time = lambda x: pd.to_datetime(x.created_time))\n",
      "     .assign(updated_time = lambda x: pd.to_datetime(x.updated_time)))\n",
      "sdf.head()\n",
      "6/23:\n",
      "%%time\n",
      "sdf = (pd.concat(shared_dfs)\n",
      "     .assign(name = lambda x: pd.Categorical(x.name))\n",
      "     .assign(sentiment = lambda x: pd.Categorical(x.sentiment))\n",
      "     .assign(created_time = lambda x: pd.to_datetime(x.created_time))\n",
      "     .assign(updated_time = lambda x: pd.to_datetime(x.updated_time)))\n",
      "sdf.head()\n",
      "6/24:\n",
      "%%time\n",
      "sdf = (pd.concat(shared_dfs)\n",
      "     .assign(name = lambda x: pd.Categorical(x.name))\n",
      "     .assign(sentiment = lambda x: pd.Categorical(x.sentiment))\n",
      "     .assign(created_time = lambda x: pd.to_datetime(x.created_time, format='%Y-%m-%d %H:%M:%S'))\n",
      "     .assign(updated_time = lambda x: pd.to_datetime(x.updated_time, format='%Y-%m-%d %H:%M:%S')))\n",
      "sdf.head()\n",
      "6/25:\n",
      "%%time\n",
      "sdf = (pd.concat(shared_dfs)\n",
      "     .assign(name = lambda x: pd.Categorical(x.name))\n",
      "     .assign(sentiment = lambda x: pd.Categorical(x.sentiment))\n",
      "     .assign(created_time = lambda x: pd.to_datetime(x.created_time, format='%Y-%m-%d %H:%M:%S'))\n",
      "     .assign(updated_time = lambda x: pd.to_datetime(x.updated_time, format='%Y-%m-%d %H:%M:%S')))\n",
      "sdf.head()\n",
      "6/26:\n",
      "%%time\n",
      "sdf = (pd.concat(shared_dfs)\n",
      "     .assign(name = lambda x: pd.Categorical(x.name))\n",
      "     .assign(sentiment = lambda x: pd.Categorical(x.sentiment))\n",
      "     .assign(created_time = lambda x: pd.to_datetime(x.created_time, infer_datetime_format=True))\n",
      "     .assign(updated_time = lambda x: pd.to_datetime(x.updated_time, infer_datetime_format=True)))\n",
      "sdf.head()\n",
      "6/27:\n",
      "%%time\n",
      "sdf = (pd.concat(shared_dfs)\n",
      "     .assign(name = lambda x: pd.Categorical(x.name))\n",
      "     .assign(sentiment = lambda x: pd.Categorical(x.sentiment))\n",
      "     .assign(created_time = lambda x: pd.to_datetime(x.created_time, infer_datetime_format=True))\n",
      "     .assign(updated_time = lambda x: pd.to_datetime(x.updated_time, infer_datetime_format=True)))\n",
      "sdf.head()\n",
      "6/28:\n",
      "%%time\n",
      "df = (pd.concat(data_dfs)\n",
      "     .assign(name = lambda x: pd.Categorical(x.name))\n",
      "     .assign(sentiment = lambda x: pd.Categorical(x.sentiment))\n",
      "     .assign(created_time = lambda x: pd.to_datetime(x.created_time, format='%Y-%m-%d %H:%M:%S'))\n",
      "     .assign(updated_time = lambda x: pd.to_datetime(x.updated_time, format='%Y-%m-%d %H:%M:%S')))\n",
      "df.head()\n",
      "6/29:\n",
      "%%time\n",
      "df = (pd.concat(data_dfs)\n",
      "     .assign(name = lambda x: pd.Categorical(x.name))\n",
      "     .assign(sentiment = lambda x: pd.Categorical(x.sentiment))\n",
      "     .assign(created_time = lambda x: pd.to_datetime(x.created_time))\n",
      "     .assign(updated_time = lambda x: pd.to_datetime(x.updated_time)))\n",
      "df.head()\n",
      "6/30:\n",
      "%%time\n",
      "df = (pd.concat(data_dfs)\n",
      "     .assign(name = lambda x: pd.Categorical(x.name))\n",
      "     .assign(sentiment = lambda x: pd.Categorical(x.sentiment))\n",
      "     .assign(created_time = lambda x: pd.to_datetime(x.created_time, infer_datetime_format=True))\n",
      "     .assign(updated_time = lambda x: pd.to_datetime(x.updated_time, infer_datetime_format=True)))\n",
      "df.head()\n",
      "6/31:\n",
      "%%time\n",
      "df = (pd.concat(data_dfs)\n",
      "     .assign(name = lambda x: pd.Categorical(x.name))\n",
      "     .assign(sentiment = lambda x: pd.Categorical(x.sentiment))\n",
      "     .assign(created_time = lambda x: pd.to_datetime(x.created_time, format='%Y-%m-%d %H:%M:%S'))\n",
      "     .assign(updated_time = lambda x: pd.to_datetime(x.updated_time, format='%Y-%m-%d %H:%M:%S')))\n",
      "df.head()\n",
      "6/32:\n",
      "%%time\n",
      "sdf = (pd.concat(shared_dfs)\n",
      "     .assign(name = lambda x: pd.Categorical(x.name))\n",
      "     .assign(sentiment = lambda x: pd.Categorical(x.sentiment))\n",
      "     .assign(created_time = lambda x: pd.to_datetime(x.created_time, format='%Y-%m-%d %H:%M:%S'))\n",
      "     .assign(updated_time = lambda x: pd.to_datetime(x.updated_time, format='%Y-%m-%d %H:%M:%S')))\n",
      "sdf.head()\n",
      "6/33: df.name.value_counts().head()\n",
      "6/34: sdf.name.value_counts().head()\n",
      "6/35: df.dtypes\n",
      "6/36: sdf.dtypes\n",
      "6/37:\n",
      "with zipfile.ZipFile('turf/id_lang.zip') as zf: \n",
      "    df = df.merge(pickle.load(zf.open('id_lang.pkl')), how='left', on='id')\n",
      "6/38:\n",
      "with zipfile.ZipFile('turf/shared_id_lang.zip') as zf: \n",
      "    sdf = sdf.merge(pickle.load(zf.open('shared_id_lang.pkl')), how='left', on='id')\n",
      "6/39: df.langdetect.value_counts().head(20)\n",
      "6/40: df.groupby(['name', 'langdetect']).size().unstack()[['he', 'ar', 'en', 'ru', 'fr', 'de', 'es']].sort_values(by='he', ascending=False).head(20)\n",
      "6/41:\n",
      "from __future__ import absolute_import, print_function, unicode_literals\n",
      "\n",
      "import os\n",
      "import re\n",
      "\n",
      "import requests\n",
      "\n",
      "\n",
      "def get_block_for_codepoint(cp):\n",
      "    \"\"\"Return the Unicode block name for the provided numeric codepoint\"\"\"\n",
      "\n",
      "    for start, end, block_name in UNICODE_BLOCKS:\n",
      "        if start <= cp <= end:\n",
      "            return block_name\n",
      "\n",
      "    return 'No_Block'\n",
      "\n",
      "\n",
      "def load_unicode_blocks_from_file(f):\n",
      "    file_contents = f.read().decode('utf-8')\n",
      "\n",
      "    blocks = []\n",
      "    for start, end, block_name in re.findall(r'([0-9A-F]+)\\.\\.([0-9A-F]+);\\ (\\S.*\\S)', file_contents):\n",
      "        if block_name == 'No_Block':\n",
      "            continue\n",
      "\n",
      "        blocks.append((int(start, 16), int(end, 16), block_name))\n",
      "\n",
      "    return blocks\n",
      "\n",
      "\n",
      "def load_unicode_blocks(block_filename):\n",
      "    if not os.path.exists(block_filename):\n",
      "        print('Unicode block file %s does not exist. Downloading…' % block_filename)\n",
      "        r = requests.get('http://unicode.org/Public/UNIDATA/Blocks.txt')\n",
      "        r.raise_for_status()\n",
      "\n",
      "        with open(block_filename, 'wb') as f:\n",
      "            for chunk in r.iter_content():\n",
      "                f.write(chunk)\n",
      "\n",
      "    with open(block_filename, 'rb') as f:\n",
      "        blocks = load_unicode_blocks_from_file(f)\n",
      "\n",
      "    return blocks\n",
      "\n",
      "UNICODE_BLOCKS = load_unicode_blocks('UNIDATA-Blocks.txt')\n",
      "6/42:\n",
      "from collections import Counter\n",
      "def get_blocks_for_string(s):\n",
      "    s = ''.join(e for e in str(s) if e.isalnum())\n",
      "    return Counter([get_block_for_codepoint(ord(c)) for c in str(s)])\n",
      "6/43: len(UNICODE_BLOCKS)\n",
      "6/44:\n",
      "shared_from = pd.Series(sdf['from'].unique())\n",
      "shared_from_blocks = (pd.concat([shared_from, shared_from.apply(get_blocks_for_string).apply(pd.Series)], axis = 1)\n",
      "                     .rename(columns={0:'from'}))\n",
      "shared_from_blocks.head()\n",
      "6/45:\n",
      "with zipfile.ZipFile('turf/from_blocks_split.zip') as zf: \n",
      "    from_blocks_split = pickle.load(zf.open('from_blocks_split.pkl'))\n",
      "6/46: from_blocks_split.head()\n",
      "6/47:\n",
      "from_blocks_split = pd.concat([from_blocks_split, shared_from_blocks], sort=False).drop_duplicates()\n",
      "from_blocks_split.tail()\n",
      "6/48: from_blocks_split.count().sort_values(ascending=False).index\n",
      "6/49:\n",
      "def clean_dataframe_column_names(df, prefix='', suffix='', exclude=[]):\n",
      "    cols = df.columns\n",
      "    new_column_names = []\n",
      "\n",
      "    for col in cols:\n",
      "        if not col in exclude:\n",
      "            new_col = prefix+col.lstrip().rstrip().lower().replace (\" \", \"_\").replace (\"-\", \"_\")+suffix #strip beginning spaces, makes lowercase, add underscpre\n",
      "        else:\n",
      "            new_col = col\n",
      "        new_column_names.append(new_col)\n",
      "\n",
      "    df.columns = new_column_names\n",
      "6/50:\n",
      "f = from_blocks_split\n",
      "clean_dataframe_column_names(f, '_from_', exclude=['from'])\n",
      "f['from_he_count'] = f.loc[:, f.columns.str.contains('hebrew')].sum(axis=1)\n",
      "f['from_latin_count'] = f.loc[:, f.columns.str.contains('latin')].sum(axis=1)\n",
      "f['from_ar_count'] = f.loc[:, f.columns.str.contains('arabic')].sum(axis=1)\n",
      "f['from_cyr_count'] = f.loc[:, f.columns.str.contains('cyrillic')].sum(axis=1)\n",
      "f['from_other_count'] = f.loc[:, ~f.columns.str.startswith('from')].sum(axis=1) - f.loc[:, f.columns.str.startswith('from_')].sum(axis=1).fillna(0) \n",
      "f['from_unicode_blocks'] =   f.loc[:,  f.columns.str.startswith('_from')].count(axis=1)\n",
      "6/51: f.columns.str.startswith('from_')\n",
      "6/52: f.loc[:, f.columns.str.startswith('from')].head()\n",
      "6/53: f.columns.str.startswith('from_')\n",
      "6/54: f.loc[:, f.columns.str.startswith('from')].head()\n",
      "6/55: f.loc[f.from_other_count>0, f.columns.str.startswith('from')].head(10)\n",
      "6/56:\n",
      "shared_text_blocks = sdf.text.apply(get_blocks_for_string)\n",
      "shared_text_blocks = shared_text_blocks.apply(pd.Series)\n",
      "shared_text_blocks.head()\n",
      "6/57:\n",
      "import gzip\n",
      "\n",
      "def save(obj, filename):\n",
      "    \"\"\"Save an object to a compressed disk file.\n",
      "       Works well with huge objects.\n",
      "    \"\"\"\n",
      "    with gzip.GzipFile(filename, 'wb') as file:\n",
      "        pickle.dump(obj, file)\n",
      "\n",
      "def load(filename):\n",
      "    \"\"\"Loads a compressed object from disk\n",
      "    \"\"\"\n",
      "    with gzip.GzipFile(filename, 'rb') as file:\n",
      "        obj = pickle.load(file)\n",
      "\n",
      "    return obj\n",
      "6/58:\n",
      "t = [1, 2, 3, 4]\n",
      "save(t, 'test.gz')\n",
      "load('test.gz')\n",
      "6/59:\n",
      "#save(splits, 'turf/data_text_blocks_splits.pkl.gz')\n",
      "#splits = load('turf/data_text_blocks_splits.pkl.gz')\n",
      "6/60:\n",
      "#save(data_text_blocks_df, 'turf/data_id_text_blocks_df.pkl.gz')\n",
      "data_text_blocks_df = load('turf/data_id_text_blocks_df.pkl.gz')\n",
      "6/61:\n",
      "clean_dataframe_column_names(data_text_blocks_df, '_text_', exclude='id')\n",
      "data_text_blocks_df.head()\n",
      "6/62:\n",
      "t = data_text_blocks_df\n",
      "t['text_he_count'] =    t.loc[:,  t.columns.str.contains('hebrew')].sum(axis=1)\n",
      "t['text_latin_count'] = t.loc[:,  t.columns.str.contains('latin')].sum(axis=1)\n",
      "t['text_ar_count'] =    t.loc[:,  t.columns.str.contains('arabic')].sum(axis=1)\n",
      "t['text_cyr_count'] =   t.loc[:,  t.columns.str.contains('cyrillic')].sum(axis=1)\n",
      "t['text_other_count'] = t.loc[:, ~((t.columns.str.startswith('text'))|(t.columns=='id'))].sum(axis=1) - t.loc[:, t.columns.str.startswith('text_')].sum(axis=1).fillna(0) \n",
      "t['text_unicode_blocks'] =   t.loc[:,  t.columns.str.startswith('_text')].count(axis=1)\n",
      "6/63: t.loc[:, t.columns.str.startswith('text')].head()\n",
      "6/64: t.loc[t.text_other_count>0, t.columns.str.startswith('text')].head(10)\n",
      "6/65:\n",
      "shared_text_blocks = sdf.text.apply(get_blocks_for_string)\n",
      "shared_text_blocks = shared_text_blocks.apply(pd.Series)\n",
      "shared_text_blocks = pd.concat([sdf['id'], shared_text_blocks], axis=1)\n",
      "6/66:\n",
      "s = shared_text_blocks\n",
      "s['text_he_count'] =    s.loc[:,  s.columns.str.contains('hebrew')].sum(axis=1)\n",
      "s['text_latin_count'] = s.loc[:,  s.columns.str.contains('latin')].sum(axis=1)\n",
      "s['text_ar_count'] =    s.loc[:,  s.columns.str.contains('arabic')].sum(axis=1)\n",
      "s['text_cyr_count'] =   s.loc[:,  s.columns.str.contains('cyrillic')].sum(axis=1)\n",
      "s['text_other_count'] = s.loc[:, ~((s.columns.str.startswith('text'))|(s.columns=='id'))].sum(axis=1) - s.loc[:, s.columns.str.startswith('text_')].sum(axis=1).fillna(0) \n",
      "s['text_unicode_blocks'] =   s.loc[:,  s.columns.str.startswith('_text')].count(axis=1)\n",
      "6/67: s.loc[:, s.columns.str.startswith('text')].head()\n",
      "6/68:\n",
      "shared_text_blocks = sdf.text.apply(get_blocks_for_string)\n",
      "shared_text_blocks = shared_text_blocks.apply(pd.Series)\n",
      "shared_text_blocks = pd.concat([sdf['id'], shared_text_blocks], axis=1)\n",
      "6/69:\n",
      "clean_dataframe_column_names(shared_text_blocks, '_text_', exclude='id')\n",
      "s = shared_text_blocks\n",
      "s['text_he_count'] =    s.loc[:,  s.columns.str.contains('hebrew')].sum(axis=1)\n",
      "s['text_latin_count'] = s.loc[:,  s.columns.str.contains('latin')].sum(axis=1)\n",
      "s['text_ar_count'] =    s.loc[:,  s.columns.str.contains('arabic')].sum(axis=1)\n",
      "s['text_cyr_count'] =   s.loc[:,  s.columns.str.contains('cyrillic')].sum(axis=1)\n",
      "s['text_other_count'] = s.loc[:, ~((s.columns.str.startswith('text'))|(s.columns=='id'))].sum(axis=1) - s.loc[:, s.columns.str.startswith('text_')].sum(axis=1).fillna(0) \n",
      "s['text_unicode_blocks'] =   s.loc[:,  s.columns.str.startswith('_text')].count(axis=1)\n",
      "6/70: s.loc[:, s.columns.str.startswith('text')].head()\n",
      "6/71: pd.concat([df.head(10, t.head(10).loc[:, t.columns.str.startswith('text')])\n",
      "6/72: pd.concat([df.head(10), t.head(10).loc[:, t.columns.str.startswith('text')]])\n",
      "6/73: pd.concat([df.head(10), t.head(10).loc[:, t.columns.str.startswith('text')]], axis=1)\n",
      "6/74: df.head(10).merge(t.head.loc[:, ((t.columns.str.startswith('text')) | (t.columns=='id'))], on='id', how='left')\n",
      "6/75: df.head(10).merge(t.loc[:, ((t.columns.str.startswith('text')) | (t.columns=='id'))], on='id', how='left')\n",
      "6/76: df = df.merge(t.loc[:, ((t.columns.str.startswith('text')) | (t.columns=='id'))], on='id', how='left')\n",
      "6/77: sdf = sdf.merge(s.loc[:, ((s.columns.str.startswith('text')) | (s.columns=='id'))], on='id', how='left')\n",
      "6/78:\n",
      "df['created_month'] = df['created_time'].dt.to_period('M')\n",
      "df['created_day'] = df['created_time'].dt.to_period('d')\n",
      "df['created_day_of_week'] = pd.Categorical(df['created_time'].dt.strftime('%A'))\n",
      "df['created_hour'] = pd.Categorical(df['created_time'].dt.strftime('%H'))\n",
      "6/79:\n",
      "sdf['created_month'] = sdf['created_time'].dt.to_period('M')\n",
      "sdf['created_day'] =   sdf['created_time'].dt.to_period('d')\n",
      "sdf['created_day_of_week'] = pd.Categorical(sdf['created_time'].dt.strftime('%A'))\n",
      "sdf['created_hour'] = pd.Categorical(sdf['created_time'].dt.strftime('%H'))\n",
      "6/80: df.dtypes\n",
      "6/81: df.to_csv('turf/data_df.csv.gz', compression='gzip')\n",
      "6/82: sdf.to_csv('turf/shared_df.csv.gz', compression='gzip')\n",
      " 7/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pickle\n",
      "import zipfile\n",
      "import re\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('husl', 12)\n",
      "%matplotlib inline\n",
      " 7/2: sdf = pd.from_csv('turf/shared_df.csv.gz', compression='gzip')\n",
      " 7/3: sdf = pd.read_csv('turf/shared_df.csv.gz', compression='gzip')\n",
      " 7/4:\n",
      "sdf = pd.read_csv('turf/shared_df.csv.gz', compression='gzip')\n",
      "sdf.dtypes\n",
      "6/83: sdf.to_csv('turf/shared_df.pkl.gz', compression='gzip')\n",
      " 7/5:\n",
      "sdf = pd.read_pickle('turf/shared_df.pkl.gz', compression='gzip')\n",
      "sdf.dtypes\n",
      "6/84: sdf.to_pickle('turf/shared_df.pkl.gz', compression='gzip')\n",
      " 7/6:\n",
      "sdf = pd.read_pickle('turf/shared_df.pkl.gz', compression='gzip')\n",
      "sdf.dtypes\n",
      "6/85: df.to_pickle('turf/data_df.pkl.gz', compression='gzip')\n",
      " 7/7:\n",
      "%%time\n",
      "df = pd.read_pickle('turf/data_df.pkl.gz', compression='gzip')\n",
      " 7/8: df.dtypes\n",
      " 7/9:\n",
      "mkdet = pd.read_csv('turf/mk_details.csv', sep='\\t')\n",
      "mkdet.head()\n",
      "7/10:\n",
      "#save(st, 'turf/total_stats.pkl.gz')\n",
      "st = pd.read_pickle( 'turf/total_stats.pkl.gz', compression='gzip')\n",
      "7/11: st.head()\n",
      "7/12: st['party'] = st['name'].map(mkdet.set_index('folder_name')['party'])\n",
      "7/13: st['party'] = st.reset_index()['name'].map(mkdet.set_index('folder_name')['party'])\n",
      "7/14:\n",
      "non_uniques = (st.reset_index()\n",
      " .loc[st.reset_index().groupby('from_id').name.transform('nunique')>1, ['from_id', 'name', 'party']])\n",
      "7/15: non_uniques.head()\n",
      "7/16: st.party.head()\n",
      "7/17: st.reset_index()['name'].head()\n",
      "7/18: st.reset_index()['name'].head().map(mkdet.set_index('folder_name')['party'])\n",
      "7/19: st['party'] = pd.concat(st, st.reset_index()['name'].map(mkdet.set_index('folder_name')['party']), axis=1)\n",
      "7/20: st = pd.concat(st, st.reset_index()['name'].map(mkdet.set_index('folder_name')['party']), axis=1)\n",
      "7/21: st = pd.concat([st, st.reset_index()['name'].map(mkdet.set_index('folder_name')['party'])], axis=1)\n",
      "7/22: st['party'] = st.reset_index()['name'].map(mkdet.set_index('folder_name')['party'])\n",
      "7/23: st.head()\n",
      "7/24: st['party', 'party'] = st.reset_index()['name'].map(mkdet.set_index('folder_name')['party'])\n",
      "7/25: st.head()\n",
      "7/26:\n",
      "st = st.reset_index()\n",
      "st['party'] = st['name'].head().map(mkdet.set_index('folder_name')['party'])\n",
      "7/27:\n",
      "non_uniques = (st.reset_index()\n",
      " .loc[st.reset_index().groupby('from_id').name.transform('nunique')>1, ['from_id', 'name', 'party']])\n",
      "7/28: non_uniques.head()\n",
      "7/29:\n",
      "#save(st, 'turf/total_stats.pkl.gz')\n",
      "st = pd.read_pickle( 'turf/total_stats.pkl.gz', compression='gzip')\n",
      "7/30:\n",
      "st = st.reset_index()\n",
      "st['party'] = st['name'].head().map(mkdet.set_index('folder_name')['party'])\n",
      "7/31:\n",
      "non_uniques = (st.reset_index()\n",
      " .loc[st.reset_index().groupby('from_id').name.transform('nunique')>1, ['from_id', 'name', 'party']])\n",
      "7/32: non_uniques.head()\n",
      "7/33: non_uniques.from_id.nunique()\n",
      "7/34: campers = non_uniques[non_uniques.groupby('from_id').party.nunique.transform('nunique')==1]\n",
      "7/35: campers = non_uniques[non_uniques.groupby('from_id').party.transform('nunique')==1]\n",
      "7/36: campers.from_id.nunique()\n",
      "7/37: non_uniques.groupby('from_id').party.transform('nunique')\n",
      "7/38: campers.nunique()\n",
      "7/39: campers = non_uniques[non_uniques.groupby('from_id').party.transform('nunique')==1]\n",
      "7/40: campers.head()\n",
      "7/41: campers.from_id.nunique()\n",
      "7/42: campers\n",
      "7/43: non_uniques.groupby('from_id').party.transform('nunique')==1\n",
      "7/44: non_uniques.groupby('from_id').party.transform('nunique')\n",
      "7/45: non_uniques.tail()\n",
      "7/46:\n",
      "st = st.reset_index()\n",
      "st['party'] = st['name'].map(mkdet.set_index('folder_name')['party'])\n",
      "7/47:\n",
      "#save(st, 'turf/total_stats.pkl.gz')\n",
      "st = pd.read_pickle( 'turf/total_stats.pkl.gz', compression='gzip')\n",
      "7/48:\n",
      "st = st.reset_index()\n",
      "st['party'] = st['name'].map(mkdet.set_index('folder_name')['party'])\n",
      "7/49:\n",
      "non_uniques = (st.reset_index()\n",
      " .loc[st.reset_index().groupby('from_id').name.transform('nunique')>1, ['from_id', 'name', 'party']])\n",
      "7/50: non_uniques.head()\n",
      "7/51: non_uniques.from_id.nunique()\n",
      "7/52: campers = non_uniques[non_uniques.groupby('from_id').party.transform('nunique')==1]\n",
      "7/53: campers.from_id.nunique()\n",
      "7/54: base = non_uniques[non_uniques.groupby('from_id').party.transform('nunique')==1]\n",
      "7/55: base.from_id.nunique()\n",
      "7/56: non_uniques.groupby('from_id').party.transform('nunique')==1\n",
      "7/57: st[['party']].drop_duplicates()\n",
      "7/58: st[['party']].unique()\n",
      "7/59: st[['party']].unique\n",
      "7/60: st.party.unique()\n",
      "7/61: st['camp'] = st['party'].map(camps)\n",
      "7/62:\n",
      "camps = { 'מרצ': 'opposition',\n",
      "          'יש עתיד': 'opposition',\n",
      "          'ליכוד': 'coalition',\n",
      "          'הבית היהודי בראשות נפתלי בנט': 'coalition',\n",
      "          'המחנה הציוני': 'opposition',\n",
      "          'הרשימה המשותפת (חד”ש, רע”מ, בל”ד, תע”ל)': 'opposition',\n",
      "          'כולנו בראשות משה כחלון': 'coalition',\n",
      "          'ישראל ביתנו': 'coalition' }\n",
      "7/63: st['camp'] = st['party'].map(camps)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/64: non_uniques['camp'] = non_uniques['party'].map(camps)\n",
      "7/65: campers = non_uniques[non_uniques.groupby('from_id').camp.transform('nunique')==1]\n",
      "7/66: campers.from_id.nunique()\n",
      "7/67: non_uniques['camp'].value_counts()\n",
      "7/68: non_uniques.camp.value_counts()\n",
      "7/69: non_uniques.party.value_counts()\n",
      "7/70: non_uniques.shape\n",
      "7/71: non_uniques.groupby('camp').from_id.nunique()\n",
      "7/72: non_uniques.groupby('party').from_id.nunique()\n",
      "7/73:\n",
      "(pd.concat([uniques, \n",
      "            uniques/st.reset_index().from_id.nunique(), \n",
      "            uniques/st.reset_index().groupby('name').from_id.nunique()],\n",
      "           axis=1, keys=['uniques', 'ratio_from_all', 'ratio_from_own']).sort_values(by='uniques', ascending=False)\n",
      ".sort_values(by='ratio_from_own', ascending=False)).head(10)\n",
      "7/74:\n",
      "uniques = (st.reset_index()\n",
      " [st.reset_index().groupby('from_id').name.transform('nunique')==1]\n",
      " .name.value_counts())\n",
      "7/75: uniques.head(10)\n",
      "7/76:\n",
      "(pd.concat([uniques, \n",
      "            uniques/st.reset_index().from_id.nunique(), \n",
      "            uniques/st.reset_index().groupby('name').from_id.nunique()],\n",
      "           axis=1, keys=['uniques', 'ratio_from_all', 'ratio_from_own']).sort_values(by='uniques', ascending=False)\n",
      ".sort_values(by='ratio_from_own', ascending=False)).head(10)\n",
      "7/77: campers.from_id.nunique() / non_uniques.from_id.nunique()\n",
      "7/78: base.from_id.nunique() / non_uniques.from_id.nunique()\n",
      "7/79: campers[campers.camp=='coalition'].from_id.nunique()\n",
      "7/80: campers[campers.camp=='opposition'].from_id.nunique()\n",
      "7/81: campers[campers.camp=='coalition'].from_id.nunique() / non_uniques.from_id.nunique()\n",
      "7/82: campers[campers.camp=='coalition'].from_id.nunique() / campers.from_id.nunique()\n",
      "7/83: non_uniques[non_uniques.party.isna()]\n",
      "7/84: non_uniques[non_uniques.party.notna()]\n",
      " 8/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('husl', 12)\n",
      "%matplotlib inline\n",
      " 8/2:\n",
      "%%time\n",
      "df = pd.read_pickle('turf/data_df.pkl.gz', compression='gzip')\n",
      " 8/3: df.dtypes\n",
      " 8/4:\n",
      "sdf = pd.read_pickle('turf/shared_df.pkl.gz', compression='gzip')\n",
      "sdf.dtypes\n",
      " 8/5:\n",
      "stats = ['mean', 'std', 'min', 'max', 'median']\n",
      "\n",
      "st = (df.loc[df.is_post==0, ['from_id', 'name', 'id', 'elapsed_time', 'length', 'like_count', 'post_id', 'parent_id', 'created_month', 'created_day', 'created_day_of_week', 'created_hour']]\n",
      "      #.dropna(subset=['post_id'])\n",
      "      .groupby(by=['from_id', 'name'])\n",
      "      .agg(\n",
      "          {'id': 'count', \n",
      "           'elapsed_time': stats, \n",
      "           'length': stats, \n",
      "           'like_count': stats, \n",
      "           'post_id': 'nunique', \n",
      "           'parent_id': 'nunique',\n",
      "           'created_month': ['nunique'],\n",
      "           'created_day': ['nunique'],\n",
      "           'created_day_of_week': ['nunique'],\n",
      "           'created_hour': ['nunique'],\n",
      "           \n",
      "          })\n",
      "     )\n",
      " 8/6:\n",
      "#save(st, 'turf/total_stats.pkl.gz')\n",
      "st = pd.read_pickle( 'turf/total_stats.pkl.gz', compression='gzip')\n",
      " 8/7: st.head()\n",
      " 8/8: st.shape\n",
      " 8/9:\n",
      "from_id_lang = df.groupby(['from_id', 'langdetect']).size().unstack()[['he', 'ar', 'en', 'ru', 'fr', 'de', 'es']].sort_values(by='he', ascending=False)\n",
      "from_id_lang.head()\n",
      "8/10: from_id_uniblock = df.loc[:,list(top_blocks.head(20).index)+ ['not_in_top20', 'from_id']].groupby('from_id').sum()\n",
      "8/11:\n",
      "top_uniblock = from_id_uniblock.idxmax(axis=1)\n",
      "top_uniblock.head()\n",
      "8/12: st[st['id', 'count']>10].sort_values(by=('elapsed_time', 'std')).head(10)\n",
      "8/13: #pickle.dump(st, open('turf/basic_stats_df.pkl', 'wb'))\n",
      "8/14: st.reset_index().from_id.nunique()\n",
      "8/15:\n",
      "counts = st.reset_index().groupby('from_id').agg({('id', 'count'): 'sum'})['id', 'count']\n",
      "counts[counts==1].shape\n",
      "8/16:\n",
      "st.columns = st.columns.droplevel()\n",
      "st = st.reset_index()\n",
      "8/17: st.head()\n",
      "8/18:\n",
      "mat = st.set_index(['from_id', 'name'])[['count']].unstack()\n",
      "mat.columns = mat.columns.droplevel()\n",
      "mat[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu==1)].shape[0]\n",
      "8/19: mat.count(axis=1)==1\n",
      "8/20:\n",
      "#no good\n",
      "st[(st['count']==1) & (st.name=='Benjamin_Netanyahu')].shape\n",
      "8/21: counts[counts==1].shape[0]/st.reset_index().from_id.nunique()\n",
      "8/22: counts[counts>10].shape[0]/st.reset_index().from_id.nunique()\n",
      "8/23: (st.reset_index().groupby('from_id').name.nunique().value_counts().head(20)/st.reset_index().from_id.nunique()).plot(kind='bar')\n",
      "8/24:\n",
      "uniques = (st.reset_index()\n",
      " [st.reset_index().groupby('from_id').name.transform('nunique')==1]\n",
      " .name.value_counts())\n",
      "8/25: uniques.head(10)\n",
      "8/26:\n",
      "(pd.concat([uniques, \n",
      "            uniques/st.reset_index().from_id.nunique(), \n",
      "            uniques/st.reset_index().groupby('name').from_id.nunique()],\n",
      "           axis=1, keys=['uniques', 'ratio_from_all', 'ratio_from_own']).sort_values(by='uniques', ascending=False)\n",
      ".sort_values(by='ratio_from_own', ascending=False)).head(10)\n",
      "8/27:\n",
      "mkdet = pd.read_csv('turf/mk_details.csv', sep='\\t')\n",
      "mkdet.head()\n",
      "8/28: df['party'] = df['name'].map(mkdet.set_index('folder_name')['party'])\n",
      "8/29: df.groupby(['party', 'langdetect']).size().unstack()\n",
      "8/30:\n",
      "st = st.reset_index()\n",
      "st['party'] = st['name'].map(mkdet.set_index('folder_name')['party'])\n",
      "8/31:\n",
      "non_uniques = (st.reset_index()\n",
      " .loc[st.reset_index().groupby('from_id').name.transform('nunique')>1, ['from_id', 'name', 'party']])\n",
      "8/32: non_uniques.head()\n",
      "8/33: non_uniques.from_id.nunique()\n",
      "8/34: non_uniques.party.value_counts()\n",
      "8/35: non_uniques.groupby('party').from_id.nunique()\n",
      "8/36: base = non_uniques[non_uniques.groupby('from_id').party.transform('nunique')==1]\n",
      "8/37: base.from_id.nunique()\n",
      "8/38: base.from_id.nunique() / non_uniques.from_id.nunique()\n",
      "8/39: st.party.unique()\n",
      "8/40:\n",
      "camps = { 'מרצ': 'opposition',\n",
      "          'יש עתיד': 'opposition',\n",
      "          'ליכוד': 'coalition',\n",
      "          'הבית היהודי בראשות נפתלי בנט': 'coalition',\n",
      "          'המחנה הציוני': 'opposition',\n",
      "          'הרשימה המשותפת (חד”ש, רע”מ, בל”ד, תע”ל)': 'opposition',\n",
      "          'כולנו בראשות משה כחלון': 'coalition',\n",
      "          'ישראל ביתנו': 'coalition' }\n",
      "8/41: st['camp'] = st['party'].map(camps)\n",
      "8/42: non_uniques['camp'] = non_uniques['party'].map(camps)\n",
      "8/43: non_uniques.groupby('camp').from_id.nunique()\n",
      "8/44: campers = non_uniques[non_uniques.groupby('from_id').camp.transform('nunique')==1]\n",
      "8/45: campers.from_id.nunique()\n",
      "8/46: campers.from_id.nunique() / non_uniques.from_id.nunique()\n",
      "8/47: campers[campers.camp=='coalition'].from_id.nunique() / campers.from_id.nunique()\n",
      "8/48: campers[campers.camp=='opposition'].from_id.nunique()\n",
      "8/49: st['id', 'count'].reset_index().groupby('name').sum().sort_values(ascending=False)\n",
      "8/50: st.head()\n",
      "8/51: st.groupby('name').count.sum().sort_values(ascending=False)\n",
      "8/52: st.groupby('name')['count'].sum().sort_values(ascending=False)\n",
      "8/53: st.groupby('name')['count'].sum().sort_values(ascending=False).head(10)\n",
      "8/54: top_10_commented = list(st.groupby('name')['count'].sum().sort_values(ascending=False).head(10).index)\n",
      "8/55:\n",
      "top_10_commented = list(st.groupby('name')['count'].sum().sort_values(ascending=False).head(10).index)\n",
      "top_10_commented\n",
      "8/56: celebz =  non_uniques[non_uniques.groupby('from_id').name.isin(top_10_commented).all()]\n",
      "8/57: celebz =  non_uniques[non_uniques.groupby('from_id').name.transform(isin(top_10_commented).all())]\n",
      "8/58: celebz =  non_uniques[non_uniques.groupby('from_id').name.transform(lambda x: x.isin(top_10_commented).all())]\n",
      "8/59: celebz =  non_uniques[non_uniques.groupby('from_id').name.transform(lambda x: x.isin(top_10_commented).all())]\n",
      "8/60: celebz =  non_uniques.head(1000)[non_uniques.head(1000).groupby('from_id').name.transform(lambda x: x.isin(top_10_commented).all())]\n",
      "8/61: celebz.head\n",
      "8/62: celebz.head()\n",
      "8/63: celebz =  non_uniques[non_uniques.groupby('from_id').name.transform(lambda x: x.isin(top_10_commented).all())]\n",
      "8/64: celebz.head()\n",
      "8/65: celebz.from_id.nunique()\n",
      "8/66: celebz.from_id.nunique() / non_uniques.from_id.nunique()\n",
      "8/67: celebz.from_id.unique() - campers.from_id.unique() - base.from_id.unique()\n",
      "8/68: celebz.from_id.unique().difference(campers.from_id.unique()).difference(base.from_id.unique())\n",
      "8/69: celebz.from_id.drop_duplicates().index.difference(campers.from_id.drop_duplicates().index).difference(base.from_id.drop_duplicates().index)\n",
      "8/70: celebz.from_id.drop_duplicates().values\n",
      "8/71: set(celebz.from_id.drop_duplicates().values).difference(set(campers.from_id.drop_duplicates().values)).difference(set(base.from_id.drop_duplicates().values))\n",
      "8/72: len(set(celebz.from_id.drop_duplicates().values).difference(set(campers.from_id.drop_duplicates().values)).difference(set(base.from_id.drop_duplicates().values)))\n",
      " 9/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('husl', 12)\n",
      "%matplotlib inline\n",
      " 9/2:\n",
      "%%time\n",
      "df = pd.read_pickle('turf/data_df.pkl.gz', compression='gzip')\n",
      " 9/3: df.dtypes\n",
      " 9/4:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('husl', 12)\n",
      "%matplotlib inline\n",
      " 9/5:\n",
      "#save(st, 'turf/total_stats.pkl.gz')\n",
      "st = pd.read_pickle( 'turf/total_stats.pkl.gz', compression='gzip')\n",
      " 9/6: st.head()\n",
      " 9/7: st.shape\n",
      " 9/8:\n",
      "from_id_lang = df.groupby(['from_id', 'langdetect']).size().unstack()[['he', 'ar', 'en', 'ru', 'fr', 'de', 'es']].sort_values(by='he', ascending=False)\n",
      "from_id_lang.head()\n",
      " 9/9:\n",
      "st.columns = st.columns.droplevel()\n",
      "st = st.reset_index()\n",
      "9/10: st.head()\n",
      "9/11:\n",
      "mat = st.set_index(['from_id', 'name'])[['count']].unstack()\n",
      "mat.columns = mat.columns.droplevel()\n",
      "mat[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu==1)].shape[0]\n",
      "9/12: mat.count(axis=1)==1\n",
      "9/13:\n",
      "#no good\n",
      "st[(st['count']==1) & (st.name=='Benjamin_Netanyahu')].shape\n",
      "9/14: counts[counts==1].shape[0]/st.reset_index().from_id.nunique()\n",
      "9/15: st['camp'] = st['party'].map(camps)\n",
      "9/16: non_uniques.groupby('camp').from_id.nunique()\n",
      "9/17: campers = non_uniques[non_uniques.groupby('from_id').camp.transform('nunique')==1]\n",
      "9/18: st[st['id', 'count']>10].sort_values(by=('elapsed_time', 'std')).head(10)\n",
      "9/19: #pickle.dump(st, open('turf/basic_stats_df.pkl', 'wb'))\n",
      "9/20: st.reset_index().from_id.nunique()\n",
      "9/21:\n",
      "counts = st.reset_index().groupby('from_id').agg({('id', 'count'): 'sum'})['id', 'count']\n",
      "counts[counts==1].shape\n",
      "9/22:\n",
      "#save(st, 'turf/total_stats.pkl.gz')\n",
      "st = pd.read_pickle( 'turf/total_stats.pkl.gz', compression='gzip')\n",
      "9/23: st.head()\n",
      "9/24: st[st['id', 'count']>10].sort_values(by=('elapsed_time', 'std')).head(10)\n",
      "9/25: #pickle.dump(st, open('turf/basic_stats_df.pkl', 'wb'))\n",
      "9/26: st.reset_index().from_id.nunique()\n",
      "9/27:\n",
      "counts = st.reset_index().groupby('from_id').agg({('id', 'count'): 'sum'})['id', 'count']\n",
      "counts[counts==1].shape\n",
      "9/28:\n",
      "st.columns = st.columns.droplevel()\n",
      "st = st.reset_index()\n",
      "9/29: st.head()\n",
      "9/30:\n",
      "mat = st.set_index(['from_id', 'name'])[['count']].unstack()\n",
      "mat.columns = mat.columns.droplevel()\n",
      "mat[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu==1)].shape[0]\n",
      "9/31: mat.count(axis=1)==1\n",
      "9/32:\n",
      "#no good\n",
      "st[(st['count']==1) & (st.name=='Benjamin_Netanyahu')].shape\n",
      "9/33: counts[counts==1].shape[0]/st.reset_index().from_id.nunique()\n",
      "9/34: counts[counts>10].shape[0]/st.reset_index().from_id.nunique()\n",
      "9/35: (st.reset_index().groupby('from_id').name.nunique().value_counts().head(20)/st.reset_index().from_id.nunique()).plot(kind='bar')\n",
      "9/36:\n",
      "uniques = (st.reset_index()\n",
      " [st.reset_index().groupby('from_id').name.transform('nunique')==1]\n",
      " .name.value_counts())\n",
      "9/37: uniques.head(10)\n",
      "9/38:\n",
      "(pd.concat([uniques, \n",
      "            uniques/st.reset_index().from_id.nunique(), \n",
      "            uniques/st.reset_index().groupby('name').from_id.nunique()],\n",
      "           axis=1, keys=['uniques', 'ratio_from_all', 'ratio_from_own']).sort_values(by='uniques', ascending=False)\n",
      ".sort_values(by='ratio_from_own', ascending=False)).head(10)\n",
      "9/39:\n",
      "mkdet = pd.read_csv('turf/mk_details.csv', sep='\\t')\n",
      "mkdet.head()\n",
      "9/40: df['party'] = df['name'].map(mkdet.set_index('folder_name')['party'])\n",
      "9/41:\n",
      "st = st.reset_index()\n",
      "st['party'] = st['name'].map(mkdet.set_index('folder_name')['party'])\n",
      "9/42:\n",
      "non_uniques = (st.reset_index()\n",
      " .loc[st.reset_index().groupby('from_id').name.transform('nunique')>1, ['from_id', 'name', 'party']])\n",
      "9/43: non_uniques.head()\n",
      "9/44: non_uniques.from_id.nunique()\n",
      "9/45: non_uniques.party.value_counts()\n",
      "9/46: non_uniques.groupby('party').from_id.nunique()\n",
      "9/47: base = non_uniques[non_uniques.groupby('from_id').party.transform('nunique')==1]\n",
      "9/48: base.from_id.nunique()\n",
      "9/49: base.from_id.nunique() / non_uniques.from_id.nunique()\n",
      "9/50: st.party.unique()\n",
      "9/51:\n",
      "camps = { 'מרצ': 'opposition',\n",
      "          'יש עתיד': 'opposition',\n",
      "          'ליכוד': 'coalition',\n",
      "          'הבית היהודי בראשות נפתלי בנט': 'coalition',\n",
      "          'המחנה הציוני': 'opposition',\n",
      "          'הרשימה המשותפת (חד”ש, רע”מ, בל”ד, תע”ל)': 'opposition',\n",
      "          'כולנו בראשות משה כחלון': 'coalition',\n",
      "          'ישראל ביתנו': 'coalition' }\n",
      "9/52: st['camp'] = st['party'].map(camps)\n",
      "9/53: non_uniques['camp'] = non_uniques['party'].map(camps)\n",
      "9/54: non_uniques.groupby('camp').from_id.nunique()\n",
      "9/55: campers = non_uniques[non_uniques.groupby('from_id').camp.transform('nunique')==1]\n",
      "9/56: campers.from_id.nunique()\n",
      "9/57: campers.from_id.nunique() / non_uniques.from_id.nunique()\n",
      "9/58: campers[campers.camp=='coalition'].from_id.nunique() / campers.from_id.nunique()\n",
      "9/59: campers[campers.camp=='opposition'].from_id.nunique()\n",
      "9/60:\n",
      "top_10_commented = list(st.groupby('name')['count'].sum().sort_values(ascending=False).head(10).index)\n",
      "top_10_commented\n",
      "9/61: celebz =  non_uniques[non_uniques.groupby('from_id').name.transform(lambda x: x.isin(top_10_commented).all())]\n",
      "9/62: celebz.head()\n",
      "9/63: celebz.from_id.nunique()\n",
      "9/64: celebz.from_id.nunique() / non_uniques.from_id.nunique()\n",
      "9/65: celebz.from_id.drop_duplicates().values\n",
      "9/66: len(set(celebz.from_id.drop_duplicates().values).difference(set(campers.from_id.drop_duplicates().values)).difference(set(base.from_id.drop_duplicates().values)))\n",
      "9/67: campers[campers.camp=='opposition'].from_id.nunique() / campers.from_id.nunique()\n",
      "9/68: print ('Total number of commenters: %d' % st.reset_index().from_id.nunique())\n",
      "9/69:\n",
      "stats = ['mean', 'std', 'min', 'max', 'median']\n",
      "\n",
      "st = (df.loc[df.is_post==0, ['from_id', 'name', 'id', 'elapsed_time', 'length', 'like_count', 'post_id', 'parent_id', 'created_month', 'created_day', 'created_day_of_week', 'created_hour']]\n",
      "      #.dropna(subset=['post_id'])\n",
      "      .groupby(by=['from_id', 'name'])\n",
      "      .agg(\n",
      "          {'id': 'count', \n",
      "           'elapsed_time': stats, \n",
      "           'length': stats, \n",
      "           'like_count': stats, \n",
      "           'post_id': 'nunique', \n",
      "           'parent_id': 'nunique',\n",
      "           'created_month': ['nunique'],\n",
      "           'created_day': ['nunique'],\n",
      "           'created_day_of_week': ['nunique'],\n",
      "           'created_hour': ['nunique'],\n",
      "           \n",
      "          })\n",
      "     )\n",
      "9/70:\n",
      "%%time\n",
      "df = pd.read_pickle('turf/data_df.pkl.gz', compression='gzip')\n",
      "10/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('husl', 12)\n",
      "%matplotlib inline\n",
      "10/2:\n",
      "%%time\n",
      "df = pd.read_pickle('turf/data_df.pkl.gz', compression='gzip')\n",
      "10/3: df.dtypes\n",
      "10/4:\n",
      "sdf = pd.read_pickle('turf/shared_df.pkl.gz', compression='gzip')\n",
      "sdf.dtypes\n",
      "10/5:\n",
      "stats = ['mean', 'std', 'min', 'max', 'median']\n",
      "\n",
      "st = (df.loc[df.is_post==0, ['from_id', 'name', 'id', 'elapsed_time', 'length', 'like_count', 'post_id', 'parent_id', 'created_month', 'created_day', 'created_day_of_week', 'created_hour']]\n",
      "      #.dropna(subset=['post_id'])\n",
      "      .groupby(by=['from_id', 'name'])\n",
      "      .agg(\n",
      "          {'id': 'count', \n",
      "           'elapsed_time': stats, \n",
      "           'length': stats, \n",
      "           'like_count': stats, \n",
      "           'post_id': 'nunique', \n",
      "           'parent_id': 'nunique',\n",
      "           'created_month': ['nunique'],\n",
      "           'created_day': ['nunique'],\n",
      "           'created_day_of_week': ['nunique'],\n",
      "           'created_hour': ['nunique'],\n",
      "           \n",
      "          })\n",
      "     )\n",
      "10/6:\n",
      "stats = ['mean', 'std', 'min', 'max', 'median']\n",
      "\n",
      "st = (df.loc[df.is_post==0, ['from_id', 'name', 'id', 'elapsed_time', 'length', 'like_count', 'post_id', 'parent_id', 'created_month', 'created_day', 'created_day_of_week', 'created_hour']]\n",
      "      #.dropna(subset=['post_id'])\n",
      "      .groupby(by=['from_id', 'name'])\n",
      "      .agg(\n",
      "          {'id': 'count', \n",
      "           'elapsed_time': 'mean', \n",
      "           'length': 'mean', \n",
      "           'like_count': 'mean', \n",
      "           'post_id': 'nunique', \n",
      "           'parent_id': 'nunique',\n",
      "           'created_month': 'nunique',\n",
      "           'created_day': 'nunique',\n",
      "           'created_day_of_week': 'nunique',\n",
      "           'created_hour': 'nunique',\n",
      "           \n",
      "          })\n",
      "     )\n",
      "10/7: st.id.sum()\n",
      "10/8: st.head()\n",
      "10/9: st.shape\n",
      "10/10: st.head()\n",
      "10/11: st.shape\n",
      "10/12: st.id.sum()\n",
      "10/13: df[df.is_post==0].shape\n",
      "10/14: print ('Total number of comments: %d' % st.id.sum())\n",
      "10/15: print ('Total number of distinct users (commenters): %d' % st.reset_index().from_id.nunique())\n",
      "10/16: st = st.reset_index()\n",
      "10/17: print ('Total number of distinct users (commenters): %d' % st.from_id.nunique())\n",
      "10/18: save(st, 'turf/total_stats_no_multiindex.pkl.gz')\n",
      "10/19: st.to_pickle('turf/total_stats_no_multiindex.pkl.gz', compression='gzip')\n",
      "10/20:\n",
      "counts = st.groupby('from_id').id.sum()\n",
      "counts[counts==1].shape\n",
      "10/21:\n",
      "mat = st.set_index(['from_id', 'name'])[['count']].unstack()\n",
      "mat.columns = mat.columns.droplevel()\n",
      "mat[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu==1)].shape[0]\n",
      "10/22:\n",
      "mat = st.pivot(index='from_id', columns='name', values='id')\n",
      "mat[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu==1)].shape[0]\n",
      "10/23: mat.count(axis=1)==1\n",
      "10/24:\n",
      "#no good\n",
      "st[(st['count']==1) & (st.name=='Benjamin_Netanyahu')].shape\n",
      "10/25: mat[(mat.count(axis=1)==1)].shape[0]\n",
      "10/26:\n",
      "counts = st.groupby('from_id').id.sum()\n",
      "counts[counts==1].shape\n",
      "10/27: counts.head()\n",
      "10/28:\n",
      "counts = st.groupby('from_id').id.sum()\n",
      "print ('Total number of one timers: %d' % counts[counts==1].shape[0])\n",
      "10/29:\n",
      "counts = st.groupby('from_id').id.sum()\n",
      "print ('Total number of one timers: %d' % counts[counts==1].shape[0])\n",
      "print ('Ratio of one timers from all commenters: %d' % counts[counts==1].shape[0] / st.from_id.nunique())\n",
      "10/30:\n",
      "counts = st.groupby('from_id').id.sum()\n",
      "print ('Total number of one timers: %d' % counts[counts==1].shape[0])\n",
      "print ('Ratio of one timers from all commenters: %d' % (counts[counts==1].shape[0] / st.from_id.nunique()))\n",
      "10/31:\n",
      "counts = st.groupby('from_id').id.sum()\n",
      "print ('Total number of one timers: %d' % counts[counts==1].shape[0])\n",
      "print ('Ratio of one timers from all commenters: %.2f' % (counts[counts==1].shape[0] / st.from_id.nunique()))\n",
      "10/32:\n",
      "print ('Ratio of Bibi from one timers: %.2f' % (mat[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu==1)].shape[0] \n",
      " / counts[counts==1].shape[0]))\n",
      "10/33:\n",
      "counts = st.groupby('from_id').id.sum()\n",
      "print ('Total number of one timers: %d' % counts[counts==1].shape[0])\n",
      "print ('Ratio of one timers from all users: %.2f' % (counts[counts==1].shape[0] / st.from_id.nunique()))\n",
      "10/34: print ('Ratio of one timers from all users: %.2f' % (counts[counts>10].shape[0]/st.from_id.nunique()))\n",
      "10/35: print ('Ratio of users that commented 10 times and up: %.2f' % (counts[counts>=10].shape[0]/st.from_id.nunique()))\n",
      "10/36:\n",
      "uniques = (st\n",
      " [st.groupby('from_id').name.transform('nunique')==1]\n",
      " .name.value_counts())\n",
      "10/37: uniques.head(10)\n",
      "10/38:\n",
      "print ('Total number of uniques: %d' % uniques.sum())\n",
      "print ('Ratio of one timers from all users: %.2f' % (counts[counts==1].shape[0] / st.from_id.nunique()))\n",
      "10/39:\n",
      "print ('Ratio of Bibi from one-timers: %.2f' % (mat[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu==1)].shape[0] \n",
      " / counts[counts==1].shape[0]))\n",
      "10/40:\n",
      "counts = st.groupby('from_id').id.sum()\n",
      "print ('Total number of one-timers: %d' % counts[counts==1].shape[0])\n",
      "print ('Ratio of one timers from all users: %.2f' % (counts[counts==1].shape[0] / st.from_id.nunique()))\n",
      "10/41:\n",
      "print ('Total number of uniques: %d' % uniques.sum())\n",
      "print ('Ratio of uniques from all users: %.2f' % (uniques.sum() / st.from_id.nunique()))\n",
      "print ('Ratio of uniques from one-timers: %.2f' % (uniques.sum() / counts[counts==1].shape[0]))\n",
      "10/42:\n",
      "print ('Total number of uniques: %d' % uniques.sum())\n",
      "print ('Ratio of uniques from all users: %.2f' % (uniques.sum() / st.from_id.nunique()))\n",
      "print ('Ratio of one-timers from uniques: %.2f' % (counts[counts==1].shape[0] / uniques.sum()))\n",
      "10/43:\n",
      "(pd.concat([uniques, \n",
      "            uniques/st.from_id.nunique(), \n",
      "            uniques/st.groupby('name').from_id.nunique()],\n",
      "           axis=1, keys=['uniques', 'ratio_from_all', 'ratio_from_own']).sort_values(by='uniques', ascending=False)\n",
      ".sort_values(by='ratio_from_own', ascending=False)).head(10)\n",
      "10/44:\n",
      "(pd.concat([uniques, \n",
      "            uniques/st.from_id.nunique(), \n",
      "            uniques/st.groupby('name').from_id.nunique()],\n",
      "           axis=1, keys=['num_uniques', 'ratio_from_all_users', 'ratio_from_own_users']).sort_values(by='uniques', ascending=False)\n",
      ".sort_values(by='ratio_from_own', ascending=False)).head(10)\n",
      "10/45:\n",
      "(pd.concat([uniques, \n",
      "            uniques/st.from_id.nunique(), \n",
      "            uniques/st.groupby('name').from_id.nunique()],\n",
      "           axis=1, keys=['num_uniques', 'ratio_from_all_users', 'ratio_from_own_users']).sort_values(by='num_uniques', ascending=False)\n",
      ".sort_values(by='ratio_from_own_users', ascending=False)).head(10)\n",
      "10/46:\n",
      "(pd.concat([uniques, \n",
      "            uniques/st.from_id.nunique(), \n",
      "            uniques/st.groupby('name').from_id.nunique()],\n",
      "           axis=1, keys=['num_uniques', 'ratio_from_all_users', 'ratio_from_own_users'])\n",
      " .sort_values(by='ratio_from_own_users', ascending=False)).head(10)\n",
      "10/47:\n",
      "mkdet = pd.read_csv('turf/mk_details.csv', sep='\\t')\n",
      "mkdet.head()\n",
      "10/48: df['party'] = df['name'].map(mkdet.set_index('folder_name')['party'])\n",
      "10/49: df.groupby(['party', 'langdetect']).size().unstack()\n",
      "10/50: st['party'] = st['name'].map(mkdet.set_index('folder_name')['party'])\n",
      "10/51:\n",
      "non_uniques = (st.reset_index()\n",
      " .loc[st.reset_index().groupby('from_id').name.transform('nunique')>1, ['from_id', 'name', 'party']])\n",
      "10/52: non_uniques.head()\n",
      "10/53:\n",
      "non_uniques = (st.reset_index()\n",
      " .loc[st.groupby('from_id').name.transform('nunique')>1, ['from_id', 'name', 'party']])\n",
      "10/54: non_uniques.head()\n",
      "10/55: non_uniques.from_id.nunique()\n",
      "10/56: non_uniques.party.value_counts()\n",
      "10/57: non_uniques.groupby('party').from_id.nunique()\n",
      "10/58: base = non_uniques[non_uniques.groupby('from_id').party.transform('nunique')==1]\n",
      "10/59: base = non_uniques[non_uniques.groupby('from_id').party.transform('nunique')==1]\n",
      "10/60:\n",
      "print ('Total number of basers: %d' % base.from_id.nunique())\n",
      "print ('Ratio of basers from all users: %.2f' % (base.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of basers from non-uniques: %.2f' % (base.from_id.nunique() / non_uniques.from_id.nunique()))\n",
      "10/61: st.party.unique()\n",
      "10/62:\n",
      "camps = { 'מרצ': 'opposition',\n",
      "          'יש עתיד': 'opposition',\n",
      "          'ליכוד': 'coalition',\n",
      "          'הבית היהודי בראשות נפתלי בנט': 'coalition',\n",
      "          'המחנה הציוני': 'opposition',\n",
      "          'הרשימה המשותפת (חד”ש, רע”מ, בל”ד, תע”ל)': 'opposition',\n",
      "          'כולנו בראשות משה כחלון': 'coalition',\n",
      "          'ישראל ביתנו': 'coalition' }\n",
      "10/63: st['camp'] = st['party'].map(camps)\n",
      "10/64: non_uniques['camp'] = non_uniques['party'].map(camps)\n",
      "10/65: non_uniques.groupby('camp').from_id.nunique()\n",
      "10/66: campers = non_uniques[non_uniques.groupby('from_id').camp.transform('nunique')==1]\n",
      "10/67: campers.from_id.nunique()\n",
      "10/68: campers.from_id.nunique() / non_uniques.from_id.nunique()\n",
      "10/69: campers[campers.camp=='coalition'].from_id.nunique() / campers.from_id.nunique()\n",
      "10/70: campers[campers.camp=='opposition'].from_id.nunique() / campers.from_id.nunique()\n",
      "10/71:\n",
      "print ('Total number of campers: %d' % campers.from_id.nunique())\n",
      "print ('Ratio of campers from all users: %.2f' % (campers.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of campers from non-uniques: %.2f' % (campers.from_id.nunique() / non_uniques.from_id.nunique()))\n",
      "print ('Coalition / Opposition campers: %.2f' % (campers.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of campers from non-uniques: %.2f / %.2f' % \n",
      "       (campers[campers.camp=='coalition'].from_id.nunique() / campers.from_id.nunique())\n",
      "       (campers[campers.camp=='opposition'].from_id.nunique() / campers.from_id.nunique()))\n",
      "10/72:\n",
      "print ('Total number of campers: %d' % campers.from_id.nunique())\n",
      "print ('Ratio of campers from all users: %.2f' % (campers.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of campers from non-uniques: %.2f' % (campers.from_id.nunique() / non_uniques.from_id.nunique()))\n",
      "print ('Coalition / Opposition campers: %.2f' % (campers.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of campers from non-uniques: %.2f / %.2f' % \n",
      "       (campers[campers.camp=='coalition'].from_id.nunique() / campers.from_id.nunique()),\n",
      "       (campers[campers.camp=='opposition'].from_id.nunique() / campers.from_id.nunique()))\n",
      "10/73:\n",
      "print ('Total number of campers: %d' % campers.from_id.nunique())\n",
      "print ('Ratio of campers from all users: %.2f' % (campers.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of campers from non-uniques: %.2f' % (campers.from_id.nunique() / non_uniques.from_id.nunique()))\n",
      "print ('Coalition / Opposition campers: %.2f' % (campers.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of campers from non-uniques: %.2f / %.2f' % \n",
      "       ((campers[campers.camp=='coalition'].from_id.nunique() / campers.from_id.nunique()),\n",
      "       (campers[campers.camp=='opposition'].from_id.nunique() / campers.from_id.nunique())))\n",
      "10/74:\n",
      "print ('Total number of campers: %d' % campers.from_id.nunique())\n",
      "print ('Ratio of campers from all users: %.2f' % (campers.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of campers from non-uniques: %.2f' % (campers.from_id.nunique() / non_uniques.from_id.nunique()))\n",
      "print ('Coalition / Opposition campers: %.2f / %.2f' % \n",
      "       ((campers[campers.camp=='coalition'].from_id.nunique() / campers.from_id.nunique()),\n",
      "       (campers[campers.camp=='opposition'].from_id.nunique() / campers.from_id.nunique())))\n",
      "10/75:\n",
      "print ('Total number of campers: %d' % campers.from_id.nunique())\n",
      "print ('Ratio of campers from all users: %.2f' % (campers.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of campers from non-uniques: %.2f' % (campers.from_id.nunique() / non_uniques.from_id.nunique()))\n",
      "print ('Coalition / Opposition campers: %.2f / %.2f' % \n",
      "       ((campers[campers.camp=='coalition'].from_id.nunique() / campers.from_id.nunique()),\n",
      "       (campers[campers.camp=='opposition'].from_id.nunique() / campers.from_id.nunique())))\n",
      "cg = st.groupby('camp')\n",
      "print ('Coalition / Opposition total users: %.2f / %.2f' % \n",
      "       ((cg.from_id.nunique()['coalition'] / st.from_id.nunique()),\n",
      "       ((cg.from_id.nunique()['opposition'] / st.from_id.nunique())))\n",
      "\n",
      "print ('Coalition / Opposition total comments: %.2f / %.2f' % \n",
      "       ((st.groupby(.from_id.nunique() / campers.from_id.nunique()),\n",
      "       (campers[campers.camp=='opposition'].from_id.nunique() / campers.from_id.nunique())))\n",
      "10/76:\n",
      "print ('Total number of campers: %d' % campers.from_id.nunique())\n",
      "print ('Ratio of campers from all users: %.2f' % (campers.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of campers from non-uniques: %.2f' % (campers.from_id.nunique() / non_uniques.from_id.nunique()))\n",
      "print ('Coalition / Opposition campers: %.2f / %.2f' % \n",
      "       ((campers[campers.camp=='coalition'].from_id.nunique() / campers.from_id.nunique()),\n",
      "       (campers[campers.camp=='opposition'].from_id.nunique() / campers.from_id.nunique())))\n",
      "cg = st.groupby('camp')\n",
      "print ('Coalition / Opposition total users: %.2f / %.2f' % \n",
      "       ((cg.from_id.nunique()['coalition'] / st.from_id.nunique()),\n",
      "       ((cg.from_id.nunique()['opposition'] / st.from_id.nunique()))))\n",
      "\n",
      "#print ('Coalition / Opposition total comments: %.2f / %.2f' %\n",
      "10/77:\n",
      "print ('Total number of campers: %d' % campers.from_id.nunique())\n",
      "print ('Ratio of campers from all users: %.2f' % (campers.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of campers from non-uniques: %.2f' % (campers.from_id.nunique() / non_uniques.from_id.nunique()))\n",
      "print ('Coalition | Opposition ratio from campers campers: %.2f | %.2f' % \n",
      "       ((campers[campers.camp=='coalition'].from_id.nunique() / campers.from_id.nunique()),\n",
      "       (campers[campers.camp=='opposition'].from_id.nunique() / campers.from_id.nunique())))\n",
      "cg = st.groupby('camp')\n",
      "print ('Coalition / Opposition from total users total users: %.2f | %.2f' % \n",
      "       ((cg.from_id.nunique()['coalition'] / st.from_id.nunique()),\n",
      "       ((cg.from_id.nunique()['opposition'] / st.from_id.nunique()))))\n",
      "\n",
      "#print ('Coalition / Opposition total comments: %.2f / %.2f' %\n",
      "10/78:\n",
      "print ('Total number of campers: %d' % campers.from_id.nunique())\n",
      "print ('Ratio of campers from all users: %.2f' % (campers.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of campers from non-uniques: %.2f' % (campers.from_id.nunique() / non_uniques.from_id.nunique()))\n",
      "print ('Coalition | Opposition ratio from campers campers: %.2f | %.2f' % \n",
      "       ((campers[campers.camp=='coalition'].from_id.nunique() / campers.from_id.nunique()),\n",
      "       (campers[campers.camp=='opposition'].from_id.nunique() / campers.from_id.nunique())))\n",
      "cg = st.groupby('camp')\n",
      "print ('Coalition | Opposition ratios from total users total users: %.2f | %.2f' % \n",
      "       ((cg.from_id.nunique()['coalition'] / st.from_id.nunique()),\n",
      "       ((cg.from_id.nunique()['opposition'] / st.from_id.nunique()))))\n",
      "\n",
      "print ('Coalition | Opposition total comments: %.2f | %.2f' % \n",
      "       ((cg.id.sum()['coalition'] / st.id.sum()),\n",
      "       ((cg.id.sum()['opposition'] / st.id.sum()))))\n",
      "10/79:\n",
      "top_10_commented = list(st.groupby('name')['count'].sum().sort_values(ascending=False).head(10).index)\n",
      "top_10_commented\n",
      "10/80:\n",
      "top_10_commented = list(st.groupby('name')['id'].sum().sort_values(ascending=False).head(10).index)\n",
      "top_10_commented\n",
      "10/81:\n",
      "top_10_commented = list(st.groupby('name').id.sum().sort_values(ascending=False).head(10).index)\n",
      "top_10_commented\n",
      "10/82: celebz =  non_uniques[non_uniques.groupby('from_id').id.transform(lambda x: x.isin(top_10_commented).all())]\n",
      "10/83: celebz =  non_uniques[non_uniques.groupby('from_id').name.transform(lambda x: x.isin(top_10_commented).all())]\n",
      "10/84: celebz.head()\n",
      "10/85: num_celebiez_no_base_camp = len(set(celebz.from_id.drop_duplicates().values).difference(set(campers.from_id.drop_duplicates().values)).difference(set(base.from_id.drop_duplicates().values)))\n",
      "10/86:\n",
      "print ('Total number of celeb followers (celebies): %d' % celebz.from_id.nunique())\n",
      "print ('Ratio of celebies from all users: %.2f' % (celebz.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of celebies from non-uniques: %.2f' % (celebz.from_id.nunique() / non_uniques.from_id.nunique()))\n",
      "print ('Remove campers and base from celebies: %d' % num_celebiez_no_base_camp)\n",
      "10/87: top_commented = st.groupby('name').id.sum().sort_values(ascending=False)\n",
      "10/88:\n",
      "top_10_commented = list(top_commented.head(10).index)\n",
      "top_10_commented\n",
      "10/89:\n",
      "top_10_commented = top_commented[:10]\n",
      "top_10_commented\n",
      "10/90: top_commented = list(st.groupby('name').id.sum().sort_values(ascending=False).head(10).index)\n",
      "10/91:\n",
      "top_10_commented = top_commented[:10]\n",
      "top_10_commented\n",
      "10/92: unique_users = (st.loc[st.groupby('from_id').name.transform('nunique')==1, ['from_id', 'name', 'party']])\n",
      "10/93: uniques\n",
      "10/94: uniques.reindex(top_commented).plot()\n",
      "10/95: uniques.reindex(top_commented).plot(kind='bar')\n",
      "10/96: top_commented\n",
      "10/97: top_commented = list(st.groupby('name').id.sum().sort_values(ascending=False).index)\n",
      "10/98:\n",
      "top_10_commented = top_commented[:10]\n",
      "top_10_commented\n",
      "10/99: uniques.reindex(top_commented).plot(kind='bar')\n",
      "10/100: uniques.reindex(top_commented).plot(kind='bar').drop('Benjamin_Netanyahu')\n",
      "10/101: uniques.reindex(top_commented).drop('Benjamin_Netanyahu').plot(kind='bar')\n",
      "10/102: uniques.reindex(top_commented).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10))\n",
      "10/103: uniques.reindex(top_commented).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=45)\n",
      "10/104: uniques.reindex(top_commented).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=135)\n",
      "10/105: uniques.reindex(top_commented).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=60)\n",
      "10/106: uniques.reindex(top_commented).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=30)\n",
      "10/107: uniques.reindex(top_commented).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "10/108: users_accum = set()\n",
      "10/109: users_accum\n",
      "10/110: mat.loc[:, top_commented[:1]].notna().all()\n",
      "10/111: mat.loc[:, top_commented[:1]].notna().count()\n",
      "10/112: mat.loc[:, top_commented[:1]].count()\n",
      "10/113: mat.loc[:, top_commented[1:]].isna().all()\n",
      "10/114: mat.loc[:, top_commented[1:]].isna().all(axis=1)\n",
      "10/115:\n",
      "only_bibi = mat.loc[:, top_commented[1:]].isna().all(axis=1)\n",
      "only_bibi[only_bibi].shape\n",
      "10/116:\n",
      "onlys = []\n",
      "\n",
      "for i, name in enumerate(top_commented):\n",
      "    only = mat.loc[:, top_commented[i+1:]].isna().all(axis=1)\n",
      "    onlys.append(only[only].shape[0])\n",
      "    \n",
      "onlys\n",
      "10/117: user_accum_df = pd.DataFrame({'mk': top_commented, 'user_accum': onlys})\n",
      "10/118:\n",
      "user_accum_df = pd.DataFrame({'mk': top_commented, 'user_accum': onlys})\n",
      "user_accum_df.head()\n",
      "10/119: user_accum_df.plot(kind='bar')\n",
      "10/120: user_accum_df.set_index('mk').plot(kind='bar')\n",
      "10/121: user_accum_df.set_index('mk').user_accum.plot(kind='bar')\n",
      "10/122: user_accum_df.set_index('mk').user_accum.plot(kind='bar', figsize=(15,10), rot=75)\n",
      "10/123:\n",
      "import altair as alt\n",
      "\n",
      "alt.Chart(user_accum_df).mark_bar().encode(\n",
      "    x = 'mk',\n",
      "    y = 'user_accum',\n",
      "    color = 'party',\n",
      "    tooltip = 'mk'\n",
      ").interactive()\n",
      "10/124:\n",
      "import altair as alt\n",
      "\n",
      "alt.Chart(user_accum_df).mark_bar().encode(\n",
      "    x = 'mk',\n",
      "    y = 'user_accum',\n",
      "    color = 'party',\n",
      "    tooltip = 'mk'\n",
      ").interactive()\n",
      "10/125: user_accum_df['party'] = user_accum_df['mk'].map(mkdet.set_index('folder_name')['party'])\n",
      "10/126:\n",
      "import altair as alt\n",
      "\n",
      "alt.Chart(user_accum_df).mark_bar().encode(\n",
      "    x = 'mk',\n",
      "    y = 'user_accum',\n",
      "    color = 'party',\n",
      "    tooltip = 'mk'\n",
      ").interactive()\n",
      "10/127:\n",
      "import altair as alt\n",
      "alt.renderers.enable('notebook')\n",
      "\n",
      "alt.Chart(user_accum_df).mark_bar().encode(\n",
      "    x = 'mk',\n",
      "    y = 'user_accum',\n",
      "    color = 'party',\n",
      "    tooltip = 'mk'\n",
      ").interactive()\n",
      "10/128:\n",
      "import altair as alt\n",
      "alt.renderers.enable('notebook')\n",
      "\n",
      "alt.Chart(user_accum_df).mark_bar().encode(\n",
      "    x = 'mk',\n",
      "    y = 'user_accum',\n",
      "    color = 'party',\n",
      "    tooltip = 'mk'\n",
      ").interactive()\n",
      "10/129:\n",
      "import altair as alt\n",
      "alt.renderers.enable('notebook')\n",
      "\n",
      "alt.Chart(user_accum_df).mark_bar().encode(\n",
      "    x = alt.X('mk', order=top_commented),\n",
      "    y = 'user_accum',\n",
      "    color = 'party',\n",
      "    tooltip = 'mk'\n",
      ").interactive()\n",
      "10/130:\n",
      "import altair as alt\n",
      "alt.renderers.enable('notebook')\n",
      "\n",
      "alt.Chart(user_accum_df).mark_bar().encode(\n",
      "    x = alt.X('mk', order=alt.Order(top_commented)),\n",
      "    y = 'user_accum',\n",
      "    color = 'party',\n",
      "    tooltip = 'mk'\n",
      ").interactive()\n",
      "10/131:\n",
      "import altair as alt\n",
      "alt.renderers.enable('notebook')\n",
      "\n",
      "alt.Chart(user_accum_df).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = 'user_accum',\n",
      "    color = 'party',\n",
      "    tooltip = 'mk'\n",
      ").interactive()\n",
      "10/132:\n",
      "user_accum_df['party'] = user_accum_df['mk'].map(mkdet.set_index('folder_name')['party'])\n",
      "user_accum_df['camp'] = user_accum_df['party'].map(camps)\n",
      "10/133:\n",
      "import altair as alt\n",
      "alt.renderers.enable('notebook')\n",
      "\n",
      "alt.Chart(user_accum_df).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = 'user_accum',\n",
      "    color = 'camp',\n",
      "    tooltip = 'mk'\n",
      ").interactive()\n",
      "10/134: user_accum_df.reset_index()\n",
      "10/135:\n",
      "alt.Chart(user_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = 'user_accum',\n",
      "    color = 'camp',\n",
      "    tooltip = alt.Tooltip(['mk', 'index'])\n",
      ").interactive()\n",
      "10/136:\n",
      "alt.Chart(user_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = 'user_accum',\n",
      "    color = 'camp',\n",
      "    tooltip = ['mk', 'index']\n",
      ").interactive()\n",
      "10/137:\n",
      "alt.Chart(user_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = alt.Y('user_accum', log=True),\n",
      "    color = 'camp',\n",
      "    tooltip = ['mk', 'index']\n",
      ").interactive()\n",
      "10/138:\n",
      "alt.Chart(user_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = alt.Y('user_accum', scale='log'),\n",
      "    color = 'camp',\n",
      "    tooltip = ['mk', 'index']\n",
      ").interactive()\n",
      "10/139:\n",
      "alt.Chart(user_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = alt.Y('user_accum', scale=alt.Scale(exponent=2)),\n",
      "    color = 'camp',\n",
      "    tooltip = ['mk', 'index']\n",
      ").interactive()\n",
      "10/140:\n",
      "alt.Chart(user_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = alt.Y('user_accum', scale=alt.Scale(exponent=5)),\n",
      "    color = 'camp',\n",
      "    tooltip = ['mk', 'index']\n",
      ").interactive()\n",
      "10/141:\n",
      "alt.Chart(user_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = alt.Y('user_accum', scale=alt.Scale(type='pow', exponent=5)),\n",
      "    color = 'camp',\n",
      "    tooltip = ['mk', 'index']\n",
      ").interactive()\n",
      "10/142:\n",
      "alt.Chart(user_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = alt.Y('user_accum', scale=alt.Scale(type='pow', exponent=1)),\n",
      "    color = 'camp',\n",
      "    tooltip = ['mk', 'index']\n",
      ").interactive()\n",
      "10/143:\n",
      "alt.Chart(user_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = alt.Y('user_accum', scale=alt.Scale(type='pow', exponent=2)),\n",
      "    color = 'camp',\n",
      "    tooltip = ['mk', 'index']\n",
      ").interactive()\n",
      "10/144:\n",
      "alt.Chart(user_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = alt.Y('user_accum', scale=alt.Scale(type='pow', exponent=0.5)),\n",
      "    color = 'camp',\n",
      "    tooltip = ['mk', 'index']\n",
      ").interactive()\n",
      "10/145:\n",
      "alt.Chart(user_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = alt.Y('user_accum', scale=alt.Scale(type='log', base=10)),\n",
      "    color = 'camp',\n",
      "    tooltip = ['mk', 'index']\n",
      ").interactive()\n",
      "10/146:\n",
      "alt.Chart(user_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = alt.Y('user_accum', scale=alt.Scale(type='log', base=2)),\n",
      "    color = 'camp',\n",
      "    tooltip = ['mk', 'index']\n",
      ").interactive()\n",
      "10/147:\n",
      "alt.Chart(user_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = alt.Y('user_accum', scale=alt.Scale(type='log', base=10)),\n",
      "    color = 'camp',\n",
      "    tooltip = ['mk', 'index']\n",
      ").interactive()\n",
      "10/148:\n",
      "alt.Chart(user_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = alt.Y('user_accum', scale=alt.Scale(type='pow', base=2)),\n",
      "    color = 'camp',\n",
      "    tooltip = ['mk', 'index']\n",
      ").interactive()\n",
      "10/149:\n",
      "alt.Chart(user_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = alt.Y('user_accum', scale=alt.Scale(type='pow', exponent=2)),\n",
      "    color = 'camp',\n",
      "    tooltip = ['mk', 'index']\n",
      ").interactive()\n",
      "10/150:\n",
      "alt.Chart(user_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = alt.Y('user_accum', scale=alt.Scale(type='pow', exponent=0.1)),\n",
      "    color = 'camp',\n",
      "    tooltip = ['mk', 'index']\n",
      ").interactive()\n",
      "10/151:\n",
      "alt.Chart(user_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = alt.Y('user_accum', scale=alt.Scale(type='pow', exponent=10)),\n",
      "    color = 'camp',\n",
      "    tooltip = ['mk', 'index']\n",
      ").interactive()\n",
      "10/152:\n",
      "alt.Chart(user_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = alt.Y('user_accum', scale=alt.Scale(type='sqrt', exponent=10)),\n",
      "    color = 'camp',\n",
      "    tooltip = ['mk', 'index']\n",
      ").interactive()\n",
      "10/153:\n",
      "alt.Chart(user_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = alt.Y('user_accum', scale=alt.Scale(domain=(2e5, 8e5))),\n",
      "    color = 'camp',\n",
      "    tooltip = ['mk', 'index']\n",
      ").interactive()\n",
      "10/154:\n",
      "alt.Chart(user_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = alt.Y('user_accum', scale=alt.Scale(domain=(2e5, 8e5), type='sqrt')),\n",
      "    color = 'camp',\n",
      "    tooltip = ['mk', 'index']\n",
      ").interactive()\n",
      "10/155:\n",
      "alt.Chart(user_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = alt.Y('user_accum',)),\n",
      "    color = 'camp',\n",
      "    tooltip = ['mk', 'index']\n",
      ").interactive()\n",
      "10/156:\n",
      "alt.Chart(user_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = alt.Y('user_accum'),\n",
      "    color = 'camp',\n",
      "    tooltip = ['mk', 'index']\n",
      ").interactive()\n",
      "10/157:\n",
      "alt.Chart(user_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = alt.Y('user_accum'),\n",
      "    color = 'camp',\n",
      "    tooltip = ['mk', 'index']\n",
      ").properties(height=800, width=600).interactive()\n",
      "10/158:\n",
      "alt.Chart(user_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = alt.Y('user_accum'),\n",
      "    color = 'camp',\n",
      "    tooltip = ['mk', 'index']\n",
      ").properties(height=600, width=800).interactive()\n",
      "10/159:\n",
      "alt.Chart(user_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = alt.Y('user_accum', scale=alt.Scale(domain=(0, 8e5))),\n",
      "    color = 'camp',\n",
      "    tooltip = ['mk', 'index']\n",
      ").properties(height=600, width=800).interactive()\n",
      "10/160:\n",
      "alt.Chart(user_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = alt.Y('user_accum',),\n",
      "    color = 'camp',\n",
      "    tooltip = ['mk', 'index']\n",
      ").properties(height=600, width=800).interactive()\n",
      "10/161:\n",
      "alt.Chart(user_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = alt.Y('user_accum',),\n",
      "    color = 'camp',\n",
      "    tooltip = ['mk', 'index']\n",
      ").properties(height=600, width=800).interactive(bind_y=False)\n",
      "10/162:\n",
      "alt.Chart(user_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = alt.Y('user_accum',),\n",
      "    color = 'camp',\n",
      "    tooltip = ['mk', 'index']\n",
      ").properties(height=600, width=800)\n",
      "10/163:\n",
      "alt.Chart(user_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = alt.Y('user_accum',),\n",
      "    color = 'camp',\n",
      "    tooltip = ['index', 'mk', 'party']\n",
      ").properties(height=600, width=800)\n",
      "10/164: user_accum_df['delta'] = user_accum_df.user_accum.delta()\n",
      "10/165: user_accum_df['delta'] = user_accum_df.user_accum.diff()\n",
      "10/166:\n",
      "alt.Chart(user_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = alt.Y('delta',),\n",
      "    color = 'camp',\n",
      "    tooltip = ['index', 'mk', 'party']\n",
      ").properties(height=600, width=800)\n",
      "10/167:\n",
      "alt.Chart(user_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = alt.Y('user_accum',),\n",
      "    color = 'camp',\n",
      "    tooltip = ['index', 'mk', 'party', 'user_accum']\n",
      ").properties(height=600, width=800)\n",
      "10/168: mat[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu==1)]\n",
      "10/169: mat.loc[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu==1), 'Benjamin_Netanyahu']\n",
      "10/170: mat.loc[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu==1), 'Benjamin_Netanyahu'].shape[0]\n",
      "12/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('husl', 12)\n",
      "%matplotlib inline\n",
      "12/2:\n",
      "%%time\n",
      "df = pd.read_pickle('turf/data_df.pkl.gz', compression='gzip')\n",
      "12/3: df.dtypes\n",
      "12/4:\n",
      "sdf = pd.read_pickle('turf/shared_df.pkl.gz', compression='gzip')\n",
      "sdf.dtypes\n",
      "12/5: st = read_pickle('turf/total_stats_no_multiindex.pkl.gz', compression='gzip')\n",
      "12/6: st = pd.read_pickle('turf/total_stats_no_multiindex.pkl.gz', compression='gzip')\n",
      "12/7:\n",
      "# OLD STUFF\n",
      "#save(st, 'turf/total_stats.pkl.gz')\n",
      "#st = pd.read_pickle( 'turf/total_stats.pkl.gz', compression='gzip')\n",
      "12/8: st.head()\n",
      "12/9: st.shape\n",
      "12/10: df[df.is_post==0].shape\n",
      "12/11: print ('Total number of comments: %d' % st.id.sum())\n",
      "12/12: st[st['id', 'count']>10].sort_values(by=('elapsed_time', 'std')).head(10)\n",
      "12/13:\n",
      "print ('Total number of celeb followers (celebies): %d' % celebz.from_id.nunique())\n",
      "print ('Ratio of celebies from all users: %.2f' % (celebz.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of celebies from non-uniques: %.2f' % (celebz.from_id.nunique() / non_uniques.from_id.nunique()))\n",
      "print ('Remove campers and base from celebies: %d' % num_celebiez_no_base_camp)\n",
      "12/14: top_commented\n",
      "12/15: uniques.reindex(top_commented).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "12/16: print ('Total number of distinct users (commenters): %d' % st.from_id.nunique())\n",
      "12/17:\n",
      "counts = st.groupby('from_id').id.sum()\n",
      "print ('Total number of one-timers: %d' % counts[counts==1].shape[0])\n",
      "print ('Ratio of one timers from all users: %.2f' % (counts[counts==1].shape[0] / st.from_id.nunique()))\n",
      "12/18: mat = st.pivot(index='from_id', columns='name', values='id')\n",
      "12/19: mat[(mat.count(axis=1)==1)].shape[0]\n",
      "12/20: mat.loc[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu==1), 'Benjamin_Netanyahu'].shape[0]\n",
      "12/21:\n",
      "print ('Ratio of Bibi from one-timers: %.2f' % (mat[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu==1)].shape[0] \n",
      " / counts[counts==1].shape[0]))\n",
      "12/22: print ('Ratio of users that commented 10 times and up: %.2f' % (counts[counts>=10].shape[0]/st.from_id.nunique()))\n",
      "12/23: (st.reset_index().groupby('from_id').name.nunique().value_counts().head(20)/st.reset_index().from_id.nunique()).plot(kind='bar')\n",
      "12/24:\n",
      "uniques = (st\n",
      " [st.groupby('from_id').name.transform('nunique')==1]\n",
      " .name.value_counts())\n",
      "12/25: uniques.head(10)\n",
      "12/26:\n",
      "print ('Total number of uniques: %d' % uniques.sum())\n",
      "print ('Ratio of uniques from all users: %.2f' % (uniques.sum() / st.from_id.nunique()))\n",
      "print ('Ratio of one-timers from uniques: %.2f' % (counts[counts==1].shape[0] / uniques.sum()))\n",
      "12/27:\n",
      "(pd.concat([uniques, \n",
      "            uniques/st.from_id.nunique(), \n",
      "            uniques/st.groupby('name').from_id.nunique()],\n",
      "           axis=1, keys=['num_uniques', 'ratio_from_all_users', 'ratio_from_own_users'])\n",
      " .sort_values(by='ratio_from_own_users', ascending=False)).head(10)\n",
      "12/28:\n",
      "mkdet = pd.read_csv('turf/mk_details.csv', sep='\\t')\n",
      "mkdet.head()\n",
      "12/29: df['party'] = df['name'].map(mkdet.set_index('folder_name')['party'])\n",
      "12/30: df.groupby(['party', 'langdetect']).size().unstack()\n",
      "12/31: st['party'] = st['name'].map(mkdet.set_index('folder_name')['party'])\n",
      "12/32:\n",
      "non_uniques = (st.reset_index()\n",
      " .loc[st.groupby('from_id').name.transform('nunique')>1, ['from_id', 'name', 'party']])\n",
      "12/33: non_uniques.head()\n",
      "12/34: non_uniques.from_id.nunique()\n",
      "12/35: non_uniques.party.value_counts()\n",
      "12/36: non_uniques.groupby('party').from_id.nunique()\n",
      "12/37: base = non_uniques[non_uniques.groupby('from_id').party.transform('nunique')==1]\n",
      "12/38:\n",
      "print ('Total number of basers: %d' % base.from_id.nunique())\n",
      "print ('Ratio of basers from all users: %.2f' % (base.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of basers from non-uniques: %.2f' % (base.from_id.nunique() / non_uniques.from_id.nunique()))\n",
      "12/39: st.party.unique()\n",
      "12/40:\n",
      "camps = { 'מרצ': 'opposition',\n",
      "          'יש עתיד': 'opposition',\n",
      "          'ליכוד': 'coalition',\n",
      "          'הבית היהודי בראשות נפתלי בנט': 'coalition',\n",
      "          'המחנה הציוני': 'opposition',\n",
      "          'הרשימה המשותפת (חד”ש, רע”מ, בל”ד, תע”ל)': 'opposition',\n",
      "          'כולנו בראשות משה כחלון': 'coalition',\n",
      "          'ישראל ביתנו': 'coalition' }\n",
      "12/41: st['camp'] = st['party'].map(camps)\n",
      "12/42: non_uniques['camp'] = non_uniques['party'].map(camps)\n",
      "12/43: non_uniques.groupby('camp').from_id.nunique()\n",
      "12/44: campers = non_uniques[non_uniques.groupby('from_id').camp.transform('nunique')==1]\n",
      "12/45:\n",
      "print ('Total number of campers: %d' % campers.from_id.nunique())\n",
      "print ('Ratio of campers from all users: %.2f' % (campers.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of campers from non-uniques: %.2f' % (campers.from_id.nunique() / non_uniques.from_id.nunique()))\n",
      "print ('Coalition | Opposition ratio from campers: %.2f | %.2f' % \n",
      "       ((campers[campers.camp=='coalition'].from_id.nunique() / campers.from_id.nunique()),\n",
      "       (campers[campers.camp=='opposition'].from_id.nunique() / campers.from_id.nunique())))\n",
      "cg = st.groupby('camp')\n",
      "print ('Coalition | Opposition ratios from total users: %.2f | %.2f' % \n",
      "       ((cg.from_id.nunique()['coalition'] / st.from_id.nunique()),\n",
      "       ((cg.from_id.nunique()['opposition'] / st.from_id.nunique()))))\n",
      "\n",
      "print ('Coalition | Opposition total comments: %.2f | %.2f' % \n",
      "       ((cg.id.sum()['coalition'] / st.id.sum()),\n",
      "       ((cg.id.sum()['opposition'] / st.id.sum()))))\n",
      "12/46: top_commented = list(st.groupby('name').id.sum().sort_values(ascending=False).index)\n",
      "12/47:\n",
      "top_10_commented = top_commented[:10]\n",
      "top_10_commented\n",
      "12/48: celebz =  non_uniques[non_uniques.groupby('from_id').name.transform(lambda x: x.isin(top_10_commented).all())]\n",
      "12/49: celebz.head()\n",
      "12/50: num_celebiez_no_base_camp = len(set(celebz.from_id.drop_duplicates().values).difference(set(campers.from_id.drop_duplicates().values)).difference(set(base.from_id.drop_duplicates().values)))\n",
      "12/51:\n",
      "print ('Total number of celeb followers (celebies): %d' % celebz.from_id.nunique())\n",
      "print ('Ratio of celebies from all users: %.2f' % (celebz.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of celebies from non-uniques: %.2f' % (celebz.from_id.nunique() / non_uniques.from_id.nunique()))\n",
      "print ('Remove campers and base from celebies: %d' % num_celebiez_no_base_camp)\n",
      "12/52: top_commented\n",
      "12/53: uniques.reindex(top_commented).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "12/54: mat.loc[:, top_commented[:1]].notna\n",
      "12/55:\n",
      "only_bibi = mat.loc[:, top_commented[1:]].isna().all(axis=1)\n",
      "only_bibi[only_bibi].shape\n",
      "12/56:\n",
      "onlys = []\n",
      "\n",
      "for i, name in enumerate(top_commented):\n",
      "    only = mat.loc[:, top_commented[i+1:]].isna().all(axis=1)\n",
      "    onlys.append(only[only].shape[0])\n",
      "    \n",
      "onlys\n",
      "12/57:\n",
      "user_accum_df = pd.DataFrame({'mk': top_commented, 'user_accum': onlys})\n",
      "user_accum_df.head()\n",
      "12/58: user_accum_df.set_index('mk').user_accum.plot(kind='bar', figsize=(15,10), rot=75)\n",
      "12/59:\n",
      "user_accum_df['party'] = user_accum_df['mk'].map(mkdet.set_index('folder_name')['party'])\n",
      "user_accum_df['camp'] = user_accum_df['party'].map(camps)\n",
      "12/60:\n",
      "import altair as alt\n",
      "alt.renderers.enable('notebook')\n",
      "12/61:\n",
      "alt.Chart(user_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = alt.Y('user_accum',),\n",
      "    color = 'camp',\n",
      "    tooltip = ['index', 'mk', 'party', 'user_accum']\n",
      ").properties(height=600, width=800)\n",
      "12/62: user_accum_df['delta'] = user_accum_df.user_accum.diff()\n",
      "12/63:\n",
      "alt.Chart(user_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = alt.Y('delta',),\n",
      "    color = 'camp',\n",
      "    tooltip = ['index', 'mk', 'party']\n",
      ").properties(height=600, width=800)\n",
      "12/64: (user_accum_df - uniques.reindex(top_commented)).drop('Benjamin_Netanyahu')\n",
      "12/65: user_accum_df\n",
      "12/66: (user_accum_df.delta - uniques.reindex(top_commented)).drop('Benjamin_Netanyahu')\n",
      "12/67: uniques.reindex(top_commented)\n",
      "12/68: user_accum_df.delta\n",
      "12/69: (user_accum_df.set_index('mk)'.delta - uniques.reindex(top_commented)).drop('Benjamin_Netanyahu')\n",
      "12/70: (user_accum_df.set_index('mk').delta - uniques.reindex(top_commented)).drop('Benjamin_Netanyahu')\n",
      "12/71: (user_accum_df.set_index('mk').delta / uniques.reindex(top_commented)).drop('Benjamin_Netanyahu')\n",
      "12/72: (user_accum_df.set_index('mk').delta / uniques.reindex(top_commented)).drop('Benjamin_Netanyahu')\n",
      "12/73: (user_accum_df.set_index('mk').delta / uniques.reindex(top_commented)).drop('Benjamin_Netanyahu')(kind='bar', figsize=(15,10), rot=75)\n",
      "12/74: (user_accum_df.set_index('mk').delta / uniques.reindex(top_commented)).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "12/75: (user_accum_df.set_index('mk').delta - uniques.reindex(top_commented)).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "12/76: user_accum_df['delta_unique'] = user_accum_df['mk'].map(delta_unique)\n",
      "12/77:\n",
      "delta_unique = (user_accum_df.set_index('mk').delta - uniques.reindex(top_commented)).drop('Benjamin_Netanyahu')\n",
      "delta_unique.plot(kind='bar', figsize=(15,10), rot=75)\n",
      "12/78: user_accum_df['delta_unique'] = user_accum_df['mk'].map(delta_unique)\n",
      "12/79:\n",
      "alt.Chart(user_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X(['mk', 'delta_unique'], sort=top_commented),\n",
      "    y = alt.Y(['delta',]),\n",
      "    color = 'camp',\n",
      "    tooltip = ['index', 'mk', 'party']\n",
      ").properties(height=600, width=800)\n",
      "12/80: user_accum_df[['delta', 'delta_unique']].plot(kind='bar')\n",
      "12/81: user_accum_df[['delta', 'delta_unique']].plot(kind='bar', figsize=(15,10), rot=75)\n",
      "12/82:\n",
      "user_accum_df['delta_unique'] = user_accum_df['mk'].map(delta_unique)\n",
      "user_accum_df['delta_unique_n'] = user_accum_df['delta_unique'].max()\n",
      "user_accum_df['delta_n'] = user_accum_df['delta'].max()\n",
      "12/83: user_accum_df[['delta_n', 'delta_unique_n']].plot(kind='bar', figsize=(15,10), rot=75)\n",
      "12/84:\n",
      "user_accum_df['delta_unique'] = user_accum_df['mk'].map(delta_unique)\n",
      "user_accum_df['delta_unique_n'] = user_accum_df['delta_unique'] / user_accum_df['delta_unique'].max()\n",
      "user_accum_df['delta_n'] = user_accum_df['delta'] / user_accum_df['delta'].max()\n",
      "12/85: user_accum_df[['delta_n', 'delta_unique_n']].plot(kind='bar', figsize=(15,10), rot=75)\n",
      "12/86: (user_accum_df.delta_n / user_accum_df.delta_unique_n).plot(kind='bar', figsize=(15,10), rot=75)\n",
      "12/87: ( user_accum_df.delta_unique_n / user_accum_df.delta_n).plot(kind='bar', figsize=(15,10), rot=75)\n",
      "12/88:\n",
      "unique_delta = (uniques.reindex(top_commented) / user_accum_df.set_index('mk').delta).drop('Benjamin_Netanyahu')\n",
      "unique_delta.plot(kind='bar', figsize=(15,10), rot=75)\n",
      "12/89:\n",
      "user_accum_df['unique_delta'] = user_accum_df['mk'].map(unique_delta)\n",
      "user_accum_df['delta_n'] = user_accum_df['delta'] / user_accum_df['delta'].max()\n",
      "12/90: user_accum_df[['delta_n', 'unique_delta']].plot(kind='bar', figsize=(15,10), rot=75)\n",
      "12/91: ( user_accum_df.unique_delta / user_accum_df.delta_n).plot(kind='bar', figsize=(15,10), rot=75)\n",
      "12/92: user_accum_df[['delta_n', 'unique_delta']].plot(kind='bar', stacked=True, figsize=(15,10), rot=75)\n",
      "12/93: user_accum_df[['delta_n', 'unique_delta']].div(p_table.sum(axis=1), axis=0).plot(kind='bar', stacked=True, figsize=(15,10), rot=75)\n",
      "12/94: user_accum_df[['delta_n', 'unique_delta']].div(user_accum_df[['delta_n', 'unique_delta']].sum(axis=1), axis=0).plot(kind='bar', stacked=True, figsize=(15,10), rot=75)\n",
      "12/95:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "12/96:\n",
      "unique_delta = (uniques.reindex(top_commented) / user_accum_df.set_index('mk').delta).drop('Benjamin_Netanyahu')\n",
      "unique_delta.plot(kind='bar', figsize=(15,10), rot=75)\n",
      "12/97: user_accum_df[['delta_n', 'unique_delta']].div(user_accum_df[['delta_n', 'unique_delta']].sum(axis=1), axis=0).plot(kind='bar', stacked=True, figsize=(15,10), rot=75)\n",
      "12/98:\n",
      "unique_delta_ratio = (uniques.reindex(top_commented) / user_accum_df.set_index('mk').delta).drop('Benjamin_Netanyahu')\n",
      "unique_delta_ratio.plot(kind='bar', figsize=(15,10), rot=75)\n",
      "12/99:\n",
      "user_accum_df['unique_delta_ratio'] = user_accum_df['mk'].map(unique_delta_ratio)\n",
      "user_accum_df['delta_n'] = user_accum_df['delta'] / user_accum_df['delta'].max()\n",
      "12/100: user_accum_df[['delta_n', 'unique_delta_ratio']].div(user_accum_df[['delta_n', 'unique_delta_ratio']].sum(axis=1), axis=0).plot(kind='bar', stacked=True, figsize=(15,10), rot=75)\n",
      "12/101: user_accum_df[['delta_n', 'unique_delta_ratio']].div(user_accum_df[['delta_n', 'unique_delta_ratio']].sum(axis=1), axis=0).plot(kind='bar', stacked=True, figsize=(15,10), rot=75, width=1.0)\n",
      "12/102: user_accum_df[['delta_n', 'unique_delta_ratio']].div(user_accum_df[['delta_n', 'unique_delta_ratio']].sum(axis=1), axis=0).plot(kind='bar', stacked=True, figsize=(15,10), rot=75, width=1.0)\n",
      "12/103: unique_delta_ratio.reset_index()\n",
      "12/104: unique_delta_ratio = (uniques.reindex(top_commented) / user_accum_df.set_index('mk').delta).drop('Benjamin_Netanyahu')\n",
      "12/105:\n",
      "unique_delta_ratio = (uniques.reindex(top_commented) / user_accum_df.set_index('mk').delta).drop('Benjamin_Netanyahu')\n",
      "user_accum_df['unique_delta_ratio'] = user_accum_df['mk'].map(unique_delta_ratio)\n",
      "\n",
      "alt.Chart(user_accum_df).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = alt.Y('unique_delta_ratio',),\n",
      "    color = 'camp',\n",
      "    tooltip = ['index', 'mk', 'party']\n",
      ").properties(height=600, width=800)\n",
      "12/106:\n",
      "unique_delta_ratio = (uniques.reindex(top_commented) / user_accum_df.set_index('mk').delta).drop('Benjamin_Netanyahu')\n",
      "user_accum_df['unique_delta_ratio'] = user_accum_df['mk'].map(unique_delta_ratio)\n",
      "\n",
      "alt.Chart(user_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = alt.Y('unique_delta_ratio',),\n",
      "    color = 'camp',\n",
      "    tooltip = ['index', 'mk', 'party']\n",
      ").properties(height=600, width=800)\n",
      "12/107:\n",
      "unique_delta_ratio = (uniques.reindex(top_commented) / user_accum_df.set_index('mk').delta).drop('Benjamin_Netanyahu')\n",
      "user_accum_df['unique_delta_ratio'] = user_accum_df['mk'].map(unique_delta_ratio)\n",
      "\n",
      "alt.Chart(user_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = alt.Y('unique_delta_ratio',),\n",
      "    color = 'camp',\n",
      "    tooltip = ['index', 'mk', 'party', 'unique_delta_ratio']\n",
      ").properties(height=600, width=800)\n",
      "12/108: print (top_commented[:5])\n",
      "12/109: print (top_commented[:6])\n",
      "12/110: print ('First celebiez group: ', top_commented[:6])\n",
      "12/111:\n",
      "print ('First celebiez group: ', top_commented[:6])\n",
      "print ('Second celebiez group: ', top_commented[:14])\n",
      "12/112:\n",
      "print ('First celebiez group: ', top_commented[:6])\n",
      "print ('Second celebiez group: ', top_commented[:15])\n",
      "12/113:\n",
      "stats = ['mean', 'std', 'min', 'max', 'median']\n",
      "\n",
      "st = (df.loc[df.is_post==0, ['from_id', 'name', 'id', 'elapsed_time', 'length', 'like_count', 'share_count', 'post_id', 'parent_id', 'created_month', 'created_day', 'created_day_of_week', 'created_hour']]\n",
      "      #.dropna(subset=['post_id'])\n",
      "      .groupby(by=['from_id', 'name'])\n",
      "      .agg(\n",
      "          {'id': 'count', \n",
      "           'elapsed_time': 'mean', \n",
      "           'length': 'mean', \n",
      "           'like_count': 'mean', \n",
      "           'share_count': 'mean', \n",
      "           'post_id': 'nunique', \n",
      "           'parent_id': 'nunique',\n",
      "           'created_month': 'nunique',\n",
      "           'created_day': 'nunique',\n",
      "           'created_day_of_week': 'nunique',\n",
      "           'created_hour': 'nunique',\n",
      "           \n",
      "          })\n",
      "     )\n",
      "st = st.reset_index()\n",
      "st.to_pickle('turf/total_stats_no_multiindex.pkl.gz', compression='gzip')\n",
      "12/114: st['party'] = st['name'].map(mkdet.set_index('folder_name')['party'])\n",
      "12/115: st['camp'] = st['party'].map(camps)\n",
      "12/116: top_liked = list(df.loc[df.is_post==1].groupby('name').like_count.mean().sort_values(ascending=False).index)\n",
      "12/117: top_liked\n",
      "12/118: uniques.reindex(top_liked).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "12/119:\n",
      "def get_accum_df(mat, order):\n",
      "    onlys = []\n",
      "\n",
      "    for i, name in enumerate(order):\n",
      "        only = mat.loc[:, order[i+1:]].isna().all(axis=1)\n",
      "        onlys.append(only[only].shape[0])\n",
      "        \n",
      "    user_accum_df = pd.DataFrame({'mk': order, 'user_accum': onlys})\n",
      "    return user_accum_df\n",
      "12/120:\n",
      "user_accum_df = get_accum_df(mat, top_commented)\n",
      "user_accum_df.head()\n",
      "12/121: like_accum_df = get_accum_df(mat, top_liked)\n",
      "12/122:\n",
      "like_accum_df = get_accum_df(mat, top_liked)\n",
      "like_accum_df.head()\n",
      "12/123:\n",
      "def get_accum_df(mat, order):\n",
      "    onlys = []\n",
      "\n",
      "    for i, name in enumerate(order):\n",
      "        only = mat.loc[:, order[i+1:]].isna().all(axis=1)\n",
      "        onlys.append(only[only].shape[0])\n",
      "        \n",
      "    user_accum_df = pd.DataFrame({'mk': order, 'user_accum': onlys})\n",
      "    user_accum_df['party'] = user_accum_df['mk'].map(mkdet.set_index('folder_name')['party'])\n",
      "    user_accum_df['camp'] = user_accum_df['party'].map(camps)\n",
      "    user_accum_df['delta'] = user_accum_df.user_accum.diff()\n",
      "    unique_delta_ratio = (uniques.reindex(top_commented) / user_accum_df.set_index('mk').delta).drop('Benjamin_Netanyahu')\n",
      "    user_accum_df['unique_delta_ratio'] = user_accum_df['mk'].map(unique_delta_ratio)\n",
      "    \n",
      "    return user_accum_df\n",
      "12/124:\n",
      "def get_accum_df(mat, order):\n",
      "    onlys = []\n",
      "\n",
      "    for i, name in enumerate(order):\n",
      "        only = mat.loc[:, order[i+1:]].isna().all(axis=1)\n",
      "        onlys.append(only[only].shape[0])\n",
      "        \n",
      "    user_accum_df = pd.DataFrame({'mk': order, 'user_accum': onlys})\n",
      "    user_accum_df['party'] = user_accum_df['mk'].map(mkdet.set_index('folder_name')['party'])\n",
      "    user_accum_df['camp'] = user_accum_df['party'].map(camps)\n",
      "    user_accum_df['delta'] = user_accum_df.user_accum.diff()\n",
      "    unique_delta_ratio = (uniques.reindex(top_commented) / user_accum_df.set_index('mk').delta).drop('Benjamin_Netanyahu')\n",
      "    user_accum_df['unique_delta_ratio'] = user_accum_df['mk'].map(unique_delta_ratio)\n",
      "    user_accum_df['delta_n'] = user_accum_df['delta'] / user_accum_df['delta'].max()\n",
      "    \n",
      "    return user_accum_df\n",
      "12/125:\n",
      "like_accum_df = get_accum_df(mat, top_liked)\n",
      "like_accum_df.head()\n",
      "12/126:\n",
      "alt.Chart(like_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = alt.Y('user_accum',),\n",
      "    color = 'camp',\n",
      "    tooltip = ['index', 'mk', 'party', 'user_accum']\n",
      ").properties(height=600, width=800)\n",
      "12/127:\n",
      "alt.Chart(like_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = alt.Y('delta',),\n",
      "    color = 'camp',\n",
      "    tooltip = ['index', 'mk', 'party']\n",
      ").properties(height=600, width=800)\n",
      "12/128:\n",
      "alt.Chart(like_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = alt.Y('unique_delta_ratio',),\n",
      "    color = 'camp',\n",
      "    tooltip = ['index', 'mk', 'party', 'unique_delta_ratio']\n",
      ").properties(height=600, width=800)\n",
      "12/129:\n",
      "alt.Chart(like_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=like_accum_df),\n",
      "    y = alt.Y('user_accum',),\n",
      "    color = 'camp',\n",
      "    tooltip = ['index', 'mk', 'party', 'user_accum']\n",
      ").properties(height=600, width=800)\n",
      "12/130:\n",
      "alt.Chart(like_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_liked),\n",
      "    y = alt.Y('user_accum',),\n",
      "    color = 'camp',\n",
      "    tooltip = ['index', 'mk', 'party', 'user_accum']\n",
      ").properties(height=600, width=800)\n",
      "12/131:\n",
      "alt.Chart(like_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_liked),\n",
      "    y = alt.Y('delta',),\n",
      "    color = 'camp',\n",
      "    tooltip = ['index', 'mk', 'party']\n",
      ").properties(height=600, width=800)\n",
      "12/132:\n",
      "alt.Chart(like_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_liked),\n",
      "    y = alt.Y('unique_delta_ratio',),\n",
      "    color = 'camp',\n",
      "    tooltip = ['index', 'mk', 'party', 'unique_delta_ratio']\n",
      ").properties(height=600, width=800)\n",
      "12/133: top_shared = list(df.loc[df.is_post==1].groupby('name').share_count.mean().sort_values(ascending=False).index)\n",
      "12/134: uniques.reindex(top_shared).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "12/135:\n",
      "share_accum_df = get_accum_df(mat, top_shared)\n",
      "share_accum_df.head()\n",
      "12/136:\n",
      "alt.Chart(share_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_shared),\n",
      "    y = alt.Y('user_accum',),\n",
      "    color = 'camp',\n",
      "    tooltip = ['index', 'mk', 'party', 'user_accum']\n",
      ").properties(height=600, width=800)\n",
      "12/137:\n",
      "alt.Chart(share_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_shared),\n",
      "    y = alt.Y('delta',),\n",
      "    color = 'camp',\n",
      "    tooltip = ['index', 'mk', 'party']\n",
      ").properties(height=600, width=800)\n",
      "12/138:\n",
      "alt.Chart(share_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_shared),\n",
      "    y = alt.Y('unique_delta_ratio',),\n",
      "    color = 'camp',\n",
      "    tooltip = ['index', 'mk', 'party', 'unique_delta_ratio']\n",
      ").properties(height=600, width=800)\n",
      "13/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "13/2:\n",
      "%%time\n",
      "df = pd.read_pickle('turf/data_df.pkl.gz', compression='gzip')\n",
      "13/3: df.dtypes\n",
      "13/4:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "13/5:\n",
      "%%time\n",
      "df = pd.read_pickle('turf/data_df.pkl.gz', compression='gzip')\n",
      "13/6: df.dtypes\n",
      "13/7:\n",
      "sdf = pd.read_pickle('turf/shared_df.pkl.gz', compression='gzip')\n",
      "sdf.dtypes\n",
      "13/8: st = pd.read_pickle('turf/total_stats_no_multiindex.pkl.gz', compression='gzip')\n",
      "13/9:\n",
      "# OLD STUFF\n",
      "#save(st, 'turf/total_stats.pkl.gz')\n",
      "#st = pd.read_pickle( 'turf/total_stats.pkl.gz', compression='gzip')\n",
      "13/10: st.head()\n",
      "13/11: st.shape\n",
      "13/12: df[df.is_post==0].shape\n",
      "13/13: print ('Total number of comments: %d' % st.id.sum())\n",
      "13/14: print ('Total number of distinct users (commenters): %d' % st.from_id.nunique())\n",
      "13/15:\n",
      "counts = st.groupby('from_id').id.sum()\n",
      "print ('Total number of one-timers: %d' % counts[counts==1].shape[0])\n",
      "print ('Ratio of one timers from all users: %.2f' % (counts[counts==1].shape[0] / st.from_id.nunique()))\n",
      "13/16: mat = st.pivot(index='from_id', columns='name', values='id')\n",
      "13/17: mat[(mat.count(axis=1)==1)].shape[0]\n",
      "13/18: mat.loc[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu==1), 'Benjamin_Netanyahu'].shape[0]\n",
      "13/19:\n",
      "print ('Ratio of Bibi from one-timers: %.2f' % (mat[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu==1)].shape[0] \n",
      " / counts[counts==1].shape[0]))\n",
      "13/20: print ('Ratio of users that commented 10 times and up: %.2f' % (counts[counts>=10].shape[0]/st.from_id.nunique()))\n",
      "13/21: (st.reset_index().groupby('from_id').name.nunique().value_counts().head(20)/st.reset_index().from_id.nunique()).plot(kind='bar')\n",
      "13/22:\n",
      "uniques = (st\n",
      " [st.groupby('from_id').name.transform('nunique')==1]\n",
      " .name.value_counts())\n",
      "13/23: uniques.head(10)\n",
      "13/24:\n",
      "print ('Total number of uniques: %d' % uniques.sum())\n",
      "print ('Ratio of uniques from all users: %.2f' % (uniques.sum() / st.from_id.nunique()))\n",
      "print ('Ratio of one-timers from uniques: %.2f' % (counts[counts==1].shape[0] / uniques.sum()))\n",
      "13/25:\n",
      "(pd.concat([uniques, \n",
      "            uniques/st.from_id.nunique(), \n",
      "            uniques/st.groupby('name').from_id.nunique()],\n",
      "           axis=1, keys=['num_uniques', 'ratio_from_all_users', 'ratio_from_own_users'])\n",
      " .sort_values(by='ratio_from_own_users', ascending=False)).head(10)\n",
      "13/26:\n",
      "mkdet = pd.read_csv('turf/mk_details.csv', sep='\\t')\n",
      "mkdet.head()\n",
      "13/27: df['party'] = df['name'].map(mkdet.set_index('folder_name')['party'])\n",
      "13/28: df.groupby(['party', 'langdetect']).size().unstack()\n",
      "13/29: st['party'] = st['name'].map(mkdet.set_index('folder_name')['party'])\n",
      "13/30:\n",
      "non_uniques = (st.reset_index()\n",
      " .loc[st.groupby('from_id').name.transform('nunique')>1, ['from_id', 'name', 'party']])\n",
      "13/31: non_uniques.head()\n",
      "13/32: non_uniques.from_id.nunique()\n",
      "13/33: non_uniques.party.value_counts()\n",
      "13/34: non_uniques.groupby('party').from_id.nunique()\n",
      "13/35: base = non_uniques[non_uniques.groupby('from_id').party.transform('nunique')==1]\n",
      "13/36:\n",
      "print ('Total number of basers: %d' % base.from_id.nunique())\n",
      "print ('Ratio of basers from all users: %.2f' % (base.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of basers from non-uniques: %.2f' % (base.from_id.nunique() / non_uniques.from_id.nunique()))\n",
      "13/37: st.party.unique()\n",
      "13/38:\n",
      "camps = { 'מרצ': 'opposition',\n",
      "          'יש עתיד': 'opposition',\n",
      "          'ליכוד': 'coalition',\n",
      "          'הבית היהודי בראשות נפתלי בנט': 'coalition',\n",
      "          'המחנה הציוני': 'opposition',\n",
      "          'הרשימה המשותפת (חד”ש, רע”מ, בל”ד, תע”ל)': 'opposition',\n",
      "          'כולנו בראשות משה כחלון': 'coalition',\n",
      "          'ישראל ביתנו': 'coalition' }\n",
      "13/39: st['camp'] = st['party'].map(camps)\n",
      "13/40: non_uniques['camp'] = non_uniques['party'].map(camps)\n",
      "13/41: non_uniques.groupby('camp').from_id.nunique()\n",
      "13/42: campers = non_uniques[non_uniques.groupby('from_id').camp.transform('nunique')==1]\n",
      "13/43:\n",
      "print ('Total number of campers: %d' % campers.from_id.nunique())\n",
      "print ('Ratio of campers from all users: %.2f' % (campers.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of campers from non-uniques: %.2f' % (campers.from_id.nunique() / non_uniques.from_id.nunique()))\n",
      "print ('Coalition | Opposition ratio from campers: %.2f | %.2f' % \n",
      "       ((campers[campers.camp=='coalition'].from_id.nunique() / campers.from_id.nunique()),\n",
      "       (campers[campers.camp=='opposition'].from_id.nunique() / campers.from_id.nunique())))\n",
      "cg = st.groupby('camp')\n",
      "print ('Coalition | Opposition ratios from total users: %.2f | %.2f' % \n",
      "       ((cg.from_id.nunique()['coalition'] / st.from_id.nunique()),\n",
      "       ((cg.from_id.nunique()['opposition'] / st.from_id.nunique()))))\n",
      "\n",
      "print ('Coalition | Opposition total comments: %.2f | %.2f' % \n",
      "       ((cg.id.sum()['coalition'] / st.id.sum()),\n",
      "       ((cg.id.sum()['opposition'] / st.id.sum()))))\n",
      "13/44: top_commented = list(st.groupby('name').id.sum().sort_values(ascending=False).index)\n",
      "13/45:\n",
      "top_10_commented = top_commented[:10]\n",
      "top_10_commented\n",
      "13/46: celebz =  non_uniques[non_uniques.groupby('from_id').name.transform(lambda x: x.isin(top_10_commented).all())]\n",
      "13/47: celebz.head()\n",
      "13/48: num_celebiez_no_base_camp = len(set(celebz.from_id.drop_duplicates().values).difference(set(campers.from_id.drop_duplicates().values)).difference(set(base.from_id.drop_duplicates().values)))\n",
      "13/49:\n",
      "print ('Total number of celeb followers (celebies): %d' % celebz.from_id.nunique())\n",
      "print ('Ratio of celebies from all users: %.2f' % (celebz.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of celebies from non-uniques: %.2f' % (celebz.from_id.nunique() / non_uniques.from_id.nunique()))\n",
      "print ('Remove campers and base from celebies: %d' % num_celebiez_no_base_camp)\n",
      "13/50: top_commented\n",
      "13/51: uniques.reindex(top_commented).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "13/52:\n",
      "only_bibi = mat.loc[:, top_commented[1:]].isna().all(axis=1)\n",
      "only_bibi[only_bibi].shape\n",
      "13/53:\n",
      "def get_accum_df(mat, order):\n",
      "    onlys = []\n",
      "\n",
      "    for i, name in enumerate(order):\n",
      "        only = mat.loc[:, order[i+1:]].isna().all(axis=1)\n",
      "        onlys.append(only[only].shape[0])\n",
      "        \n",
      "    user_accum_df = pd.DataFrame({'mk': order, 'user_accum': onlys})\n",
      "    user_accum_df['party'] = user_accum_df['mk'].map(mkdet.set_index('folder_name')['party'])\n",
      "    user_accum_df['camp'] = user_accum_df['party'].map(camps)\n",
      "    user_accum_df['delta'] = user_accum_df.user_accum.diff()\n",
      "    unique_delta_ratio = (uniques.reindex(top_commented) / user_accum_df.set_index('mk').delta).drop('Benjamin_Netanyahu')\n",
      "    user_accum_df['unique_delta_ratio'] = user_accum_df['mk'].map(unique_delta_ratio)\n",
      "    user_accum_df['delta_n'] = user_accum_df['delta'] / user_accum_df['delta'].max()\n",
      "    \n",
      "    return user_accum_df\n",
      "13/54:\n",
      "user_accum_df = get_accum_df(mat, top_commented)\n",
      "user_accum_df.head()\n",
      "13/55: user_accum_df.set_index('mk').user_accum.plot(kind='bar', figsize=(15,10), rot=75)\n",
      "13/56:\n",
      "import altair as alt\n",
      "alt.renderers.enable('notebook')\n",
      "13/57:\n",
      "alt.Chart(user_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = alt.Y('user_accum',),\n",
      "    color = 'camp',\n",
      "    tooltip = ['index', 'mk', 'party', 'user_accum']\n",
      ").properties(height=600, width=800)\n",
      "13/58:\n",
      "alt.Chart(user_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = alt.Y('delta',),\n",
      "    color = 'camp',\n",
      "    tooltip = ['index', 'mk', 'party']\n",
      ").properties(height=600, width=800)\n",
      "13/59:\n",
      "alt.Chart(user_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = alt.Y('unique_delta_ratio',),\n",
      "    color = 'camp',\n",
      "    tooltip = ['index', 'mk', 'party', 'unique_delta_ratio']\n",
      ").properties(height=600, width=800)\n",
      "13/60: user_accum_df[['delta_n', 'unique_delta_ratio']].div(user_accum_df[['delta_n', 'unique_delta_ratio']].sum(axis=1), axis=0).plot(kind='bar', stacked=True, figsize=(15,10), rot=75, width=1.0)\n",
      "13/61:\n",
      "alt.Chart(user_accum_df.reset_index().stack('unique_delta_ratio')).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_commented),\n",
      "    y = alt.Y('delta',),\n",
      "    color = 'camp',\n",
      "    tooltip = ['index', 'mk', 'party']\n",
      ").properties(height=600, width=800)\n",
      "13/62:\n",
      "print ('First celebiez group: ', top_commented[:6])\n",
      "print ('Second celebiez group: ', top_commented[:15])\n",
      "13/63: user_accum_df.reset_index().head()\n",
      "13/64: top_liked = list(df.loc[df.is_post==1].groupby('name').like_count.mean().sort_values(ascending=False).index)\n",
      "13/65: uniques.reindex(top_liked).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "13/66:\n",
      "like_accum_df = get_accum_df(mat, top_liked)\n",
      "like_accum_df.head()\n",
      "13/67:\n",
      "alt.Chart(like_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_liked),\n",
      "    y = alt.Y('user_accum',),\n",
      "    color = 'camp',\n",
      "    tooltip = ['index', 'mk', 'party', 'user_accum']\n",
      ").properties(height=600, width=800)\n",
      "13/68:\n",
      "alt.Chart(like_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_liked),\n",
      "    y = alt.Y('delta',),\n",
      "    color = 'camp',\n",
      "    tooltip = ['index', 'mk', 'party']\n",
      ").properties(height=600, width=800)\n",
      "13/69:\n",
      "alt.Chart(like_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_liked),\n",
      "    y = alt.Y('unique_delta_ratio',),\n",
      "    color = 'camp',\n",
      "    tooltip = ['index', 'mk', 'party', 'unique_delta_ratio']\n",
      ").properties(height=600, width=800)\n",
      "13/70: top_shared = list(df.loc[df.is_post==1].groupby('name').share_count.mean().sort_values(ascending=False).index)\n",
      "13/71: uniques.reindex(top_shared).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "13/72:\n",
      "share_accum_df = get_accum_df(mat, top_shared)\n",
      "share_accum_df.head()\n",
      "13/73:\n",
      "alt.Chart(share_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_shared),\n",
      "    y = alt.Y('user_accum',),\n",
      "    color = 'camp',\n",
      "    tooltip = ['index', 'mk', 'party', 'user_accum']\n",
      ").properties(height=600, width=800)\n",
      "13/74:\n",
      "alt.Chart(share_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_shared),\n",
      "    y = alt.Y('delta',),\n",
      "    color = 'camp',\n",
      "    tooltip = ['index', 'mk', 'party']\n",
      ").properties(height=600, width=800)\n",
      "13/75:\n",
      "alt.Chart(share_accum_df.reset_index()).mark_bar().encode(\n",
      "    x = alt.X('mk', sort=top_shared),\n",
      "    y = alt.Y('unique_delta_ratio',),\n",
      "    color = 'camp',\n",
      "    tooltip = ['index', 'mk', 'party', 'unique_delta_ratio']\n",
      ").properties(height=600, width=800)\n",
      "13/76: top_commented[:10]\n",
      "13/77: df.head()\n",
      "13/78: top_mean_commented = list(df.loc[df.is_post==1].groupby('name').comment_count.mean().sort_values(ascending=False).index)\n",
      "13/79: uniques.reindex(top_mean_commented).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "13/80:\n",
      "def chart_user_accum(accum_df, order)\n",
      "    alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('user_accum',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'user_accum']\n",
      "    ).properties(height=600, width=800)\n",
      "    \n",
      "chart_user_accum(like_accum_df, top_liked)\n",
      "13/81:\n",
      "def chart_user_accum(accum_df, order):\n",
      "    alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('user_accum',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'user_accum']\n",
      "    ).properties(height=600, width=800)\n",
      "    \n",
      "chart_user_accum(like_accum_df, top_liked)\n",
      "13/82:\n",
      "def chart_user_accum(accum_df, order):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('user_accum',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'user_accum']\n",
      "    ).properties(height=600, width=800)\n",
      "    \n",
      "chart_user_accum(like_accum_df, top_liked)\n",
      "13/83:\n",
      "def chart_user_accum(accum_df, order):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('user_accum',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'user_accum']\n",
      "    ).properties(height=600, width=800)\n",
      "\n",
      "chart_user_accum(user_accum_df, top_commented)\n",
      "13/84:\n",
      "def chart_user_accum_delta(accum_df, order):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('delta',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'delta']\n",
      "    ).properties(height=600, width=800)\n",
      "\n",
      "chart_user_accum_delta(user_accum_df, top_commented)\n",
      "13/85:\n",
      "def chart_user_accum_unique_delta_ratio(accum_df, order):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('unique_delta_ratio',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'unique_delta_ratio']\n",
      "    ).properties(height=600, width=800)\n",
      "\n",
      "chart_user_accum_unique_delta_ratio(user_accum_df, top_commented)\n",
      "13/86:\n",
      "mean_com_accum_df = get_accum_df(mat, top_mean_commented)\n",
      "(chart_user_accum(mean_com_accum_df, top_mean_commented) \n",
      " & chart_user_accum_delta(mean_com_accum_df, top_mean_commented) \n",
      " & chart_user_accum_unique_delta_ratio(mean_com_accum_df, top_mean_commented))\n",
      "13/87:\n",
      "like_accum_df = get_accum_df(mat, top_liked)\n",
      "(chart_user_accum(like_accum_df, top_liked) \n",
      " & chart_user_accum_delta(like_accum_df, top_liked) \n",
      " & chart_user_accum_unique_delta_ratio(like_accum_df, top_liked))\n",
      "13/88:\n",
      "(chart_user_accum(share_accum_df, top_shared) \n",
      " & chart_user_accum_delta(share_accum_df, top_shared) \n",
      " & chart_user_accum_unique_delta_ratio(share_accum_df, top_shared))\n",
      "13/89:\n",
      "def chart_user_accum(accum_df, order):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('user_accum',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'user_accum']\n",
      "    ).properties(height=600, width=800)\n",
      "\n",
      "\n",
      "def chart_user_accum_delta(accum_df, order):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('delta',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'delta']\n",
      "    ).properties(height=600, width=800)\n",
      "\n",
      "\n",
      "# divide unique count and user_accum delta, to measure how much a page is part of the group\n",
      "\n",
      "def chart_user_accum_unique_delta_ratio(accum_df, order):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('unique_delta_ratio',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'unique_delta_ratio']\n",
      "    ).properties(height=600, width=800)\n",
      "13/90:\n",
      "def chart_user_accum(accum_df, order):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('user_accum',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'user_accum']\n",
      "    ).properties(height=600, width=800)\n",
      "\n",
      "\n",
      "def chart_user_accum_delta(accum_df, order):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('delta',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'delta']\n",
      "    ).properties(height=600, width=800)\n",
      "\n",
      "\n",
      "def chart_user_accum_unique_delta_ratio(accum_df, order):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('unique_delta_ratio',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'unique_delta_ratio']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = 'Page unique count divided by user_accum delta (to measure how much a page is part of the group)')\n",
      "13/91:\n",
      "(chart_user_accum(user_accum_df, top_commented) \n",
      " & chart_user_accum_delta(user_accum_df, top_commented)\n",
      " & chart_user_accum_unique_delta_ratio(user_accum_df, top_commented))\n",
      "13/92:\n",
      "def chart_user_accum(accum_df, order, order_name):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('user_accum',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'user_accum']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = f'User accumulation by {order_name} order\\nleft to right, only users that comment inside the group until the current column')\n",
      "\n",
      "\n",
      "def chart_user_accum_delta(accum_df, order, order_name):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('delta',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'delta']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = f'User accumulation delta by {order_name} order\\nleft to right, only users that comment inside the group until the current column')\n",
      "\n",
      "\n",
      "def chart_user_accum_unique_delta_ratio(accum_df, order, order_name):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('unique_delta_ratio',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'unique_delta_ratio']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = f'Page unique count divided by user_accum delta by {order_name} order\\n(to measure how much a page is part of the group)')\n",
      "13/93:\n",
      "(chart_user_accum(user_accum_df, top_commented, 'Top Commented') \n",
      " & chart_user_accum_delta(user_accum_df, top_commented, 'Top Commented')\n",
      " & chart_user_accum_unique_delta_ratio(user_accum_df, top_commented, 'Top Commented'))\n",
      "13/94:\n",
      "(chart_user_accum(user_accum_df, top_commented, 'Top Commented') \n",
      " & chart_user_accum_delta(user_accum_df, top_commented, 'Top Commented')\n",
      " & chart_user_accum_unique_delta_ratio(user_accum_df, top_commented, 'Top Commented')).properties(title='WOOOOOOOOOOOOOO')\n",
      "13/95:\n",
      "(chart_user_accum(user_accum_df, top_commented, 'Top Commented') \n",
      " & chart_user_accum_delta(user_accum_df, top_commented, 'Top Commented')\n",
      " & chart_user_accum_unique_delta_ratio(user_accum_df, top_commented, 'Top Commented')).properties(title='Top Commented')\n",
      "13/96:\n",
      "def chart_user_accum(accum_df, order, order_name):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('user_accum',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'user_accum']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = f'User accumulation by {order_name} order - left to right, only users that comment inside the group until the current column')\n",
      "\n",
      "\n",
      "def chart_user_accum_delta(accum_df, order, order_name):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('delta',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'delta']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = f'User accumulation delta by {order_name} order - left to right, only users that comment inside the group until the current column')\n",
      "\n",
      "\n",
      "def chart_user_accum_unique_delta_ratio(accum_df, order, order_name):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('unique_delta_ratio',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'unique_delta_ratio']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = f'Page unique count divided by user_accum delta by {order_name} order (to measure how much a page is part of the group)')\n",
      "13/97:\n",
      "(chart_user_accum(user_accum_df, top_commented, 'Top Commented') \n",
      " & chart_user_accum_delta(user_accum_df, top_commented, 'Top Commented')\n",
      " & chart_user_accum_unique_delta_ratio(user_accum_df, top_commented, 'Top Commented')).properties(title='Top Commented')\n",
      "13/98:\n",
      "(chart_user_accum(mean_com_accum_df, top_mean_commented) \n",
      " & chart_user_accum_delta(mean_com_accum_df, top_mean_commented) \n",
      " & chart_user_accum_unique_delta_ratio(mean_com_accum_df, top_mean_commented))\n",
      "13/99:\n",
      "(chart_user_accum(mean_com_accum_df, top_mean_commented, 'Top Mean Commented') \n",
      " & chart_user_accum_delta(mean_com_accum_df, top_mean_commented, 'Top Mean Commented') \n",
      " & chart_user_accum_unique_delta_ratio(mean_com_accum_df, top_mean_commented, 'Top Mean Commented')).properties(title='Top Mean Commented')\n",
      "13/100:\n",
      "(chart_user_accum(like_accum_df, top_liked, 'Top Liked') \n",
      " & chart_user_accum_delta(like_accum_df, top_liked, 'Top Liked') \n",
      " & chart_user_accum_unique_delta_ratio(like_accum_df, top_liked, 'Top Liked')).properties(title='Top Liked')\n",
      "13/101:\n",
      "(chart_user_accum(share_accum_df, top_shared, 'Top Shared') \n",
      " & chart_user_accum_delta(share_accum_df, top_shared, 'Top Shared') \n",
      " & chart_user_accum_unique_delta_ratio(share_accum_df, top_shared, 'Top Shared')).properties(title='Top Shared')\n",
      "13/102:\n",
      "non_uniques = (st.reset_index()\n",
      " .loc[st.groupby('from_id').name.transform('nunique')==1, ['from_id', 'name', 'party']])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/103:\n",
      "non_uniques = (st.reset_index()\n",
      " .loc[st.groupby('from_id').name.transform('nunique')>1, ['from_id', 'name', 'party']])\n",
      "13/104:\n",
      "unique_users = (st.reset_index()\n",
      " .loc[st.groupby('from_id').name.transform('nunique')==1, ['from_id', 'name', 'party']])\n",
      "13/105: uniques = ( unique_users.name.value_counts())\n",
      "13/106: uniques.head(10)\n",
      "13/107:\n",
      "counts = st.groupby('from_id').id.sum()\n",
      "one_timers = counts[counts==1]\n",
      "\n",
      "print ('Total number of one-timers: %d' % one_timers.shape[0])\n",
      "print ('Ratio of one timers from all users: %.2f' % one_timers.shape[0] / st.from_id.nunique()))\n",
      "13/108:\n",
      "counts = st.groupby('from_id').id.sum()\n",
      "one_timers = counts[counts==1]\n",
      "\n",
      "print ('Total number of one-timers: %d' % one_timers.shape[0])\n",
      "print ('Ratio of one timers from all users: %.2f' % (one_timers.shape[0] / st.from_id.nunique()))\n",
      "13/109: set(one_timers.index)\n",
      "13/110:\n",
      "from matplotlib_venn import venn2\n",
      " \n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn2([set(one_timers.index), set(unique_users.index), set(base.index), set(campers.index)], set_labels = ('One Timers', 'Uniques', 'Basers', 'Campers'), ax=ax)\n",
      "13/111:\n",
      "from matplotlib_venn import venn2\n",
      " \n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn2([set(one_timers.index), set(unique_users.index), set(base.index), set(campers.index)], set_labels = ('One Timers', 'Uniques', 'Basers', 'Campers'), ax=ax)\n",
      "13/112: [set(one_timers.index), set(unique_users.index), set(base.index), set(campers.index)]\n",
      "13/113: unique_users.head()\n",
      "13/114: one_timers\n",
      "13/115: base.head()\n",
      "13/116:\n",
      "from matplotlib_venn import venn2\n",
      " \n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn2([set(one_timers.index), set(unique_users.from_id.values), set(base.from_id.values), set(campers.from_id.values)], set_labels = ('One Timers', 'Uniques', 'Basers', 'Campers'), ax=ax)\n",
      "13/117:\n",
      "from matplotlib_venn import venn2\n",
      " \n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn2([set(one_timers.index), set(unique_users.from_id.values)], set_labels = ('One Timers', 'Uniques', 'Basers', 'Campers'), ax=ax)\n",
      "13/118:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn2([set(base.from_id.values), set(campers.from_id.values)], set_labels = ('Basers', 'Campers'), ax=ax)\n",
      "13/119:\n",
      "from matplotlib_venn import venn2\n",
      " \n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn2([set(st.from_id.values), set(one_timers.index), set(unique_users.from_id.values), set(base.from_id.values), set(campers.from_id.values)], set_labels = ('All', 'One Timers', 'Uniques', 'Basers', 'Campers'), ax=ax)\n",
      "13/120: from matplotlib_venn import venn3\n",
      "13/121:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(one_timers.index), set(unique_users.from_id.values)], set_labels = ('One Timers', 'Uniques', ), ax=ax)\n",
      "13/122:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(one_timers.index), set(unique_users.from_id.values)], set_labels = ('All', 'One Timers', 'Uniques', ), ax=ax)\n",
      "13/123:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(base.from_id.values), set(campers.from_id.values)], set_labels = ('Basers', 'Campers'), ax=ax)\n",
      "13/124:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(base.from_id.values), set(campers.from_id.values)], set_labels = ('All', 'Basers', 'Campers'), ax=ax)\n",
      "13/125:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(one_timers.index), set(unique_users.from_id.values)], set_labels = ('All', 'One Timers', 'Uniques', ), ax=ax)\n",
      "ax.get_figure().savefig('turf/venn_one_timers_uniques.png')\n",
      "13/126:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(base.from_id.values), set(campers.from_id.values)], set_labels = ('All', 'Basers', 'Campers'), ax=ax)\n",
      "\n",
      "ax.get_figure().savefig('turf/venn_basers_campers.png')\n",
      "13/127:\n",
      "labels = ('All', 'One Timers', 'Uniques', 'Basers', 'Campers')\n",
      "sets = [set(st.from_id.values), set(one_timers.index), set(unique_users.from_id.values), set(base.from_id.values), set(campers.from_id.values)]\n",
      "\n",
      "set_dict = dict(zip(labels, sets))\n",
      "13/128:\n",
      "inter_set_dict = {}\n",
      "for i in set_dict:\n",
      "    inter_set_dict[i] = {}\n",
      "    for j in set_dict:\n",
      "        inter_set_dict[i][j] = set_dict[i].intersect(set_dict[j])\n",
      "13/129:\n",
      "inter_set_dict = {}\n",
      "for i in set_dict:\n",
      "    inter_set_dict[i] = {}\n",
      "    for j in set_dict:\n",
      "        inter_set_dict[i][j] = set_dict[i].intersection(set_dict[j])\n",
      "13/130: inter_set_dict\n",
      "13/131:\n",
      "inter_set_dict = {}\n",
      "for i in set_dict:\n",
      "    inter_set_dict[i] = {}\n",
      "    for j in set_dict:\n",
      "        inter_set_dict[i][j] = size(set_dict[i].intersection(set_dict[j]))\n",
      "13/132:\n",
      "inter_set_dict = {}\n",
      "for i in set_dict:\n",
      "    inter_set_dict[i] = {}\n",
      "    for j in set_dict:\n",
      "        inter_set_dict[i][j] = len(set_dict[i].intersection(set_dict[j]))\n",
      "13/133: inter_set_dict\n",
      "13/134: pd.DataFrame(inter_set_dict)\n",
      "13/135: pd.DataFrame(inter_set_dict).to_csv('group_intersections.csv')\n",
      "13/136: pd.DataFrame(inter_set_dict, sort=False)\n",
      "13/137: pd.DataFrame(inter_set_dict)\n",
      "13/138: pd.DataFrame(inter_set_dict).reindex(labels)\n",
      "13/139: pd.DataFrame(inter_set_dict).reindex(labels).to_csv('group_intersections.csv')\n",
      "13/140: pd.DataFrame(inter_set_dict).reindex(labels).to_csv('turf/group_intersections.csv')\n",
      "14/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "14/2:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "14/3:\n",
      "%%time\n",
      "df = pd.read_pickle('turf/data_df.pkl.gz', compression='gzip')\n",
      "14/4: df.dtypes\n",
      "14/5:\n",
      "sdf = pd.read_pickle('turf/shared_df.pkl.gz', compression='gzip')\n",
      "sdf.dtypes\n",
      "14/6: st = pd.read_pickle('turf/total_stats_no_multiindex.pkl.gz', compression='gzip')\n",
      "14/7:\n",
      "# OLD STUFF\n",
      "#save(st, 'turf/total_stats.pkl.gz')\n",
      "#st = pd.read_pickle( 'turf/total_stats.pkl.gz', compression='gzip')\n",
      "14/8: st.head()\n",
      "14/9: st.shape\n",
      "14/10: df[df.is_post==0].shape\n",
      "14/11: print ('Total number of comments: %d' % st.id.sum())\n",
      "14/12:\n",
      "counts = st.groupby('from_id').id.sum()\n",
      "one_timers = counts[counts==1]\n",
      "\n",
      "print ('Total number of one-timers: %d' % one_timers.shape[0])\n",
      "print ('Ratio of one timers from all users: %.2f' % (one_timers.shape[0] / st.from_id.nunique()))\n",
      "14/13: print ('Total number of distinct users (commenters): %d' % st.from_id.nunique())\n",
      "14/14:\n",
      "counts = st.groupby('from_id').id.sum()\n",
      "one_timers = counts[counts==1]\n",
      "\n",
      "print ('Total number of one-timers: %d' % one_timers.shape[0])\n",
      "print ('Ratio of one timers from all users: %.2f' % (one_timers.shape[0] / st.from_id.nunique()))\n",
      "14/15: mat = st.pivot(index='from_id', columns='name', values='id')\n",
      "14/16: mat[(mat.count(axis=1)==1)].shape[0]\n",
      "14/17: mat.loc[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu==1), 'Benjamin_Netanyahu'].shape[0]\n",
      "14/18:\n",
      "print ('Ratio of Bibi from one-timers: %.2f' % (mat[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu==1)].shape[0] \n",
      " / counts[counts==1].shape[0]))\n",
      "14/19: print ('Ratio of users that commented 10 times and up: %.2f' % (counts[counts>=10].shape[0]/st.from_id.nunique()))\n",
      "14/20: (st.reset_index().groupby('from_id').name.nunique().value_counts().head(20)/st.reset_index().from_id.nunique()).plot(kind='bar')\n",
      "14/21:\n",
      "unique_users = (st.reset_index()\n",
      " .loc[st.groupby('from_id').name.transform('nunique')==1, ['from_id', 'name', 'party']])\n",
      "14/22: uniques = ( unique_users.name.value_counts())\n",
      "14/23: uniques.head(10)\n",
      "14/24:\n",
      "print ('Total number of uniques: %d' % uniques.sum())\n",
      "print ('Ratio of uniques from all users: %.2f' % (uniques.sum() / st.from_id.nunique()))\n",
      "print ('Ratio of one-timers from uniques: %.2f' % (counts[counts==1].shape[0] / uniques.sum()))\n",
      "14/25:\n",
      "(pd.concat([uniques, \n",
      "            uniques/st.from_id.nunique(), \n",
      "            uniques/st.groupby('name').from_id.nunique()],\n",
      "           axis=1, keys=['num_uniques', 'ratio_from_all_users', 'ratio_from_own_users'])\n",
      " .sort_values(by='ratio_from_own_users', ascending=False)).head(10)\n",
      "14/26:\n",
      "mkdet = pd.read_csv('turf/mk_details.csv', sep='\\t')\n",
      "mkdet.head()\n",
      "14/27: df['party'] = df['name'].map(mkdet.set_index('folder_name')['party'])\n",
      "14/28: df.groupby(['party', 'langdetect']).size().unstack()\n",
      "14/29: st['party'] = st['name'].map(mkdet.set_index('folder_name')['party'])\n",
      "14/30:\n",
      "non_uniques = (st.reset_index()\n",
      " .loc[st.groupby('from_id').name.transform('nunique')>1, ['from_id', 'name', 'party']])\n",
      "14/31: non_uniques.head()\n",
      "14/32: non_uniques.from_id.nunique()\n",
      "14/33: non_uniques.party.value_counts()\n",
      "14/34: non_uniques.groupby('party').from_id.nunique()\n",
      "14/35: base = non_uniques[non_uniques.groupby('from_id').party.transform('nunique')==1]\n",
      "14/36:\n",
      "print ('Total number of basers: %d' % base.from_id.nunique())\n",
      "print ('Ratio of basers from all users: %.2f' % (base.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of basers from non-uniques: %.2f' % (base.from_id.nunique() / non_uniques.from_id.nunique()))\n",
      "14/37: st.party.unique()\n",
      "14/38:\n",
      "camps = { 'מרצ': 'opposition',\n",
      "          'יש עתיד': 'opposition',\n",
      "          'ליכוד': 'coalition',\n",
      "          'הבית היהודי בראשות נפתלי בנט': 'coalition',\n",
      "          'המחנה הציוני': 'opposition',\n",
      "          'הרשימה המשותפת (חד”ש, רע”מ, בל”ד, תע”ל)': 'opposition',\n",
      "          'כולנו בראשות משה כחלון': 'coalition',\n",
      "          'ישראל ביתנו': 'coalition' }\n",
      "14/39: st['camp'] = st['party'].map(camps)\n",
      "14/40: non_uniques['camp'] = non_uniques['party'].map(camps)\n",
      "14/41: non_uniques.groupby('camp').from_id.nunique()\n",
      "14/42: campers = non_uniques[non_uniques.groupby('from_id').camp.transform('nunique')==1]\n",
      "14/43:\n",
      "print ('Total number of campers: %d' % campers.from_id.nunique())\n",
      "print ('Ratio of campers from all users: %.2f' % (campers.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of campers from non-uniques: %.2f' % (campers.from_id.nunique() / non_uniques.from_id.nunique()))\n",
      "print ('Coalition | Opposition ratio from campers: %.2f | %.2f' % \n",
      "       ((campers[campers.camp=='coalition'].from_id.nunique() / campers.from_id.nunique()),\n",
      "       (campers[campers.camp=='opposition'].from_id.nunique() / campers.from_id.nunique())))\n",
      "cg = st.groupby('camp')\n",
      "print ('Coalition | Opposition ratios from total users: %.2f | %.2f' % \n",
      "       ((cg.from_id.nunique()['coalition'] / st.from_id.nunique()),\n",
      "       ((cg.from_id.nunique()['opposition'] / st.from_id.nunique()))))\n",
      "\n",
      "print ('Coalition | Opposition total comments: %.2f | %.2f' % \n",
      "       ((cg.id.sum()['coalition'] / st.id.sum()),\n",
      "       ((cg.id.sum()['opposition'] / st.id.sum()))))\n",
      "14/44: top_commented = list(st.groupby('name').id.sum().sort_values(ascending=False).index)\n",
      "14/45:\n",
      "top_10_commented = top_commented[:10]\n",
      "top_10_commented\n",
      "14/46: celebz =  non_uniques[non_uniques.groupby('from_id').name.transform(lambda x: x.isin(top_10_commented).all())]\n",
      "14/47: celebz.head()\n",
      "14/48: num_celebiez_no_base_camp = len(set(celebz.from_id.drop_duplicates().values).difference(set(campers.from_id.drop_duplicates().values)).difference(set(base.from_id.drop_duplicates().values)))\n",
      "14/49:\n",
      "print ('Total number of celeb followers (celebies): %d' % celebz.from_id.nunique())\n",
      "print ('Ratio of celebies from all users: %.2f' % (celebz.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of celebies from non-uniques: %.2f' % (celebz.from_id.nunique() / non_uniques.from_id.nunique()))\n",
      "print ('Remove campers and base from celebies: %d' % num_celebiez_no_base_camp)\n",
      "14/50: top_commented[:10]\n",
      "14/51: uniques.reindex(top_commented).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "14/52:\n",
      "only_bibi = mat.loc[:, top_commented[1:]].isna().all(axis=1)\n",
      "only_bibi[only_bibi].shape\n",
      "14/53:\n",
      "def get_accum_df(mat, order):\n",
      "    onlys = []\n",
      "\n",
      "    for i, name in enumerate(order):\n",
      "        only = mat.loc[:, order[i+1:]].isna().all(axis=1)\n",
      "        onlys.append(only[only].shape[0])\n",
      "        \n",
      "    user_accum_df = pd.DataFrame({'mk': order, 'user_accum': onlys})\n",
      "    user_accum_df['party'] = user_accum_df['mk'].map(mkdet.set_index('folder_name')['party'])\n",
      "    user_accum_df['camp'] = user_accum_df['party'].map(camps)\n",
      "    user_accum_df['delta'] = user_accum_df.user_accum.diff()\n",
      "    unique_delta_ratio = (uniques.reindex(top_commented) / user_accum_df.set_index('mk').delta).drop('Benjamin_Netanyahu')\n",
      "    user_accum_df['unique_delta_ratio'] = user_accum_df['mk'].map(unique_delta_ratio)\n",
      "    user_accum_df['delta_n'] = user_accum_df['delta'] / user_accum_df['delta'].max()\n",
      "    \n",
      "    return user_accum_df\n",
      "14/54:\n",
      "user_accum_df = get_accum_df(mat, top_commented)\n",
      "user_accum_df.head()\n",
      "14/55: user_accum_df.set_index('mk').user_accum.plot(kind='bar', figsize=(15,10), rot=75)\n",
      "14/56:\n",
      "import altair as alt\n",
      "alt.renderers.enable('notebook')\n",
      "14/57: base.head()\n",
      "14/58: non_uniques.head()\n",
      "14/59:\n",
      "labels = ('All', 'One Timers', 'Uniques', 'Non-Uniques', 'Basers', 'Campers')\n",
      "sets = [set(st.from_id.values), set(one_timers.index), set(unique_users.from_id.values), set(non_uniques.from_id.values), set(base.from_id.values), set(campers.from_id.values)]\n",
      "\n",
      "set_dict = dict(zip(labels, sets))\n",
      "14/60:\n",
      "inter_set_dict = {}\n",
      "for i in set_dict:\n",
      "    inter_set_dict[i] = {}\n",
      "    for j in set_dict:\n",
      "        inter_set_dict[i][j] = len(set_dict[i].intersection(set_dict[j]))\n",
      "14/61: pd.DataFrame(inter_set_dict).reindex(labels)\n",
      "14/62: pd.DataFrame(inter_set_dict).reindex(labels).to_csv('turf/group_intersections.csv')\n",
      "14/63: set(st.from_id.values)\n",
      "14/64:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(one_timers.index), set(unique_users.from_id.values)], set_labels = ('All', 'One Timers', 'Uniques', ), ax=ax)\n",
      "14/65: from matplotlib_venn import venn3\n",
      "14/66:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(one_timers.index), set(unique_users.from_id.values)], set_labels = ('All', 'One Timers', 'Uniques', ), ax=ax)\n",
      "14/67:\n",
      "fig, ax = plt.subplots(figsize=[15,10])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(one_timers.index), set(unique_users.from_id.values)], set_labels = ('All', 'One Timers', 'Uniques', ), ax=ax)\n",
      "14/68:\n",
      "fig, ax = plt.subplots(figsize=[15,15])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(one_timers.index), set(unique_users.from_id.values)], set_labels = ('All', 'One Timers', 'Uniques', ), ax=ax)\n",
      "14/69: v.get_label_by_id('100').set_text('Unknown')\n",
      "14/70:\n",
      "fig, ax = plt.subplots(figsize=[15,15])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(one_timers.index), set(unique_users.from_id.values)], set_labels = ('All', 'One Timers', 'Uniques', ), ax=ax)\n",
      "14/71: v.get_label_by_id('100')\n",
      "14/72: v.get_label_by_id('111')\n",
      "14/73: dir(v.get_label_by_id('111'))\n",
      "14/74: v.get_label_by_id('111').set_x(v.get_label_by_id('111')[0]+0.05)\n",
      "14/75: dir(v.get_label_by_id('111'))\n",
      "14/76: v.get_label_by_id('111').get_position\n",
      "14/77: v.get_label_by_id('111').get_position()\n",
      "14/78: v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.05)\n",
      "14/79: v\n",
      "14/80:\n",
      "fig, ax = plt.subplots(figsize=[15,15])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(one_timers.index), set(unique_users.from_id.values)], set_labels = ('All', 'One Timers', 'Uniques', ), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.05)\n",
      "14/81:\n",
      "fig, ax = plt.subplots(figsize=[15,15])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(one_timers.index), set(unique_users.from_id.values)], set_labels = ('All', 'One Timers', 'Uniques', ), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.25)\n",
      "14/82: dir(v)\n",
      "14/83: v.get_label_by_id('001')\n",
      "14/84: v.get_label_by_id('110')\n",
      "14/85: v.get_label_by_id('010')\n",
      "14/86: v.get_label_by_id('110')\n",
      "14/87:\n",
      "fig, ax = plt.subplots(figsize=[15,15])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(one_timers.index), set(unique_users.from_id.values)], set_labels = ('All', 'One Timers', 'Uniques', ), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.25)\n",
      "14/88: v.get_label_by_id('100')\n",
      "14/89: v.get_label_by_id('001')\n",
      "14/90: v.get_label_by_id('101')\n",
      "14/91:\n",
      "fig, ax = plt.subplots(figsize=[15,15])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(one_timers.index), set(unique_users.from_id.values)], set_labels = ('All', 'One Timers', 'Uniques', ), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]-0.1)\n",
      "14/92:\n",
      "fig, ax = plt.subplots(figsize=[15,15])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(one_timers.index), set(unique_users.from_id.values)], set_labels = ('All', 'One Timers', 'Uniques', ), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.25)\n",
      "14/93: dir(v)\n",
      "14/94: v.get_patch_by_id('111')\n",
      "14/95: dir(v.get_patch_by_id('111'))\n",
      "14/96: dir(v)\n",
      "14/97:\n",
      "fig, ax = plt.subplots(figsize=[15,15])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(one_timers.index), set(unique_users.from_id.values)], set_labels = ('All', 'One Timers', 'Uniques', ), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.25)\n",
      "v.get_label_by_id('111').set_text('')\n",
      "14/98:\n",
      "fig, ax = plt.subplots(figsize=[15,15])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(one_timers.index), set(unique_users.from_id.values)], set_labels = ('All', 'One Timers', 'Uniques', ), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.25)\n",
      "v.get_label_by_id('111').set_text(' ')\n",
      "14/99:\n",
      "fig, ax = plt.subplots(figsize=[15,15])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(one_timers.index), set(unique_users.from_id.values)], set_labels = ('All', 'One Timers', 'Uniques', ), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.25).set_text(' ')\n",
      "v.get_label_by_id('111')\n",
      "14/100:\n",
      "fig, ax = plt.subplots(figsize=[15,15])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(one_timers.index), set(unique_users.from_id.values)], set_labels = ('All', 'One Timers', 'Uniques', ), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.25)\n",
      "v.get_label_by_id('101').set_text(' ')\n",
      "14/101:\n",
      "fig, ax = plt.subplots(figsize=[15,15])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(one_timers.index), set(unique_users.from_id.values)], set_labels = ('All', 'One Timers', 'Uniques', ), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.25)\n",
      "v.get_label_by_id('A').set_text(' ')\n",
      "14/102:\n",
      "fig, ax = plt.subplots(figsize=[15,15])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(one_timers.index), set(unique_users.from_id.values)], set_labels = ('All', 'One Timers', 'Uniques', ), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.25)\n",
      "v.get_label_by_id('A').set_x(v.get_label_by_id('A').get_position()[0]+0.25)\n",
      "14/103:\n",
      "fig, ax = plt.subplots(figsize=[15,15])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(one_timers.index), set(unique_users.from_id.values)], set_labels = ('All', 'One Timers', 'Uniques', ), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.25)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]+0.25)\n",
      "14/104:\n",
      "fig, ax = plt.subplots(figsize=[15,15])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(one_timers.index), set(unique_users.from_id.values)], set_labels = ('All', 'One Timers', 'Uniques', ), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.25)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.25)\n",
      "14/105:\n",
      "fig, ax = plt.subplots(figsize=[15,15])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(one_timers.index), set(unique_users.from_id.values)], set_labels = ('All', 'One Timers', 'Uniques', ), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.25)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.20)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]-0.20)\n",
      "14/106:\n",
      "fig, ax = plt.subplots(figsize=[15,15])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(one_timers.index), set(unique_users.from_id.values)], set_labels = ('All', 'One Timers', 'Uniques', ), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.25)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.20)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.20)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]-0.20)\n",
      "14/107:\n",
      "fig, ax = plt.subplots(figsize=[15,15])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(one_timers.index), set(unique_users.from_id.values)], set_labels = ('All', 'One Timers', 'Uniques', ), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.25)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.20)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.20)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]-0.10)\n",
      "14/108:\n",
      "fig, ax = plt.subplots(figsize=[15,15])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(one_timers.index), set(unique_users.from_id.values)], set_labels = ('All', 'One Timers', 'Uniques', ), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.25)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.20)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.20)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]-0.03)\n",
      "14/109:\n",
      "fig, ax = plt.subplots(figsize=[15,15])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(one_timers.index), set(unique_users.from_id.values)], set_labels = ('All', 'One Timers', 'Uniques', ), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.25)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.20)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.20)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.03)\n",
      "14/110:\n",
      "fig, ax = plt.subplots(figsize=[15,15])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(one_timers.index), set(unique_users.from_id.values)], set_labels = ('All', 'One Timers', 'Uniques', ), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.25)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.20)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.20)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.07)\n",
      "14/111:\n",
      "fig, ax = plt.subplots(figsize=[15,15])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(one_timers.index), set(unique_users.from_id.values)], set_labels = ('All', 'One Timers', 'Uniques', ), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.25)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.20)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.20)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.09)\n",
      "14/112:\n",
      "fig, ax = plt.subplots(figsize=[15,15])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(one_timers.index), set(unique_users.from_id.values)], set_labels = ('All', 'One Timers', 'Uniques', ), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.25)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.20)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.20)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.09)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]-0.09)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.09)\n",
      "14/113:\n",
      "fig, ax = plt.subplots(figsize=[15,15])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(one_timers.index), set(unique_users.from_id.values)], set_labels = ('All', 'One Timers', 'Uniques', ), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.25)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.20)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.20)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.09)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]-0.09)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.19)\n",
      "14/114:\n",
      "fig, ax = plt.subplots(figsize=[15,15])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(one_timers.index), set(unique_users.from_id.values)], set_labels = ('All', 'One Timers', 'Uniques', ), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.25)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.20)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.20)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.09)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]-0.19)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.19)\n",
      "14/115: ax.get_figure().savefig('turf/venn_one_timers_uniques.png')\n",
      "14/116:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(one_timers.index), set(unique_users.from_id.values)], set_labels = ('All', 'One Timers', 'Uniques', ), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.25)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.20)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.20)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.09)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]-0.19)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.19)\n",
      "14/117: ax.get_figure().savefig('turf/venn_one_timers_uniques.png')\n",
      "14/118:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(base.from_id.values), set(campers.from_id.values)], set_labels = ('All', 'Basers', 'Campers'), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.25)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.20)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.20)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.09)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]-0.19)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.19)\n",
      "14/119:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(base.from_id.values), set(campers.from_id.values)], set_labels = ('All', 'Basers', 'Campers'), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.25)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.20)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.20)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.09)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]-0.19)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.19)\n",
      "14/120:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(base.from_id.values), set(campers.from_id.values)], set_labels = ('All', 'Basers', 'Campers'), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.25)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.20)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.20)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.09)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]-0.19)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.89)\n",
      "14/121:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(base.from_id.values), set(campers.from_id.values)], set_labels = ('All', 'Basers', 'Campers'), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.25)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.20)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.20)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.09)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]+0.19)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.09)\n",
      "14/122:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(base.from_id.values), set(campers.from_id.values)], set_labels = ('All', 'Basers', 'Campers'), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.25)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.20)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.20)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.09)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]+0.19)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.1)\n",
      "14/123:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(base.from_id.values), set(campers.from_id.values)], set_labels = ('All', 'Basers', 'Campers'), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.25)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.20)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.20)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.09)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]+0.25)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.1)\n",
      "14/124:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(base.from_id.values), set(campers.from_id.values)], set_labels = ('All', 'Basers', 'Campers'), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.15)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.20)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.20)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.09)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]+0.25)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.1)\n",
      "14/125:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(base.from_id.values), set(campers.from_id.values)], set_labels = ('All', 'Basers', 'Campers'), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.15)\n",
      "v.get_label_by_id('111').set_y(v.get_label_by_id('111').get_position()[1]-0.05)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.20)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.20)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.09)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]+0.25)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.1)\n",
      "14/126:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(base.from_id.values), set(campers.from_id.values)], set_labels = ('All', 'Basers', 'Campers'), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.15)\n",
      "v.get_label_by_id('111').set_y(v.get_label_by_id('111').get_position()[1]-0.05)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.20)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.30)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.09)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]+0.25)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.1)\n",
      "14/127:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(base.from_id.values), set(campers.from_id.values)], set_labels = ('All', 'Basers', 'Campers'), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.15)\n",
      "v.get_label_by_id('111').set_y(v.get_label_by_id('111').get_position()[1]-0.05)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.20)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.35)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.09)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]+0.25)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.1)\n",
      "14/128:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(base.from_id.values), set(campers.from_id.values)], set_labels = ('All', 'Basers', 'Campers'), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.15)\n",
      "v.get_label_by_id('111').set_y(v.get_label_by_id('111').get_position()[1]-0.05)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.20)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.35)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.15)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]+0.25)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.1)\n",
      "14/129: ax.get_figure().savefig('turf/venn_basers_campers.png')\n",
      "14/130:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(non_uniques.from_id.values), set(base.from_id.values), set(campers.from_id.values)], set_labels = ('Non-Uniques', 'Basers', 'Campers'), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.15)\n",
      "v.get_label_by_id('111').set_y(v.get_label_by_id('111').get_position()[1]-0.05)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.20)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.35)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.15)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]+0.25)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.1)\n",
      "14/131:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(non_uniques.from_id.values), set(base.from_id.values), set(campers.from_id.values)], set_labels = ('Non-Uniques', 'Basers', 'Campers'), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.15)\n",
      "v.get_label_by_id('111').set_y(v.get_label_by_id('111').get_position()[1]-0.05)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.05)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.35)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.15)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]+0.25)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.1)\n",
      "14/132:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(non_uniques.from_id.values), set(base.from_id.values), set(campers.from_id.values)], set_labels = ('Non-Uniques', 'Basers', 'Campers'), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.15)\n",
      "v.get_label_by_id('111').set_y(v.get_label_by_id('111').get_position()[1]-0.05)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.35)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.35)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.15)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]+0.25)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.1)\n",
      "14/133:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(non_uniques.from_id.values), set(base.from_id.values), set(campers.from_id.values)], set_labels = ('Non-Uniques', 'Basers', 'Campers'), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.15)\n",
      "v.get_label_by_id('111').set_y(v.get_label_by_id('111').get_position()[1]-0.05)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.35)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.35)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.15)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]+0.15)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.1)\n",
      "14/134:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(non_uniques.from_id.values), set(base.from_id.values), set(campers.from_id.values)], set_labels = ('Non-Uniques', 'Basers', 'Campers'), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.15)\n",
      "v.get_label_by_id('111').set_y(v.get_label_by_id('111').get_position()[1]-0.05)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.35)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.35)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.15)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]+0.05)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.1)\n",
      "14/135:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(non_uniques.from_id.values), set(base.from_id.values), set(campers.from_id.values)], set_labels = ('Non-Uniques', 'Basers', 'Campers'), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.15)\n",
      "v.get_label_by_id('111').set_y(v.get_label_by_id('111').get_position()[1]-0.05)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.35)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.35)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.05)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]+0.05)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.1)\n",
      "14/136:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(non_uniques.from_id.values), set(base.from_id.values), set(campers.from_id.values)], set_labels = ('Non-Uniques', 'Basers', 'Campers'), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.15)\n",
      "v.get_label_by_id('111').set_y(v.get_label_by_id('111').get_position()[1]-0.05)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.35)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.25)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.05)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]+0.05)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.1)\n",
      "14/137:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(non_uniques.from_id.values), set(base.from_id.values), set(campers.from_id.values)], set_labels = ('Non-Uniques', 'Basers', 'Campers'), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.15)\n",
      "v.get_label_by_id('111').set_y(v.get_label_by_id('111').get_position()[1]-0.05)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.35)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.25)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.15)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]+0.05)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.1)\n",
      "14/138: ax.get_figure().savefig('turf/venn_nonuniq_basers_campers.png')\n",
      "14/139: top_commented = list(st.groupby('name').id.sum().sort_values(ascending=False).index)\n",
      "14/140:\n",
      "top_10_commented = top_commented[:10]\n",
      "top_10_commented\n",
      "14/141: celebz =  non_uniques[non_uniques.groupby('from_id').name.transform(lambda x: x.isin(top_10_commented).all())]\n",
      "14/142: top_commented[:10]\n",
      "14/143: uniques.reindex(top_commented).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "14/144:\n",
      "only_bibi = mat.loc[:, top_commented[1:]].isna().all(axis=1)\n",
      "only_bibi[only_bibi].shape\n",
      "14/145:\n",
      "def get_accum_df(mat, order):\n",
      "    onlys = []\n",
      "\n",
      "    for i, name in enumerate(order):\n",
      "        only = mat.loc[:, order[i+1:]].isna().all(axis=1)\n",
      "        onlys.append(only[only].shape[0])\n",
      "        \n",
      "    user_accum_df = pd.DataFrame({'mk': order, 'user_accum': onlys})\n",
      "    user_accum_df['party'] = user_accum_df['mk'].map(mkdet.set_index('folder_name')['party'])\n",
      "    user_accum_df['camp'] = user_accum_df['party'].map(camps)\n",
      "    user_accum_df['delta'] = user_accum_df.user_accum.diff()\n",
      "    unique_delta_ratio = (uniques.reindex(top_commented) / user_accum_df.set_index('mk').delta).drop('Benjamin_Netanyahu')\n",
      "    user_accum_df['unique_delta_ratio'] = user_accum_df['mk'].map(unique_delta_ratio)\n",
      "    user_accum_df['delta_n'] = user_accum_df['delta'] / user_accum_df['delta'].max()\n",
      "    \n",
      "    return user_accum_df\n",
      "14/146:\n",
      "user_accum_df = get_accum_df(mat, top_commented)\n",
      "user_accum_df.head()\n",
      "14/147: user_accum_df.set_index('mk').user_accum.plot(kind='bar', figsize=(15,10), rot=75)\n",
      "14/148:\n",
      "import altair as alt\n",
      "alt.renderers.enable('notebook')\n",
      "14/149:\n",
      "def chart_user_accum(accum_df, order, order_name):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('user_accum',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'user_accum']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = f'User accumulation by {order_name} order - left to right, only users that comment inside the group until the current column')\n",
      "\n",
      "\n",
      "def chart_user_accum_delta(accum_df, order, order_name):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('delta',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'delta']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = f'User accumulation delta by {order_name} order - left to right, only users that comment inside the group until the current column')\n",
      "\n",
      "\n",
      "def chart_user_accum_unique_delta_ratio(accum_df, order, order_name):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('unique_delta_ratio',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'unique_delta_ratio']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = f'Page unique count divided by user_accum delta by {order_name} order (to measure how much a page is part of the group)')\n",
      "14/150:\n",
      "(chart_user_accum(user_accum_df, top_commented, 'Top Commented') \n",
      " & chart_user_accum_delta(user_accum_df, top_commented, 'Top Commented')\n",
      " & chart_user_accum_unique_delta_ratio(user_accum_df, top_commented, 'Top Commented')).properties(title='Top Commented')\n",
      "14/151: user_accum_df[['delta_n', 'unique_delta_ratio']].div(user_accum_df[['delta_n', 'unique_delta_ratio']].sum(axis=1), axis=0).plot(kind='bar', stacked=True, figsize=(15,10), rot=75, width=1.0)\n",
      "14/152:\n",
      "print ('First celebiez group: ', top_commented[:6])\n",
      "print ('Second celebiez group: ', top_commented[:15])\n",
      "14/153: top_mean_commented = list(df.loc[df.is_post==1].groupby('name').comment_count.mean().sort_values(ascending=False).index)\n",
      "14/154: uniques.reindex(top_mean_commented).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "14/155: mean_com_accum_df = get_accum_df(mat, top_mean_commented)\n",
      "14/156:\n",
      "(chart_user_accum(mean_com_accum_df, top_mean_commented, 'Top Mean Commented') \n",
      " & chart_user_accum_delta(mean_com_accum_df, top_mean_commented, 'Top Mean Commented') \n",
      " & chart_user_accum_unique_delta_ratio(mean_com_accum_df, top_mean_commented, 'Top Mean Commented')).properties(title='Top Mean Commented')\n",
      "14/157: top_liked = list(df.loc[df.is_post==1].groupby('name').like_count.mean().sort_values(ascending=False).index)\n",
      "14/158: uniques.reindex(top_liked).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "14/159: like_accum_df = get_accum_df(mat, top_liked)\n",
      "14/160:\n",
      "(chart_user_accum(like_accum_df, top_liked, 'Top Liked') \n",
      " & chart_user_accum_delta(like_accum_df, top_liked, 'Top Liked') \n",
      " & chart_user_accum_unique_delta_ratio(like_accum_df, top_liked, 'Top Liked')).properties(title='Top Liked')\n",
      "14/161: top_shared = list(df.loc[df.is_post==1].groupby('name').share_count.mean().sort_values(ascending=False).index)\n",
      "14/162: uniques.reindex(top_shared).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "14/163:\n",
      "share_accum_df = get_accum_df(mat, top_shared)\n",
      "share_accum_df.head()\n",
      "14/164:\n",
      "(chart_user_accum(share_accum_df, top_shared, 'Top Shared') \n",
      " & chart_user_accum_delta(share_accum_df, top_shared, 'Top Shared') \n",
      " & chart_user_accum_unique_delta_ratio(share_accum_df, top_shared, 'Top Shared')).properties(title='Top Shared')\n",
      "14/165: #### Top uniques\n",
      "15/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "15/2:\n",
      "%%time\n",
      "df = pd.read_pickle('turf/data_df.pkl.gz', compression='gzip')\n",
      "15/3: df.dtypes\n",
      "15/4:\n",
      "sdf = pd.read_pickle('turf/shared_df.pkl.gz', compression='gzip')\n",
      "sdf.dtypes\n",
      "15/5: st = pd.read_pickle('turf/total_stats_no_multiindex.pkl.gz', compression='gzip')\n",
      "15/6:\n",
      "# OLD STUFF\n",
      "#save(st, 'turf/total_stats.pkl.gz')\n",
      "#st = pd.read_pickle( 'turf/total_stats.pkl.gz', compression='gzip')\n",
      "15/7: st.head()\n",
      "15/8: st.shape\n",
      "15/9: df[df.is_post==0].shape\n",
      "15/10: print ('Total number of comments: %d' % st.id.sum())\n",
      "15/11: print ('Total number of distinct users (commenters): %d' % st.from_id.nunique())\n",
      "15/12:\n",
      "counts = st.groupby('from_id').id.sum()\n",
      "one_timers = counts[counts==1]\n",
      "\n",
      "print ('Total number of one-timers: %d' % one_timers.shape[0])\n",
      "print ('Ratio of one timers from all users: %.2f' % (one_timers.shape[0] / st.from_id.nunique()))\n",
      "15/13: mat = st.pivot(index='from_id', columns='name', values='id')\n",
      "15/14: mat[(mat.count(axis=1)==1)].shape[0]\n",
      "15/15: mat.loc[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu==1), 'Benjamin_Netanyahu'].shape[0]\n",
      "15/16:\n",
      "print ('Ratio of Bibi from one-timers: %.2f' % (mat[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu==1)].shape[0] \n",
      " / counts[counts==1].shape[0]))\n",
      "15/17: print ('Ratio of users that commented 10 times and up: %.2f' % (counts[counts>=10].shape[0]/st.from_id.nunique()))\n",
      "15/18: (st.reset_index().groupby('from_id').name.nunique().value_counts().head(20)/st.reset_index().from_id.nunique()).plot(kind='bar')\n",
      "15/19:\n",
      "unique_users = (st.reset_index()\n",
      " .loc[st.groupby('from_id').name.transform('nunique')==1, ['from_id', 'name', 'party']])\n",
      "15/20: uniques = ( unique_users.name.value_counts())\n",
      "15/21: uniques.head(10)\n",
      "15/22:\n",
      "print ('Total number of uniques: %d' % uniques.sum())\n",
      "print ('Ratio of uniques from all users: %.2f' % (uniques.sum() / st.from_id.nunique()))\n",
      "print ('Ratio of one-timers from uniques: %.2f' % (counts[counts==1].shape[0] / uniques.sum()))\n",
      "15/23:\n",
      "(pd.concat([uniques, \n",
      "            uniques/st.from_id.nunique(), \n",
      "            uniques/st.groupby('name').from_id.nunique()],\n",
      "           axis=1, keys=['num_uniques', 'ratio_from_all_users', 'ratio_from_own_users'])\n",
      " .sort_values(by='ratio_from_own_users', ascending=False)).head(10)\n",
      "15/24:\n",
      "mkdet = pd.read_csv('turf/mk_details.csv', sep='\\t')\n",
      "mkdet.head()\n",
      "15/25: df['party'] = df['name'].map(mkdet.set_index('folder_name')['party'])\n",
      "15/26: df.groupby(['party', 'langdetect']).size().unstack()\n",
      "15/27: st['party'] = st['name'].map(mkdet.set_index('folder_name')['party'])\n",
      "15/28:\n",
      "non_uniques = (st.reset_index()\n",
      " .loc[st.groupby('from_id').name.transform('nunique')>1, ['from_id', 'name', 'party']])\n",
      "15/29: non_uniques.head()\n",
      "15/30: non_uniques.from_id.nunique()\n",
      "15/31: non_uniques.party.value_counts()\n",
      "15/32: non_uniques.groupby('party').from_id.nunique()\n",
      "15/33: base = non_uniques[non_uniques.groupby('from_id').party.transform('nunique')==1]\n",
      "15/34:\n",
      "print ('Total number of basers: %d' % base.from_id.nunique())\n",
      "print ('Ratio of basers from all users: %.2f' % (base.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of basers from non-uniques: %.2f' % (base.from_id.nunique() / non_uniques.from_id.nunique()))\n",
      "15/35: st.party.unique()\n",
      "15/36:\n",
      "camps = { 'מרצ': 'opposition',\n",
      "          'יש עתיד': 'opposition',\n",
      "          'ליכוד': 'coalition',\n",
      "          'הבית היהודי בראשות נפתלי בנט': 'coalition',\n",
      "          'המחנה הציוני': 'opposition',\n",
      "          'הרשימה המשותפת (חד”ש, רע”מ, בל”ד, תע”ל)': 'opposition',\n",
      "          'כולנו בראשות משה כחלון': 'coalition',\n",
      "          'ישראל ביתנו': 'coalition' }\n",
      "15/37: st['camp'] = st['party'].map(camps)\n",
      "15/38: non_uniques['camp'] = non_uniques['party'].map(camps)\n",
      "15/39: non_uniques.groupby('camp').from_id.nunique()\n",
      "15/40: campers = non_uniques[non_uniques.groupby('from_id').camp.transform('nunique')==1]\n",
      "15/41:\n",
      "print ('Total number of campers: %d' % campers.from_id.nunique())\n",
      "print ('Ratio of campers from all users: %.2f' % (campers.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of campers from non-uniques: %.2f' % (campers.from_id.nunique() / non_uniques.from_id.nunique()))\n",
      "print ('Coalition | Opposition ratio from campers: %.2f | %.2f' % \n",
      "       ((campers[campers.camp=='coalition'].from_id.nunique() / campers.from_id.nunique()),\n",
      "       (campers[campers.camp=='opposition'].from_id.nunique() / campers.from_id.nunique())))\n",
      "cg = st.groupby('camp')\n",
      "print ('Coalition | Opposition ratios from total users: %.2f | %.2f' % \n",
      "       ((cg.from_id.nunique()['coalition'] / st.from_id.nunique()),\n",
      "       ((cg.from_id.nunique()['opposition'] / st.from_id.nunique()))))\n",
      "\n",
      "print ('Coalition | Opposition total comments: %.2f | %.2f' % \n",
      "       ((cg.id.sum()['coalition'] / st.id.sum()),\n",
      "       ((cg.id.sum()['opposition'] / st.id.sum()))))\n",
      "15/42: top_commented = list(st.groupby('name').id.sum().sort_values(ascending=False).index)\n",
      "15/43:\n",
      "top_10_commented = top_commented[:10]\n",
      "top_10_commented\n",
      "15/44: celebz =  non_uniques[non_uniques.groupby('from_id').name.transform(lambda x: x.isin(top_10_commented).all())]\n",
      "15/45: celebz.head()\n",
      "15/46: num_celebiez_no_base_camp = len(set(celebz.from_id.drop_duplicates().values).difference(set(campers.from_id.drop_duplicates().values)).difference(set(base.from_id.drop_duplicates().values)))\n",
      "15/47:\n",
      "print ('Total number of celeb followers (celebies): %d' % celebz.from_id.nunique())\n",
      "print ('Ratio of celebies from all users: %.2f' % (celebz.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of celebies from non-uniques: %.2f' % (celebz.from_id.nunique() / non_uniques.from_id.nunique()))\n",
      "print ('Remove campers and base from celebies: %d' % num_celebiez_no_base_camp)\n",
      "15/48: top_commented[:10]\n",
      "15/49: uniques.reindex(top_commented).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "15/50:\n",
      "only_bibi = mat.loc[:, top_commented[1:]].isna().all(axis=1)\n",
      "only_bibi[only_bibi].shape\n",
      "15/51:\n",
      "def get_accum_df(mat, order):\n",
      "    onlys = []\n",
      "\n",
      "    for i, name in enumerate(order):\n",
      "        only = mat.loc[:, order[i+1:]].isna().all(axis=1)\n",
      "        onlys.append(only[only].shape[0])\n",
      "        \n",
      "    user_accum_df = pd.DataFrame({'mk': order, 'user_accum': onlys})\n",
      "    user_accum_df['party'] = user_accum_df['mk'].map(mkdet.set_index('folder_name')['party'])\n",
      "    user_accum_df['camp'] = user_accum_df['party'].map(camps)\n",
      "    user_accum_df['delta'] = user_accum_df.user_accum.diff()\n",
      "    unique_delta_ratio = (uniques.reindex(top_commented) / user_accum_df.set_index('mk').delta).drop('Benjamin_Netanyahu')\n",
      "    user_accum_df['unique_delta_ratio'] = user_accum_df['mk'].map(unique_delta_ratio)\n",
      "    user_accum_df['delta_n'] = user_accum_df['delta'] / user_accum_df['delta'].max()\n",
      "    \n",
      "    return user_accum_df\n",
      "15/52:\n",
      "user_accum_df = get_accum_df(mat, top_commented)\n",
      "user_accum_df.head()\n",
      "15/53: user_accum_df.set_index('mk').user_accum.plot(kind='bar', figsize=(15,10), rot=75)\n",
      "15/54:\n",
      "import altair as alt\n",
      "alt.renderers.enable('notebook')\n",
      "15/55:\n",
      "def chart_user_accum(accum_df, order, order_name):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('user_accum',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'user_accum']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = f'User accumulation by {order_name} order - left to right, only users that comment inside the group until the current column')\n",
      "\n",
      "\n",
      "def chart_user_accum_delta(accum_df, order, order_name):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('delta',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'delta']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = f'User accumulation delta by {order_name} order - left to right, only users that comment inside the group until the current column')\n",
      "\n",
      "\n",
      "def chart_user_accum_unique_delta_ratio(accum_df, order, order_name):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('unique_delta_ratio',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'unique_delta_ratio']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = f'Page unique count divided by user_accum delta by {order_name} order (to measure how much a page is part of the group)')\n",
      "15/56:\n",
      "(chart_user_accum(user_accum_df, top_commented, 'Top Commented') \n",
      " & chart_user_accum_delta(user_accum_df, top_commented, 'Top Commented')\n",
      " & chart_user_accum_unique_delta_ratio(user_accum_df, top_commented, 'Top Commented')).properties(title='Top Commented')\n",
      "15/57: user_accum_df[['delta_n', 'unique_delta_ratio']].div(user_accum_df[['delta_n', 'unique_delta_ratio']].sum(axis=1), axis=0).plot(kind='bar', stacked=True, figsize=(15,10), rot=75, width=1.0)\n",
      "15/58:\n",
      "print ('First celebiez group: ', top_commented[:6])\n",
      "print ('Second celebiez group: ', top_commented[:15])\n",
      "15/59: top_mean_commented = list(df.loc[df.is_post==1].groupby('name').comment_count.mean().sort_values(ascending=False).index)\n",
      "15/60: uniques.reindex(top_mean_commented).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "15/61: mean_com_accum_df = get_accum_df(mat, top_mean_commented)\n",
      "15/62:\n",
      "(chart_user_accum(mean_com_accum_df, top_mean_commented, 'Top Mean Commented') \n",
      " & chart_user_accum_delta(mean_com_accum_df, top_mean_commented, 'Top Mean Commented') \n",
      " & chart_user_accum_unique_delta_ratio(mean_com_accum_df, top_mean_commented, 'Top Mean Commented')).properties(title='Top Mean Commented')\n",
      "15/63: top_liked = list(df.loc[df.is_post==1].groupby('name').like_count.mean().sort_values(ascending=False).index)\n",
      "15/64: uniques.reindex(top_liked).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "15/65: like_accum_df = get_accum_df(mat, top_liked)\n",
      "15/66:\n",
      "(chart_user_accum(like_accum_df, top_liked, 'Top Liked') \n",
      " & chart_user_accum_delta(like_accum_df, top_liked, 'Top Liked') \n",
      " & chart_user_accum_unique_delta_ratio(like_accum_df, top_liked, 'Top Liked')).properties(title='Top Liked')\n",
      "15/67: top_shared = list(df.loc[df.is_post==1].groupby('name').share_count.mean().sort_values(ascending=False).index)\n",
      "15/68: uniques.reindex(top_shared).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "15/69:\n",
      "share_accum_df = get_accum_df(mat, top_shared)\n",
      "share_accum_df.head()\n",
      "15/70:\n",
      "(chart_user_accum(share_accum_df, top_shared, 'Top Shared') \n",
      " & chart_user_accum_delta(share_accum_df, top_shared, 'Top Shared') \n",
      " & chart_user_accum_unique_delta_ratio(share_accum_df, top_shared, 'Top Shared')).properties(title='Top Shared')\n",
      "15/71:\n",
      "labels = ('All', 'One Timers', 'Uniques', 'Non-Uniques', 'Basers', 'Campers')\n",
      "sets = [set(st.from_id.values), set(one_timers.index), set(unique_users.from_id.values), set(non_uniques.from_id.values), set(base.from_id.values), set(campers.from_id.values)]\n",
      "\n",
      "set_dict = dict(zip(labels, sets))\n",
      "15/72:\n",
      "inter_set_dict = {}\n",
      "for i in set_dict:\n",
      "    inter_set_dict[i] = {}\n",
      "    for j in set_dict:\n",
      "        inter_set_dict[i][j] = len(set_dict[i].intersection(set_dict[j]))\n",
      "15/73: pd.DataFrame(inter_set_dict).reindex(labels)\n",
      "15/74: pd.DataFrame(inter_set_dict).reindex(labels).to_csv('turf/group_intersections.csv')\n",
      "15/75: from matplotlib_venn import venn3\n",
      "15/76:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(one_timers.index), set(unique_users.from_id.values)], set_labels = ('All', 'One Timers', 'Uniques', ), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.25)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.20)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.20)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.09)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]-0.19)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.19)\n",
      "15/77: ax.get_figure().savefig('turf/venn_one_timers_uniques.png')\n",
      "15/78:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(base.from_id.values), set(campers.from_id.values)], set_labels = ('All', 'Basers', 'Campers'), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.15)\n",
      "v.get_label_by_id('111').set_y(v.get_label_by_id('111').get_position()[1]-0.05)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.20)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.35)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.15)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]+0.25)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.1)\n",
      "15/79: ax.get_figure().savefig('turf/venn_basers_campers.png')\n",
      "15/80:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(non_uniques.from_id.values), set(base.from_id.values), set(campers.from_id.values)], set_labels = ('Non-Uniques', 'Basers', 'Campers'), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.15)\n",
      "v.get_label_by_id('111').set_y(v.get_label_by_id('111').get_position()[1]-0.05)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.35)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.25)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.15)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]+0.05)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.1)\n",
      "15/81: ax.get_figure().savefig('turf/venn_nonuniq_basers_campers.png')\n",
      "15/82:\n",
      "from_id_lang = df.groupby(['from_id', 'langdetect']).size().unstack()[['he', 'ar', 'en', 'ru', 'fr', 'de', 'es']].sort_values(by='he', ascending=False)\n",
      "from_id_lang.head()\n",
      "15/83: like_accum_df = get_accum_df(mat, top_liked)\n",
      "15/84:\n",
      "(chart_user_accum(like_accum_df, top_liked, 'Top Liked') \n",
      " & chart_user_accum_delta(like_accum_df, top_liked, 'Top Liked') \n",
      " & chart_user_accum_unique_delta_ratio(like_accum_df, top_liked, 'Top Liked')).properties(title='Top Liked')\n",
      "15/85: top_shared = list(df.loc[df.is_post==1].groupby('name').share_count.mean().sort_values(ascending=False).index)\n",
      "15/86: uniques.reindex(top_shared).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "15/87:\n",
      "share_accum_df = get_accum_df(mat, top_shared)\n",
      "share_accum_df.head()\n",
      "15/88:\n",
      "(chart_user_accum(share_accum_df, top_shared, 'Top Shared') \n",
      " & chart_user_accum_delta(share_accum_df, top_shared, 'Top Shared') \n",
      " & chart_user_accum_unique_delta_ratio(share_accum_df, top_shared, 'Top Shared')).properties(title='Top Shared')\n",
      "15/89:\n",
      "labels = ('All', 'One Timers', 'Uniques', 'Non-Uniques', 'Basers', 'Campers')\n",
      "sets = [set(st.from_id.values), set(one_timers.index), set(unique_users.from_id.values), set(non_uniques.from_id.values), set(base.from_id.values), set(campers.from_id.values)]\n",
      "\n",
      "set_dict = dict(zip(labels, sets))\n",
      "15/90:\n",
      "inter_set_dict = {}\n",
      "for i in set_dict:\n",
      "    inter_set_dict[i] = {}\n",
      "    for j in set_dict:\n",
      "        inter_set_dict[i][j] = len(set_dict[i].intersection(set_dict[j]))\n",
      "15/91: pd.DataFrame(inter_set_dict).reindex(labels)\n",
      "15/92: pd.DataFrame(inter_set_dict).reindex(labels).to_csv('turf/group_intersections.csv')\n",
      "15/93: from matplotlib_venn import venn3\n",
      "15/94:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(one_timers.index), set(unique_users.from_id.values)], set_labels = ('All', 'One Timers', 'Uniques', ), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.25)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.20)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.20)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.09)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]-0.19)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.19)\n",
      "15/95: ax.get_figure().savefig('turf/venn_one_timers_uniques.png')\n",
      "15/96:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(base.from_id.values), set(campers.from_id.values)], set_labels = ('All', 'Basers', 'Campers'), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.15)\n",
      "v.get_label_by_id('111').set_y(v.get_label_by_id('111').get_position()[1]-0.05)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.20)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.35)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.15)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]+0.25)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.1)\n",
      "15/97: ax.get_figure().savefig('turf/venn_basers_campers.png')\n",
      "15/98:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(non_uniques.from_id.values), set(base.from_id.values), set(campers.from_id.values)], set_labels = ('Non-Uniques', 'Basers', 'Campers'), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.15)\n",
      "v.get_label_by_id('111').set_y(v.get_label_by_id('111').get_position()[1]-0.05)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.35)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.25)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.15)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]+0.05)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.1)\n",
      "15/99: ax.get_figure().savefig('turf/venn_nonuniq_basers_campers.png')\n",
      "15/100:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(base.from_id.values), set(campers.from_id.values)], set_labels = ('All', 'Basers', 'Campers'), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.15)\n",
      "v.get_label_by_id('111').set_y(v.get_label_by_id('111').get_position()[1]-0.05)\n",
      "v.get_patch_by_id('101').set_color('blue')\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.20)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.35)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.15)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]+0.25)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.1)\n",
      "15/101:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(base.from_id.values), set(campers.from_id.values)], set_labels = ('All', 'Basers', 'Campers'), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.15)\n",
      "v.get_label_by_id('111').set_y(v.get_label_by_id('111').get_position()[1]-0.05)\n",
      "v.get_patch_by_id('101').set_color('blue')\n",
      "v.get_patch_by_id('111').set_color('green')\n",
      "\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.20)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.35)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.15)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]+0.25)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.1)\n",
      "15/102:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(non_uniques.from_id.values), set(base.from_id.values), set(campers.from_id.values)], set_labels = ('Non-Uniques', 'Basers', 'Campers'), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.15)\n",
      "v.get_label_by_id('111').set_y(v.get_label_by_id('111').get_position()[1]-0.05)\n",
      "v.get_patch_by_id('101').set_color('blue')\n",
      "v.get_patch_by_id('111').set_color('green')\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.35)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.25)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.15)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]+0.05)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.1)\n",
      "15/103: st[st.from_id.isin(base.from_id)].groupby('from_id').id.sum()\n",
      "15/104: st[st.from_id.isin(base.from_id)].groupby('from_id').id.sum().mean()\n",
      "15/105: st[st.from_id.isin(campers.from_id)].groupby('from_id').id.sum().mean()\n",
      "15/106: st[st.from_id.isin(unique_users.from_id)].groupby('from_id').id.sum().mean()\n",
      "15/107: (st.reset_index().groupby('from_id').name.nunique().value_counts().head(20)/st.reset_index().from_id.nunique()).plot(kind='bar')\n",
      "15/108:\n",
      "fans_users = (st.reset_index()\n",
      " .loc[st.groupby('from_id').name.transform('nunique')==1, ['from_id', 'name', 'party']])\n",
      "15/109: fans = ( fans_users.name.value_counts())\n",
      "15/110: fans.head(10)\n",
      "15/111:\n",
      "print ('Total number of uniques: %d' % fans.sum())\n",
      "print ('Ratio of fans from all users: %.2f' % (fans.sum() / st.from_id.nunique()))\n",
      "print ('Ratio of one-timers from fans: %.2f' % (counts[counts==1].shape[0] / fans.sum()))\n",
      "15/112:\n",
      "print ('Total number of fans: %d' % fans.sum())\n",
      "print ('Ratio of fans from all users: %.2f' % (fans.sum() / st.from_id.nunique()))\n",
      "print ('Ratio of one-timers from fans: %.2f' % (counts[counts==1].shape[0] / fans.sum()))\n",
      "15/113:\n",
      "(pd.concat([fans, \n",
      "            fans/st.from_id.nunique(), \n",
      "            fans/st.groupby('name').from_id.nunique()],\n",
      "           axis=1, keys=['num_uniques', 'ratio_from_all_users', 'ratio_from_own_users'])\n",
      " .sort_values(by='ratio_from_own_users', ascending=False)).head(10)\n",
      "15/114:\n",
      "two_mks_or_more = (st.reset_index()\n",
      " .loc[st.groupby('from_id').name.transform('nunique')>1, ['from_id', 'name', 'party']])\n",
      "15/115: two_mks_or_more.head()\n",
      "15/116: two_mks_or_more.from_id.nunique()\n",
      "15/117: two_mks_or_more.party.value_counts()\n",
      "15/118: two_mks_or_more.groupby('party').from_id.nunique()\n",
      "15/119: base = two_mks_or_more[two_mks_or_more.groupby('from_id').party.transform('nunique')==1]\n",
      "15/120:\n",
      "print ('Total number of fans: %d' % fans.sum())\n",
      "print ('Ratio of uniques from all users: %.2f' % (fans.sum() / st.from_id.nunique()))\n",
      "print ('Ratio of one-timers from fans: %.2f' % (counts[counts==1].shape[0] / fans.sum()))\n",
      "15/121: (st.reset_index().groupby('from_id').id.sum().value_counts().head(20)/st.reset_index().from_id.nunique()).plot(kind='bar')\n",
      "15/122:\n",
      "fans_users = (st.reset_index()\n",
      " .loc[st.groupby('from_id').name.transform('nunique')==1, ['from_id', 'name', 'party']])\n",
      "15/123: fans = ( fans_users.name.value_counts())\n",
      "15/124: uniques.head(10)\n",
      "15/125: fans.head(10)\n",
      "15/126:\n",
      "print ('Total number of fans: %d' % fans.sum())\n",
      "print ('Ratio of fans from all users: %.2f' % (fans.sum() / st.from_id.nunique()))\n",
      "print ('Ratio of one-timers from fans: %.2f' % (counts[counts==1].shape[0] / fans.sum()))\n",
      "15/127:\n",
      "(pd.concat([fans, \n",
      "            fans/st.from_id.nunique(), \n",
      "            fans/st.groupby('name').from_id.nunique()],\n",
      "            axis=1, keys=['num_uniques', 'ratio_from_all_users', 'ratio_from_own_users'])\n",
      "             .sort_values(by='ratio_from_own_users', ascending=False)).head(10)\n",
      "15/128:\n",
      "two_mks_plus = (st.reset_index()\n",
      " .loc[st.groupby('from_id').name.transform('nunique')>1, ['from_id', 'name', 'party']])\n",
      "15/129: two_mks_plus.head()\n",
      "15/130: base = two_mks_plus[two_mks_plus.groupby('from_id').party.transform('nunique')==1]\n",
      "15/131:\n",
      "print ('Total number of basers: %d' % base.from_id.nunique())\n",
      "print ('Ratio of basers from all users: %.2f' % (base.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of basers from two-mks-plus: %.2f' % (base.from_id.nunique() / two_mks_plus.from_id.nunique()))\n",
      "15/132: two_mks_plus['camp'] = two_mks_plus['party'].map(camps)\n",
      "15/133: non_uniques.groupby('camp').from_id.nunique()\n",
      "15/134: campers = two_mks_plus[two_mks_plus.groupby('from_id').camp.transform('nunique')==1]\n",
      "15/135: two_mks_plus|.groupby('camp').from_id.nunique()\n",
      "15/136: two_mks_plus.groupby('camp').from_id.nunique()\n",
      "15/137: campers = two_mks_plus[two_mks_plus.groupby('from_id').camp.transform('nunique')==1]\n",
      "15/138:\n",
      "print ('Total number of campers: %d' % campers.from_id.nunique())\n",
      "print ('Ratio of campers from all users: %.2f' % (campers.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of campers from two_mks_plus: %.2f' % (campers.from_id.nunique() / two_mks_plus.from_id.nunique()))\n",
      "print ('Coalition | Opposition ratio from campers: %.2f | %.2f' % \n",
      "       ((campers[campers.camp=='coalition'].from_id.nunique() / campers.from_id.nunique()),\n",
      "       (campers[campers.camp=='opposition'].from_id.nunique() / campers.from_id.nunique())))\n",
      "cg = st.groupby('camp')\n",
      "print ('Coalition | Opposition ratios from total users: %.2f | %.2f' % \n",
      "       ((cg.from_id.nunique()['coalition'] / st.from_id.nunique()),\n",
      "       ((cg.from_id.nunique()['opposition'] / st.from_id.nunique()))))\n",
      "\n",
      "print ('Coalition | Opposition total comments: %.2f | %.2f' % \n",
      "       ((cg.id.sum()['coalition'] / st.id.sum()),\n",
      "       ((cg.id.sum()['opposition'] / st.id.sum()))))\n",
      "15/139: top_commented = list(st.groupby('name').id.sum().sort_values(ascending=False).index)\n",
      "15/140:\n",
      "top_10_commented = top_commented[:10]\n",
      "top_10_commented\n",
      "15/141: celebz =  two_mks_plus[two_mks_plus.groupby('from_id').name.transform(lambda x: x.isin(top_10_commented).all())]\n",
      "15/142: celebz.head()\n",
      "15/143: num_celebiez_no_base_camp = len(set(celebz.from_id.drop_duplicates().values).difference(set(campers.from_id.drop_duplicates().values)).difference(set(base.from_id.drop_duplicates().values)))\n",
      "15/144:\n",
      "print ('Total number of celeb followers (celebies): %d' % celebz.from_id.nunique())\n",
      "print ('Ratio of celebies from all users: %.2f' % (celebz.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of celebies from two_mks_plus: %.2f' % (celebz.from_id.nunique() / two_mks_plus.from_id.nunique()))\n",
      "print ('Remove campers and base from celebies: %d' % num_celebiez_no_base_camp)\n",
      "15/145: fans.reindex(top_commented).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "15/146:\n",
      "only_bibi = mat.loc[:, top_commented[1:]].isna().all(axis=1)\n",
      "only_bibi[only_bibi].shape\n",
      "15/147:\n",
      "def get_accum_df(mat, order):\n",
      "    onlys = []\n",
      "\n",
      "    for i, name in enumerate(order):\n",
      "        only = mat.loc[:, order[i+1:]].isna().all(axis=1)\n",
      "        onlys.append(only[only].shape[0])\n",
      "        \n",
      "    user_accum_df = pd.DataFrame({'mk': order, 'user_accum': onlys})\n",
      "    user_accum_df['party'] = user_accum_df['mk'].map(mkdet.set_index('folder_name')['party'])\n",
      "    user_accum_df['camp'] = user_accum_df['party'].map(camps)\n",
      "    user_accum_df['delta'] = user_accum_df.user_accum.diff()\n",
      "    unique_delta_ratio = (uniques.reindex(top_commented) / user_accum_df.set_index('mk').delta).drop('Benjamin_Netanyahu')\n",
      "    user_accum_df['unique_delta_ratio'] = user_accum_df['mk'].map(unique_delta_ratio)\n",
      "    user_accum_df['delta_n'] = user_accum_df['delta'] / user_accum_df['delta'].max()\n",
      "    \n",
      "    return user_accum_df\n",
      "15/148:\n",
      "user_accum_df = get_accum_df(mat, top_commented)\n",
      "user_accum_df.head()\n",
      "15/149: user_accum_df.set_index('mk').user_accum.plot(kind='bar', figsize=(15,10), rot=75)\n",
      "15/150:\n",
      "import altair as alt\n",
      "alt.renderers.enable('notebook')\n",
      "15/151:\n",
      "def chart_user_accum(accum_df, order, order_name):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('user_accum',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'user_accum']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = f'User accumulation by {order_name} order - left to right, only users that comment inside the group until the current column')\n",
      "\n",
      "\n",
      "def chart_user_accum_delta(accum_df, order, order_name):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('delta',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'delta']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = f'User accumulation delta by {order_name} order - left to right, only users that comment inside the group until the current column')\n",
      "\n",
      "\n",
      "def chart_user_accum_unique_delta_ratio(accum_df, order, order_name):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('unique_delta_ratio',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'unique_delta_ratio']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = f'Page unique count divided by user_accum delta by {order_name} order (to measure how much a page is part of the group)')\n",
      "15/152:\n",
      "(chart_user_accum(user_accum_df, top_commented, 'Top Commented') \n",
      " & chart_user_accum_delta(user_accum_df, top_commented, 'Top Commented')\n",
      " & chart_user_accum_unique_delta_ratio(user_accum_df, top_commented, 'Top Commented')).properties(title='Top Commented')\n",
      "15/153: user_accum_df[['delta_n', 'unique_delta_ratio']].div(user_accum_df[['delta_n', 'unique_delta_ratio']].sum(axis=1), axis=0).plot(kind='bar', stacked=True, figsize=(15,10), rot=75, width=1.0)\n",
      "15/154:\n",
      "print ('First celebiez group: ', top_commented[:6])\n",
      "print ('Second celebiez group: ', top_commented[:15])\n",
      "15/155:\n",
      "labels = ('All', 'One Timers', 'Fans', 'two_mks_plus', 'Basers', 'Campers')\n",
      "sets = [set(st.from_id.values), set(one_timers.index), set(unique_users.from_id.values), set(non_uniques.from_id.values), set(base.from_id.values), set(campers.from_id.values)]\n",
      "\n",
      "set_dict = dict(zip(labels, sets))\n",
      "15/156:\n",
      "inter_set_dict = {}\n",
      "for i in set_dict:\n",
      "    inter_set_dict[i] = {}\n",
      "    for j in set_dict:\n",
      "        inter_set_dict[i][j] = len(set_dict[i].intersection(set_dict[j]))\n",
      "15/157: pd.DataFrame(inter_set_dict).reindex(labels)\n",
      "15/158:\n",
      "labels = ('All', 'One Timers', 'Fans', 'two_mks_plus', 'Basers', 'Campers')\n",
      "sets = [set(st.from_id.values), set(one_timers.index), set(unique_users.from_id.values), set(non_uniques.from_id.values), set(base.from_id.values), set(campers.from_id.values)]\n",
      "\n",
      "set_dict = dict(zip(labels, sets))\n",
      "15/159:\n",
      "inter_set_dict = {}\n",
      "for i in set_dict:\n",
      "    inter_set_dict[i] = {}\n",
      "    for j in set_dict:\n",
      "        inter_set_dict[i][j] = len(set_dict[i].intersection(set_dict[j]))\n",
      "15/160: pd.DataFrame(inter_set_dict).reindex(labels)\n",
      "15/161: pd.DataFrame(inter_set_dict).reindex(labels).to_csv('turf/group_intersections.csv')\n",
      "15/162:\n",
      "campers = campers[campers.from_id.notin(base.from_id)]\n",
      "campers.shape\n",
      "15/163:\n",
      "campers = campers[~campers.from_id.isin(base.from_id)]\n",
      "campers.shape\n",
      "15/164:\n",
      "campers = campers[~campers.from_id.isin(base.from_id)]\n",
      "campers.from_id.nunique()\n",
      "15/165: two_mks_plus = two_mks_plus[(~two_mks_plus.from_id.isin(base.from_id)) & (~two_mks_plus.from_id.isin(campers.from_id))]\n",
      "15/166:\n",
      "two_mks_plus = two_mks_plus[(~two_mks_plus.from_id.isin(base.from_id)) \n",
      "                            & (~two_mks_plus.from_id.isin(campers.from_id))]\n",
      "two_mks_plus.from_id.nunique()\n",
      "15/167:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(one_timers.index), set(unique_users.from_id.values)], set_labels = ('All', 'One Timers', 'Fans', ), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.25)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.20)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.20)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.09)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]-0.19)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.19)\n",
      "15/168: ax.get_figure().savefig('turf/venn_one_timers_fans.png')\n",
      "15/169: fans = fans[~fans.from_id.isin(one_timers.from_id)]\n",
      "15/170: fans_users = fans_users[~fans_users.from_id.isin(one_timers.from_id)]\n",
      "15/171: one_timers.head()\n",
      "15/172: fans_users = fans_users[~fans_users.from_id.isin(one_timers.index)]\n",
      "15/173:\n",
      "fans_users = fans_users[~fans_users.from_id.isin(one_timers.index)]\n",
      "fans_users.from_id.nunique()\n",
      "15/174: st[st.from_id.isin(unique_users.from_id)].groupby('from_id').id.sum().mean()\n",
      "15/175: st[st.from_id.isin(fans_users.from_id)].groupby('from_id').id.sum().mean()\n",
      "15/176: st[st.from_id.isin(campers.from_id)].groupby('from_id').id.sum().mean()\n",
      "15/177: st[st.from_id.isin(base.from_id)].groupby('from_id').id.sum().mean()\n",
      "15/178: top_commented[:10]\n",
      "15/179: fans.reindex(top_commented).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "15/180:\n",
      "only_bibi = mat.loc[:, top_commented[1:]].isna().all(axis=1)\n",
      "only_bibi[only_bibi].shape\n",
      "15/181:\n",
      "def get_accum_df(mat, order):\n",
      "    onlys = []\n",
      "\n",
      "    for i, name in enumerate(order):\n",
      "        only = mat.loc[:, order[i+1:]].isna().all(axis=1)\n",
      "        onlys.append(only[only].shape[0])\n",
      "        \n",
      "    user_accum_df = pd.DataFrame({'mk': order, 'user_accum': onlys})\n",
      "    user_accum_df['party'] = user_accum_df['mk'].map(mkdet.set_index('folder_name')['party'])\n",
      "    user_accum_df['camp'] = user_accum_df['party'].map(camps)\n",
      "    user_accum_df['delta'] = user_accum_df.user_accum.diff()\n",
      "    unique_delta_ratio = (uniques.reindex(top_commented) / user_accum_df.set_index('mk').delta).drop('Benjamin_Netanyahu')\n",
      "    user_accum_df['unique_delta_ratio'] = user_accum_df['mk'].map(unique_delta_ratio)\n",
      "    user_accum_df['delta_n'] = user_accum_df['delta'] / user_accum_df['delta'].max()\n",
      "    \n",
      "    return user_accum_df\n",
      "15/182:\n",
      "user_accum_df = get_accum_df(mat, top_commented)\n",
      "user_accum_df.head()\n",
      "15/183: user_accum_df.set_index('mk').user_accum.plot(kind='bar', figsize=(15,10), rot=75)\n",
      "15/184:\n",
      "import altair as alt\n",
      "alt.renderers.enable('notebook')\n",
      "15/185:\n",
      "def chart_user_accum(accum_df, order, order_name):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('user_accum',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'user_accum']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = f'User accumulation by {order_name} order - left to right, only users that comment inside the group until the current column')\n",
      "\n",
      "\n",
      "def chart_user_accum_delta(accum_df, order, order_name):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('delta',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'delta']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = f'User accumulation delta by {order_name} order - left to right, only users that comment inside the group until the current column')\n",
      "\n",
      "\n",
      "def chart_user_accum_unique_delta_ratio(accum_df, order, order_name):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('unique_delta_ratio',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'unique_delta_ratio']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = f'Page unique count divided by user_accum delta by {order_name} order (to measure how much a page is part of the group)')\n",
      "15/186:\n",
      "(chart_user_accum(user_accum_df, top_commented, 'Top Commented') \n",
      " & chart_user_accum_delta(user_accum_df, top_commented, 'Top Commented')\n",
      " & chart_user_accum_unique_delta_ratio(user_accum_df, top_commented, 'Top Commented')).properties(title='Top Commented')\n",
      "15/187: user_accum_df[['delta_n', 'unique_delta_ratio']].div(user_accum_df[['delta_n', 'unique_delta_ratio']].sum(axis=1), axis=0).plot(kind='bar', stacked=True, figsize=(15,10), rot=75, width=1.0)\n",
      "15/188:\n",
      "print ('First celebiez group: ', top_commented[:6])\n",
      "print ('Second celebiez group: ', top_commented[:15])\n",
      "15/189: top_mean_commented = list(df.loc[df.is_post==1].groupby('name').comment_count.mean().sort_values(ascending=False).index)\n",
      "15/190: uniques.reindex(top_mean_commented).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "15/191: mean_com_accum_df = get_accum_df(mat, top_mean_commented)\n",
      "15/192:\n",
      "(chart_user_accum(mean_com_accum_df, top_mean_commented, 'Top Mean Commented') \n",
      " & chart_user_accum_delta(mean_com_accum_df, top_mean_commented, 'Top Mean Commented') \n",
      " & chart_user_accum_unique_delta_ratio(mean_com_accum_df, top_mean_commented, 'Top Mean Commented')).properties(title='Top Mean Commented')\n",
      "15/193: top_liked = list(df.loc[df.is_post==1].groupby('name').like_count.mean().sort_values(ascending=False).index)\n",
      "15/194: uniques.reindex(top_liked).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "15/195: like_accum_df = get_accum_df(mat, top_liked)\n",
      "15/196:\n",
      "(chart_user_accum(like_accum_df, top_liked, 'Top Liked') \n",
      " & chart_user_accum_delta(like_accum_df, top_liked, 'Top Liked') \n",
      " & chart_user_accum_unique_delta_ratio(like_accum_df, top_liked, 'Top Liked')).properties(title='Top Liked')\n",
      "15/197: top_shared = list(df.loc[df.is_post==1].groupby('name').share_count.mean().sort_values(ascending=False).index)\n",
      "15/198: uniques.reindex(top_shared).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "15/199:\n",
      "share_accum_df = get_accum_df(mat, top_shared)\n",
      "share_accum_df.head()\n",
      "15/200:\n",
      "(chart_user_accum(share_accum_df, top_shared, 'Top Shared') \n",
      " & chart_user_accum_delta(share_accum_df, top_shared, 'Top Shared') \n",
      " & chart_user_accum_unique_delta_ratio(share_accum_df, top_shared, 'Top Shared')).properties(title='Top Shared')\n",
      "15/201: top_shared\n",
      "15/202: st.groupby('from_id').id.sum().mean()\n",
      "15/203: st[st.from_id.isin(two_mks_plus.from_id)].groupby('from_id').id.sum().mean()\n",
      "15/204: st[(st.is_post==0) & (st.from_id.isin(two_mks_plus.from_id))].groupby('from_id').id.sum().mean()\n",
      "15/205: st[(st.from_id.isin(two_mks_plus.from_id))].groupby('from_id').id.sum().mean()\n",
      "15/206: st[(st.from_id.isin(two_mks_plus.from_id))].groupby('from_id').id.sum().hist()\n",
      "15/207: st[(st.from_id.isin(two_mks_plus.from_id))].groupby('from_id').id.sum().sort_values(ascending=False)\n",
      "15/208: st[(st.from_id.isin(two_mks_plus.from_id))].groupby('from_id').id.sum().sort_values(ascending=False).head(20)\n",
      "15/209: st[(st.from_id.isin(fans_users.from_id))].groupby('from_id').id.sum().sort_values(ascending=False).head(20)\n",
      "15/210: st[(st.from_id.isin(base.from_id))].groupby('from_id').id.sum().sort_values(ascending=False).head(20)\n",
      "15/211: st[(st.from_id.isin(two_mks_plus.from_id))].groupby('from_id').id.sum().sort_values(ascending=False).head(20)\n",
      "15/212: st[(st.from_id.isin(campers.from_id))].groupby('from_id').id.sum().sort_values(ascending=False).head(20)\n",
      "15/213: st[(st.from_id.isin(two_mks_plus.from_id))].groupby('from_id').id.sum().hist(bins=10)\n",
      "15/214: st[(st.from_id.isin(two_mks_plus.from_id))].groupby('from_id').id.sum().hist(bins=20)\n",
      "15/215: st.groupby('from_id').id.sum()\n",
      "15/216: user_counts = st.groupby('from_id').id.sum()\n",
      "15/217: binning = pd.qcut(user_counts)\n",
      "15/218: binning = pd.qcut(user_counts, 5)\n",
      "15/219: binning = pd.qcut(user_counts, 9)\n",
      "15/220: binning = pd.cut(user_counts, 9)\n",
      "15/221: binning.hist()\n",
      "15/222: binning.value_counts().plot()\n",
      "15/223: binning.value_counts().plot(kind='bar')\n",
      "15/224: binning = pd.qcut(user_counts, 9)\n",
      "15/225: binning = pd.qcut(user_counts, 9, duplicates='drop')\n",
      "15/226: binning.value_counts().plot(kind='bar')\n",
      "15/227: binning = pd.qcut(user_counts, 15, duplicates='drop')\n",
      "15/228: binning.plot(kind='bar')\n",
      "15/229: binning = pd.qcut(user_counts, 3, duplicates='drop')\n",
      "15/230: binning.plot(kind='bar')\n",
      "15/231: binning = pd.cut(user_counts, range(10)*1000)\n",
      "15/232: binning = pd.cut(user_counts, np.arange(0,1000,1000))\n",
      "15/233: binning.plot(kind='bar')\n",
      "15/234: binning = pd.cut(user_counts, [0,1,5,10,50,100,250,500,750,1000,1500,2000,10000])\n",
      "15/235: binning.plot(kind='bar')\n",
      "15/236: user_counts = st.groupby('from_id').id.sum()\n",
      "15/237: binning = pd.cut(user_counts, [0,1,5,10,50,100,250,500,750,1000,1500,2000,10000])\n",
      "15/238: binning.plot(kind='bar')\n",
      "15/239: binning\n",
      "15/240: binning.value_counts().plot(kind='bar')\n",
      "15/241: binning.value_counts()\n",
      "15/242: binning.value_counts(sort=False)\n",
      "15/243: binning = pd.cut(user_counts, [0,1,5,10,20,30,40,50,100,250,500,750,1000,1500,2000,10000])\n",
      "15/244: binning.value_counts(sort=False)\n",
      "15/245: user_counts[user_counts>1].hist()\n",
      "15/246: user_counts[user_counts>1].value_counts.plot()\n",
      "15/247: user_counts[user_counts>1].value_counts().plot()\n",
      "15/248: user_counts[user_counts>=10].value_counts().plot()\n",
      "15/249: binning.value_counts(sort=False).plot()\n",
      "15/250: binning.value_counts(sort=False).plot(kind='bar')\n",
      "15/251: binning.value_counts(sort=False).reset_index().rename(columns={0:'index'})\n",
      "15/252:\n",
      "al.Chart(binning.value_counts(sort=False).reset_index().rename(columns={0:'index'})).mark_bar().encode(\n",
      "    x = 'index'\n",
      "    y = 'id'\n",
      ")\n",
      "15/253:\n",
      "al.Chart(binning.value_counts(sort=False).reset_index().rename(columns={0:'index'})).mark_bar().encode(\n",
      "    x = 'index'\n",
      "    y = 'id'\n",
      ").interactive()\n",
      "15/254:\n",
      "al.Chart(binning.value_counts(sort=False).cumsum().reset_index().rename(columns={0:'index'})).mark_bar().encode(\n",
      "    x = 'index'\n",
      "    y = 'id'\n",
      ").interactive()\n",
      "15/255:\n",
      "al.Chart(binning.value_counts(sort=False).cumsum().reset_index().rename(columns={0:'index'})).mark_bar().encode(\n",
      "    x = 'index',\n",
      "    y = 'id'\n",
      ").interactive()\n",
      "15/256:\n",
      "alt.Chart(binning.value_counts(sort=False).cumsum().reset_index().rename(columns={0:'index'})).mark_bar().encode(\n",
      "    x = 'index',\n",
      "    y = 'id'\n",
      ").interactive()\n",
      "15/257:\n",
      "alt.Chart(binning.value_counts(sort=False).reset_index().rename(columns={0:'index'})).mark_bar().encode(\n",
      "    x = 'index',\n",
      "    y = 'id'\n",
      ").interactive()\n",
      "15/258:\n",
      "alt.Chart(binning.value_counts(sort=False).reset_index().rename(columns={0:'index'})).mark_bar().encode(\n",
      "    x = 'index:O',\n",
      "    y = 'id'\n",
      ").interactive()\n",
      "15/259: binning.value_counts(sort=False).plot(kind='bar',figsize=(10,7))\n",
      "15/260: perc = pd.qcut(user_counts, 19)\n",
      "15/261: perc = pd.qcut(user_counts, 20)\n",
      "15/262: perc = pd.qcut(user_counts, 20, percision=5)\n",
      "15/263: perc = pd.qcut(user_counts, 20, precision=5)\n",
      "15/264: perc = pd.qcut(user_counts, 20, duplicates='drop')\n",
      "15/265: perc.value_counts(sort=False).plot(kind='bar',figsize=(10,7))\n",
      "15/266: perc = pd.qcut(user_counts, 17, duplicates='drop')\n",
      "15/267: perc.value_counts(sort=False).plot(kind='bar',figsize=(10,7))\n",
      "15/268: perc = pd.qcut(user_counts, 9, duplicates='drop')\n",
      "15/269:\n",
      "perc = pd.qcut(user_counts, 9, duplicates='drop')\n",
      "perc.value_counts(sort=False).plot(kind='bar',figsize=(10,7))\n",
      "15/270:\n",
      "perc = pd.qcut(user_counts, 15, duplicates='drop')\n",
      "perc.value_counts(sort=False).plot(kind='bar',figsize=(10,7))\n",
      "15/271:\n",
      "perc = pd.qcut(user_counts, 14, duplicates='drop')\n",
      "perc.value_counts(sort=False).plot(kind='bar',figsize=(10,7))\n",
      "15/272:\n",
      "perc = pd.qcut(user_counts, 30, duplicates='drop')\n",
      "perc.value_counts(sort=False).plot(kind='bar',figsize=(10,7))\n",
      "15/273:\n",
      "alt.Chart(perc.value_counts(sort=False).reset_index().rename(columns={0:'index'})).mark_bar().encode(\n",
      "    x = alt.X('index:O', sort=),\n",
      "    y = 'id'\n",
      ").interactive()\n",
      "15/274:\n",
      "alt.Chart(perc.value_counts(sort=False).reset_index().rename(columns={0:'index'})).mark_bar().encode(\n",
      "    x = alt.X('index:O'),\n",
      "    y = 'id'\n",
      ").interactive()\n",
      "15/275:\n",
      "perc = pd.qcut(user_counts, 20, duplicates='drop')\n",
      "perc.value_counts(sort=False).plot(kind='bar',figsize=(10,7))\n",
      "15/276:\n",
      "perc = pd.qcut(user_counts, 20, duplicates='drop')\n",
      "perc.value_counts(sort=False)\n",
      "15/277: perc_bah = pd.cut(user_counts, [0,1,2,3,4,5,7,11,24,9359])\n",
      "15/278:\n",
      "perc_bah = pd.cut(user_counts, [0,1,2,3,4,5,7,11,24,9359])\n",
      "binning.value_counts(sort=False).plot(kind='bar',figsize=(10,7))\n",
      "15/279:\n",
      "perc_bah = pd.cut(user_counts, [0,1,2,3,4,5,7,11,24,9359])\n",
      "perc_bah.value_counts(sort=False).plot(kind='bar',figsize=(10,7))\n",
      "15/280:\n",
      "perc_bah = pd.cut(user_counts, [0,1,2,3,4,5,7,11,24,9359])\n",
      "perc_bah.value_counts(sort=False).plot(kind='bar',figsize=(10,7)) / user_counts.shape[0]\n",
      "15/281:\n",
      "perc_bah = pd.cut(user_counts, [0,1,2,3,4,5,7,11,24,9359])\n",
      "(perc_bah.value_counts(sort=False)/ user_counts.shape[0]).plot(kind='bar',figsize=(10,7))\n",
      "15/282:\n",
      "perc_bah = pd.cut(user_counts, [0,1,2,3,4,5,7,11,24,9359])\n",
      "(perc_bah.value_counts(sort=False)/ user_counts.shape[0])\n",
      "15/283:\n",
      "perc_bah = pd.cut(user_counts, [0,1,2,3,4,5,7,11,24,9359])\n",
      "(perc_bah.value_counts(sort=False))\n",
      "15/284: (perc_bah.value_counts(sort=False)/ user_counts.shape[0])\n",
      "15/285:\n",
      "dedicated = st[st.from_id.isin(user_counts[user_counts>24].index)]\n",
      "dedicated.shape\n",
      "15/286: st.from_id.isin(user_counts[user_counts>24].index\n",
      "15/287: st[st.from_id.isin(user_counts[user_counts>24].index)]\n",
      "15/288:\n",
      "dedicated = st[st.from_id.isin(user_counts[user_counts>24].index)]\n",
      "dedicated.from_id.nunique()\n",
      "15/289: dedicated.head()\n",
      "15/290:\n",
      "campers = campers[(~campers.from_id.isin(base.from_id))& (~campers.from_id.isin(dedicated.from_id))]\n",
      "campers.from_id.nunique()\n",
      "15/291:\n",
      "base = base[(~base.from_id.isin(dedicated.from_id))]\n",
      "base.from_id.nunique()\n",
      "15/292:\n",
      "two_mks_plus = two_mks_plus[(~two_mks_plus.from_id.isin(base.from_id)) \n",
      "                            & (~two_mks_plus.from_id.isin(campers.from_id))\n",
      "                            & (~two_mks_plus.from_id.isin(dedicated.from_id))]\n",
      "two_mks_plus.from_id.nunique()\n",
      "15/293:\n",
      "fans_users = fans_users[~fans_users.from_id.isin(one_timers.index)\n",
      "                        & (~fans_users.from_id.isin(dedicated.from_id))]\n",
      "fans_users.from_id.nunique()\n",
      "15/294: campers.from_id.nunique() + base.from_id.nunique() + two_mks_plus.from_id.nunique()\n",
      "15/295: st[st.from_id.isin(fans_users.from_id)].groupby('from_id').id.sum().mean()\n",
      "15/296: st[(st.from_id.isin(fans_users.from_id))].groupby('from_id').id.sum().sort_values(ascending=False).head(20)\n",
      "15/297: st[st.from_id.isin(base.from_id)].groupby('from_id').id.sum().mean()\n",
      "15/298: st[st.from_id.isin(campers.from_id)].groupby('from_id').id.sum().mean()\n",
      "15/299: st[(st.from_id.isin(two_mks_plus.from_id))].groupby('from_id').id.sum().mean()\n",
      "15/300: st[(st.from_id.isin(two_mks_plus.from_id))].groupby('from_id').id.sum().sort_values(ascending=False).head(20)\n",
      "15/301: st[(st.from_id.isin(dedicated.from_id))].groupby('from_id').id.sum().mean()\n",
      "15/302: dedicated.groupby('from_id').id.sum().mean()\n",
      "15/303: two_mks_plus.groupby('from_id').id.sum().mean()\n",
      "15/304: two_mks_plus.head()\n",
      "15/305: st[(st.from_id.isin(two_mks_plus.from_id))].groupby('from_id').id.sum().mean()\n",
      "15/306: st[(st.from_id.isin(two_mks_plus.from_id))].groupby('from_id').id.sum().\n",
      "15/307: st[(st.from_id.isin(two_mks_plus.from_id))].groupby('from_id').id.sum()\n",
      "15/308: st[(st.from_id.isin(two_mks_plus.from_id))].id.sum()\n",
      "15/309: dedicated.id.sum()\n",
      "15/310: st[st.from_id.isin(fans_users.from_id)].id.sum()\n",
      "15/311: one_timers.head()\n",
      "15/312: st[st.from_id.isin(one_timers.index)].id.sum()\n",
      "15/313: one_timers.shape\n",
      "15/314: st[st.from_id.isin(base.from_id)].id.sum()\n",
      "15/315: st[st.from_id.isin(campers.from_id)].id.sum()\n",
      "15/316: st.id.sum()\n",
      "15/317: st[(st.from_id.isin(two_mks_plus.from_id))].groupby('from_id').post_id.sum().mean()\n",
      "15/318:\n",
      "gtwo = st[(st.from_id.isin(two_mks_plus.from_id))].groupby('from_id')\n",
      "gtwo.id.sum().mean(), .post_id.sum().mean()\n",
      "15/319:\n",
      "gtwo = st[(st.from_id.isin(two_mks_plus.from_id))].groupby('from_id')\n",
      "gtwo.id.sum().mean(), gtwo.post_id.sum().mean()\n",
      "15/320:\n",
      "gcamp = st[st.from_id.isin(campers.from_id)].groupby('from_id')\n",
      "gcamp.id.sum().mean(), gcamp.post_id.sum().mean()\n",
      "15/321:\n",
      "gbase = st[st.from_id.isin(base.from_id)].groupby('from_id')\n",
      "gbase.id.sum().mean(), gbase.post_id.sum().mean()\n",
      "15/322:\n",
      "gfans = st[st.from_id.isin(fans_users.from_id)].groupby('from_id')\n",
      "gfans.id.sum().mean(), gfans.post_id.sum().mean()\n",
      "15/323: df[df.is_post==1].shape\n",
      "15/324: df[df.is_post==1].shape[0], df[df.is_post==1].comment_count.mean()\n",
      "15/325: df[df.is_post==1].groupby('name').comment_count.mean()\n",
      "15/326: df[df.is_post==1].groupby('name').comment_count.mean().sort_values(ascending=False).plot(kind='bar')\n",
      "15/327: df[df.is_post==1].groupby('name').comment_count.mean().sort_values(ascending=False).plot(kind='bar', figsize=(15,10))\n",
      "15/328:\n",
      "gded = dedicated.groupby('from_id')\n",
      "gded.id.sum().mean(), gded.post_id.sum().mean()\n",
      "15/329:\n",
      "gded = dedicated.groupby('from_id')\n",
      "gded.id.sum().mean(), gded.post_id.sum().mean(), gded.id.sum().mean()/ gded.post_id.sum().mean()\n",
      "15/330:\n",
      "gtwo = st[(st.from_id.isin(two_mks_plus.from_id))].groupby('from_id')\n",
      "gtwo.id.sum().mean(), gtwo.post_id.sum().mean(), gtwo.id.sum().mean()/gtwo.post_id.sum().mean()\n",
      "15/331:\n",
      "gcamp = st[st.from_id.isin(campers.from_id)].groupby('from_id')\n",
      "gcamp.id.sum().mean(), gcamp.post_id.sum().mean(), gcamp.id.sum().mean()/ gcamp.post_id.sum().mean()\n",
      "15/332:\n",
      "gbase = st[st.from_id.isin(base.from_id)].groupby('from_id')\n",
      "gbase.id.sum().mean(), gbase.post_id.sum().mean(), gbase.id.sum().mean()/ gbase.post_id.sum().mean()\n",
      "15/333:\n",
      "gfans = st[st.from_id.isin(fans_users.from_id)].groupby('from_id')\n",
      "gfans.id.sum().mean(), gfans.post_id.sum().mean(), gfans.id.sum().mean(), gfans.post_id.sum().mean()\n",
      "15/334:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gfans = st[st.from_id.isin(fans_users.from_id)].groupby('from_id')\n",
      "gfans.id.sum().mean(), gfans.post_id.sum().mean(), gfans.id.sum().mean()/ gfans.post_id.sum().mean()\n",
      "15/335: gfans = st[st.from_id.isin(fans_users.from_id)].groupby('name').size().plot(kind='bar', figsize=(10,7))\n",
      "15/336:\n",
      "gfans = st[st.from_id.isin(fans_users.from_id)].groupby('from_id')\n",
      "gfans.id.sum().mean(), gfans.post_id.sum().mean(), gfans.id.sum().mean()/ gfans.post_id.sum().mean()\n",
      "15/337: st[st.from_id.isin(fans_users.from_id)].groupby('name').size().plot(kind='bar', figsize=(10,7))\n",
      "15/338: st[st.from_id.isin(fans_users.from_id)].groupby('name').size().sort_values(ascending=False).plot(kind='bar', figsize=(15,10))\n",
      "15/339: st[st.from_id.isin(one_timers.index)].groupby('name').size().sort_values(ascending=False).plot(kind='bar', figsize=(15,10))\n",
      "15/340: st[st.from_id.isin(base.from_id)].groupby('name').size().sort_values(ascending=False).plot(kind='bar', figsize=(15,10))\n",
      "15/341: st[st.from_id.isin(campers.from_id)].groupby('name').size().sort_values(ascending=False).plot(kind='bar', figsize=(15,10))\n",
      "15/342: st[st.from_id.isin(two_mks_plus.from_id)].groupby('name').size().sort_values(ascending=False).plot(kind='bar', figsize=(15,10))\n",
      "15/343: st[st.from_id.isin(dedicated.from_id)].groupby('name').size().sort_values(ascending=False).plot(kind='bar', figsize=(15,10))\n",
      "15/344: st[st.from_id.isin(base.from_id)].groupby('name').size().sort_values(ascending=False).plot(kind='barh', figsize=(15,10))\n",
      "15/345: st[st.from_id.isin(base.from_id)].groupby('name').size().sort_values(ascending=False).plot(kind='barh', figsize=(10,15))\n",
      "15/346: st[st.from_id.isin(base.from_id)].groupby('name').size().sort_values().plot(kind='barh', figsize=(10,15))\n",
      "15/347: df[df.is_post==1].groupby('name').comment_count.mean().sort_values().plot(kind='barh', figsize=(15,10))\n",
      "15/348: df[df.is_post==1].groupby('name').comment_count.mean().sort_values().plot(kind='barh', figsize=(10,15))\n",
      "15/349: st[st.from_id.isin(one_timers.index)].groupby('name').size().sort_values().plot(kind='barh', figsize=(10,15))\n",
      "15/350: st[st.from_id.isin(fans_users.from_id)].groupby('name').size().sort_values().plot(kind='barh', figsize=(10,15))\n",
      "15/351: st[st.from_id.isin(campers.from_id)].groupby('name').size().sort_values().plot(kind='barh', figsize=(10,15))\n",
      "15/352: st[st.from_id.isin(two_mks_plus.from_id)].groupby('name').size().sort_values().plot(kind='barh', figsize=(10,15))\n",
      "15/353: st[st.from_id.isin(dedicated.from_id)].groupby('name').size().sort_values().plot(kind='barh', figsize=(10,15))\n",
      "15/354:\n",
      "twomk_page = st[st.from_id.isin(two_mks_plus.from_id)].groupby('name').size()\n",
      "twomk_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "15/355:\n",
      "ded_page = st[st.from_id.isin(dedicated.from_id)].groupby('name').size()\n",
      "ded_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "15/356:\n",
      "camp_page = st[st.from_id.isin(campers.from_id)].groupby('name')\n",
      "camp_page.size().sort_values().plot(kind='barh', figsize=(10,15))\n",
      "15/357:\n",
      "fans_page = st[st.from_id.isin(fans_users.from_id)].groupby('name').size()\n",
      "fans_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "15/358:\n",
      "one_timers_page = st[st.from_id.isin(one_timers.index)].groupby('name').size()\n",
      "one_timers_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "15/359: pd.concat([one_timers_page, fans_page, camp_page, twomk_page, ded_page])\n",
      "15/360:\n",
      "camp_page = st[st.from_id.isin(campers.from_id)].groupby('name').size()\n",
      "camp_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "15/361:\n",
      "base_page = st[st.from_id.isin(base.from_id)].groupby('name').size()\n",
      "base_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "15/362: pd.concat([one_timers_page, fans_page, camp_page, twomk_page, ded_page])\n",
      "15/363: pd.concat([one_timers_page, fans_page, camp_page, twomk_page, ded_page], axis=1)\n",
      "15/364:\n",
      "pd.concat([one_timers_page, fans_page, base_page, camp_page, twomk_page, ded_page], axis=1,\n",
      "          keys=['One-timers', 'Fans', 'Basers', 'Campers', 'Two-MK-Plus', 'Dedicated'])\n",
      "15/365:\n",
      "page_cats = pd.concat([one_timers_page, fans_page, base_page, camp_page, twomk_page, ded_page], axis=1,\n",
      "          keys=['One-timers', 'Fans', 'Basers', 'Campers', 'Two-MK-Plus', 'Dedicated'])\n",
      "15/366: page_cats.plot(kind='bar')\n",
      "15/367: page_cats.plot(kind='bar', stacked=True)\n",
      "15/368: page_cats.div(page_cats.sum(axis=1), axis=0).plot(kind='bar', stacked=True, figsize=(15,10))\n",
      "15/369: page_cats.div(page_cats.sum(axis=1), axis=0).plot(kind='bar', stacked=True, figsize=(15,10)).reindex(top_commented)\n",
      "15/370: page_cats.reindex(top_commented).div(page_cats.sum(axis=1), axis=0).plot(kind='bar', stacked=True, figsize=(15,10))\n",
      "15/371: page_cats.div(page_cats.sum(axis=1), axis=0).plot(kind='bar', stacked=True, figsize=(15,10))\n",
      "15/372: page_cats\n",
      "15/373: page_cats.reindex(top_commented)\n",
      "15/374: page_cats.div(page_cats.sum(axis=1), axis=0).reindex(top_commented).plot(kind='bar', stacked=True, figsize=(15,10))\n",
      "15/375: page_cats.div(page_cats.sum(axis=1), axis=0).reindex(top_commented).plot(kind='barh', stacked=True, figsize=(15,10))\n",
      "15/376: page_cats.div(page_cats.sum(axis=1), axis=0).reindex(top_commented)\n",
      "15/377: page_cats.div(page_cats.sum(axis=1), axis=0).reindex(top_commented).plot(kind='barh', stacked=True, figsize=(15,10))\n",
      "15/378: page_cats.div(page_cats.sum(axis=1), axis=0)\n",
      "15/379: page_cats.div(page_cats.sum(axis=1), axis=0).reindex(top_commented)\n",
      "15/380: page_cat_ratios = page_cats.div(page_cats.sum(axis=1), axis=0).reindex(top_commented)\n",
      "15/381:\n",
      "page_cat_ratios = page_cats.div(page_cats.sum(axis=1), axis=0).reindex(top_commented)\n",
      "page_cat_ratios.stack()\n",
      "15/382:\n",
      "page_cat_ratios = page_cats.div(page_cats.sum(axis=1), axis=0).reindex(top_commented)\n",
      "page_cat_ratios.stack().reset_index\n",
      "15/383:\n",
      "page_cat_ratios = page_cats.div(page_cats.sum(axis=1), axis=0).reindex(top_commented)\n",
      "page_cat_ratios.stack().reset_index()\n",
      "15/384:\n",
      "page_cat_ratios = page_cats.div(page_cats.sum(axis=1), axis=0).reindex(top_commented)\n",
      "page_cat_ratios.stack().reset_index().rename(columns={'level_1': 'Category', 0: 'ratio'})\n",
      "15/385:\n",
      "page_cat_ratios = page_cats.div(page_cats.sum(axis=1), axis=0).reindex(top_commented).stack().reset_index().rename(columns={'level_1': 'Category', 0: 'ratio'})\n",
      "page_cat_ratios\n",
      "15/386:\n",
      "page_cat_ratios = page_cats.div(page_cats.sum(axis=1), axis=0).reindex(top_commented).stack().reset_index().rename(columns={'level_1': 'Category', 0: 'ratio'})\n",
      "page_cat_ratios.head(10)\n",
      "15/387:\n",
      "regular = alt.Chart(page_cat_ratios).mark_bar().encode(\n",
      "    alt.X('ratio:Q', title=None),\n",
      "    alt.Y('name:N',\n",
      "        axis=alt.Axis(title='מספר קווים'),\n",
      "        #stack='normalize'\n",
      "    ),\n",
      "    alt.Color('Category:O'),\n",
      "    #tooltip = ['start_zone','mh_cut', 'size']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('ratio:Q', title=None),\n",
      "    alt.Y('name:N',\n",
      "        axis=alt.Axis(title='מספר קווים'),\n",
      "        stack='normalize'\n",
      "    ),\n",
      "    alt.Color('Category:O'),\n",
      "(regular & normalized).configure_axisX(labelAngle=-45)\n",
      "15/388:\n",
      "regular = alt.Chart(page_cat_ratios).mark_bar().encode(\n",
      "    alt.X('ratio:Q'),\n",
      "    alt.Y('name:N',\n",
      "\n",
      "    ),\n",
      "    alt.Color('Category:O'),\n",
      "    #tooltip = ['start_zone','mh_cut', 'size']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('ratio:Q',\n",
      "        axis=alt.Axis(title='אחוזים'),\n",
      "        stack='normalize'\n",
      "    ),)\n",
      "(regular & normalized).configure_axisX(labelAngle=-45).properties(title='תדירויות קווים לפי אזור התחלה')\n",
      "15/389:\n",
      "regular = alt.Chart(page_cat_ratios).mark_bar().encode(\n",
      "    alt.X('ratio:Q'),\n",
      "    alt.Y('name:N',\n",
      "\n",
      "    ),\n",
      "    alt.Color('Category:N'),\n",
      "    #tooltip = ['start_zone','mh_cut', 'size']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('ratio:Q',\n",
      "        axis=alt.Axis(title='אחוזים'),\n",
      "        stack='normalize'\n",
      "    ),)\n",
      "(regular & normalized).configure_axisX(labelAngle=-45).properties(title='תדירויות קווים לפי אזור התחלה')\n",
      "15/390:\n",
      "regular = alt.Chart(page_cat_ratios).mark_bar().encode(\n",
      "    alt.X('ratio:Q'),\n",
      "    alt.Y('name:N',\n",
      "\n",
      "    ),\n",
      "    alt.Color('Category:N'),\n",
      "    #tooltip = ['start_zone','mh_cut', 'size']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('ratio:Q',\n",
      "        axis=alt.Axis(title='אחוזים'),\n",
      "        stack='normalize'\n",
      "    ),)\n",
      "(regular & normalized)\n",
      "15/391:\n",
      "regular = alt.Chart(page_cat_ratios).mark_bar().encode(\n",
      "    alt.X('ratio:Q'),\n",
      "    alt.Y('name:N', sort=top_commented),\n",
      "    alt.Color('Category:N'),\n",
      "    #tooltip = ['start_zone','mh_cut', 'size']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('ratio:Q',\n",
      "        axis=alt.Axis(title='אחוזים'),\n",
      "        stack='normalize'\n",
      "    ),)\n",
      "(regular & normalized)\n",
      "15/392: page_cats = page_cat.stack().reset_index().rename(columns={'level_1': 'Category', 0: 'ratio'})\n",
      "15/393: page_cats = page_cats.stack().reset_index().rename(columns={'level_1': 'Category', 0: 'ratio'})\n",
      "15/394:\n",
      "regular = alt.Chart(page_cast).mark_bar().encode(\n",
      "    alt.X('ratio:Q'),\n",
      "    alt.Y('name:N', sort=top_commented),\n",
      "    alt.Color('Category:N'),\n",
      "    #tooltip = ['start_zone','mh_cut', 'size']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('ratio:Q',\n",
      "        axis=alt.Axis(title='אחוזים'),\n",
      "        stack='normalize'\n",
      "    ),)\n",
      "(regular & normalized)\n",
      "15/395:\n",
      "regular = alt.Chart(page_cats).mark_bar().encode(\n",
      "    alt.X('ratio:Q'),\n",
      "    alt.Y('name:N', sort=top_commented),\n",
      "    alt.Color('Category:N'),\n",
      "    #tooltip = ['start_zone','mh_cut', 'size']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('ratio:Q',\n",
      "        axis=alt.Axis(title='אחוזים'),\n",
      "        stack='normalize'\n",
      "    ),)\n",
      "(regular & normalized)\n",
      "15/396:\n",
      "regular = alt.Chart(page_cats).mark_bar().encode(\n",
      "    alt.X('ratio:Q'),\n",
      "    alt.Y('name:N', sort=top_commented),\n",
      "    alt.Color('Category:N'),\n",
      "    #tooltip = ['start_zone','mh_cut', 'size']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('ratio:Q',\n",
      "        axis=alt.Axis(title='אחוזים'),\n",
      "        stack='normalize'\n",
      "    ),)\n",
      "(regular | normalized)\n",
      "15/397:\n",
      "regular = alt.Chart(page_cats).mark_bar().encode(\n",
      "    alt.X('ratio:Q'),\n",
      "    alt.Y('name:N', sort=top_commented),\n",
      "    alt.Color('Category:N'),\n",
      "    #tooltip = ['start_zone','mh_cut', 'size']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('ratio:Q',\n",
      "        axis=alt.Axis(title='אחוזים'),\n",
      "        stack='normalize'\n",
      "    ),)\n",
      "(regular | normalized).properties(height=1200, width=600)\n",
      "15/398:\n",
      "regular = alt.Chart(page_cats).mark_bar().encode(\n",
      "    alt.X('ratio:Q'),\n",
      "    alt.Y('name:N', sort=top_commented),\n",
      "    alt.Color('Category:N'),\n",
      "    #tooltip = ['start_zone','mh_cut', 'size']\n",
      ").properties(height=1200, width=700)\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('ratio:Q',\n",
      "        axis=alt.Axis(title='אחוזים'),\n",
      "        stack='normalize'\n",
      "    ),)\n",
      "(regular | normalized)\n",
      "15/399:\n",
      "regular = alt.Chart(page_cats).mark_bar().encode(\n",
      "    alt.X('ratio:Q'),\n",
      "    alt.Y('name:N', sort=top_commented),\n",
      "    alt.Color('Category:N'),\n",
      "    #tooltip = ['start_zone','mh_cut', 'size']\n",
      ").properties(height=1200, width=350)\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('ratio:Q',\n",
      "        axis=alt.Axis(title='אחוזים'),\n",
      "        stack='normalize'\n",
      "    ),)\n",
      "(regular | normalized)\n",
      "15/400:\n",
      "regular = alt.Chart(page_cats).mark_bar().encode(\n",
      "    alt.X('ratio:Q'),\n",
      "    alt.Y('name:N', sort=top_commented),\n",
      "    alt.Color('Category:N'),\n",
      "    #tooltip = ['start_zone','mh_cut', 'size']\n",
      ").properties(height=1200, width=300)\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('ratio:Q',\n",
      "        axis=alt.Axis(title='אחוזים'),\n",
      "        stack='normalize'\n",
      "    ),)\n",
      "(regular | normalized)\n",
      "15/401:\n",
      "regular = alt.Chart(page_cats).mark_bar().encode(\n",
      "    alt.X('ratio:Q'),\n",
      "    alt.Y('name:N', sort=top_commented),\n",
      "    alt.Color('Category:N'),\n",
      "    #tooltip = ['start_zone','mh_cut', 'size']\n",
      ").properties(height=1200, width=300)\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('ratio:Q',\n",
      "        axis=alt.Axis(title='אחוזים'),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y(legend=None)\n",
      ")\n",
      "(regular | normalized)\n",
      "15/402:\n",
      "regular = alt.Chart(page_cats).mark_bar().encode(\n",
      "    alt.X('ratio:Q'),\n",
      "    alt.Y('name:N', sort=top_commented),\n",
      "    alt.Color('Category:N'),\n",
      "    #tooltip = ['start_zone','mh_cut', 'size']\n",
      ").properties(height=1200, width=300)\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('ratio:Q',\n",
      "        axis=alt.Axis(title='אחוזים'),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y(labels=None)\n",
      ")\n",
      "(regular | normalized)\n",
      "15/403:\n",
      "regular = alt.Chart(page_cats).mark_bar().encode(\n",
      "    alt.X('ratio:Q'),\n",
      "    alt.Y('name:N', sort=top_commented),\n",
      "    alt.Color('Category:N'),\n",
      "    #tooltip = ['start_zone','mh_cut', 'size']\n",
      ").properties(height=1200, width=300)\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('ratio:Q',\n",
      "        axis=alt.Axis(title='אחוזים'),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y(axis=None)\n",
      ")\n",
      "(regular | normalized)\n",
      "15/404:\n",
      "regular = alt.Chart(page_cats).mark_bar().encode(\n",
      "    alt.X('ratio:Q'),\n",
      "    alt.Y('name:N', sort=top_commented),\n",
      "    alt.Color('Category:N'),\n",
      "    #tooltip = ['start_zone','mh_cut', 'size']\n",
      ").properties(height=1200, width=300)\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('ratio:Q',\n",
      "        axis=alt.Axis(title='אחוזים'),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented, axis=None),\n",
      ")\n",
      "(regular | normalized)\n",
      "15/405:\n",
      "regular = alt.Chart(page_cats).mark_bar().encode(\n",
      "    alt.X('ratio:Q'),\n",
      "    alt.Y('name:N', sort=top_commented),\n",
      "    alt.Color('Category:N'),\n",
      "    #tooltip = ['start_zone','mh_cut', 'size']\n",
      ").properties(height=1200, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('ratio:Q',\n",
      "        axis=alt.Axis(title='אחוזים'),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented, axis=None),\n",
      ").properties(title='User Ratio')\n",
      "(regular | normalized)\n",
      "15/406:\n",
      "regular = alt.Chart(page_cats).mark_bar().encode(\n",
      "    alt.X('ratio:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented),\n",
      "    alt.Color('Category:N'),\n",
      "    #tooltip = ['start_zone','mh_cut', 'size']\n",
      ").properties(height=1200, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('ratio:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented, axis=None),\n",
      ").properties(title='User Ratio')\n",
      "(regular | normalized)\n",
      "15/407:\n",
      "regular = alt.Chart(page_cats).mark_bar().encode(\n",
      "    alt.X('ratio:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None),\n",
      "    alt.Color('Category:N'),\n",
      "    #tooltip = ['start_zone','mh_cut', 'size']\n",
      ").properties(height=1200, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('ratio:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented, axis=None),\n",
      ").properties(title='User Ratio')\n",
      "(regular | normalized)\n",
      "16/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "16/2:\n",
      "%%time\n",
      "df = pd.read_pickle('turf/data_df.pkl.gz', compression='gzip')\n",
      "16/3: df.dtypes\n",
      "16/4:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "16/5:\n",
      "%%time\n",
      "df = pd.read_pickle('turf/data_df.pkl.gz', compression='gzip')\n",
      "16/6: df.dtypes\n",
      "16/7:\n",
      "sdf = pd.read_pickle('turf/shared_df.pkl.gz', compression='gzip')\n",
      "sdf.dtypes\n",
      "16/8: st = pd.read_pickle('turf/total_stats_no_multiindex.pkl.gz', compression='gzip')\n",
      "16/9:\n",
      "# OLD STUFF\n",
      "#save(st, 'turf/total_stats.pkl.gz')\n",
      "#st = pd.read_pickle( 'turf/total_stats.pkl.gz', compression='gzip')\n",
      "16/10: st.head()\n",
      "16/11: st.shape\n",
      "16/12: df[df.is_post==0].shape\n",
      "16/13: print ('Total number of comments: %d' % st.id.sum())\n",
      "16/14: print ('Total number of distinct users (commenters): %d' % st.from_id.nunique())\n",
      "16/15:\n",
      "counts = st.groupby('from_id').id.sum()\n",
      "one_timers = counts[counts==1]\n",
      "\n",
      "print ('Total number of one-timers: %d' % one_timers.shape[0])\n",
      "print ('Ratio of one timers from all users: %.2f' % (one_timers.shape[0] / st.from_id.nunique()))\n",
      "16/16: mat = st.pivot(index='from_id', columns='name', values='id')\n",
      "16/17: mat[(mat.count(axis=1)==1)].shape[0]\n",
      "16/18: mat.loc[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu==1), 'Benjamin_Netanyahu'].shape[0]\n",
      "16/19:\n",
      "print ('Ratio of Bibi from one-timers: %.2f' % (mat[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu==1)].shape[0] \n",
      " / counts[counts==1].shape[0]))\n",
      "16/20: print ('Ratio of users that commented 10 times and up: %.2f' % (counts[counts>=10].shape[0]/st.from_id.nunique()))\n",
      "16/21: (st.reset_index().groupby('from_id').name.nunique().value_counts().head(20)/st.reset_index().from_id.nunique()).plot(kind='bar')\n",
      "16/22: (st.reset_index().groupby('from_id').id.sum().value_counts().head(20)/st.reset_index().from_id.nunique()).plot(kind='bar')\n",
      "16/23:\n",
      "fans_users = (st.reset_index()\n",
      " .loc[st.groupby('from_id').name.transform('nunique')==1, ['from_id', 'name', 'party']])\n",
      "16/24: fans = ( fans_users.name.value_counts())\n",
      "16/25: fans.head(10)\n",
      "16/26:\n",
      "print ('Total number of fans: %d' % fans.sum())\n",
      "print ('Ratio of fans from all users: %.2f' % (fans.sum() / st.from_id.nunique()))\n",
      "print ('Ratio of one-timers from fans: %.2f' % (counts[counts==1].shape[0] / fans.sum()))\n",
      "16/27:\n",
      "(pd.concat([fans, \n",
      "            fans/st.from_id.nunique(), \n",
      "            fans/st.groupby('name').from_id.nunique()],\n",
      "            axis=1, keys=['num_uniques', 'ratio_from_all_users', 'ratio_from_own_users'])\n",
      "             .sort_values(by='ratio_from_own_users', ascending=False)).head(10)\n",
      "16/28:\n",
      "mkdet = pd.read_csv('turf/mk_details.csv', sep='\\t')\n",
      "mkdet.head()\n",
      "16/29: df['party'] = df['name'].map(mkdet.set_index('folder_name')['party'])\n",
      "16/30: df.groupby(['party', 'langdetect']).size().unstack()\n",
      "16/31: st['party'] = st['name'].map(mkdet.set_index('folder_name')['party'])\n",
      "16/32:\n",
      "two_mks_plus = (st.reset_index()\n",
      " .loc[st.groupby('from_id').name.transform('nunique')>1, ['from_id', 'name', 'party']])\n",
      "16/33: two_mks_plus.head()\n",
      "16/34: two_mks_plus.from_id.nunique()\n",
      "16/35: two_mks_plus.party.value_counts()\n",
      "16/36: two_mks_plus.groupby('party').from_id.nunique()\n",
      "16/37: base = two_mks_plus[two_mks_plus.groupby('from_id').party.transform('nunique')==1]\n",
      "16/38:\n",
      "print ('Total number of basers: %d' % base.from_id.nunique())\n",
      "print ('Ratio of basers from all users: %.2f' % (base.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of basers from two-mks-plus: %.2f' % (base.from_id.nunique() / two_mks_plus.from_id.nunique()))\n",
      "16/39: st.party.unique()\n",
      "16/40:\n",
      "camps = { 'מרצ': 'opposition',\n",
      "          'יש עתיד': 'opposition',\n",
      "          'ליכוד': 'coalition',\n",
      "          'הבית היהודי בראשות נפתלי בנט': 'coalition',\n",
      "          'המחנה הציוני': 'opposition',\n",
      "          'הרשימה המשותפת (חד”ש, רע”מ, בל”ד, תע”ל)': 'opposition',\n",
      "          'כולנו בראשות משה כחלון': 'coalition',\n",
      "          'ישראל ביתנו': 'coalition' }\n",
      "16/41: st['camp'] = st['party'].map(camps)\n",
      "16/42: two_mks_plus['camp'] = two_mks_plus['party'].map(camps)\n",
      "16/43: two_mks_plus.groupby('camp').from_id.nunique()\n",
      "16/44: campers = two_mks_plus[two_mks_plus.groupby('from_id').camp.transform('nunique')==1]\n",
      "16/45:\n",
      "print ('Total number of campers: %d' % campers.from_id.nunique())\n",
      "print ('Ratio of campers from all users: %.2f' % (campers.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of campers from two_mks_plus: %.2f' % (campers.from_id.nunique() / two_mks_plus.from_id.nunique()))\n",
      "print ('Coalition | Opposition ratio from campers: %.2f | %.2f' % \n",
      "       ((campers[campers.camp=='coalition'].from_id.nunique() / campers.from_id.nunique()),\n",
      "       (campers[campers.camp=='opposition'].from_id.nunique() / campers.from_id.nunique())))\n",
      "cg = st.groupby('camp')\n",
      "print ('Coalition | Opposition ratios from total users: %.2f | %.2f' % \n",
      "       ((cg.from_id.nunique()['coalition'] / st.from_id.nunique()),\n",
      "       ((cg.from_id.nunique()['opposition'] / st.from_id.nunique()))))\n",
      "\n",
      "print ('Coalition | Opposition total comments: %.2f | %.2f' % \n",
      "       ((cg.id.sum()['coalition'] / st.id.sum()),\n",
      "       ((cg.id.sum()['opposition'] / st.id.sum()))))\n",
      "16/46: top_commented = list(st.groupby('name').id.sum().sort_values(ascending=False).index)\n",
      "16/47:\n",
      "top_10_commented = top_commented[:10]\n",
      "top_10_commented\n",
      "16/48: celebz =  two_mks_plus[two_mks_plus.groupby('from_id').name.transform(lambda x: x.isin(top_10_commented).all())]\n",
      "16/49: mat.head()\n",
      "16/50:\n",
      "#celebz =  two_mks_plus[two_mks_plus.groupby('from_id').name.transform(lambda x: x.isin(top_10_commented).all())]\n",
      "celebs = two_mks_plus[two_mks_plus.from_id.isin(mat.loc[:, order[i+1:]].isna().all(axis=1).reset_index().from_id.index)]\n",
      "16/51:\n",
      "#celebz =  two_mks_plus[two_mks_plus.groupby('from_id').name.transform(lambda x: x.isin(top_10_commented).all())]\n",
      "celebs = two_mks_plus[two_mks_plus.from_id.isin(mat.loc[:, top_commented[i+1:]].isna().all(axis=1).reset_index().from_id.index)]\n",
      "16/52:\n",
      "#celebz =  two_mks_plus[two_mks_plus.groupby('from_id').name.transform(lambda x: x.isin(top_10_commented).all())]\n",
      "mat = mat[top_commented]\n",
      "celebs = two_mks_plus[two_mks_plus.from_id.isin(mat.loc[:, top_commented[i+1:]].isna().all(axis=1).reset_index().from_id.index)]\n",
      "16/53:\n",
      "#celebz =  two_mks_plus[two_mks_plus.groupby('from_id').name.transform(lambda x: x.isin(top_10_commented).all())]\n",
      "mat = mat[top_commented]\n",
      "celebs = two_mks_plus[two_mks_plus.from_id.isin(mat.loc[:, top_commented[10:]].isna().all(axis=1).reset_index().from_id.index)]\n",
      "16/54: celebz.head()\n",
      "16/55:\n",
      "#celebz =  two_mks_plus[two_mks_plus.groupby('from_id').name.transform(lambda x: x.isin(top_10_commented).all())]\n",
      "mat = mat[top_commented]\n",
      "celebz = two_mks_plus[two_mks_plus.from_id.isin(mat.loc[:, top_commented[10:]].isna().all(axis=1).reset_index().from_id.index)]\n",
      "16/56: celebz.head()\n",
      "16/57: mat.head()\n",
      "16/58: mat.loc[:, top_commented[10:]].isna().all(axis=1).reset_index().from_id.index\n",
      "16/59: mat.loc[:, top_commented[10:]].isna().all(axis=1).from_id.index\n",
      "16/60: mat.loc[:, top_commented[10:]].isna().all(axis=1).index\n",
      "16/61:\n",
      "#celebz =  two_mks_plus[two_mks_plus.groupby('from_id').name.transform(lambda x: x.isin(top_10_commented).all())]\n",
      "mat = mat[top_commented]\n",
      "celebz = two_mks_plus[two_mks_plus.from_id.isin(mat.loc[:, top_commented[10:]].isna().all(axis=1).index)]\n",
      "16/62: num_celebiez_no_base_camp = len(set(celebz.from_id.drop_duplicates().values).difference(set(campers.from_id.drop_duplicates().values)).difference(set(base.from_id.drop_duplicates().values)))\n",
      "16/63:\n",
      "print ('Total number of celeb followers (celebies): %d' % celebz.from_id.nunique())\n",
      "print ('Ratio of celebies from all users: %.2f' % (celebz.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of celebies from two_mks_plus: %.2f' % (celebz.from_id.nunique() / two_mks_plus.from_id.nunique()))\n",
      "print ('Remove campers and base from celebies: %d' % num_celebiez_no_base_camp)\n",
      "16/64:\n",
      "#celebz =  two_mks_plus[two_mks_plus.groupby('from_id').name.transform(lambda x: x.isin(top_10_commented).all())]\n",
      "mat = mat[top_commented]\n",
      "celebz = two_mks_plus[two_mks_plus.from_id.isin(mat[mat.loc[:, top_commented[10:]].isna().all(axis=1)].index)]\n",
      "16/65: num_celebiez_no_base_camp = len(set(celebz.from_id.drop_duplicates().values).difference(set(campers.from_id.drop_duplicates().values)).difference(set(base.from_id.drop_duplicates().values)))\n",
      "16/66:\n",
      "print ('Total number of celeb followers (celebies): %d' % celebz.from_id.nunique())\n",
      "print ('Ratio of celebies from all users: %.2f' % (celebz.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of celebies from two_mks_plus: %.2f' % (celebz.from_id.nunique() / two_mks_plus.from_id.nunique()))\n",
      "print ('Remove campers and base from celebies: %d' % num_celebiez_no_base_camp)\n",
      "16/67: top_commented[:10]\n",
      "16/68: fans.reindex(top_commented).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "16/69:\n",
      "only_bibi = mat.loc[:, top_commented[1:]].isna().all(axis=1)\n",
      "only_bibi[only_bibi].shape\n",
      "16/70:\n",
      "def get_accum_df(mat, order):\n",
      "    onlys = []\n",
      "    mat_ordered = mat.copy()\n",
      "    mat_ordered = mat_ordered[order]\n",
      "    \n",
      "    for i, name in enumerate(order):\n",
      "        only = mat.loc[:, order[i+1:]].isna().all(axis=1)\n",
      "        onlys.append(only[only].shape[0])\n",
      "        \n",
      "    user_accum_df = pd.DataFrame({'mk': order, 'user_accum': onlys})\n",
      "    user_accum_df['party'] = user_accum_df['mk'].map(mkdet.set_index('folder_name')['party'])\n",
      "    user_accum_df['camp'] = user_accum_df['party'].map(camps)\n",
      "    user_accum_df['delta'] = user_accum_df.user_accum.diff()\n",
      "    unique_delta_ratio = (uniques.reindex(top_commented) / user_accum_df.set_index('mk').delta).drop('Benjamin_Netanyahu')\n",
      "    user_accum_df['unique_delta_ratio'] = user_accum_df['mk'].map(unique_delta_ratio)\n",
      "    user_accum_df['delta_n'] = user_accum_df['delta'] / user_accum_df['delta'].max()\n",
      "    \n",
      "    return user_accum_df\n",
      "16/71:\n",
      "user_accum_df = get_accum_df(mat, top_commented)\n",
      "user_accum_df.head()\n",
      "18/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "18/2:\n",
      "%%time\n",
      "df = pd.read_pickle('turf/data_df.pkl.gz', compression='gzip')\n",
      "18/3: df.dtypes\n",
      "18/4:\n",
      "sdf = pd.read_pickle('turf/shared_df.pkl.gz', compression='gzip')\n",
      "sdf.dtypes\n",
      "18/5: st = pd.read_pickle('turf/total_stats_no_multiindex.pkl.gz', compression='gzip')\n",
      "18/6:\n",
      "# OLD STUFF\n",
      "#save(st, 'turf/total_stats.pkl.gz')\n",
      "#st = pd.read_pickle( 'turf/total_stats.pkl.gz', compression='gzip')\n",
      "18/7: st.head()\n",
      "18/8: st.shape\n",
      "18/9: df[df.is_post==0].shape\n",
      "18/10: print ('Total number of comments: %d' % st.id.sum())\n",
      "18/11: print ('Total number of distinct users (commenters): %d' % st.from_id.nunique())\n",
      "18/12:\n",
      "counts = st.groupby('from_id').id.sum()\n",
      "one_timers = counts[counts==1]\n",
      "\n",
      "print ('Total number of one-timers: %d' % one_timers.shape[0])\n",
      "print ('Ratio of one timers from all users: %.2f' % (one_timers.shape[0] / st.from_id.nunique()))\n",
      "18/13: mat = st.pivot(index='from_id', columns='name', values='id')\n",
      "18/14: mat[(mat.count(axis=1)==1)].shape[0]\n",
      "18/15: mat.loc[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu==1), 'Benjamin_Netanyahu'].shape[0]\n",
      "18/16:\n",
      "print ('Ratio of Bibi from one-timers: %.2f' % (mat[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu==1)].shape[0] \n",
      " / counts[counts==1].shape[0]))\n",
      "18/17: print ('Ratio of users that commented 10 times and up: %.2f' % (counts[counts>=10].shape[0]/st.from_id.nunique()))\n",
      "18/18: (st.reset_index().groupby('from_id').name.nunique().value_counts().head(20)/st.reset_index().from_id.nunique()).plot(kind='bar')\n",
      "18/19: (st.reset_index().groupby('from_id').id.sum().value_counts().head(20)/st.reset_index().from_id.nunique()).plot(kind='bar')\n",
      "18/20:\n",
      "fans_users = (st.reset_index()\n",
      " .loc[st.groupby('from_id').name.transform('nunique')==1, ['from_id', 'name', 'party']])\n",
      "18/21: fans = ( fans_users.name.value_counts())\n",
      "18/22: fans.head(10)\n",
      "18/23:\n",
      "print ('Total number of fans: %d' % fans.sum())\n",
      "print ('Ratio of fans from all users: %.2f' % (fans.sum() / st.from_id.nunique()))\n",
      "print ('Ratio of one-timers from fans: %.2f' % (counts[counts==1].shape[0] / fans.sum()))\n",
      "18/24:\n",
      "(pd.concat([fans, \n",
      "            fans/st.from_id.nunique(), \n",
      "            fans/st.groupby('name').from_id.nunique()],\n",
      "            axis=1, keys=['num_uniques', 'ratio_from_all_users', 'ratio_from_own_users'])\n",
      "             .sort_values(by='ratio_from_own_users', ascending=False)).head(10)\n",
      "18/25:\n",
      "mkdet = pd.read_csv('turf/mk_details.csv', sep='\\t')\n",
      "mkdet.head()\n",
      "18/26: df['party'] = df['name'].map(mkdet.set_index('folder_name')['party'])\n",
      "18/27: df.groupby(['party', 'langdetect']).size().unstack()\n",
      "18/28: st['party'] = st['name'].map(mkdet.set_index('folder_name')['party'])\n",
      "18/29:\n",
      "two_mks_plus = (st.reset_index()\n",
      " .loc[st.groupby('from_id').name.transform('nunique')>1, ['from_id', 'name', 'party']])\n",
      "18/30: two_mks_plus.head()\n",
      "18/31: two_mks_plus.from_id.nunique()\n",
      "18/32: two_mks_plus.party.value_counts()\n",
      "18/33: two_mks_plus.groupby('party').from_id.nunique()\n",
      "18/34: base = two_mks_plus[two_mks_plus.groupby('from_id').party.transform('nunique')==1]\n",
      "18/35:\n",
      "print ('Total number of basers: %d' % base.from_id.nunique())\n",
      "print ('Ratio of basers from all users: %.2f' % (base.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of basers from two-mks-plus: %.2f' % (base.from_id.nunique() / two_mks_plus.from_id.nunique()))\n",
      "18/36: st.party.unique()\n",
      "18/37:\n",
      "camps = { 'מרצ': 'opposition',\n",
      "          'יש עתיד': 'opposition',\n",
      "          'ליכוד': 'coalition',\n",
      "          'הבית היהודי בראשות נפתלי בנט': 'coalition',\n",
      "          'המחנה הציוני': 'opposition',\n",
      "          'הרשימה המשותפת (חד”ש, רע”מ, בל”ד, תע”ל)': 'opposition',\n",
      "          'כולנו בראשות משה כחלון': 'coalition',\n",
      "          'ישראל ביתנו': 'coalition' }\n",
      "18/38: st['camp'] = st['party'].map(camps)\n",
      "18/39: two_mks_plus['camp'] = two_mks_plus['party'].map(camps)\n",
      "18/40: two_mks_plus.groupby('camp').from_id.nunique()\n",
      "18/41: campers = two_mks_plus[two_mks_plus.groupby('from_id').camp.transform('nunique')==1]\n",
      "18/42:\n",
      "print ('Total number of campers: %d' % campers.from_id.nunique())\n",
      "print ('Ratio of campers from all users: %.2f' % (campers.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of campers from two_mks_plus: %.2f' % (campers.from_id.nunique() / two_mks_plus.from_id.nunique()))\n",
      "print ('Coalition | Opposition ratio from campers: %.2f | %.2f' % \n",
      "       ((campers[campers.camp=='coalition'].from_id.nunique() / campers.from_id.nunique()),\n",
      "       (campers[campers.camp=='opposition'].from_id.nunique() / campers.from_id.nunique())))\n",
      "cg = st.groupby('camp')\n",
      "print ('Coalition | Opposition ratios from total users: %.2f | %.2f' % \n",
      "       ((cg.from_id.nunique()['coalition'] / st.from_id.nunique()),\n",
      "       ((cg.from_id.nunique()['opposition'] / st.from_id.nunique()))))\n",
      "\n",
      "print ('Coalition | Opposition total comments: %.2f | %.2f' % \n",
      "       ((cg.id.sum()['coalition'] / st.id.sum()),\n",
      "       ((cg.id.sum()['opposition'] / st.id.sum()))))\n",
      "18/43: top_commented = list(st.groupby('name').id.sum().sort_values(ascending=False).index)\n",
      "18/44:\n",
      "top_10_commented = top_commented[:10]\n",
      "top_10_commented\n",
      "18/45:\n",
      "#celebz =  two_mks_plus[two_mks_plus.groupby('from_id').name.transform(lambda x: x.isin(top_10_commented).all())]\n",
      "mat = mat[top_commented]\n",
      "celebz = two_mks_plus[two_mks_plus.from_id.isin(mat[mat.loc[:, top_commented[10:]].isna().all(axis=1)].index)]\n",
      "18/46: num_celebiez_no_base_camp = len(set(celebz.from_id.drop_duplicates().values).difference(set(campers.from_id.drop_duplicates().values)).difference(set(base.from_id.drop_duplicates().values)))\n",
      "18/47:\n",
      "print ('Total number of celeb followers (celebies): %d' % celebz.from_id.nunique())\n",
      "print ('Ratio of celebies from all users: %.2f' % (celebz.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of celebies from two_mks_plus: %.2f' % (celebz.from_id.nunique() / two_mks_plus.from_id.nunique()))\n",
      "print ('Remove campers and base from celebies: %d' % num_celebiez_no_base_camp)\n",
      "18/48: top_commented[:10]\n",
      "18/49: fans.reindex(top_commented).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "18/50:\n",
      "only_bibi = mat.loc[:, top_commented[1:]].isna().all(axis=1)\n",
      "only_bibi[only_bibi].shape\n",
      "18/51:\n",
      "def get_accum_df(mat, order):\n",
      "    onlys = []\n",
      "    mat_ordered = mat.copy()\n",
      "    mat_ordered = mat_ordered[order]\n",
      "    \n",
      "    for i, name in enumerate(order):\n",
      "        only = mat_ordered.loc[:, order[i+1:]].isna().all(axis=1)\n",
      "        onlys.append(only[only].shape[0])\n",
      "        \n",
      "    user_accum_df = pd.DataFrame({'mk': order, 'user_accum': onlys})\n",
      "    user_accum_df['party'] = user_accum_df['mk'].map(mkdet.set_index('folder_name')['party'])\n",
      "    user_accum_df['camp'] = user_accum_df['party'].map(camps)\n",
      "    user_accum_df['delta'] = user_accum_df.user_accum.diff()\n",
      "    unique_delta_ratio = (uniques.reindex(top_commented) / user_accum_df.set_index('mk').delta).drop('Benjamin_Netanyahu')\n",
      "    user_accum_df['unique_delta_ratio'] = user_accum_df['mk'].map(unique_delta_ratio)\n",
      "    user_accum_df['delta_n'] = user_accum_df['delta'] / user_accum_df['delta'].max()\n",
      "    \n",
      "    return user_accum_df\n",
      "18/52:\n",
      "user_accum_df = get_accum_df(mat, top_commented)\n",
      "user_accum_df.head()\n",
      "18/53:\n",
      "def get_accum_df(mat, order):\n",
      "    onlys = []\n",
      "    \n",
      "    for i, name in enumerate(order):\n",
      "        only = mat.loc[:, order[i+1:]].isna().all(axis=1)\n",
      "        onlys.append(only[only].shape[0])\n",
      "        \n",
      "    user_accum_df = pd.DataFrame({'mk': order, 'user_accum': onlys})\n",
      "    user_accum_df['party'] = user_accum_df['mk'].map(mkdet.set_index('folder_name')['party'])\n",
      "    user_accum_df['camp'] = user_accum_df['party'].map(camps)\n",
      "    user_accum_df['delta'] = user_accum_df.user_accum.diff()\n",
      "    unique_delta_ratio = (fans.reindex(top_commented) / user_accum_df.set_index('mk').delta).drop('Benjamin_Netanyahu')\n",
      "    user_accum_df['unique_delta_ratio'] = user_accum_df['mk'].map(unique_delta_ratio)\n",
      "    user_accum_df['delta_n'] = user_accum_df['delta'] / user_accum_df['delta'].max()\n",
      "    \n",
      "    return user_accum_df\n",
      "18/54:\n",
      "user_accum_df = get_accum_df(mat, top_commented)\n",
      "user_accum_df.head()\n",
      "18/55: user_accum_df.set_index('mk').user_accum.plot(kind='bar', figsize=(15,10), rot=75)\n",
      "18/56:\n",
      "import altair as alt\n",
      "alt.renderers.enable('notebook')\n",
      "18/57:\n",
      "def chart_user_accum(accum_df, order, order_name):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('user_accum',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'user_accum']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = f'User accumulation by {order_name} order - left to right, only users that comment inside the group until the current column')\n",
      "\n",
      "\n",
      "def chart_user_accum_delta(accum_df, order, order_name):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('delta',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'delta']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = f'User accumulation delta by {order_name} order - left to right, only users that comment inside the group until the current column')\n",
      "\n",
      "\n",
      "def chart_user_accum_unique_delta_ratio(accum_df, order, order_name):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('unique_delta_ratio',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'unique_delta_ratio']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = f'Page unique count divided by user_accum delta by {order_name} order (to measure how much a page is part of the group)')\n",
      "18/58:\n",
      "(chart_user_accum(user_accum_df, top_commented, 'Top Commented') \n",
      " & chart_user_accum_delta(user_accum_df, top_commented, 'Top Commented')\n",
      " & chart_user_accum_unique_delta_ratio(user_accum_df, top_commented, 'Top Commented')).properties(title='Top Commented')\n",
      "18/59: user_accum_df[['delta_n', 'unique_delta_ratio']].div(user_accum_df[['delta_n', 'unique_delta_ratio']].sum(axis=1), axis=0).plot(kind='bar', stacked=True, figsize=(15,10), rot=75, width=1.0)\n",
      "18/60:\n",
      "print ('First celebiez group: ', top_commented[:6])\n",
      "print ('Second celebiez group: ', top_commented[:15])\n",
      "18/61: top_mean_commented = list(df.loc[df.is_post==1].groupby('name').comment_count.mean().sort_values(ascending=False).index)\n",
      "18/62: uniques.reindex(top_mean_commented).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "18/63: fans.reindex(top_mean_commented).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "18/64: mean_com_accum_df = get_accum_df(mat, top_mean_commented)\n",
      "18/65:\n",
      "(chart_user_accum(mean_com_accum_df, top_mean_commented, 'Top Mean Commented') \n",
      " & chart_user_accum_delta(mean_com_accum_df, top_mean_commented, 'Top Mean Commented') \n",
      " & chart_user_accum_unique_delta_ratio(mean_com_accum_df, top_mean_commented, 'Top Mean Commented')).properties(title='Top Mean Commented')\n",
      "18/66: top_liked = list(df.loc[df.is_post==1].groupby('name').like_count.mean().sort_values(ascending=False).index)\n",
      "18/67: uniques.reindex(top_liked).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "18/68:\n",
      "labels = ('All', 'One Timers', 'Fans', 'two_mks_plus', 'Basers', 'Campers')\n",
      "sets = [set(st.from_id.values), set(one_timers.index), set(fans_users.from_id.values), set(two_mks_plus.from_id.values), set(base.from_id.values), set(campers.from_id.values)]\n",
      "\n",
      "set_dict = dict(zip(labels, sets))\n",
      "18/69:\n",
      "inter_set_dict = {}\n",
      "for i in set_dict:\n",
      "    inter_set_dict[i] = {}\n",
      "    for j in set_dict:\n",
      "        inter_set_dict[i][j] = len(set_dict[i].intersection(set_dict[j]))\n",
      "18/70: pd.DataFrame(inter_set_dict).reindex(labels)\n",
      "18/71: pd.DataFrame(inter_set_dict).reindex(labels).to_csv('turf/group_intersections.csv')\n",
      "18/72: from matplotlib_venn import venn3\n",
      "18/73: ax.get_figure().savefig('turf/venn_one_timers_fans.png')\n",
      "18/74: from matplotlib_venn import venn3\n",
      "18/75:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(one_timers.index), set(fans_users.from_id.values)], set_labels = ('All', 'One Timers', 'Fans', ), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.25)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.20)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.20)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.09)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]-0.19)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.19)\n",
      "18/76: ax.get_figure().savefig('turf/venn_one_timers_fans.png')\n",
      "18/77:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(base.from_id.values), set(campers.from_id.values)], set_labels = ('All', 'Basers', 'Campers'), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.15)\n",
      "v.get_label_by_id('111').set_y(v.get_label_by_id('111').get_position()[1]-0.05)\n",
      "v.get_patch_by_id('101').set_color('blue')\n",
      "v.get_patch_by_id('111').set_color('green')\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.20)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.35)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.15)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]+0.25)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.1)\n",
      "18/78: ax.get_figure().savefig('turf/venn_basers_campers.png')\n",
      "18/79:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(two_mks_plus.from_id.values), set(base.from_id.values), set(campers.from_id.values)], set_labels = ('two_mks_plus', 'Basers', 'Campers'), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.15)\n",
      "v.get_label_by_id('111').set_y(v.get_label_by_id('111').get_position()[1]-0.05)\n",
      "v.get_patch_by_id('101').set_color('blue')\n",
      "v.get_patch_by_id('111').set_color('green')\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.35)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.25)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.15)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]+0.05)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.1)\n",
      "18/80: ax.get_figure().savefig('turf/venn_two_mks_plus_basers_campers.png')\n",
      "18/81: user_counts = st.groupby('from_id').id.sum()\n",
      "18/82:\n",
      "perc = pd.qcut(user_counts, 20, duplicates='drop')\n",
      "perc.value_counts(sort=False)\n",
      "18/83:\n",
      "perc_bah = pd.cut(user_counts, [0,1,2,3,4,5,7,11,24,9359])\n",
      "(perc_bah.value_counts(sort=False))\n",
      "18/84: (perc_bah.value_counts(sort=False)/ user_counts.shape[0])\n",
      "18/85: (perc_bah.value_counts(sort=False)/ user_counts.shape[0]).plot(kind='bar',figsize=(10,7))\n",
      "18/86:\n",
      "dedicated = st[st.from_id.isin(user_counts[user_counts>24].index)]\n",
      "dedicated.from_id.nunique()\n",
      "18/87:\n",
      "campers = campers[(~campers.from_id.isin(base.from_id))& (~campers.from_id.isin(dedicated.from_id))]\n",
      "campers.from_id.nunique()\n",
      "18/88:\n",
      "base = base[(~base.from_id.isin(dedicated.from_id))]\n",
      "base.from_id.nunique()\n",
      "18/89:\n",
      "two_mks_plus = two_mks_plus[(~two_mks_plus.from_id.isin(base.from_id)) \n",
      "                            & (~two_mks_plus.from_id.isin(campers.from_id))\n",
      "                            & (~two_mks_plus.from_id.isin(dedicated.from_id))]\n",
      "two_mks_plus.from_id.nunique()\n",
      "18/90: campers.from_id.nunique() + base.from_id.nunique() + two_mks_plus.from_id.nunique()\n",
      "18/91:\n",
      "fans_users = fans_users[~fans_users.from_id.isin(one_timers.index)\n",
      "                        & (~fans_users.from_id.isin(dedicated.from_id))]\n",
      "fans_users.from_id.nunique()\n",
      "18/92: df[df.is_post==1].shape[0], df[df.is_post==1].comment_count.mean()\n",
      "18/93: df[df.is_post==1].groupby('name').comment_count.mean().sort_values().plot(kind='barh', figsize=(10,15))\n",
      "18/94: one_timers.shape\n",
      "18/95:\n",
      "one_timers_page = st[st.from_id.isin(one_timers.index)].groupby('name').size()\n",
      "one_timers_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "18/96:\n",
      "gfans = st[st.from_id.isin(fans_users.from_id)].groupby('from_id')\n",
      "gfans.id.sum().mean(), gfans.post_id.sum().mean(), gfans.id.sum().mean()/ gfans.post_id.sum().mean()\n",
      "18/97:\n",
      "fans_page = st[st.from_id.isin(fans_users.from_id)].groupby('name').size()\n",
      "fans_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "18/98:\n",
      "gbase = st[st.from_id.isin(base.from_id)].groupby('from_id')\n",
      "gbase.id.sum().mean(), gbase.post_id.sum().mean(), gbase.id.sum().mean()/ gbase.post_id.sum().mean()\n",
      "18/99:\n",
      "base_page = st[st.from_id.isin(base.from_id)].groupby('name').size()\n",
      "base_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "18/100:\n",
      "gcamp = st[st.from_id.isin(campers.from_id)].groupby('from_id')\n",
      "gcamp.id.sum().mean(), gcamp.post_id.sum().mean(), gcamp.id.sum().mean()/ gcamp.post_id.sum().mean()\n",
      "18/101: st[st.from_id.isin(campers.from_id)].id.sum()\n",
      "18/102:\n",
      "camp_page = st[st.from_id.isin(campers.from_id)].groupby('name').size()\n",
      "camp_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "18/103: st[(st.from_id.isin(two_mks_plus.from_id))].id.sum()\n",
      "18/104:\n",
      "gtwo = st[(st.from_id.isin(two_mks_plus.from_id))].groupby('from_id')\n",
      "gtwo.id.sum().mean(), gtwo.post_id.sum().mean(), gtwo.id.sum().mean()/gtwo.post_id.sum().mean()\n",
      "18/105:\n",
      "twomk_page = st[st.from_id.isin(two_mks_plus.from_id)].groupby('name').size()\n",
      "twomk_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "18/106:\n",
      "gded = dedicated.groupby('from_id')\n",
      "gded.id.sum().mean(), gded.post_id.sum().mean(), gded.id.sum().mean()/ gded.post_id.sum().mean()\n",
      "18/107:\n",
      "ded_page = st[st.from_id.isin(dedicated.from_id)].groupby('name').size()\n",
      "ded_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "18/108:\n",
      "page_cats = pd.concat([one_timers_page, fans_page, base_page, camp_page, twomk_page, ded_page], axis=1,\n",
      "          keys=['One-timers', 'Fans', 'Basers', 'Campers', 'Two-MK-Plus', 'Dedicated'])\n",
      "18/109: page_cats = page_cats.stack().reset_index().rename(columns={'level_1': 'Category', 0: 'ratio'})\n",
      "18/110:\n",
      "page_cat_ratios = page_cats.div(page_cats.sum(axis=1), axis=0).reindex(top_commented).stack().reset_index().rename(columns={'level_1': 'Category', 0: 'ratio'})\n",
      "page_cat_ratios.head(10)\n",
      "18/111:\n",
      "page_cat_ratios = page_cats.div(page_cats.sum(axis=1), axis=0).reindex(top_commented).stack().reset_index().rename(columns={'level_1': 'Category', 0: 'ratio'})\n",
      "page_cat_ratios.head(10)\n",
      "18/112: page_cats\n",
      "18/113:\n",
      "regular = alt.Chart(page_cats).mark_bar().encode(\n",
      "    alt.X('ratio:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None),\n",
      "    alt.Color('Category:N'),\n",
      "    #tooltip = ['start_zone','mh_cut', 'size']\n",
      ").properties(height=1200, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('ratio:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented, axis=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "18/114:\n",
      "regular = alt.Chart(page_cats).mark_bar().encode(\n",
      "    alt.X('ratio:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('Category:N'),\n",
      "    #tooltip = ['start_zone','mh_cut', 'size']\n",
      ").properties(height=1200, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('ratio:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "18/115:\n",
      "regular = alt.Chart(page_cats).mark_bar().encode(\n",
      "    alt.X('ratio:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('Category:N'),\n",
      "    #tooltip = ['start_zone','mh_cut', 'size']\n",
      ").properties(height=1200, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('ratio:Q',\n",
      "        axis=alt.Axis(),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "18/116:\n",
      "regular = alt.Chart(page_cats).mark_bar().encode(\n",
      "    alt.X('ratio:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('Category:N'),\n",
      "    #tooltip = ['start_zone','mh_cut', 'size']\n",
      ").properties(height=1200, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('ratio:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "18/117:\n",
      "regular = alt.Chart(page_cats).mark_bar().encode(\n",
      "    alt.X('ratio:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('Category:N'),\n",
      "    #tooltip = ['start_zone','mh_cut', 'size']\n",
      ").properties(height=1200, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('ratio:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "18/118:\n",
      "regular = alt.Chart(page_cats).mark_bar().encode(\n",
      "    alt.X('ratio:Q', axis=alt.Axis(title=None, position='top')),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('Category:N'),\n",
      "    #tooltip = ['start_zone','mh_cut', 'size']\n",
      ").properties(height=1200, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('ratio:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "18/119:\n",
      "regular = alt.Chart(page_cats).mark_bar().encode(\n",
      "    alt.X('ratio:Q', axis=alt.Axis(title=None, position='top')),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('Category:N'),\n",
      "    #tooltip = ['start_zone','mh_cut', 'size']\n",
      ").properties(height=1200, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('ratio:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "18/120:\n",
      "regular = alt.Chart(page_cats).mark_bar().encode(\n",
      "    alt.X('ratio:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('Category:N'),\n",
      "    #tooltip = ['start_zone','mh_cut', 'size']\n",
      ").properties(height=1200, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('ratio:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "18/121:\n",
      "regular = alt.Chart(page_cats).mark_bar().encode(\n",
      "    alt.X('user_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('Category:N'),\n",
      "    tooltip = ['Category','user_count']\n",
      ").properties(height=1200, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('user_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "18/122: page_cats = page_cats.stack().reset_index().rename(columns={'level_1': 'Category', 0: 'user_count'})\n",
      "18/123:\n",
      "regular = alt.Chart(page_cats).mark_bar().encode(\n",
      "    alt.X('user_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('Category:N'),\n",
      "    tooltip = ['Category','user_count']\n",
      ").properties(height=1200, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('user_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "18/124:\n",
      "page_cats = pd.concat([one_timers_page, fans_page, base_page, camp_page, twomk_page, ded_page], axis=1,\n",
      "          keys=['One-timers', 'Fans', 'Basers', 'Campers', 'Two-MK-Plus', 'Dedicated'])\n",
      "18/125: page_cats = page_cats.stack().reset_index().rename(columns={'level_1': 'Category', 0: 'user_count'})\n",
      "18/126:\n",
      "regular = alt.Chart(page_cats).mark_bar().encode(\n",
      "    alt.X('user_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('Category:N'),\n",
      "    tooltip = ['Category','user_count']\n",
      ").properties(height=1200, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('user_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "18/127:\n",
      "regular = alt.Chart(page_cats).mark_bar().encode(\n",
      "    alt.X('user_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('Category:N'),\n",
      "    tooltip = ['name', 'Category','user_count']\n",
      ").properties(height=1200, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('user_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "19/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "19/2:\n",
      "%%time\n",
      "df = pd.read_pickle('turf/data_df.pkl.gz', compression='gzip')\n",
      "19/3: df.dtypes\n",
      "19/4:\n",
      "sdf = pd.read_pickle('turf/shared_df.pkl.gz', compression='gzip')\n",
      "sdf.dtypes\n",
      "19/5: st = pd.read_pickle('turf/total_stats_no_multiindex.pkl.gz', compression='gzip')\n",
      "19/6:\n",
      "# OLD STUFF\n",
      "#save(st, 'turf/total_stats.pkl.gz')\n",
      "#st = pd.read_pickle( 'turf/total_stats.pkl.gz', compression='gzip')\n",
      "19/7: st.head()\n",
      "19/8: st.shape\n",
      "19/9: df[df.is_post==0].shape\n",
      "19/10: print ('Total number of comments: %d' % st.id.sum())\n",
      "19/11: print ('Total number of distinct users (commenters): %d' % st.from_id.nunique())\n",
      "19/12:\n",
      "counts = st.groupby('from_id').id.sum()\n",
      "one_timers = counts[counts==1]\n",
      "\n",
      "print ('Total number of one-timers: %d' % one_timers.shape[0])\n",
      "print ('Ratio of one timers from all users: %.2f' % (one_timers.shape[0] / st.from_id.nunique()))\n",
      "19/13: mat = st.pivot(index='from_id', columns='name', values='id')\n",
      "19/14: mat[(mat.count(axis=1)==1)].shape[0]\n",
      "19/15: mat.loc[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu==1), 'Benjamin_Netanyahu'].shape[0]\n",
      "19/16:\n",
      "print ('Ratio of Bibi from one-timers: %.2f' % (mat[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu==1)].shape[0] \n",
      " / counts[counts==1].shape[0]))\n",
      "19/17: print ('Ratio of users that commented 10 times and up: %.2f' % (counts[counts>=10].shape[0]/st.from_id.nunique()))\n",
      "19/18: (st.reset_index().groupby('from_id').name.nunique().value_counts().head(20)/st.reset_index().from_id.nunique()).plot(kind='bar')\n",
      "19/19: (st.reset_index().groupby('from_id').id.sum().value_counts().head(20)/st.reset_index().from_id.nunique()).plot(kind='bar')\n",
      "19/20:\n",
      "fans_users = (st.reset_index()\n",
      " .loc[st.groupby('from_id').name.transform('nunique')==1, ['from_id', 'name', 'party']])\n",
      "19/21: fans = ( fans_users.name.value_counts())\n",
      "19/22: fans.head(10)\n",
      "19/23:\n",
      "print ('Total number of fans: %d' % fans.sum())\n",
      "print ('Ratio of fans from all users: %.2f' % (fans.sum() / st.from_id.nunique()))\n",
      "print ('Ratio of one-timers from fans: %.2f' % (counts[counts==1].shape[0] / fans.sum()))\n",
      "19/24:\n",
      "(pd.concat([fans, \n",
      "            fans/st.from_id.nunique(), \n",
      "            fans/st.groupby('name').from_id.nunique()],\n",
      "            axis=1, keys=['num_uniques', 'ratio_from_all_users', 'ratio_from_own_users'])\n",
      "             .sort_values(by='ratio_from_own_users', ascending=False)).head(10)\n",
      "19/25:\n",
      "mkdet = pd.read_csv('turf/mk_details.csv', sep='\\t')\n",
      "mkdet.head()\n",
      "19/26: df['party'] = df['name'].map(mkdet.set_index('folder_name')['party'])\n",
      "19/27: df.groupby(['party', 'langdetect']).size().unstack()\n",
      "19/28: st['party'] = st['name'].map(mkdet.set_index('folder_name')['party'])\n",
      "19/29:\n",
      "two_mks_plus = (st.reset_index()\n",
      " .loc[st.groupby('from_id').name.transform('nunique')>1, ['from_id', 'name', 'party']])\n",
      "19/30: two_mks_plus.head()\n",
      "19/31: two_mks_plus.from_id.nunique()\n",
      "19/32: two_mks_plus.party.value_counts()\n",
      "19/33: two_mks_plus.groupby('party').from_id.nunique()\n",
      "19/34: base = two_mks_plus[two_mks_plus.groupby('from_id').party.transform('nunique')==1]\n",
      "19/35:\n",
      "print ('Total number of basers: %d' % base.from_id.nunique())\n",
      "print ('Ratio of basers from all users: %.2f' % (base.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of basers from two-mks-plus: %.2f' % (base.from_id.nunique() / two_mks_plus.from_id.nunique()))\n",
      "19/36: st.party.unique()\n",
      "19/37:\n",
      "camps = { 'מרצ': 'opposition',\n",
      "          'יש עתיד': 'opposition',\n",
      "          'ליכוד': 'coalition',\n",
      "          'הבית היהודי בראשות נפתלי בנט': 'coalition',\n",
      "          'המחנה הציוני': 'opposition',\n",
      "          'הרשימה המשותפת (חד”ש, רע”מ, בל”ד, תע”ל)': 'opposition',\n",
      "          'כולנו בראשות משה כחלון': 'coalition',\n",
      "          'ישראל ביתנו': 'coalition' }\n",
      "19/38: st['camp'] = st['party'].map(camps)\n",
      "19/39: two_mks_plus['camp'] = two_mks_plus['party'].map(camps)\n",
      "19/40: two_mks_plus.groupby('camp').from_id.nunique()\n",
      "19/41: campers = two_mks_plus[two_mks_plus.groupby('from_id').camp.transform('nunique')==1]\n",
      "19/42:\n",
      "print ('Total number of campers: %d' % campers.from_id.nunique())\n",
      "print ('Ratio of campers from all users: %.2f' % (campers.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of campers from two_mks_plus: %.2f' % (campers.from_id.nunique() / two_mks_plus.from_id.nunique()))\n",
      "print ('Coalition | Opposition ratio from campers: %.2f | %.2f' % \n",
      "       ((campers[campers.camp=='coalition'].from_id.nunique() / campers.from_id.nunique()),\n",
      "       (campers[campers.camp=='opposition'].from_id.nunique() / campers.from_id.nunique())))\n",
      "cg = st.groupby('camp')\n",
      "print ('Coalition | Opposition ratios from total users: %.2f | %.2f' % \n",
      "       ((cg.from_id.nunique()['coalition'] / st.from_id.nunique()),\n",
      "       ((cg.from_id.nunique()['opposition'] / st.from_id.nunique()))))\n",
      "\n",
      "print ('Coalition | Opposition total comments: %.2f | %.2f' % \n",
      "       ((cg.id.sum()['coalition'] / st.id.sum()),\n",
      "       ((cg.id.sum()['opposition'] / st.id.sum()))))\n",
      "19/43: top_commented = list(st.groupby('name').id.sum().sort_values(ascending=False).index)\n",
      "19/44:\n",
      "top_10_commented = top_commented[:10]\n",
      "top_10_commented\n",
      "19/45:\n",
      "#celebz =  two_mks_plus[two_mks_plus.groupby('from_id').name.transform(lambda x: x.isin(top_10_commented).all())]\n",
      "celebz = two_mks_plus[two_mks_plus.from_id.isin(mat[mat.loc[:, top_commented[10:]].isna().all(axis=1)].index)]\n",
      "19/46: num_celebiez_no_base_camp = len(set(celebz.from_id.drop_duplicates().values).difference(set(campers.from_id.drop_duplicates().values)).difference(set(base.from_id.drop_duplicates().values)))\n",
      "19/47:\n",
      "print ('Total number of celeb followers (celebies): %d' % celebz.from_id.nunique())\n",
      "print ('Ratio of celebies from all users: %.2f' % (celebz.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of celebies from two_mks_plus: %.2f' % (celebz.from_id.nunique() / two_mks_plus.from_id.nunique()))\n",
      "print ('Remove campers and base from celebies: %d' % num_celebiez_no_base_camp)\n",
      "19/48: top_commented[:10]\n",
      "19/49: fans.reindex(top_commented).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "19/50:\n",
      "only_bibi = mat.loc[:, top_commented[1:]].isna().all(axis=1)\n",
      "only_bibi[only_bibi].shape\n",
      "19/51:\n",
      "def get_accum_df(mat, order):\n",
      "    onlys = []\n",
      "    \n",
      "    for i, name in enumerate(order):\n",
      "        only = mat.loc[:, order[i+1:]].isna().all(axis=1)\n",
      "        onlys.append(only[only].shape[0])\n",
      "        \n",
      "    user_accum_df = pd.DataFrame({'mk': order, 'user_accum': onlys})\n",
      "    user_accum_df['party'] = user_accum_df['mk'].map(mkdet.set_index('folder_name')['party'])\n",
      "    user_accum_df['camp'] = user_accum_df['party'].map(camps)\n",
      "    user_accum_df['delta'] = user_accum_df.user_accum.diff()\n",
      "    unique_delta_ratio = (fans.reindex(top_commented) / user_accum_df.set_index('mk').delta).drop('Benjamin_Netanyahu')\n",
      "    user_accum_df['unique_delta_ratio'] = user_accum_df['mk'].map(unique_delta_ratio)\n",
      "    user_accum_df['delta_n'] = user_accum_df['delta'] / user_accum_df['delta'].max()\n",
      "    \n",
      "    return user_accum_df\n",
      "19/52:\n",
      "user_accum_df = get_accum_df(mat, top_commented)\n",
      "user_accum_df.head()\n",
      "19/53: user_accum_df.set_index('mk').user_accum.plot(kind='bar', figsize=(15,10), rot=75)\n",
      "19/54:\n",
      "import altair as alt\n",
      "alt.renderers.enable('notebook')\n",
      "19/55:\n",
      "def chart_user_accum(accum_df, order, order_name):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('user_accum',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'user_accum']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = f'User accumulation by {order_name} order - left to right, only users that comment inside the group until the current column')\n",
      "\n",
      "\n",
      "def chart_user_accum_delta(accum_df, order, order_name):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('delta',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'delta']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = f'User accumulation delta by {order_name} order - left to right, only users that comment inside the group until the current column')\n",
      "\n",
      "\n",
      "def chart_user_accum_unique_delta_ratio(accum_df, order, order_name):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('unique_delta_ratio',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'unique_delta_ratio']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = f'Page unique count divided by user_accum delta by {order_name} order (to measure how much a page is part of the group)')\n",
      "19/56:\n",
      "(chart_user_accum(user_accum_df, top_commented, 'Top Commented') \n",
      " & chart_user_accum_delta(user_accum_df, top_commented, 'Top Commented')\n",
      " & chart_user_accum_unique_delta_ratio(user_accum_df, top_commented, 'Top Commented')).properties(title='Top Commented')\n",
      "19/57: user_accum_df[['delta_n', 'unique_delta_ratio']].div(user_accum_df[['delta_n', 'unique_delta_ratio']].sum(axis=1), axis=0).plot(kind='bar', stacked=True, figsize=(15,10), rot=75, width=1.0)\n",
      "19/58:\n",
      "print ('First celebiez group: ', top_commented[:6])\n",
      "print ('Second celebiez group: ', top_commented[:15])\n",
      "19/59: top_mean_commented = list(df.loc[df.is_post==1].groupby('name').comment_count.mean().sort_values(ascending=False).index)\n",
      "19/60: fans.reindex(top_mean_commented).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "19/61: mean_com_accum_df = get_accum_df(mat, top_mean_commented)\n",
      "19/62:\n",
      "(chart_user_accum(mean_com_accum_df, top_mean_commented, 'Top Mean Commented') \n",
      " & chart_user_accum_delta(mean_com_accum_df, top_mean_commented, 'Top Mean Commented') \n",
      " & chart_user_accum_unique_delta_ratio(mean_com_accum_df, top_mean_commented, 'Top Mean Commented')).properties(title='Top Mean Commented')\n",
      "19/63: top_liked = list(df.loc[df.is_post==1].groupby('name').like_count.mean().sort_values(ascending=False).index)\n",
      "19/64: fans.reindex(top_liked).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "19/65: like_accum_df = get_accum_df(mat, top_liked)\n",
      "19/66:\n",
      "(chart_user_accum(like_accum_df, top_liked, 'Top Liked') \n",
      " & chart_user_accum_delta(like_accum_df, top_liked, 'Top Liked') \n",
      " & chart_user_accum_unique_delta_ratio(like_accum_df, top_liked, 'Top Liked')).properties(title='Top Liked')\n",
      "19/67: top_shared = list(df.loc[df.is_post==1].groupby('name').share_count.mean().sort_values(ascending=False).index)\n",
      "19/68: fans.reindex(top_shared).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "19/69:\n",
      "share_accum_df = get_accum_df(mat, top_shared)\n",
      "share_accum_df.head()\n",
      "19/70:\n",
      "(chart_user_accum(share_accum_df, top_shared, 'Top Shared') \n",
      " & chart_user_accum_delta(share_accum_df, top_shared, 'Top Shared') \n",
      " & chart_user_accum_unique_delta_ratio(share_accum_df, top_shared, 'Top Shared')).properties(title='Top Shared')\n",
      "19/71: top_shared\n",
      "19/72:\n",
      "labels = ('All', 'One Timers', 'Fans', 'two_mks_plus', 'Basers', 'Campers')\n",
      "sets = [set(st.from_id.values), set(one_timers.index), set(fans_users.from_id.values), set(two_mks_plus.from_id.values), set(base.from_id.values), set(campers.from_id.values)]\n",
      "\n",
      "set_dict = dict(zip(labels, sets))\n",
      "19/73:\n",
      "inter_set_dict = {}\n",
      "for i in set_dict:\n",
      "    inter_set_dict[i] = {}\n",
      "    for j in set_dict:\n",
      "        inter_set_dict[i][j] = len(set_dict[i].intersection(set_dict[j]))\n",
      "19/74: pd.DataFrame(inter_set_dict).reindex(labels)\n",
      "19/75: pd.DataFrame(inter_set_dict).reindex(labels).to_csv('turf/group_intersections.csv')\n",
      "19/76: from matplotlib_venn import venn3\n",
      "19/77:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(one_timers.index), set(fans_users.from_id.values)], set_labels = ('All', 'One Timers', 'Fans', ), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.25)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.20)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.20)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.09)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]-0.19)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.19)\n",
      "19/78: ax.get_figure().savefig('turf/venn_one_timers_fans.png')\n",
      "19/79:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(base.from_id.values), set(campers.from_id.values)], set_labels = ('All', 'Basers', 'Campers'), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.15)\n",
      "v.get_label_by_id('111').set_y(v.get_label_by_id('111').get_position()[1]-0.05)\n",
      "v.get_patch_by_id('101').set_color('blue')\n",
      "v.get_patch_by_id('111').set_color('green')\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.20)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.35)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.15)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]+0.25)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.1)\n",
      "19/80: ax.get_figure().savefig('turf/venn_basers_campers.png')\n",
      "19/81:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(two_mks_plus.from_id.values), set(base.from_id.values), set(campers.from_id.values)], set_labels = ('two_mks_plus', 'Basers', 'Campers'), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.15)\n",
      "v.get_label_by_id('111').set_y(v.get_label_by_id('111').get_position()[1]-0.05)\n",
      "v.get_patch_by_id('101').set_color('blue')\n",
      "v.get_patch_by_id('111').set_color('green')\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.35)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.25)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.15)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]+0.05)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.1)\n",
      "19/82: ax.get_figure().savefig('turf/venn_two_mks_plus_basers_campers.png')\n",
      "19/83: user_counts = st.groupby('from_id').id.sum()\n",
      "19/84:\n",
      "perc = pd.qcut(user_counts, 20, duplicates='drop')\n",
      "perc.value_counts(sort=False)\n",
      "19/85:\n",
      "perc_bah = pd.cut(user_counts, [0,1,2,3,4,5,7,11,24,9359])\n",
      "(perc_bah.value_counts(sort=False))\n",
      "19/86: (perc_bah.value_counts(sort=False)/ user_counts.shape[0])\n",
      "19/87: (perc_bah.value_counts(sort=False)/ user_counts.shape[0]).plot(kind='bar',figsize=(10,7))\n",
      "19/88:\n",
      "dedicated = st[st.from_id.isin(user_counts[user_counts>24].index)]\n",
      "dedicated.from_id.nunique()\n",
      "19/89:\n",
      "campers = campers[(~campers.from_id.isin(base.from_id))& (~campers.from_id.isin(dedicated.from_id))]\n",
      "campers.from_id.nunique()\n",
      "19/90:\n",
      "base = base[(~base.from_id.isin(dedicated.from_id))]\n",
      "base.from_id.nunique()\n",
      "19/91:\n",
      "two_mks_plus = two_mks_plus[(~two_mks_plus.from_id.isin(base.from_id)) \n",
      "                            & (~two_mks_plus.from_id.isin(campers.from_id))\n",
      "                            & (~two_mks_plus.from_id.isin(dedicated.from_id))]\n",
      "two_mks_plus.from_id.nunique()\n",
      "19/92: campers.from_id.nunique() + base.from_id.nunique() + two_mks_plus.from_id.nunique()\n",
      "19/93:\n",
      "fans_users = fans_users[~fans_users.from_id.isin(one_timers.index)\n",
      "                        & (~fans_users.from_id.isin(dedicated.from_id))]\n",
      "fans_users.from_id.nunique()\n",
      "19/94: df[df.is_post==1].shape[0], df[df.is_post==1].comment_count.mean()\n",
      "19/95: df[df.is_post==1].groupby('name').comment_count.mean().sort_values().plot(kind='barh', figsize=(10,15))\n",
      "19/96: one_timers.shape\n",
      "19/97:\n",
      "one_timers_page = st[st.from_id.isin(one_timers.index)].groupby('name').size()\n",
      "one_timers_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "19/98:\n",
      "gfans = st[st.from_id.isin(fans_users.from_id)].groupby('from_id')\n",
      "gfans.id.sum().mean(), gfans.post_id.sum().mean(), gfans.id.sum().mean()/ gfans.post_id.sum().mean()\n",
      "19/99:\n",
      "fans_page = st[st.from_id.isin(fans_users.from_id)].groupby('name').size()\n",
      "fans_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "19/100:\n",
      "gbase = st[st.from_id.isin(base.from_id)].groupby('from_id')\n",
      "gbase.id.sum().mean(), gbase.post_id.sum().mean(), gbase.id.sum().mean()/ gbase.post_id.sum().mean()\n",
      "19/101:\n",
      "base_page = st[st.from_id.isin(base.from_id)].groupby('name').size()\n",
      "base_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "19/102:\n",
      "gcamp = st[st.from_id.isin(campers.from_id)].groupby('from_id')\n",
      "gcamp.id.sum().mean(), gcamp.post_id.sum().mean(), gcamp.id.sum().mean()/ gcamp.post_id.sum().mean()\n",
      "19/103: st[st.from_id.isin(campers.from_id)].id.sum()\n",
      "19/104:\n",
      "camp_page = st[st.from_id.isin(campers.from_id)].groupby('name').size()\n",
      "camp_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "19/105: st[(st.from_id.isin(two_mks_plus.from_id))].id.sum()\n",
      "19/106:\n",
      "gtwo = st[(st.from_id.isin(two_mks_plus.from_id))].groupby('from_id')\n",
      "gtwo.id.sum().mean(), gtwo.post_id.sum().mean(), gtwo.id.sum().mean()/gtwo.post_id.sum().mean()\n",
      "19/107:\n",
      "twomk_page = st[st.from_id.isin(two_mks_plus.from_id)].groupby('name').size()\n",
      "twomk_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "19/108:\n",
      "gded = dedicated.groupby('from_id')\n",
      "gded.id.sum().mean(), gded.post_id.sum().mean(), gded.id.sum().mean()/ gded.post_id.sum().mean()\n",
      "19/109:\n",
      "ded_page = st[st.from_id.isin(dedicated.from_id)].groupby('name').size()\n",
      "ded_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "19/110:\n",
      "page_cats = pd.concat([one_timers_page, fans_page, base_page, camp_page, twomk_page, ded_page], axis=1,\n",
      "          keys=['One-timers', 'Fans', 'Basers', 'Campers', 'Two-MK-Plus', 'Dedicated'])\n",
      "19/111: page_cats = page_cats.stack().reset_index().rename(columns={'level_1': 'Category', 0: 'user_count'})\n",
      "19/112:\n",
      "regular = alt.Chart(page_cats).mark_bar().encode(\n",
      "    alt.X('user_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('Category:N'),\n",
      "    tooltip = ['name', 'Category','user_count']\n",
      ").properties(height=1200, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('user_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "19/113:\n",
      "from_id_lang = df.groupby(['from_id', 'langdetect']).size().unstack()[['he', 'ar', 'en', 'ru', 'fr', 'de', 'es']].sort_values(by='he', ascending=False)\n",
      "from_id_lang.head()\n",
      "19/114:\n",
      "for l, s in zip(labels, sets):\n",
      "    df.loc[df.from_id.isin(s), 'group_cat'] = l\n",
      "    \n",
      "df.group_cat.value_counts()\n",
      "19/115:\n",
      "labels = ('One Timers', 'Fans', 'two_mks_plus', 'Basers', 'Campers')\n",
      "sets = [set(one_timers.index), set(fans_users.from_id.values), set(two_mks_plus.from_id.values), set(base.from_id.values), set(campers.from_id.values)]\n",
      "\n",
      "for l, s in zip(labels, sets):\n",
      "    df.loc[df.from_id.isin(s), 'group_cat'] = l\n",
      "    \n",
      "df.group_cat.value_counts()\n",
      "19/116:\n",
      "labels = ('One Timers', 'Fans', 'two_mks_plus', 'Basers', 'Campers')\n",
      "sets = [set(one_timers.index), set(fans_users.from_id.values), set(two_mks_plus.from_id.values), set(base.from_id.values), set(campers.from_id.values)]\n",
      "\n",
      "for l, s in zip(labels, sets):\n",
      "    df.loc[df.from_id.isin(s), 'group_cat'] = l\n",
      "\n",
      "df['group_cat'] = pd.Categorical(df.group_cat)\n",
      "df.group_cat.value_counts()\n",
      "19/117:\n",
      "labels = ('One Timers', 'Fans', 'Two-MKs-Plus', 'Basers', 'Campers', 'Dedicated')\n",
      "sets = [set(one_timers.index), set(fans_users.from_id.values), set(two_mks_plus.from_id.values), set(base.from_id.values), set(campers.from_id.values), set(dedicated.from_id.values)]\n",
      "\n",
      "for l, s in zip(labels, sets):\n",
      "    df.loc[df.from_id.isin(s), 'group_cat'] = l\n",
      "\n",
      "df['group_cat'] = pd.Categorical(df.group_cat)\n",
      "df.group_cat.value_counts()\n",
      "19/118:\n",
      "labels = ('One Timers', 'Fans', 'Two-MKs-Plus', 'Basers', 'Campers', 'Dedicated')\n",
      "sets = [set(one_timers.index), set(fans_users.from_id.values), set(two_mks_plus.from_id.values), set(base.from_id.values), set(campers.from_id.values), set(dedicated.from_id.values)]\n",
      "df = df.drop('group_cat', axis=1)\n",
      "for l, s in zip(labels, sets):\n",
      "    df.loc[df.from_id.isin(s), 'group_cat'] = l\n",
      "\n",
      "df['group_cat'] = pd.Categorical(df.group_cat)\n",
      "df.group_cat.value_counts()\n",
      "19/119: dedicated.shape\n",
      "19/120: df.groupby(['name', 'group_cat', 'langdetect']).size()\n",
      "19/121: df.groupby(['name', 'group_cat', 'langdetect']).size().reset_index().head()\n",
      "19/122:\n",
      "group_lang = df.groupby(['name', 'group_cat', 'langdetect']).size().reset_index().rename(columns = {0: 'post_count'})\n",
      "group_lang.head()\n",
      "19/123:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('post_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('langdetect:N'),\n",
      "    tooltip = ['name', 'group_cat', 'langdetect', 'post_count']\n",
      ").properties(height=1200, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('post_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized).facet(row='name')\n",
      "19/124:\n",
      "regular = alt.Chart(group_lang[group_lang.comment_count>1]).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('langdetect:N'),\n",
      "    tooltip = ['name', 'group_cat', 'langdetect', 'comment_count']\n",
      ").properties(height=1200, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized).facet(row='name')\n",
      "19/125:\n",
      "group_lang = df.groupby(['name', 'group_cat', 'langdetect']).size().reset_index().rename(columns = {0: 'comment_count'})\n",
      "group_lang.head()\n",
      "19/126:\n",
      "regular = alt.Chart(group_lang[group_lang.comment_count>1]).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('langdetect:N'),\n",
      "    tooltip = ['name', 'group_cat', 'langdetect', 'comment_count']\n",
      ").properties(height=1200, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized).facet(row='name')\n",
      "19/127: group_lang[group_lang.comment_count>1].shape\n",
      "19/128: group_lang[group_lang.comment_count>2].shape\n",
      "19/129: alt.data_transformers.enable('json')\n",
      "19/130:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('langdetect:N'),\n",
      "    tooltip = ['name', 'group_cat', 'langdetect', 'comment_count']\n",
      ").properties(height=1200, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized).facet(row='name')\n",
      "19/131:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('langdetect:N'),\n",
      "    tooltip = ['name', 'group_cat', 'langdetect', 'comment_count']\n",
      ").properties(height=1200, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "(normalized).facet(row='name')\n",
      "19/132:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('langdetect:N'),\n",
      "    tooltip = ['name', 'group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "(normalized).facet(row='name')\n",
      "19/133:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('langdetect:N'),\n",
      "    tooltip = ['group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "(normalized).facet(row='name')\n",
      "19/134:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('langdetect:N'),\n",
      "    tooltip = ['group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "(normalized)\n",
      "19/135:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('langdetect:N'),\n",
      "    tooltip = ['group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "(normalized).facet(row='name')\n",
      "19/136:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('langdetect:N', legend=None),\n",
      "    tooltip = ['group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "(normalized).facet(row='name')\n",
      "19/137: df.langdetect.value_counts()\n",
      "19/138: df.langdetect.value_counts().reset_index()\n",
      "19/139: df.langdetect.value_counts().head(15)\n",
      "19/140: df.langdetect.value_counts().head(15).index\n",
      "19/141:\n",
      "group_lang.loc[~group_lang.langdetect.isin(top), 'langdetect'] = 'other'\n",
      "group_lang.langdetect.value_counts()\n",
      "19/142: top_lang = df.langdetect.value_counts().head(15).index\n",
      "19/143:\n",
      "group_lang.loc[~group_lang.langdetect.isin(top_lang), 'langdetect'] = 'other'\n",
      "group_lang.langdetect.value_counts()\n",
      "19/144:\n",
      "labels = ('One Timers', 'Fans', 'Two-MKs-Plus', 'Basers', 'Campers', 'Dedicated')\n",
      "sets = [set(one_timers.index), \n",
      "        set(fans_users.from_id.values), \n",
      "        set(two_mks_plus.from_id.values), \n",
      "        set(base.from_id.values), \n",
      "        set(campers.from_id.values), \n",
      "        set(dedicated.from_id.values)]\n",
      "df = df.drop('group_cat', axis=1)\n",
      "for l, s in zip(labels, sets):\n",
      "    df.loc[df.from_id.isin(s), 'group_cat'] = l\n",
      "\n",
      "df.group_cat.value_counts()\n",
      "19/145:\n",
      "group_lang = df.groupby(['name', 'group_cat', 'langdetect']).size().reset_index().rename(columns = {0: 'comment_count'})\n",
      "group_lang.head()\n",
      "19/146:\n",
      "group_lang.loc[~group_lang.langdetect.isin(top_lang), 'langdetect'] = 'other'\n",
      "group_lang.langdetect.value_counts()\n",
      "19/147: top_lang = df.langdetect.value_counts().head(15).index\n",
      "19/148:\n",
      "group_lang.loc[~group_lang.langdetect.isin(top_lang), 'langdetect'] = 'other'\n",
      "group_lang.langdetect.value_counts()\n",
      "19/149:\n",
      "group_lang = df.groupby(['name', 'group_cat', 'langdetect']).size().reset_index().rename(columns = {0: 'comment_count'})\n",
      "group_lang.head()\n",
      "19/150: top_lang = df.langdetect.value_counts().head(15).index\n",
      "19/151:\n",
      "group_lang.loc[~group_lang.langdetect.isin(top_lang), 'langdetect'] = 'other'\n",
      "group_lang.langdetect.value_counts()\n",
      "19/152: group_lang.dtypes\n",
      "19/153:\n",
      "group_lang.langdetect.add_categories(['other'])\n",
      "group_lang.loc[~group_lang.langdetect.isin(top_lang), 'langdetect'] = 'other'\n",
      "group_lang.langdetect.value_counts()\n",
      "19/154:\n",
      "group_lang.langdetect.cat.add_categories(['other'])\n",
      "group_lang.loc[~group_lang.langdetect.isin(top_lang), 'langdetect'] = 'other'\n",
      "group_lang.langdetect.value_counts()\n",
      "19/155:\n",
      "group_lang['langdetect'] = group_lang.langdetect.cat.add_categories(['other'])\n",
      "group_lang.loc[~group_lang.langdetect.isin(top_lang), 'langdetect'] = 'other'\n",
      "group_lang.langdetect.value_counts()\n",
      "19/156: alt.data_transformers.enable('json')\n",
      "19/157:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('langdetect:N', legend=None),\n",
      "    tooltip = ['group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "(normalized).facet(row='name')\n",
      "19/158:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('langdetect:N',),\n",
      "    tooltip = ['group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "(normalized).facet(row='name')\n",
      "19/159:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('langdetect:N',),\n",
      "    tooltip = ['group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "(normalized).facet(row='name', sort=top_commented)\n",
      "19/160:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('langdetect:N',),\n",
      "    tooltip = ['group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "(normalized).facet(row='name')\n",
      "19/161:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('langdetect:N',),\n",
      "    tooltip = ['name', 'group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "(normalized).facet(row='name')\n",
      "19/162:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat:N', sort=top_commented),\n",
      "    alt.Color('langdetect:N',),\n",
      "    tooltip = ['name', 'group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "(normalized).facet(row='name')\n",
      "19/163:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat:N', sort=top_commented),\n",
      "    alt.Color('langdetect:N',),\n",
      "    tooltip = ['name', 'group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "(normalized).facet(row='name')\n",
      "19/164:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat:N', sort=top_commented, title=None),\n",
      "    alt.Color('langdetect:N',),\n",
      "    tooltip = ['name', 'group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "(normalized).facet(row='name')\n",
      "19/165:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat:N', title=None),\n",
      "    alt.Color('langdetect:N',),\n",
      "    tooltip = ['name', 'group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "(normalized).facet(row='name')\n",
      "19/166: group_lang.groupby(['name', 'group_cat', 'langdetect']).size().shape\n",
      "19/167: group_lang = group_lang.groupby(['name', 'group_cat', 'langdetect']).size().reset_index()\n",
      "19/168: alt.data_transformers.enable('json')\n",
      "19/169:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat:N', title=None),\n",
      "    alt.Color('langdetect:N',),\n",
      "    tooltip = ['name', 'group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "(normalized).facet(row='name')\n",
      "19/170: group_lang = group_lang.groupby(['name', 'group_cat', 'langdetect']).comment_count.sum().reset_index()\n",
      "19/171:\n",
      "group_lang['langdetect'] = group_lang.langdetect.cat.add_categories(['other'])\n",
      "group_lang.loc[~group_lang.langdetect.isin(top_lang), 'langdetect'] = 'other'\n",
      "group_lang.langdetect.value_counts()\n",
      "19/172: group_lang = group_lang.groupby(['name', 'group_cat', 'langdetect']).comment_count.sum().reset_index()\n",
      "19/173:\n",
      "#group_lang['langdetect'] = group_lang.langdetect.cat.add_categories(['other'])\n",
      "group_lang.loc[~group_lang.langdetect.isin(top_lang), 'langdetect'] = 'other'\n",
      "group_lang.langdetect.value_counts()\n",
      "19/174: group_lang = group_lang.groupby(['name', 'group_cat', 'langdetect']).comment_count.sum().reset_index()\n",
      "19/175:\n",
      "group_lang = df.groupby(['name', 'group_cat', 'langdetect']).size().reset_index().rename(columns = {0: 'comment_count'})\n",
      "group_lang.head()\n",
      "19/176: top_lang = df.langdetect.value_counts().head(15).index\n",
      "19/177:\n",
      "group_lang['langdetect'] = group_lang.langdetect.cat.add_categories(['other'])\n",
      "group_lang.loc[~group_lang.langdetect.isin(top_lang), 'langdetect'] = 'other'\n",
      "group_lang.langdetect.value_counts()\n",
      "19/178: group_lang = group_lang.groupby(['name', 'group_cat', 'langdetect']).comment_count.sum().reset_index()\n",
      "19/179: alt.data_transformers.enable('json')\n",
      "19/180:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat:N', title=None),\n",
      "    alt.Color('langdetect:N',),\n",
      "    tooltip = ['name', 'group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "(normalized).facet(row='name')\n",
      "19/181:\n",
      "group_lang = df.groupby(['name', 'group_cat', 'langdetect']).size().reset_index().rename(columns = {0: 'comment_count'})\n",
      "group_lang.head()\n",
      "19/182: top_lang = df.langdetect.value_counts().head(15).index\n",
      "19/183:\n",
      "group_lang['langdetect'] = group_lang.langdetect.cat.add_categories(['other'])\n",
      "group_lang.loc[~group_lang.langdetect=='cy', 'langdetect'] = 'ru'\n",
      "group_lang.loc[~group_lang.langdetect.isin(top_lang), 'langdetect'] = 'other'\n",
      "group_lang.langdetect.value_counts()\n",
      "19/184:\n",
      "group_lang['langdetect'] = group_lang.langdetect.cat.add_categories(['other'])\n",
      "group_lang.loc[group_lang.langdetect=='cy', 'langdetect'] = 'ru'\n",
      "group_lang.loc[~group_lang.langdetect.isin(top_lang), 'langdetect'] = 'other'\n",
      "group_lang.langdetect.value_counts()\n",
      "19/185:\n",
      "group_lang = df.groupby(['name', 'group_cat', 'langdetect']).size().reset_index().rename(columns = {0: 'comment_count'})\n",
      "group_lang.head()\n",
      "19/186: top_lang = df.langdetect.value_counts().head(15).index\n",
      "19/187:\n",
      "group_lang['langdetect'] = group_lang.langdetect.cat.add_categories(['other'])\n",
      "group_lang.loc[group_lang.langdetect=='cy', 'langdetect'] = 'ru'\n",
      "group_lang.loc[~group_lang.langdetect.isin(top_lang), 'langdetect'] = 'other'\n",
      "group_lang.langdetect.value_counts()\n",
      "19/188:\n",
      "group_lang['langdetect'] = group_lang.langdetect.cat.add_categories(['other'])\n",
      "group_lang.loc[group_lang.langdetect=='cy', 'langdetect'] = 'ru'\n",
      "\n",
      "top_lang = df.langdetect.value_counts().head(9).index\n",
      "19/189:\n",
      "\n",
      "group_lang.loc[~group_lang.langdetect.isin(top_lang), 'langdetect'] = 'other'\n",
      "group_lang.langdetect.value_counts()\n",
      "19/190:\n",
      "labels = ('One Timers', 'Fans', 'Two-MKs-Plus', 'Basers', 'Campers', 'Dedicated')\n",
      "sets = [set(one_timers.index), \n",
      "        set(fans_users.from_id.values), \n",
      "        set(two_mks_plus.from_id.values), \n",
      "        set(base.from_id.values), \n",
      "        set(campers.from_id.values), \n",
      "        set(dedicated.from_id.values)]\n",
      "df = df.drop('group_cat', axis=1)\n",
      "for l, s in zip(labels, sets):\n",
      "    df.loc[df.from_id.isin(s), 'group_cat'] = l\n",
      "\n",
      "df['group_cat'] = pd.Categorical(df.group_cat)\n",
      "df.group_cat.value_counts()\n",
      "19/191:\n",
      "group_lang = df.groupby(['name', 'group_cat', 'langdetect']).size().reset_index().rename(columns = {0: 'comment_count'})\n",
      "group_lang.head()\n",
      "19/192:\n",
      "labels = ('One Timers', 'Fans', 'Two-MKs-Plus', 'Basers', 'Campers', 'Dedicated')\n",
      "sets = [set(one_timers.index), \n",
      "        set(fans_users.from_id.values), \n",
      "        set(two_mks_plus.from_id.values), \n",
      "        set(base.from_id.values), \n",
      "        set(campers.from_id.values), \n",
      "        set(dedicated.from_id.values)]\n",
      "df = df.drop('group_cat', axis=1)\n",
      "for l, s in zip(labels, sets):\n",
      "    df.loc[df.from_id.isin(s), 'group_cat'] = l\n",
      "\n",
      "df['group_cat'] = pd.Categorical(df.group_cat)\n",
      "df.group_cat.value_counts()\n",
      "19/193:\n",
      "group_lang = df.groupby(['name', 'group_cat', 'langdetect']).size().reset_index().rename(columns = {0: 'comment_count'})\n",
      "group_lang.head()\n",
      "19/194:\n",
      "#group_lang['langdetect'] = group_lang.langdetect.cat.add_categories(['other'])\n",
      "group_lang.loc[group_lang.langdetect=='cy', 'langdetect'] = 'ru'\n",
      "\n",
      "top_lang = df.langdetect.value_counts().head(9).index\n",
      "19/195:\n",
      "\n",
      "group_lang.loc[~group_lang.langdetect.isin(top_lang), 'langdetect'] = 'other'\n",
      "group_lang.langdetect.value_counts()\n",
      "19/196:\n",
      "group_lang['langdetect'] = group_lang.langdetect.cat.add_categories(['other'])\n",
      "group_lang.loc[group_lang.langdetect=='cy', 'langdetect'] = 'ru'\n",
      "\n",
      "top_lang = df.langdetect.value_counts().head(9).index\n",
      "19/197:\n",
      "\n",
      "group_lang.loc[~group_lang.langdetect.isin(top_lang), 'langdetect'] = 'other'\n",
      "group_lang.langdetect.value_counts()\n",
      "19/198: group_lang = group_lang.groupby(['name', 'group_cat', 'langdetect']).comment_count.sum().reset_index()\n",
      "19/199: alt.data_transformers.enable('json')\n",
      "19/200:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat:N', title=None),\n",
      "    alt.Color('langdetect:N',),\n",
      "    tooltip = ['name', 'group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "(normalized).facet(row='name')\n",
      "19/201:\n",
      "group_lang = df.groupby(['name', 'group_cat', 'langdetect']).size().reset_index().rename(columns = {0: 'comment_count'})\n",
      "group_lang.head()\n",
      "19/202:\n",
      "group_lang['langdetect'] = group_lang.langdetect.cat.add_categories(['other'])\n",
      "group_lang.loc[group_lang.langdetect=='cy', 'langdetect'] = 'ru'\n",
      "group_lang = group_lang.groupby(['name', 'group_cat', 'langdetect']).comment_count.sum().reset_index()\n",
      "top_lang = df.langdetect.value_counts().head(9).index\n",
      "19/203:\n",
      "\n",
      "group_lang.loc[~group_lang.langdetect.isin(top_lang), 'langdetect'] = 'other'\n",
      "group_lang.langdetect.value_counts()\n",
      "19/204:\n",
      "group_lang.loc[~group_lang.langdetect.isin(top_lang), 'langdetect'] = 'other'\n",
      "group_lang.langdetect.value_counts()\n",
      "19/205: alt.data_transformers.enable('json')\n",
      "19/206:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat:N', title=None),\n",
      "    alt.Color('langdetect:N',),\n",
      "    tooltip = ['name', 'group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "(normalized).facet(row='name')\n",
      "19/207:\n",
      "top_lang = top_lang.groupby('langdetect').size().sort_valus(ascending=False).head(9).index\n",
      "group_lang.loc[~group_lang.langdetect.isin(top_lang), 'langdetect'] = 'other'\n",
      "group_lang.langdetect.value_counts()\n",
      "19/208:\n",
      "top_lang = group_lang.groupby('langdetect').size().sort_valus(ascending=False).head(9).index\n",
      "group_lang.loc[~group_lang.langdetect.isin(top_lang), 'langdetect'] = 'other'\n",
      "group_lang.langdetect.value_counts()\n",
      "19/209:\n",
      "top_lang = group_lang.groupby('langdetect').size().sort_values(ascending=False).head(9).index\n",
      "group_lang.loc[~group_lang.langdetect.isin(top_lang), 'langdetect'] = 'other'\n",
      "group_lang.langdetect.value_counts()\n",
      "19/210:\n",
      "group_lang = df.groupby(['name', 'group_cat', 'langdetect']).size().reset_index().rename(columns = {0: 'comment_count'})\n",
      "group_lang.head()\n",
      "19/211:\n",
      "group_lang['langdetect'] = group_lang.langdetect.cat.add_categories(['other'])\n",
      "group_lang.loc[group_lang.langdetect=='cy', 'langdetect'] = 'ru'\n",
      "group_lang = group_lang.groupby(['name', 'group_cat', 'langdetect']).comment_count.sum().reset_index()\n",
      "19/212:\n",
      "top_lang = group_lang.groupby('langdetect').size().sort_values(ascending=False).head(9).index\n",
      "group_lang.loc[~group_lang.langdetect.isin(top_lang), 'langdetect'] = 'other'\n",
      "group_lang.langdetect.value_counts()\n",
      "19/213: alt.data_transformers.enable('json')\n",
      "19/214:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat:N', title=None),\n",
      "    alt.Color('langdetect:N',),\n",
      "    tooltip = ['name', 'group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "(normalized).facet(row='name')\n",
      "19/215: group_lang.shape\n",
      "19/216: group_lang\n",
      "19/217:\n",
      "group_lang = df.groupby(['name', 'group_cat', 'langdetect']).size().reset_index().rename(columns = {0: 'comment_count'})\n",
      "group_lang.head()\n",
      "19/218:\n",
      "group_lang['langdetect'] = group_lang.langdetect.cat.add_categories(['other'])\n",
      "group_lang.loc[group_lang.langdetect=='cy', 'langdetect'] = 'ru'\n",
      "19/219:\n",
      "top_lang = group_lang.groupby('langdetect').size().sort_values(ascending=False).head(9).index\n",
      "group_lang.loc[~group_lang.langdetect.isin(top_lang), 'langdetect'] = 'other'\n",
      "group_lang = group_lang.groupby(['name', 'group_cat', 'langdetect']).comment_count.sum().reset_index()\n",
      "\n",
      "group_lang.langdetect.value_counts()\n",
      "19/220: group_lang.shape\n",
      "19/221: alt.data_transformers.enable('defalt')\n",
      "19/222: alt.data_transformers.disable('json')\n",
      "19/223: alt.data_transformers.active()\n",
      "19/224: alt.data_transformers.active\n",
      "19/225: alt.data_transformers.enable('default', max_rows=None)\n",
      "19/226:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat:N', title=None),\n",
      "    alt.Color('langdetect:N',),\n",
      "    tooltip = ['name', 'group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "(normalized).facet(row='name')\n",
      "19/227:\n",
      "top_lang = group_lang.groupby('langdetect').sum().sort_values(ascending=False).head(9).index\n",
      "group_lang.loc[~group_lang.langdetect.isin(top_lang), 'langdetect'] = 'other'\n",
      "group_lang = group_lang.groupby(['name', 'group_cat', 'langdetect']).comment_count.sum().reset_index()\n",
      "\n",
      "group_lang.langdetect.value_counts()\n",
      "19/228:\n",
      "top_lang = group_lang.groupby('langdetect').comment_count.sum().sort_values(ascending=False).head(9).index\n",
      "group_lang.loc[~group_lang.langdetect.isin(top_lang), 'langdetect'] = 'other'\n",
      "group_lang = group_lang.groupby(['name', 'group_cat', 'langdetect']).comment_count.sum().reset_index()\n",
      "\n",
      "group_lang.langdetect.value_counts()\n",
      "19/229:\n",
      "top_lang = group_lang.groupby('langdetect').comment_count.sum().sort_values(ascending=False).head(10).index\n",
      "group_lang.loc[~group_lang.langdetect.isin(top_lang), 'langdetect'] = 'other'\n",
      "group_lang = group_lang.groupby(['name', 'group_cat', 'langdetect']).comment_count.sum().reset_index()\n",
      "\n",
      "group_lang.langdetect.value_counts()\n",
      "19/230: alt.data_transformers.enable('default', max_rows=None)\n",
      "19/231:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat:N', title=None),\n",
      "    alt.Color('langdetect:N',),\n",
      "    tooltip = ['name', 'group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "(normalized).facet(row='name')\n",
      "19/232:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat:N', title=None),\n",
      "    alt.Color('langdetect:N',),\n",
      "    tooltip = ['name', 'group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ")\n",
      "\n",
      "(normalized).facet(row='name')\n",
      "19/233:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat:N', title=None),\n",
      "    alt.Color('langdetect:N',),\n",
      "    tooltip = ['name', 'group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ")\n",
      "\n",
      "(normalized).facet(row='name').properties(title='Comment Count by MK, User Group Category and Language Detected')\n",
      "19/234:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', title=None),\n",
      "    alt.Color('langdetect:N',),\n",
      "    tooltip = ['name', 'group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ")\n",
      "\n",
      "(normalized).facet(column='group_cat').properties(title='Comment Count by MK, User Group Category and Language Detected')\n",
      "19/235:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', title=None),\n",
      "    alt.Color('langdetect:N',),\n",
      "    tooltip = ['name', 'group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(width=200)\n",
      "\n",
      "(normalized).facet(column='group_cat').properties(title='Comment Count by MK, User Group Category and Language Detected')\n",
      "19/236:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', title=None),\n",
      "    alt.Color('langdetect:N',),\n",
      "    tooltip = ['name', 'group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(width=150)\n",
      "\n",
      "(normalized).facet(column='group_cat').properties(title='Comment Count by MK, User Group Category and Language Detected')\n",
      "19/237:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', title=None),\n",
      "    alt.Color('langdetect:N',),\n",
      "    tooltip = ['name', 'group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(width=100)\n",
      "\n",
      "(normalized).facet(column='group_cat').properties(title='Comment Count by MK, User Group Category and Language Detected')\n",
      "19/238:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', title=None),\n",
      "    alt.Color('langdetect:N',),\n",
      "    tooltip = ['name', 'group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(width=150)\n",
      "\n",
      "(normalized).facet(column='group_cat').properties(title='Comment Count by MK, User Group Category and Language Detected')\n",
      "19/239:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', title=None),\n",
      "    alt.Color('langdetect:N',),\n",
      "    tooltip = ['name', 'group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(width=150)\n",
      "\n",
      "(normalized).facet(column='group_cat', title=None).properties(title='Comment Count by MK, User Group Category and Language Detected')\n",
      "19/240:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', title=None),\n",
      "    alt.Color('langdetect:N',),\n",
      "    tooltip = ['name', 'group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(width=150)\n",
      "\n",
      "(normalized).facet(column='group_cat').properties(title='Comment Count by MK, User Group Category and Language Detected')\n",
      "19/241: top_lang = group_lang.groupby('langdetect').comment_count.sum().sort_values(ascending=False).head(10).index\n",
      "19/242:\n",
      "top_lang = group_lang.groupby('langdetect').comment_count.sum().sort_values(ascending=False).head(10).index\n",
      "group_lang.loc[~group_lang.langdetect.isin(top_lang), 'langdetect'] = 'other'\n",
      "group_lang = group_lang.groupby(['name', 'group_cat', 'langdetect']).comment_count.sum().reset_index()\n",
      "\n",
      "group_lang.langdetect.value_counts()\n",
      "19/243: top_lang.head()\n",
      "19/244: top_lang\n",
      "19/245: group_lang.groupby('langdetect').comment_count.sum().sort_values(ascending=False).head(10)\n",
      "19/246:\n",
      "group_lang = df.groupby(['name', 'group_cat', 'langdetect']).size().reset_index().rename(columns = {0: 'comment_count'})\n",
      "group_lang.head()\n",
      "19/247:\n",
      "group_lang['langdetect'] = group_lang.langdetect.cat.add_categories(['other'])\n",
      "group_lang.loc[group_lang.langdetect=='cy', 'langdetect'] = 'ru'\n",
      "19/248: group_lang.groupby('langdetect').comment_count.sum().sort_values(ascending=False).head(10)\n",
      "19/249:\n",
      "top_lang = group_lang.groupby('langdetect').comment_count.sum().sort_values(ascending=False).head(10).index\n",
      "group_lang.loc[~group_lang.langdetect.isin(top_lang), 'langdetect'] = 'other'\n",
      "group_lang = group_lang.groupby(['name', 'group_cat', 'langdetect']).comment_count.sum().reset_index()\n",
      "\n",
      "group_lang.langdetect.value_counts()\n",
      "19/250: alt.data_transformers.enable('default', max_rows=None)\n",
      "19/251:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', title=None),\n",
      "    alt.Color('langdetect:N',),\n",
      "    tooltip = ['name', 'group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(width=150)\n",
      "\n",
      "(normalized).facet(column='group_cat').properties(title='Comment Count by MK, User Group Category and Language Detected')\n",
      "19/252:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#a6cee3','#1f78b4','#b2df8a','#33a02c','#fb9a99','#e31a1c','#fdbf6f','#ff7f00','#cab2d6','#6a3d9a']),\n",
      "    ),\n",
      "    tooltip = ['name', 'group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(width=150)\n",
      "\n",
      "(normalized).facet(column='group_cat').properties(title='Comment Count by MK, User Group Category and Language Detected')\n",
      "19/253:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#a6cee3','#1f78b4','#b2df8a','#33a02c','#fb9a99','#e31a1c','#fdbf6f','#ff7f00','#cab2d6','#6a3d9a'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['name', 'group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(width=150)\n",
      "\n",
      "(normalized).facet(column='group_cat').properties(title='Comment Count by MK, User Group Category and Language Detected')\n",
      "19/254:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#a6cee3','#fb9a99','#b2df8a','#33a02c','#1f78b4','#e31a1c','#fdbf6f','#ff7f00','#cab2d6','#6a3d9a'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['name', 'group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(width=150)\n",
      "\n",
      "(normalized).facet(column='group_cat').properties(title='Comment Count by MK, User Group Category and Language Detected')\n",
      "19/255:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#33a02c','#1f78b4','#e31a1c','#fdbf6f','#ff7f00','#cab2d6','#a6cee3'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['name', 'group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(width=150)\n",
      "\n",
      "(normalized).facet(column='group_cat').properties(title='Comment Count by MK, User Group Category and Language Detected')\n",
      "19/256:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#a6cee3','#1f78b4','#e31a1c','#fdbf6f','#ff7f00','#cab2d6','#33a02c'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['name', 'group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(width=150)\n",
      "\n",
      "(normalized).facet(column='group_cat').properties(title='Comment Count by MK, User Group Category and Language Detected')\n",
      "19/257:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#a6cee3','#1f78b4','#ff7f00','#fdbf6f','#e31a1c','#cab2d6','#33a02c'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['name', 'group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(width=150)\n",
      "\n",
      "(normalized).facet(column='group_cat').properties(title='Comment Count by MK, User Group Category and Language Detected')\n",
      "19/258:\n",
      "group_lang['langdetect'] = group_lang.langdetect.cat.add_categories(['other'])\n",
      "group_lang.loc[group_lang.langdetect=='cy', 'langdetect'] = 'ru'\n",
      "19/259: group_lang.groupby('langdetect').comment_count.sum().sort_values(ascending=False).head(9)\n",
      "19/260:\n",
      "group_lang = df.groupby(['name', 'group_cat', 'langdetect']).size().reset_index().rename(columns = {0: 'comment_count'})\n",
      "group_lang.head()\n",
      "19/261:\n",
      "group_lang['langdetect'] = group_lang.langdetect.cat.add_categories(['other'])\n",
      "group_lang.loc[group_lang.langdetect=='cy', 'langdetect'] = 'ru'\n",
      "19/262: group_lang.groupby('langdetect').comment_count.sum().sort_values(ascending=False).head(9)\n",
      "19/263:\n",
      "top_lang = group_lang.groupby('langdetect').comment_count.sum().sort_values(ascending=False).head(9).index\n",
      "group_lang.loc[~group_lang.langdetect.isin(top_lang), 'langdetect'] = 'other'\n",
      "group_lang = group_lang.groupby(['name', 'group_cat', 'langdetect']).comment_count.sum().reset_index()\n",
      "\n",
      "group_lang.langdetect.value_counts()\n",
      "19/264: alt.data_transformers.enable('default', max_rows=None)\n",
      "19/265:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#a6cee3','#1f78b4','#ff7f00','#fdbf6f','#e31a1c','#cab2d6','#33a02c'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['name', 'group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(width=150)\n",
      "\n",
      "(normalized).facet(column='group_cat').properties(title='Comment Count by MK, User Group Category and Language Detected')\n",
      "19/266:\n",
      "regular = alt.Chart(group_lang.groupby(['group_cat', 'langdetect']).comment_count.sum().reset_index()).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#a6cee3','#1f78b4','#ff7f00','#fdbf6f','#e31a1c','#cab2d6','#33a02c'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['name', 'group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(width=150)\n",
      "\n",
      "(normalized).facet(column='group_cat').properties(title='Comment Count by User Group Category and Language Detected')\n",
      "19/267:\n",
      "regular = alt.Chart(group_lang.groupby(['group_cat', 'langdetect']).comment_count.sum().reset_index()).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#a6cee3','#1f78b4','#ff7f00','#fdbf6f','#e31a1c','#cab2d6','#33a02c'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(width=150)\n",
      "\n",
      "(normalized).facet(column='group_cat').properties(title='Comment Count by User Group Category and Language Detected')\n",
      "19/268:\n",
      "regular = alt.Chart(group_lang.groupby(['group_cat', 'langdetect']).comment_count.sum().reset_index()).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#a6cee3','#1f78b4','#ff7f00','#fdbf6f','#e31a1c','#cab2d6','#33a02c'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ")\n",
      "\n",
      "(normalized).properties(title='Comment Count by User Group Category and Language Detected')\n",
      "19/269:\n",
      "regular = alt.Chart(group_lang.groupby(['group_cat', 'langdetect']).comment_count.sum().reset_index()).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#a6cee3','#1f78b4','#ff7f00','#fdbf6f','#e31a1c','#cab2d6','#33a02c'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ")\n",
      "\n",
      "(normalized).properties(title='Comment Count by User Group Category and Language Detected')\n",
      "19/270:\n",
      "regular = alt.Chart(group_lang[group_lang.name!='Benjamin_Netanyahu'].groupby(['group_cat', 'langdetect']).comment_count.sum().reset_index()).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#a6cee3','#1f78b4','#ff7f00','#fdbf6f','#e31a1c','#cab2d6','#33a02c'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ")\n",
      "\n",
      "(normalized).properties(title='Comment Count by User Group Category and Language Detected')\n",
      "19/271:\n",
      "regular = alt.Chart(group_lang[group_lang.name!='Benjamin_Netanyahu'].groupby(['group_cat', 'langdetect']).comment_count.sum().reset_index()).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#a6cee3','#1f78b4','#ff7f00','#fdbf6f','#e31a1c','#cab2d6','#33a02c'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ")\n",
      "\n",
      "(normalized).properties(title='Comment Count by User Group Category and Language Detected (No Netanyahu)')\n",
      "19/272: df.head()\n",
      "19/273: df.name.value_counts()\n",
      "19/274: df.head()\n",
      "19/275: df[(df.name.isin(['Benjamin_Netanyahu', 'Isaac_Herzog'])) & (df.created_time.dt.month.isin([6,12]))]\n",
      "19/276: df[(df.name.isin(['Benjamin_Netanyahu', 'Isaac_Herzog'])) & (df.created_month.isin('2015-06', '2015-12', '2016-06'))]\n",
      "19/277: df[(df.name.isin(['Benjamin_Netanyahu', 'Isaac_Herzog'])) & (df.created_month.isin(['2015-06', '2015-12', '2016-06']))]\n",
      "19/278: df[(df.name.isin(['Benjamin_Netanyahu', 'Isaac_Herzog'])) & (df.created_month.isin(['2015-06', '2015-12', '2016-06']))]\n",
      "19/279: df.dtypes\n",
      "19/280: df.created_month.value_counts\n",
      "19/281: df.created_month.value_counts()\n",
      "19/282: df[(df.name.isin(['Benjamin_Netanyahu', 'Isaac_Herzog'])) & (df.created_month.dt.isin(['2015-06', '2015-12', '2016-06']))]\n",
      "19/283: df[(df.name.isin(['Benjamin_Netanyahu', 'Isaac_Herzog'])) & (df.created_month.dt.isin(pd.to_datetime('2015-06').to_period('M')))]\n",
      "19/284: df[(df.name.isin(['Benjamin_Netanyahu', 'Isaac_Herzog'])) & (df.created_month.isin([pd.to_datetime('2015-06').to_period('M')]))]\n",
      "19/285: df[(df.name.isin(['Benjamin_Netanyahu', 'Isaac_Herzog'])) & (df.created_month.isin([pd.to_datetime('2015-06').to_period('M'), pd.to_datetime('2015-12').to_period('M'), pd.to_datetime('2016-06').to_period('M')]))]\n",
      "19/286: net_herz = df[(df.name.isin(['Benjamin_Netanyahu', 'Isaac_Herzog'])) & (df.created_time.isin([pd.to_datetime('2015-06').to_period('M'), pd.to_datetime('2015-12').to_period('M'), pd.to_datetime('2016-06').to_period('M')]))]\n",
      "19/287: net_herz = df[(df.name.isin(['Benjamin_Netanyahu', 'Isaac_Herzog'])) & (df.created_date.isin([pd.to_datetime('2015-06').to_period('M'), pd.to_datetime('2015-12').to_period('M'), pd.to_datetime('2016-06').to_period('M')]))]\n",
      "19/288: df.head()\n",
      "19/289: net_herz = df[(df.name.isin(['Benjamin_Netanyahu', 'Isaac_Herzog'])) & (df.created_month.isin([pd.to_datetime('2015-06').to_period('M'), pd.to_datetime('2015-12').to_period('M'), pd.to_datetime('2016-06').to_period('M')]))]\n",
      "19/290: net_herz.shape\n",
      "19/291: net_herz[net_herz.is_post==1].groupby('name').size()\n",
      "19/292: net_herz[net_herz.is_post==1].groupby('name').cat.remove_unused_categories().size()\n",
      "19/293: net_herz[net_herz.is_post==1].groupby('name').size().cat.remove_unused_categories()\n",
      "19/294: net_herz[net_herz.is_post==1].groupby('name').size()\n",
      "19/295: net_herz['name'] = net_herz.name.cat.remove_unused_categories()\n",
      "19/296: net_herz[net_herz.is_post==1].groupby('name').size()\n",
      "19/297: net_herz.to_csv('turf/netanyahu_herzog_6mo.csv')\n",
      "19/298:\n",
      "net_herz = df[(df.name.isin(['Benjamin_Netanyahu', 'Isaac_Herzog'])) \n",
      "              & (df.created_month.isin([pd.to_datetime('2015-06').to_period('7d'), \n",
      "                                        pd.to_datetime('2015-12').to_period('7d'), \n",
      "                                        pd.to_datetime('2016-06').to_period('7d')]))]\n",
      "19/299: net_herz['name'] = net_herz.name.cat.remove_unused_categories()\n",
      "19/300: net_herz[net_herz.is_post==1].groupby('name').size()\n",
      "19/301:\n",
      "net_herz = df[(df.name.isin(['Benjamin_Netanyahu', 'Isaac_Herzog'])) \n",
      "              & (df.created_month.isin([pd.to_datetime('2015-06-01').to_period('7d'), \n",
      "                                        pd.to_datetime('2015-12-01').to_period('7d'), \n",
      "                                        pd.to_datetime('2016-06-01').to_period('7d')]))]\n",
      "19/302: net_herz['name'] = net_herz.name.cat.remove_unused_categories()\n",
      "19/303: net_herz[net_herz.is_post==1].groupby('name').size()\n",
      "19/304: net_herz.to_csv('turf/netanyahu_herzog_6mo.csv')\n",
      "19/305:\n",
      "net_herz = df[(df.name.isin(['Benjamin_Netanyahu', 'Isaac_Herzog'])) \n",
      "              & (df.created_time.isin([pd.to_datetime('2015-06-01').to_period('7d'), \n",
      "                                       pd.to_datetime('2015-12-01').to_period('7d'), \n",
      "                                       pd.to_datetime('2016-06-01').to_period('7d')]))]\n",
      "19/306:\n",
      "net_herz = df[(df.name.isin(['Benjamin_Netanyahu', 'Isaac_Herzog'])) \n",
      "              & (df.created_day.isin([pd.to_datetime('2015-06-01').to_period('7d'), \n",
      "                                       pd.to_datetime('2015-12-01').to_period('7d'), \n",
      "                                       pd.to_datetime('2016-06-01').to_period('7d')]))]\n",
      "19/307: net_herz['name'] = net_herz.name.cat.remove_unused_categories()\n",
      "19/308: net_herz[net_herz.is_post==1].groupby('name').size()\n",
      "19/309:\n",
      "net_herz = df[(df.name.isin(['Benjamin_Netanyahu', 'Isaac_Herzog'])) \n",
      "              & (df.created_time.dt.between('2015-06-01', '2015-06-07'))]\n",
      "19/310:\n",
      "net_herz = df[(df.name.isin(['Benjamin_Netanyahu', 'Isaac_Herzog'])) \n",
      "              & (df.created_time.between('2015-06-01', '2015-06-07'))]\n",
      "19/311: net_herz['name'] = net_herz.name.cat.remove_unused_categories()\n",
      "19/312: net_herz[net_herz.is_post==1].groupby('name').size()\n",
      "19/313:\n",
      "net_herz = df[(df.name.isin(['Benjamin_Netanyahu', 'Isaac_Herzog'])) \n",
      "              & (  (df.created_time.between('2015-06-01', '2015-06-07'))\n",
      "                 | (df.created_time.between('2015-12-01', '2015-12-07'))\n",
      "                 | (df.created_time.between('2016-06-01', '2016-06-07')))]\n",
      "19/314: net_herz['name'] = net_herz.name.cat.remove_unused_categories()\n",
      "19/315: net_herz[net_herz.is_post==1].groupby('name').size()\n",
      "19/316: net_herz[net_herz.is_post==1].date.value_counts()\n",
      "19/317:\n",
      "net_herz_posts = df[(df.is_post==1) & (df.name.isin(['Benjamin_Netanyahu', 'Isaac_Herzog'])) \n",
      "              & (  (df.created_time.between('2015-06-01', '2015-06-07'))\n",
      "                 | (df.created_time.between('2015-12-01', '2015-12-07'))\n",
      "                 | (df.created_time.between('2016-06-01', '2016-06-07')))]\n",
      "19/318: net_herz[net_herz.is_post==1].created_day.value_counts()\n",
      "19/319:\n",
      "net_herz_posts = df[(df.is_post==1) & (df.name.isin(['Benjamin_Netanyahu', 'Isaac_Herzog'])) \n",
      "              & (  (df.created_time.between('2015-06-01', '2015-06-08'))\n",
      "                 | (df.created_time.between('2015-12-01', '2015-12-08'))\n",
      "                 | (df.created_time.between('2016-06-01', '2016-06-08')))]\n",
      "19/320: net_herz_posts.post_id\n",
      "19/321: net_herz_comments = df[df.post_id.isin(net_herz_posts.post_id)]\n",
      "19/322: net_herz_comments.shape\n",
      "19/323: net_herz_posts.head()\n",
      "19/324: net_herz_posts.shape\n",
      "19/325: net_herz_posts.name.value_counts()\n",
      "19/326: net_herz_comments.to_csv('turf/netanyahu_herzog_comments_jun15_dec15_jun16_1st_week.csv')\n",
      "19/327: net_herz_posts.to_csv('turf/netanyahu_herzog_posts_jun15_dec15_jun16_1st_week.csv')\n",
      "19/328: df[df.text.str.contains('superman', case=False)]\n",
      "19/329: df[df.text.str.contains('superman', case=False, na=False)]\n",
      "19/330: superman = df[df.text.str.contains('superman', case=False, na=False)]\n",
      "19/331: superman.name.value_counts()\n",
      "19/332: df[(df.is_post==1) & (df.text.str.contains('superman', case=False, na=False))]\n",
      "19/333: superman.groupby('post_id').size()\n",
      "19/334: df[df.post_id=='268108602075_10153538217037076']\n",
      "19/335: df[df.post_id=='268108602075_10153538217037076'].text\n",
      "19/336: df[((df.is_post==1) & (df.post_id=='268108602075_10153538217037076')].text\n",
      "19/337: df[(df.is_post==1) & (df.post_id=='268108602075_10153538217037076')].text\n",
      "19/338: df[(df.is_post==1) & (df.post_id=='268108602075_10153538217037076')].text.iat[0]\n",
      "19/339: superman.groupby(['name', 'group_cat']).size()\n",
      "19/340: superman[superman.post_id=='268108602075_10153538217037076']\n",
      "19/341: superman[superman.post_id=='268108602075_10153538217037076'].T\n",
      "19/342: df[df.id=='268108602075_10153538217037076']\n",
      "19/343: superman[superman.post_id!='268108602075_10153538217037076'].T\n",
      "19/344: trump = df[df.text.str.contains('Trump')]\n",
      "19/345: trump = df[df.text.str.contains('Trump', na=False)]\n",
      "19/346: trump.name.value_counts()\n",
      "19/347: trump.date.value_counts(sort=False)\n",
      "19/348: trump.created_month.value_counts(sort=False)\n",
      "19/349: trump.created_month.value_counts().sort_index()\n",
      "19/350: trump.sort_values(by='created_month')\n",
      "19/351: trump.sort_values(by='created_month')[['created_day', 'text']].to_tuple()\n",
      "19/352: trump.sort_values(by='created_month')[['created_day', 'text']].totuple()\n",
      "19/353: [i in trump.sort_values(by='created_month')[['created_day', 'text']].itertuples()]\n",
      "19/354: [i for i in trump.sort_values(by='created_month')[['created_day', 'text']].itertuples()]\n",
      "19/355: trump.post_id.value_counts()\n",
      "19/356: trump[trump.created_time<'2016'].post_id.value_counts()\n",
      "19/357: trump[trump.post_id=='268108602075_10153048937867076']\n",
      "19/358: trump[(trump.post_id=='268108602075_10153048937867076') & (trump.post_id==1)]\n",
      "19/359: df[(df.post_id=='268108602075_10153048937867076') & (trump.post_id==1)]\n",
      "19/360: df[(df.post_id=='268108602075_10153048937867076') & (df.post_id==1)].text.iat[0]\n",
      "19/361: df[(df.post_id=='268108602075_10153048937867076') & (df.is_post==1)].text.iat[0]\n",
      "19/362: trump.created_month.value_counts().sort_index()\n",
      "19/363: hashem = df[df.text.str.contains('Hashem', case=False, na=False)]\n",
      "19/364: hashem.name.value_counts()\n",
      "19/365: hashem.created_month.value_counts().sort_index()\n",
      "19/366: hashem.head()\n",
      "19/367: top_shared\n",
      "19/368: pd.DataFrame([top_mean_commented, top_liked, top_shared], columns['commented', 'liked', 'shared'])\n",
      "19/369: pd.DataFrame([top_mean_commented, top_liked, top_shared], columns=['commented', 'liked', 'shared'])\n",
      "19/370: pd.DataFrame([[top_mean_commented, top_liked, top_shared]], columns=['commented', 'liked', 'shared'])\n",
      "19/371: pd.DataFrame([top_mean_commented, top_liked, top_shared],)\n",
      "19/372:\n",
      "all_rank = pd.DataFrame([top_mean_commented, top_liked, top_shared],)\n",
      "all_rank = all_rank.T\n",
      "all_rank.columns=['commented', 'liked', 'shared']\n",
      "19/373:\n",
      "all_rank = pd.DataFrame([top_mean_commented, top_liked, top_shared],)\n",
      "all_rank = all_rank.T\n",
      "all_rank.columns=['commented', 'liked', 'shared']\n",
      "all_rank.head()\n",
      "19/374:\n",
      "ts = (df.loc[df.is_post==1].groupby('name').share_count.mean().sort_values(ascending=False))\n",
      "tmc = (df.loc[df.is_post==1].groupby('name').comment_count.mean().sort_values(ascending=False))\n",
      "tl = (df.loc[df.is_post==1].groupby('name').like_count.mean().sort_values(ascending=False))\n",
      "all_points = pd.concat([tmc, tl, ts], axis=1)\n",
      "all_points.head()\n",
      "19/375:\n",
      "ts = (df.loc[df.is_post==1].groupby('name').share_count.mean().sort_values(ascending=False))\n",
      "tmc = (df.loc[df.is_post==1].groupby('name').comment_count.mean().sort_values(ascending=False))\n",
      "tl = (df.loc[df.is_post==1].groupby('name').like_count.mean().sort_values(ascending=False))\n",
      "all_scores = pd.concat([tmc, tl, ts], axis=1)\n",
      "all_scores['total_score'] = all_scores.sum(axis=1)\n",
      "all_scores.head()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/376:\n",
      "ts = (df.loc[df.is_post==1].groupby('name').share_count.mean().sort_values(ascending=False))\n",
      "tmc = (df.loc[df.is_post==1].groupby('name').comment_count.mean().sort_values(ascending=False))\n",
      "tl = (df.loc[df.is_post==1].groupby('name').like_count.mean().sort_values(ascending=False))\n",
      "all_scores = pd.concat([tmc, tl, ts], axis=1)\n",
      "all_scores['total_score'] = all_scores.sum(axis=1)\n",
      "all_scores.sort_values(by='total_score', ascending=False)\n",
      "19/377:\n",
      "ts = (df.loc[df.is_post==1].groupby('name').share_count.mean().sort_values(ascending=False))\n",
      "tmc = (df.loc[df.is_post==1].groupby('name').comment_count.mean().sort_values(ascending=False))\n",
      "tl = (df.loc[df.is_post==1].groupby('name').like_count.mean().sort_values(ascending=False))\n",
      "all_scores = pd.concat([tmc, tl, ts], axis=1)\n",
      "all_scores_div = all_scores.div(all_scores.sum(axis=0), axis=1)\n",
      "all_scores['total_score'] = all_scores.sum(axis=1)\n",
      "all_scores_div['total_score'] = all_scores_div.sum(axis=1)\n",
      "all_scores.sort_values(by='total_score', ascending=False)\n",
      "19/378: all_scores_div.sort_values(by='total_score', ascending=False)\n",
      "19/379:\n",
      "ts = (df.loc[df.is_post==1].groupby('name').share_count.mean().sort_values(ascending=False))\n",
      "tmc = (df.loc[df.is_post==1].groupby('name').comment_count.mean().sort_values(ascending=False))\n",
      "tl = (df.loc[df.is_post==1].groupby('name').like_count.mean().sort_values(ascending=False))\n",
      "all_scores = pd.concat([tmc, tl, ts], axis=1)\n",
      "all_scores_div = all_scores.div(all_scores.sum(axis=0), axis=1)\n",
      "all_scores['total_score'] = all_scores.sum(axis=1)\n",
      "all_scores_div['total_score'] = all_scores_div.sum(axis=1)\n",
      "all_scores.sort_values(by='total_score', ascending=False).head(10)\n",
      "19/380: all_scores_div.sort_values(by='total_score', ascending=False).head(10)\n",
      "19/381:\n",
      "ts = (df.loc[df.is_post==1].groupby('name').share_count.mean().sort_values(ascending=False))\n",
      "tmc = (df.loc[df.is_post==1].groupby('name').comment_count.mean().sort_values(ascending=False))\n",
      "tl = (df.loc[df.is_post==1].groupby('name').like_count.mean().sort_values(ascending=False))\n",
      "all_scores = pd.concat([tmc, tl, ts], axis=1)\n",
      "all_scores_div = all_scores.div(all_scores.sum(axis=0), axis=1)\n",
      "all_scores['total_score'] = all_scores['like_count'] + 1.1*all_scores['comment_count'] + 1.2*all_scores['share_count']\n",
      "all_scores_div['total_score'] = all_scores_div.sum(axis=1)\n",
      "all_scores.sort_values(by='total_score', ascending=False).head(10)\n",
      "19/382:\n",
      "ts = (df.loc[df.is_post==1].groupby('name').share_count.mean().sort_values(ascending=False))\n",
      "tmc = (df.loc[df.is_post==1].groupby('name').comment_count.mean().sort_values(ascending=False))\n",
      "tl = (df.loc[df.is_post==1].groupby('name').like_count.mean().sort_values(ascending=False))\n",
      "all_scores = pd.concat([tmc, tl, ts], axis=1)\n",
      "all_scores_div = all_scores.div(all_scores.sum(axis=0), axis=1)\n",
      "all_scores['total_score'] = all_scores['like_count'] + 1.1*all_scores['comment_count'] + 1.2*all_scores['share_count']\n",
      "all_scores_div['total_score'] = all_scores_div['like_count'] + 1.1*all_scores_div['comment_count'] + 1.2*all_scores_div['share_count']\n",
      "all_scores.sort_values(by='total_score', ascending=False).head(10)\n",
      "19/383: all_scores_div.sort_values(by='total_score', ascending=False).head(10)\n",
      "19/384:\n",
      "all_accum_df = get_accum_df(mat, list(all_scores_div.index))\n",
      "(chart_user_accum(all_accum_df, list(all_scores_div.index), 'All Score') \n",
      " & chart_user_accum_delta(share_accum_df, top_shared, 'All Score') \n",
      " & chart_user_accum_unique_delta_ratio(share_accum_df, top_shared, 'All Score')).properties(title='All Score')\n",
      "19/385:\n",
      "all_accum_df = get_accum_df(mat, list(all_scores_div.sort_values(by='total_score', ascending=False).index))\n",
      "(chart_user_accum(all_accum_df, list(all_scores_div.sort_values(by='total_score', ascending=False).index), 'All Score') \n",
      " & chart_user_accum_delta(share_accum_df, top_shared, 'All Score') \n",
      " & chart_user_accum_unique_delta_ratio(share_accum_df, top_shared, 'All Score')).properties(title='All Score')\n",
      "19/386:\n",
      "all_accum_df = get_accum_df(mat, list(all_scores_div.sort_values(by='total_score', ascending=False).index))\n",
      "(chart_user_accum(all_accum_df, list(all_scores_div.sort_values(by='total_score', ascending=False).index), 'All Score') \n",
      " & chart_user_accum_delta(all_accum_df, list(all_scores_div.sort_values(by='total_score', ascending=False).index), 'All Score') \n",
      " & chart_user_accum_unique_delta_ratio(share_accum_df, list(all_scores_div.sort_values(by='total_score', ascending=False).index), 'All Score')).properties(title='All Score')\n",
      "19/387:\n",
      "all_accum_df = get_accum_df(mat, list(all_scores_div.sort_values(by='total_score', ascending=False).index))\n",
      "(chart_user_accum(all_accum_df, list(all_scores_div.sort_values(by='total_score', ascending=False).index), 'All Score') \n",
      " & chart_user_accum_delta(all_accum_df, list(all_scores_div.sort_values(by='total_score', ascending=False).index), 'All Score') \n",
      " & chart_user_accum_unique_delta_ratio(all_accum_df, list(all_scores_div.sort_values(by='total_score', ascending=False).index), 'All Score')).properties(title='All Score')\n",
      "19/388: all_scores_div.sort_values(by='total_score', ascending=False).head(20)\n",
      "19/389:\n",
      "#celebz =  two_mks_plus[two_mks_plus.groupby('from_id').name.transform(lambda x: x.isin(top_10_commented).all())]\n",
      "new_celebies = two_mks_plus[two_mks_plus.from_id.isin(mat[mat.loc[:, list(all_scores_div.sort_values(by='total_score', ascending=False).index)[12:]].isna().all(axis=1)].index)]\n",
      "19/390: new_celebies.size()\n",
      "19/391: new_celebies.shape\n",
      "19/392: list(all_scores_div.sort_values(by='total_score', ascending=False).index)[12:]\n",
      "19/393: new_celebies.from_id.nunique()\n",
      "19/394: new_celebies.from_id.nunique(), two_mks_plus.from_id.nunique()\n",
      "19/395:\n",
      "ded_celebies = dedicated[dedicated.from_id.isin(mat[mat.loc[:, list(all_scores_div.sort_values(by='total_score', ascending=False).index)[12:]].isna().all(axis=1)].index)]\n",
      "ded_celebies.from_id.nunique(), dedicated.from_id.nunique()\n",
      "19/396:\n",
      "ded_fans_users = (st.dedicated()\n",
      " .loc[st.groupby('from_id').name.transform('nunique')==1, ['from_id', 'name', 'party']])\n",
      "ded_fans_users.from_id.nunique(), dedicated.from_id.nunique()\n",
      "19/397:\n",
      "ded_fans_users = (dedicated.loc[st.groupby('from_id').name.transform('nunique')==1, ['from_id', 'name', 'party']])\n",
      "ded_fans_users.from_id.nunique(), dedicated.from_id.nunique()\n",
      "19/398:\n",
      "ded_two_mks_plus = (dedicated.loc[st.groupby('from_id').name.transform('nunique')>1, ['from_id', 'name', 'party']])\n",
      "ded_base = ded_two_mks_plus[ded_two_mks_plus.groupby('from_id').party.transform('nunique')==1]\n",
      "ded_campers = ded_two_mks_plus[ded_two_mks_plus.groupby('from_id').camp.transform('nunique')==1]\n",
      "ded_celebies = ded_two_mks_plus[ded_two_mks_plus.from_id.isin(mat[mat.loc[:, list(all_scores_div.sort_values(by='total_score', ascending=False).index)[12:]].isna().all(axis=1)].index)]\n",
      "dedicated.from_id.nunique(), ded_two_mks_plus.from_id.nunique(), ded_base.from_id.nunique(), ded_campers.from_id.nunique(), ded_celebies.from_id.nunique()\n",
      "19/399:\n",
      "ded_two_mks_plus = (dedicated.loc[st.groupby('from_id').name.transform('nunique')>1, ['from_id', 'name', 'party', 'camp']])\n",
      "ded_base = ded_two_mks_plus[ded_two_mks_plus.groupby('from_id').party.transform('nunique')==1]\n",
      "ded_campers = ded_two_mks_plus[ded_two_mks_plus.groupby('from_id').camp.transform('nunique')==1]\n",
      "ded_celebies = ded_two_mks_plus[ded_two_mks_plus.from_id.isin(mat[mat.loc[:, list(all_scores_div.sort_values(by='total_score', ascending=False).index)[12:]].isna().all(axis=1)].index)]\n",
      "dedicated.from_id.nunique(), ded_two_mks_plus.from_id.nunique(), ded_base.from_id.nunique(), ded_campers.from_id.nunique(), ded_celebies.from_id.nunique()\n",
      "20/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "20/2:\n",
      "%%time\n",
      "df = pd.read_pickle('turf/data_df.pkl.gz', compression='gzip')\n",
      "20/3: df.dtypes\n",
      "20/4:\n",
      "sdf = pd.read_pickle('turf/shared_df.pkl.gz', compression='gzip')\n",
      "sdf.dtypes\n",
      "20/5: st = pd.read_pickle('turf/total_stats_no_multiindex.pkl.gz', compression='gzip')\n",
      "20/6:\n",
      "# OLD STUFF\n",
      "#save(st, 'turf/total_stats.pkl.gz')\n",
      "#st = pd.read_pickle( 'turf/total_stats.pkl.gz', compression='gzip')\n",
      "20/7: st.head()\n",
      "20/8: st.shape\n",
      "20/9: df[df.is_post==0].shape\n",
      "20/10: print ('Total number of comments: %d' % st.id.sum())\n",
      "20/11: print ('Total number of distinct users (commenters): %d' % st.from_id.nunique())\n",
      "20/12:\n",
      "counts = st.groupby('from_id').id.sum()\n",
      "one_timers = counts[counts==1]\n",
      "\n",
      "print ('Total number of one-timers: %d' % one_timers.shape[0])\n",
      "print ('Ratio of one timers from all users: %.2f' % (one_timers.shape[0] / st.from_id.nunique()))\n",
      "20/13: mat = st.pivot(index='from_id', columns='name', values='id')\n",
      "20/14: mat[(mat.count(axis=1)==1)].shape[0]\n",
      "20/15: mat.loc[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu==1), 'Benjamin_Netanyahu'].shape[0]\n",
      "20/16:\n",
      "print ('Ratio of Bibi from one-timers: %.2f' % (mat[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu==1)].shape[0] \n",
      " / counts[counts==1].shape[0]))\n",
      "20/17: print ('Ratio of users that commented 10 times and up: %.2f' % (counts[counts>=10].shape[0]/st.from_id.nunique()))\n",
      "20/18: (st.reset_index().groupby('from_id').name.nunique().value_counts().head(20)/st.reset_index().from_id.nunique()).plot(kind='bar')\n",
      "20/19: (st.reset_index().groupby('from_id').id.sum().value_counts().head(20)/st.reset_index().from_id.nunique()).plot(kind='bar')\n",
      "20/20:\n",
      "fans_users = (st.reset_index()\n",
      " .loc[st.groupby('from_id').name.transform('nunique')==1, ['from_id', 'name', 'party']])\n",
      "20/21: fans = ( fans_users.name.value_counts())\n",
      "20/22: fans.head(10)\n",
      "20/23:\n",
      "print ('Total number of fans: %d' % fans.sum())\n",
      "print ('Ratio of fans from all users: %.2f' % (fans.sum() / st.from_id.nunique()))\n",
      "print ('Ratio of one-timers from fans: %.2f' % (counts[counts==1].shape[0] / fans.sum()))\n",
      "20/24:\n",
      "(pd.concat([fans, \n",
      "            fans/st.from_id.nunique(), \n",
      "            fans/st.groupby('name').from_id.nunique()],\n",
      "            axis=1, keys=['num_uniques', 'ratio_from_all_users', 'ratio_from_own_users'])\n",
      "             .sort_values(by='ratio_from_own_users', ascending=False)).head(10)\n",
      "20/25:\n",
      "mkdet = pd.read_csv('turf/mk_details.csv', sep='\\t')\n",
      "mkdet.head()\n",
      "20/26: df['party'] = df['name'].map(mkdet.set_index('folder_name')['party'])\n",
      "20/27: df.groupby(['party', 'langdetect']).size().unstack()\n",
      "20/28: st['party'] = st['name'].map(mkdet.set_index('folder_name')['party'])\n",
      "20/29:\n",
      "two_mks_plus = (st.reset_index()\n",
      " .loc[st.groupby('from_id').name.transform('nunique')>1, ['from_id', 'name', 'party']])\n",
      "20/30: two_mks_plus.head()\n",
      "20/31: two_mks_plus.from_id.nunique()\n",
      "20/32: two_mks_plus.party.value_counts()\n",
      "20/33: two_mks_plus.groupby('party').from_id.nunique()\n",
      "20/34: base = two_mks_plus[two_mks_plus.groupby('from_id').party.transform('nunique')==1]\n",
      "20/35:\n",
      "print ('Total number of basers: %d' % base.from_id.nunique())\n",
      "print ('Ratio of basers from all users: %.2f' % (base.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of basers from two-mks-plus: %.2f' % (base.from_id.nunique() / two_mks_plus.from_id.nunique()))\n",
      "20/36: st.party.unique()\n",
      "20/37:\n",
      "camps = { 'מרצ': 'opposition',\n",
      "          'יש עתיד': 'opposition',\n",
      "          'ליכוד': 'coalition',\n",
      "          'הבית היהודי בראשות נפתלי בנט': 'coalition',\n",
      "          'המחנה הציוני': 'opposition',\n",
      "          'הרשימה המשותפת (חד”ש, רע”מ, בל”ד, תע”ל)': 'opposition',\n",
      "          'כולנו בראשות משה כחלון': 'coalition',\n",
      "          'ישראל ביתנו': 'coalition' }\n",
      "20/38: st['camp'] = st['party'].map(camps)\n",
      "20/39: two_mks_plus['camp'] = two_mks_plus['party'].map(camps)\n",
      "20/40: two_mks_plus.groupby('camp').from_id.nunique()\n",
      "20/41: campers = two_mks_plus[two_mks_plus.groupby('from_id').camp.transform('nunique')==1]\n",
      "20/42:\n",
      "print ('Total number of campers: %d' % campers.from_id.nunique())\n",
      "print ('Ratio of campers from all users: %.2f' % (campers.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of campers from two_mks_plus: %.2f' % (campers.from_id.nunique() / two_mks_plus.from_id.nunique()))\n",
      "print ('Coalition | Opposition ratio from campers: %.2f | %.2f' % \n",
      "       ((campers[campers.camp=='coalition'].from_id.nunique() / campers.from_id.nunique()),\n",
      "       (campers[campers.camp=='opposition'].from_id.nunique() / campers.from_id.nunique())))\n",
      "cg = st.groupby('camp')\n",
      "print ('Coalition | Opposition ratios from total users: %.2f | %.2f' % \n",
      "       ((cg.from_id.nunique()['coalition'] / st.from_id.nunique()),\n",
      "       ((cg.from_id.nunique()['opposition'] / st.from_id.nunique()))))\n",
      "\n",
      "print ('Coalition | Opposition total comments: %.2f | %.2f' % \n",
      "       ((cg.id.sum()['coalition'] / st.id.sum()),\n",
      "       ((cg.id.sum()['opposition'] / st.id.sum()))))\n",
      "20/43: top_commented = list(st.groupby('name').id.sum().sort_values(ascending=False).index)\n",
      "20/44:\n",
      "top_10_commented = top_commented[:10]\n",
      "top_10_commented\n",
      "20/45:\n",
      "#celebz =  two_mks_plus[two_mks_plus.groupby('from_id').name.transform(lambda x: x.isin(top_10_commented).all())]\n",
      "celebz = two_mks_plus[two_mks_plus.from_id.isin(mat[mat.loc[:, top_commented[10:]].isna().all(axis=1)].index)]\n",
      "20/46: num_celebiez_no_base_camp = len(set(celebz.from_id.drop_duplicates().values).difference(set(campers.from_id.drop_duplicates().values)).difference(set(base.from_id.drop_duplicates().values)))\n",
      "20/47:\n",
      "print ('Total number of celeb followers (celebies): %d' % celebz.from_id.nunique())\n",
      "print ('Ratio of celebies from all users: %.2f' % (celebz.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of celebies from two_mks_plus: %.2f' % (celebz.from_id.nunique() / two_mks_plus.from_id.nunique()))\n",
      "print ('Remove campers and base from celebies: %d' % num_celebiez_no_base_camp)\n",
      "20/48: top_commented[:10]\n",
      "20/49: fans.reindex(top_commented).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "20/50:\n",
      "only_bibi = mat.loc[:, top_commented[1:]].isna().all(axis=1)\n",
      "only_bibi[only_bibi].shape\n",
      "20/51:\n",
      "def get_accum_df(mat, order):\n",
      "    onlys = []\n",
      "    \n",
      "    for i, name in enumerate(order):\n",
      "        only = mat.loc[:, order[i+1:]].isna().all(axis=1)\n",
      "        onlys.append(only[only].shape[0])\n",
      "        \n",
      "    user_accum_df = pd.DataFrame({'mk': order, 'user_accum': onlys})\n",
      "    user_accum_df['party'] = user_accum_df['mk'].map(mkdet.set_index('folder_name')['party'])\n",
      "    user_accum_df['camp'] = user_accum_df['party'].map(camps)\n",
      "    user_accum_df['delta'] = user_accum_df.user_accum.diff()\n",
      "    unique_delta_ratio = (fans.reindex(top_commented) / user_accum_df.set_index('mk').delta).drop('Benjamin_Netanyahu')\n",
      "    user_accum_df['unique_delta_ratio'] = user_accum_df['mk'].map(unique_delta_ratio)\n",
      "    user_accum_df['delta_n'] = user_accum_df['delta'] / user_accum_df['delta'].max()\n",
      "    \n",
      "    return user_accum_df\n",
      "20/52:\n",
      "user_accum_df = get_accum_df(mat, top_commented)\n",
      "user_accum_df.head()\n",
      "20/53: user_accum_df.set_index('mk').user_accum.plot(kind='bar', figsize=(15,10), rot=75)\n",
      "20/54:\n",
      "import altair as alt\n",
      "alt.renderers.enable('notebook')\n",
      "20/55:\n",
      "def chart_user_accum(accum_df, order, order_name):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('user_accum',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'user_accum']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = f'User accumulation by {order_name} order - left to right, only users that comment inside the group until the current column')\n",
      "\n",
      "\n",
      "def chart_user_accum_delta(accum_df, order, order_name):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('delta',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'delta']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = f'User accumulation delta by {order_name} order - left to right, only users that comment inside the group until the current column')\n",
      "\n",
      "\n",
      "def chart_user_accum_unique_delta_ratio(accum_df, order, order_name):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('unique_delta_ratio',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'unique_delta_ratio']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = f'Page unique count divided by user_accum delta by {order_name} order (to measure how much a page is part of the group)')\n",
      "20/56:\n",
      "(chart_user_accum(user_accum_df, top_commented, 'Top Commented') \n",
      " & chart_user_accum_delta(user_accum_df, top_commented, 'Top Commented')\n",
      " & chart_user_accum_unique_delta_ratio(user_accum_df, top_commented, 'Top Commented')).properties(title='Top Commented')\n",
      "20/57: user_accum_df[['delta_n', 'unique_delta_ratio']].div(user_accum_df[['delta_n', 'unique_delta_ratio']].sum(axis=1), axis=0).plot(kind='bar', stacked=True, figsize=(15,10), rot=75, width=1.0)\n",
      "20/58:\n",
      "print ('First celebiez group: ', top_commented[:6])\n",
      "print ('Second celebiez group: ', top_commented[:15])\n",
      "20/59: top_mean_commented = list(df.loc[df.is_post==1].groupby('name').comment_count.mean().sort_values(ascending=False).index)\n",
      "20/60: fans.reindex(top_mean_commented).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "20/61: mean_com_accum_df = get_accum_df(mat, top_mean_commented)\n",
      "20/62:\n",
      "(chart_user_accum(mean_com_accum_df, top_mean_commented, 'Top Mean Commented') \n",
      " & chart_user_accum_delta(mean_com_accum_df, top_mean_commented, 'Top Mean Commented') \n",
      " & chart_user_accum_unique_delta_ratio(mean_com_accum_df, top_mean_commented, 'Top Mean Commented')).properties(title='Top Mean Commented')\n",
      "20/63: top_liked = list(df.loc[df.is_post==1].groupby('name').like_count.mean().sort_values(ascending=False).index)\n",
      "20/64: fans.reindex(top_liked).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "20/65: like_accum_df = get_accum_df(mat, top_liked)\n",
      "20/66:\n",
      "(chart_user_accum(like_accum_df, top_liked, 'Top Liked') \n",
      " & chart_user_accum_delta(like_accum_df, top_liked, 'Top Liked') \n",
      " & chart_user_accum_unique_delta_ratio(like_accum_df, top_liked, 'Top Liked')).properties(title='Top Liked')\n",
      "20/67: top_shared = list(df.loc[df.is_post==1].groupby('name').share_count.mean().sort_values(ascending=False).index)\n",
      "20/68: fans.reindex(top_shared).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "20/69:\n",
      "share_accum_df = get_accum_df(mat, top_shared)\n",
      "share_accum_df.head()\n",
      "20/70:\n",
      "(chart_user_accum(share_accum_df, top_shared, 'Top Shared') \n",
      " & chart_user_accum_delta(share_accum_df, top_shared, 'Top Shared') \n",
      " & chart_user_accum_unique_delta_ratio(share_accum_df, top_shared, 'Top Shared')).properties(title='Top Shared')\n",
      "20/71:\n",
      "ts = (df.loc[df.is_post==1].groupby('name').share_count.mean().sort_values(ascending=False))\n",
      "tmc = (df.loc[df.is_post==1].groupby('name').comment_count.mean().sort_values(ascending=False))\n",
      "tl = (df.loc[df.is_post==1].groupby('name').like_count.mean().sort_values(ascending=False))\n",
      "all_scores = pd.concat([tmc, tl, ts], axis=1)\n",
      "all_scores_div = all_scores.div(all_scores.sum(axis=0), axis=1)\n",
      "all_scores['total_score'] = all_scores['like_count'] + 1.1*all_scores['comment_count'] + 1.2*all_scores['share_count']\n",
      "all_scores_div['total_score'] = all_scores_div['like_count'] + 1.1*all_scores_div['comment_count'] + 1.2*all_scores_div['share_count']\n",
      "all_scores.sort_values(by='total_score', ascending=False).head(10)\n",
      "20/72: all_scores_div.sort_values(by='total_score', ascending=False).head(20)\n",
      "20/73:\n",
      "all_accum_df = get_accum_df(mat, list(all_scores_div.sort_values(by='total_score', ascending=False).index))\n",
      "(chart_user_accum(all_accum_df, list(all_scores_div.sort_values(by='total_score', ascending=False).index), 'All Score') \n",
      " & chart_user_accum_delta(all_accum_df, list(all_scores_div.sort_values(by='total_score', ascending=False).index), 'All Score') \n",
      " & chart_user_accum_unique_delta_ratio(all_accum_df, list(all_scores_div.sort_values(by='total_score', ascending=False).index), 'All Score')).properties(title='All Score')\n",
      "20/74: new_celebies = two_mks_plus[two_mks_plus.from_id.isin(mat[mat.loc[:, list(all_scores_div.sort_values(by='total_score', ascending=False).index)[12:]].isna().all(axis=1)].index)]\n",
      "20/75: new_celebies.from_id.nunique(), two_mks_plus.from_id.nunique()\n",
      "20/76:\n",
      "ded_fans_users = (dedicated.loc[st.groupby('from_id').name.transform('nunique')==1, ['from_id', 'name', 'party']])\n",
      "ded_fans_users.from_id.nunique(), dedicated.from_id.nunique()\n",
      "20/77: user_counts = st.groupby('from_id').id.sum()\n",
      "20/78:\n",
      "perc = pd.qcut(user_counts, 20, duplicates='drop')\n",
      "perc.value_counts(sort=False)\n",
      "20/79:\n",
      "perc_bah = pd.cut(user_counts, [0,1,2,3,4,5,7,11,24,9359])\n",
      "(perc_bah.value_counts(sort=False))\n",
      "20/80: (perc_bah.value_counts(sort=False)/ user_counts.shape[0])\n",
      "20/81: (perc_bah.value_counts(sort=False)/ user_counts.shape[0]).plot(kind='bar',figsize=(10,7))\n",
      "20/82:\n",
      "dedicated = st[st.from_id.isin(user_counts[user_counts>24].index)]\n",
      "dedicated.from_id.nunique()\n",
      "20/83:\n",
      "campers = campers[(~campers.from_id.isin(base.from_id))& (~campers.from_id.isin(dedicated.from_id))]\n",
      "campers.from_id.nunique()\n",
      "20/84:\n",
      "base = base[(~base.from_id.isin(dedicated.from_id))]\n",
      "base.from_id.nunique()\n",
      "20/85:\n",
      "two_mks_plus = two_mks_plus[(~two_mks_plus.from_id.isin(base.from_id)) \n",
      "                            & (~two_mks_plus.from_id.isin(campers.from_id))\n",
      "                            & (~two_mks_plus.from_id.isin(dedicated.from_id))]\n",
      "two_mks_plus.from_id.nunique()\n",
      "20/86: campers.from_id.nunique() + base.from_id.nunique() + two_mks_plus.from_id.nunique()\n",
      "20/87:\n",
      "fans_users = fans_users[~fans_users.from_id.isin(one_timers.index)\n",
      "                        & (~fans_users.from_id.isin(dedicated.from_id))]\n",
      "fans_users.from_id.nunique()\n",
      "20/88:\n",
      "ded_fans_users = (dedicated.loc[st.groupby('from_id').name.transform('nunique')==1, ['from_id', 'name', 'party']])\n",
      "ded_fans_users.from_id.nunique(), dedicated.from_id.nunique()\n",
      "20/89:\n",
      "ded_two_mks_plus = (dedicated.loc[st.groupby('from_id').name.transform('nunique')>1, ['from_id', 'name', 'party', 'camp']])\n",
      "ded_base = ded_two_mks_plus[ded_two_mks_plus.groupby('from_id').party.transform('nunique')==1]\n",
      "ded_campers = ded_two_mks_plus[ded_two_mks_plus.groupby('from_id').camp.transform('nunique')==1]\n",
      "ded_celebies = ded_two_mks_plus[ded_two_mks_plus.from_id.isin(mat[mat.loc[:, list(all_scores_div.sort_values(by='total_score', ascending=False).index)[12:]].isna().all(axis=1)].index)]\n",
      "dedicated.from_id.nunique(), ded_two_mks_plus.from_id.nunique(), ded_base.from_id.nunique(), ded_campers.from_id.nunique(), ded_celebies.from_id.nunique()\n",
      "20/90:\n",
      "labels = ('All', 'One Timers', 'Fans', 'two_mks_plus', 'Basers', 'Campers')\n",
      "sets = [set(st.from_id.values), set(one_timers.index), set(fans_users.from_id.values), set(two_mks_plus.from_id.values), set(base.from_id.values), set(campers.from_id.values)]\n",
      "\n",
      "set_dict = dict(zip(labels, sets))\n",
      "20/91:\n",
      "inter_set_dict = {}\n",
      "for i in set_dict:\n",
      "    inter_set_dict[i] = {}\n",
      "    for j in set_dict:\n",
      "        inter_set_dict[i][j] = len(set_dict[i].intersection(set_dict[j]))\n",
      "20/92: pd.DataFrame(inter_set_dict).reindex(labels)\n",
      "20/93: pd.DataFrame(inter_set_dict).reindex(labels).to_csv('turf/group_intersections.csv')\n",
      "20/94: from matplotlib_venn import venn3\n",
      "20/95:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(one_timers.index), set(fans_users.from_id.values)], set_labels = ('All', 'One Timers', 'Fans', ), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.25)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.20)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.20)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.09)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]-0.19)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.19)\n",
      "20/96: df[df.is_post==1].groupby('name').comment_count.mean().sort_values().plot(kind='barh', figsize=(10,15))\n",
      "20/97: one_timers.shape\n",
      "20/98:\n",
      "one_timers_page = st[st.from_id.isin(one_timers.index)].groupby('name').size()\n",
      "one_timers_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "20/99:\n",
      "gfans = st[st.from_id.isin(fans_users.from_id)].groupby('from_id')\n",
      "gfans.id.sum().mean(), gfans.post_id.sum().mean(), gfans.id.sum().mean()/ gfans.post_id.sum().mean()\n",
      "20/100:\n",
      "fans_page = st[st.from_id.isin(fans_users.from_id)].groupby('name').size()\n",
      "fans_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "20/101:\n",
      "gbase = st[st.from_id.isin(base.from_id)].groupby('from_id')\n",
      "gbase.id.sum().mean(), gbase.post_id.sum().mean(), gbase.id.sum().mean()/ gbase.post_id.sum().mean()\n",
      "20/102:\n",
      "base_page = st[st.from_id.isin(base.from_id)].groupby('name').size()\n",
      "base_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "20/103:\n",
      "gcamp = st[st.from_id.isin(campers.from_id)].groupby('from_id')\n",
      "gcamp.id.sum().mean(), gcamp.post_id.sum().mean(), gcamp.id.sum().mean()/ gcamp.post_id.sum().mean()\n",
      "20/104: st[st.from_id.isin(campers.from_id)].id.sum()\n",
      "20/105:\n",
      "camp_page = st[st.from_id.isin(campers.from_id)].groupby('name').size()\n",
      "camp_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "20/106: st[(st.from_id.isin(two_mks_plus.from_id))].id.sum()\n",
      "20/107:\n",
      "gtwo = st[(st.from_id.isin(two_mks_plus.from_id))].groupby('from_id')\n",
      "gtwo.id.sum().mean(), gtwo.post_id.sum().mean(), gtwo.id.sum().mean()/gtwo.post_id.sum().mean()\n",
      "20/108:\n",
      "twomk_page = st[st.from_id.isin(two_mks_plus.from_id)].groupby('name').size()\n",
      "twomk_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "20/109:\n",
      "gded = dedicated.groupby('from_id')\n",
      "gded.id.sum().mean(), gded.post_id.sum().mean(), gded.id.sum().mean()/ gded.post_id.sum().mean()\n",
      "20/110:\n",
      "ded_page = st[st.from_id.isin(dedicated.from_id)].groupby('name').size()\n",
      "ded_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "20/111:\n",
      "page_cats = pd.concat([one_timers_page, fans_page, base_page, camp_page, twomk_page, ded_page], axis=1,\n",
      "          keys=['One-timers', 'Fans', 'Basers', 'Campers', 'Two-MK-Plus', 'Dedicated'])\n",
      "20/112: page_cats = page_cats.stack().reset_index().rename(columns={'level_1': 'Category', 0: 'user_count'})\n",
      "20/113:\n",
      "regular = alt.Chart(page_cats).mark_bar().encode(\n",
      "    alt.X('user_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('Category:N'),\n",
      "    tooltip = ['name', 'Category','user_count']\n",
      ").properties(height=1200, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('user_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "20/114:\n",
      "labels = ('One Timers', 'Fans', 'Two-MKs-Plus', 'Basers', 'Campers', 'Dedicated')\n",
      "sets = [set(one_timers.index), \n",
      "        set(fans_users.from_id.values), \n",
      "        set(two_mks_plus.from_id.values), \n",
      "        set(base.from_id.values), \n",
      "        set(campers.from_id.values), \n",
      "        set(dedicated.from_id.values)]\n",
      "df = df.drop('group_cat', axis=1)\n",
      "for l, s in zip(labels, sets):\n",
      "    df.loc[df.from_id.isin(s), 'group_cat'] = l\n",
      "\n",
      "df['group_cat'] = pd.Categorical(df.group_cat)\n",
      "df.group_cat.value_counts()\n",
      "20/115:\n",
      "labels = ('One Timers', 'Fans', 'Two-MKs-Plus', 'Basers', 'Campers', 'Dedicated')\n",
      "sets = [set(one_timers.index), \n",
      "        set(fans_users.from_id.values), \n",
      "        set(two_mks_plus.from_id.values), \n",
      "        set(base.from_id.values), \n",
      "        set(campers.from_id.values), \n",
      "        set(dedicated.from_id.values)]\n",
      "#df = df.drop('group_cat', axis=1)\n",
      "for l, s in zip(labels, sets):\n",
      "    df.loc[df.from_id.isin(s), 'group_cat'] = l\n",
      "\n",
      "df['group_cat'] = pd.Categorical(df.group_cat)\n",
      "df.group_cat.value_counts()\n",
      "20/116:\n",
      "group_lang = df.groupby(['name', 'group_cat', 'langdetect']).size().reset_index().rename(columns = {0: 'comment_count'})\n",
      "group_lang['langdetect'] = group_lang.langdetect.cat.add_categories(['other'])\n",
      "group_lang.loc[group_lang.langdetect=='cy', 'langdetect'] = 'ru'\n",
      "top_lang = group_lang.groupby('langdetect').comment_count.sum().sort_values(ascending=False).head(9).index\n",
      "group_lang.loc[~group_lang.langdetect.isin(top_lang), 'langdetect'] = 'other'\n",
      "group_lang = group_lang.groupby(['name', 'group_cat', 'langdetect']).comment_count.sum().reset_index()\n",
      "\n",
      "group_lang.langdetect.value_counts()\n",
      "20/117: df.groupby('group_cat').id.sum()\n",
      "20/118: st.head()\n",
      "20/119: df.group_cat.value_counts()/df.shape[0]\n",
      "20/120: st[st.from_id.isin(user_counts[user_counts>24].index)].id.sum()\n",
      "20/121: st[st.from_id.isin(user_counts[user_counts>24].index)].id.sum() / st.id.sum()\n",
      "20/122: st[st.from_id.isin(user_counts[user_counts>10].index)].id.sum() / st.id.sum()\n",
      "20/123: st[st.from_id.isin(user_counts[user_counts>12].index)].id.sum() / st.id.sum()\n",
      "20/124: st[st.from_id.isin(user_counts[user_counts>12].index)].from_id.nunique() / st.from_id.nunique()\n",
      "20/125: st[st.from_id.isin(user_counts[user_counts>11].index)].from_id.nunique() / st.from_id.nunique()\n",
      "20/126: st[st.from_id.isin(user_counts[user_counts>11].index)].id.sum() / st.id.sum()\n",
      "21/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "21/2:\n",
      "%%time\n",
      "df = pd.read_pickle('turf/data_df.pkl.gz', compression='gzip')\n",
      "21/3: df.dtypes\n",
      "21/4:\n",
      "sdf = pd.read_pickle('turf/shared_df.pkl.gz', compression='gzip')\n",
      "sdf.dtypes\n",
      "21/5: st = pd.read_pickle('turf/total_stats_no_multiindex.pkl.gz', compression='gzip')\n",
      "21/6:\n",
      "# OLD STUFF\n",
      "#save(st, 'turf/total_stats.pkl.gz')\n",
      "#st = pd.read_pickle( 'turf/total_stats.pkl.gz', compression='gzip')\n",
      "21/7: st.head()\n",
      "22/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "22/2:\n",
      "%%time\n",
      "df = pd.read_pickle('turf/data_df.pkl.gz', compression='gzip')\n",
      "22/3: df.dtypes\n",
      "22/4:\n",
      "sdf = pd.read_pickle('turf/shared_df.pkl.gz', compression='gzip')\n",
      "sdf.dtypes\n",
      "22/5:\n",
      "def percentile(n):\n",
      "    def percentile_(x):\n",
      "        return np.percentile(x, n)\n",
      "    percentile_.__name__ = 'percentile_%s' % n\n",
      "    return percentile_\n",
      "22/6:\n",
      "stats = ['mean', 'std', 'min', 'max', 'median', percentile(25), percentile(75), percentile(95)]\n",
      "\n",
      "times = (df.loc[df.is_post==0, ['from_id', 'name', 'elapsed_time',  'created_time', 'created_month', 'created_day', 'created_day_of_week', 'created_hour',]]\n",
      "      #.dropna(subset=['post_id'])\n",
      "      .groupby(by=['from_id', 'name'])\n",
      "      .agg(stats)\n",
      "     )\n",
      "times = times.reset_index()\n",
      "times.to_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "23/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "23/2: times = pd.read_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "23/3:\n",
      "%%time\n",
      "df = pd.read_pickle('turf/data_df.pkl.gz', compression='gzip')\n",
      "23/4: df.dtypes\n",
      "23/5:\n",
      "sdf = pd.read_pickle('turf/shared_df.pkl.gz', compression='gzip')\n",
      "sdf.dtypes\n",
      "23/6:\n",
      "def percentile(n):\n",
      "    def percentile_(x):\n",
      "        return np.percentile(x, n)\n",
      "    percentile_.__name__ = 'percentile_%s' % n\n",
      "    return percentile_\n",
      "23/7:\n",
      "stats = ['mean', 'std', 'min', 'max', 'median', percentile(25), percentile(75), percentile(95)]\n",
      "\n",
      "times = (df.loc[df.is_post==0, ['from_id', 'name', 'elapsed_time',  'created_time', 'created_month', 'created_day', 'created_day_of_week', 'created_hour',]]\n",
      "      #.dropna(subset=['post_id'])\n",
      "      .groupby(by=['from_id', 'name'])\n",
      "      .agg(stats)\n",
      "     )\n",
      "times = times.reset_index()\n",
      "times.to_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "23/8: times = pd.read_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "23/9: times.head()\n",
      "23/10: times.head()\n",
      "23/11: times.dtypes\n",
      "23/12: df.loc[df.is_post==0, ['from_id', 'name', 'elapsed_time',  'created_time', 'created_month', 'created_day', 'created_day_of_week', 'created_hour',]].head()\n",
      "23/13:\n",
      "stats = ['mean', 'std', 'min', 'max', 'median', percentile(25), percentile(75), percentile(95)]\n",
      "\n",
      "times = (df.loc[df.is_post==0, ['from_id', 'name', 'elapsed_time',  'created_time', 'created_month', 'created_day', 'created_day_of_week', 'created_hour',]]\n",
      "      #.dropna(subset=['post_id'])\n",
      "      .groupby(by=['from_id', 'name'])\n",
      "      .agg(stats)\n",
      "     )\n",
      "times = times.reset_index()\n",
      "times.to_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "23/14: df.loc[df.is_post==0, ['from_id', 'name', 'elapsed_time',  'created_time', 'created_month', 'created_day', 'created_day_of_week', 'created_hour',]].dtypes\n",
      "23/15:\n",
      "stats = ['std', 'min', 'max', 'median', percentile(25), percentile(75), percentile(95)]\n",
      "\n",
      "times = (df.loc[df.is_post==0, ['from_id', 'name', 'elapsed_time',  'created_time', 'created_month', 'created_day', 'created_day_of_week', 'created_hour',]]\n",
      "      #.dropna(subset=['post_id'])\n",
      "      .groupby(by=['from_id', 'name'])\n",
      "      .agg(stats)\n",
      "     )\n",
      "times = times.reset_index()\n",
      "times.to_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "23/16:\n",
      "stats = ['nunique', 'min', 'max', 'median', percentile(25), percentile(75), percentile(95)]\n",
      "\n",
      "times = (df.loc[df.is_post==0, ['from_id', 'name', 'elapsed_time',  'created_time', 'created_month', 'created_day', 'created_day_of_week', 'created_hour',]]\n",
      "      #.dropna(subset=['post_id'])\n",
      "      .groupby(by=['from_id', 'name'])\n",
      "      .agg(stats)\n",
      "     )\n",
      "times = times.reset_index()\n",
      "times.to_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "23/17:\n",
      "stats = ['nunique', 'min', 'max', 'median', percentile(25), percentile(75), percentile(95)]\n",
      "\n",
      "times = (df.loc[df.is_post==0, ['from_id', 'name', 'elapsed_time',  'created_time', 'created_month', 'created_day', 'created_day_of_week', 'created_hour',]]\n",
      "      #.dropna(subset=['post_id'])\n",
      "      .head(10000)\n",
      "      .groupby(by=['from_id', 'name'])\n",
      "      .agg(stats)\n",
      "     )\n",
      "times = times.reset_index()\n",
      "times.to_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "23/18: times.head()\n",
      "23/19:\n",
      "stats = ['nunique', 'min', 'max', 'median', percentile(25), percentile(75), percentile(95)]\n",
      "\n",
      "times = (df.loc[df.is_post==0, ['from_id', 'name', 'elapsed_time',  'created_time', 'created_month', 'created_day', 'created_day_of_week', 'created_hour',]]\n",
      "      #.dropna(subset=['post_id'])\n",
      "      .head(10000)\n",
      "      .groupby(by=['from_id', 'name'])\n",
      "      .agg({created_time:stats})\n",
      "     )\n",
      "times = times.reset_index()\n",
      "times.to_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "23/20:\n",
      "stats = ['nunique', 'min', 'max', 'median', percentile(25), percentile(75), percentile(95)]\n",
      "\n",
      "times = (df.loc[df.is_post==0, ['from_id', 'name', 'elapsed_time',  'created_time', 'created_month', 'created_day', 'created_day_of_week', 'created_hour',]]\n",
      "      #.dropna(subset=['post_id'])\n",
      "      .head(10000)\n",
      "      .groupby(by=['from_id', 'name'])\n",
      "      .agg({'created_time':stats})\n",
      "     )\n",
      "times = times.reset_index()\n",
      "times.to_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "23/21:\n",
      "stats = ['nunique', 'median', percentile(25), percentile(75), percentile(95)]\n",
      "\n",
      "times = (df.loc[df.is_post==0, ['from_id', 'name', 'elapsed_time',  'created_time', 'created_month', 'created_day', 'created_day_of_week', 'created_hour',]]\n",
      "      #.dropna(subset=['post_id'])\n",
      "      .head(10000)\n",
      "      .groupby(by=['from_id', 'name'])\n",
      "      .agg({'created_time':stats})\n",
      "     )\n",
      "times = times.reset_index()\n",
      "times.to_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "23/22:\n",
      "stats = ['nunique', 'min', 'max', 'median', percentile(25), percentile(75), percentile(95)]\n",
      "\n",
      "times = (df.loc[df.is_post==0, ['from_id', 'name', 'elapsed_time',  'created_time', 'created_month', 'created_day', 'created_day_of_week', 'created_hour',]]\n",
      "      #.dropna(subset=['post_id'])\n",
      "      .head(10000)\n",
      "      .groupby(by=['from_id', 'name'])\n",
      "      .agg({'created_time': stats})\n",
      "     )\n",
      "times = times.reset_index()\n",
      "times.to_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "23/23:\n",
      "stats = ['nunique', 'min', 'max', 'median', percentile(25), percentile(75), percentile(95)]\n",
      "\n",
      "times = (df.loc[df.is_post==0, ['from_id', 'name', 'elapsed_time',  'created_time', 'created_month', 'created_day', 'created_day_of_week', 'created_hour',]]\n",
      "      #.dropna(subset=['post_id'])\n",
      "      .head(10000)\n",
      "      .groupby(by=['from_id', 'name'])\n",
      "      .agg({'created_time': stats})\n",
      "     )\n",
      "times = times.reset_index()\n",
      "times.to_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "23/24:\n",
      "stats = ['nunique', 'min', 'max', 'median', percentile(25), percentile(75), percentile(95)]\n",
      "\n",
      "times = (df.loc[df.is_post==0, ['from_id', 'name', 'elapsed_time',  'created_time', 'created_month', 'created_day', 'created_day_of_week', 'created_hour',]]\n",
      "      #.dropna(subset=['post_id'])\n",
      "      .head(10000)\n",
      "      .groupby(by=['from_id', 'name'])\n",
      "      .agg({'created_time': 'nunique'})\n",
      "     )\n",
      "times = times.reset_index()\n",
      "times.to_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "23/25: times.head()\n",
      "23/26:\n",
      "stats = ['nunique', 'min', 'max', 'median']\n",
      "\n",
      "times = (df.loc[df.is_post==0, ['from_id', 'name', 'elapsed_time',  'created_time', 'created_month', 'created_day', 'created_day_of_week', 'created_hour',]]\n",
      "      #.dropna(subset=['post_id'])\n",
      "      .head(10000)\n",
      "      .groupby(by=['from_id', 'name'])\n",
      "      .agg({'created_time': stats})\n",
      "     )\n",
      "times = times.reset_index()\n",
      "times.to_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "23/27:\n",
      "stats = ['nunique',]\n",
      "\n",
      "times = (df.loc[df.is_post==0, ['from_id', 'name', 'elapsed_time',  'created_time', 'created_month', 'created_day', 'created_day_of_week', 'created_hour',]]\n",
      "      #.dropna(subset=['post_id'])\n",
      "      .head(10000)\n",
      "      .groupby(by=['from_id', 'name'])\n",
      "      .agg({'created_time': stats})\n",
      "     )\n",
      "times = times.reset_index()\n",
      "times.to_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "23/28:\n",
      "stats = ['nunique', 'min', 'max', 'median', percentile(25), percentile(75), percentile(95)]\n",
      "\n",
      "times = (df.loc[df.is_post==0, ['from_id', 'name', 'elapsed_time',  'created_time', 'created_month', 'created_day', 'created_day_of_week', 'created_hour',]]\n",
      "      #.dropna(subset=['post_id'])\n",
      "      .head(10000)\n",
      "      .groupby(by=['from_id', 'name'])\n",
      "      .agg({'created_time': stats})\n",
      "     )\n",
      "times = times.reset_index()\n",
      "times.to_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "23/29:\n",
      "stats = ['nunique', 'min', ]\n",
      "\n",
      "times = (df.loc[df.is_post==0, ['from_id', 'name', 'elapsed_time',  'created_time', 'created_month', 'created_day', 'created_day_of_week', 'created_hour',]]\n",
      "      #.dropna(subset=['post_id'])\n",
      "      .head(10000)\n",
      "      .groupby(by=['from_id', 'name'])\n",
      "      .agg({'created_time': stats})\n",
      "     )\n",
      "times = times.reset_index()\n",
      "times.to_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "23/30: times.head()\n",
      "23/31:\n",
      "stats = ['nunique', 'min', 'max', 'std']\n",
      "\n",
      "times = (df.loc[df.is_post==0, ['from_id', 'name', 'elapsed_time',  'created_time', 'created_month', 'created_day', 'created_day_of_week', 'created_hour',]]\n",
      "      #.dropna(subset=['post_id'])\n",
      "      .head(10000)\n",
      "      .groupby(by=['from_id', 'name'])\n",
      "      .agg({'created_time': stats})\n",
      "     )\n",
      "times = times.reset_index()\n",
      "times.to_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "23/32:\n",
      "stats = ['nunique', 'min', 'max']\n",
      "\n",
      "times = (df.loc[df.is_post==0, ['from_id', 'name', 'elapsed_time',  'created_time', 'created_month', 'created_day', 'created_day_of_week', 'created_hour',]]\n",
      "      #.dropna(subset=['post_id'])\n",
      "      .head(10000)\n",
      "      .groupby(by=['from_id', 'name'])\n",
      "      .agg({stats})\n",
      "     )\n",
      "times = times.reset_index()\n",
      "times.to_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "23/33:\n",
      "stats = ['nunique', 'min', 'max']\n",
      "\n",
      "times = (df.loc[df.is_post==0, ['from_id', 'name', 'elapsed_time',  'created_time', 'created_month', 'created_day', 'created_day_of_week', 'created_hour',]]\n",
      "      #.dropna(subset=['post_id'])\n",
      "      .head(10000)\n",
      "      .groupby(by=['from_id', 'name'])\n",
      "      .agg(stats)\n",
      "     )\n",
      "times = times.reset_index()\n",
      "times.to_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "23/34: times.head()\n",
      "23/35:\n",
      "stats = ['nunique', 'min', 'max']\n",
      "\n",
      "times = (df.loc[df.is_post==0, ['from_id', 'name', 'elapsed_time',  'created_time', 'created_month', 'created_day', 'created_day_of_week', 'created_hour',]]\n",
      "      #.dropna(subset=['post_id'])\n",
      "      .head(10000)\n",
      "      .groupby(by=['from_id', 'name'])\n",
      "      .agg(stats)\n",
      "      .assign(period_length = lambda x: x['created_time']['max'] - x['created_time']['min'])\n",
      "     )\n",
      "times = times.reset_index()\n",
      "times.to_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "23/36: times.head()\n",
      "23/37: times.groupby('from_id').period_length.mean()\n",
      "23/38: times.groupby('from_id').period_length..total_seconds().mean()\n",
      "23/39: times.groupby('from_id').period_length.total_seconds().mean()\n",
      "23/40:\n",
      "stats = ['nunique', 'min', 'max']\n",
      "\n",
      "times = (df.loc[df.is_post==0, ['from_id', 'name', 'elapsed_time',  'created_time', 'created_month', 'created_day', 'created_day_of_week', 'created_hour',]]\n",
      "      #.dropna(subset=['post_id'])\n",
      "      .head(10000)\n",
      "      .groupby(by=['from_id', 'name'])\n",
      "      .agg(stats)\n",
      "      .assign(period_length = lambda x: (x['created_time']['max'] - x['created_time']['min']).dt.total_seconds())\n",
      "     )\n",
      "times = times.reset_index()\n",
      "times.to_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "23/41: times.groupby('from_id').period_length.mean()\n",
      "23/42: times.groupby('from_id').period_length.mean().sort_values(descending=False)\n",
      "23/43: times.groupby('from_id').period_length.mean().sort_values(ascending=False)\n",
      "23/44: times.groupby('from_id').period_length.mean().hist()\n",
      "23/45: times.groupby('from_id').period_length.mean().hist(bins=30)\n",
      "23/46: times.groupby('from_id').period_length.mean().to_timedelta().hist(bins=30)\n",
      "23/47: times.groupby('from_id').period_length.mean().hist(bins=30)\n",
      "23/48: pd.to_timedelta(times.groupby('from_id').period_length.mean()).hist(bins=30)\n",
      "23/49: pd.to_timedelta(times.groupby('from_id').period_length.mean())\n",
      "23/50: pd.to_timedelta(times.groupby('from_id').period_length.mean(), unit='s')\n",
      "23/51: pd.to_timedelta(times.groupby('from_id').period_length.mean(), unit='s').hist(bins=30)\n",
      "23/52: times.groupby('from_id').period_length.mean().hist(bins=30)\n",
      "23/53:\n",
      "stats = ['nunique', 'min', 'max']\n",
      "\n",
      "times = (df.loc[df.is_post==0, ['from_id', 'name', 'elapsed_time',  'created_time', 'created_month', 'created_day', 'created_day_of_week', 'created_hour',]]\n",
      "      #.dropna(subset=['post_id'])\n",
      "      .head(10000)\n",
      "      .groupby(by=['from_id'])\n",
      "      .agg(stats)\n",
      "      .assign(period_length = lambda x: (x['created_time']['max'] - x['created_time']['min']).dt.total_seconds())\n",
      "     )\n",
      "times = times.reset_index()\n",
      "times.to_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "23/54: times.period_length.mean().hist(bins=30)\n",
      "23/55: times.period_length.hist(bins=30)\n",
      "23/56:\n",
      "stats = ['nunique', 'min', 'max']\n",
      "\n",
      "times = (df.loc[df.is_post==0, ['from_id', 'name', 'elapsed_time',  'created_time', 'created_month', 'created_day', 'created_day_of_week', 'created_hour',]]\n",
      "      #.dropna(subset=['post_id'])\n",
      "      .groupby(by=['from_id'])\n",
      "      .agg(stats)\n",
      "      .assign(period_length = lambda x: (x['created_time']['max'] - x['created_time']['min']).dt.total_seconds())\n",
      "     )\n",
      "times = times.reset_index()\n",
      "times.to_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "24/1: import wikipediaapi\n",
      "24/2: import wikipediaapi as wa\n",
      "24/3: wiki = wa.Wikipedia('he')\n",
      "24/4: zuk_he = wiki.page('מבצע_צוק_איתן')\n",
      "24/5: dir(zuk_he)\n",
      "24/6: zuk_he.langlinks\n",
      "24/7:\n",
      "def print_langlinks(page):\n",
      "    langlinks = page.langlinks\n",
      "    for k in sorted(langlinks.keys()):\n",
      "        v = langlinks[k]\n",
      "        print(\"%s: %s - %s: %s\" % (k, v.language, v.title, v.fullurl))\n",
      "\n",
      "print_langlinks(zuk_he)\n",
      "24/8: zuk_he.langlinks['ar'].summary\n",
      "24/9: zuk_he.langlinks['ar'].links\n",
      "24/10: len(zuk_he.langlinks['ar'].links)\n",
      "24/11: zuk_he.langlinks['ar'].links\n",
      "24/12: zuk_he.langlinks['ar'].extlinks\n",
      "24/13: zuk_he.langlinks['ar']\n",
      "24/14: dir(zuk_he.langlinks['ar'])\n",
      "24/15: zuk_he.langlinks['ar'].sections_by_title\n",
      "24/16: zuk_he.langlinks['ar'].sections\n",
      "24/17:\n",
      "def print_sections(sections, level=0):\n",
      "        for s in sections:\n",
      "                print(\"%s: %s - %s\" % (\"*\" * (level + 1), s.title, s.text[0:40]))\n",
      "                print_sections(s.sections, level + 1)\n",
      "print_sections(zuk_he)\n",
      "24/18:\n",
      "def print_sections(sections, level=0):\n",
      "        for s in sections:\n",
      "                print(\"%s: %s - %s\" % (\"*\" * (level + 1), s.title, s.text[0:40]))\n",
      "                print_sections(s.sections, level + 1)\n",
      "print_sections(zuk_he.sections)\n",
      "24/19: zuk_he.section_by_title\n",
      "24/20: zuk_he.section_by_title('קישורים חיצוניים')\n",
      "24/21: dir(zuk_he.section_by_title('קישורים חיצוניים'))\n",
      "24/22: zuk_he.section_by_title('קישורים חיצוניים')\n",
      "24/23: dir(zuk_he.section_by_title('קישורים חיצוניים'))\n",
      "24/24: zuk_he.section_by_title('קישורים חיצוניים').text\n",
      "24/25: zuk_he.section_by_title('קישורים חיצוניים')\n",
      "24/26: zuk_he.section_by_title('קישורים חיצוניים').sections\n",
      "24/27: zuk_he.section_by_title('קישורים חיצוניים').sections[0]\n",
      "24/28: zuk_he.section_by_title('קישורים חיצוניים').sections[0].text\n",
      "24/29: dir(zuk_he.section_by_title('קישורים חיצוניים').sections[0])\n",
      "24/30: dir(zuk_he.section_by_title('קישורים חיצוניים').sections[0].level)\n",
      "24/31: zuk_he.section_by_title('קישורים חיצוניים').sections[0].level\n",
      "24/32: zuk_he.section_by_title('קישורים חיצוניים').sections[0].text\n",
      "24/33: zuk_he.section_by_title('קישורים חיצוניים').sections[0]\n",
      "24/34: dir(zuk_he.links)\n",
      "24/35: zuk_he.links\n",
      "24/36: import requests\n",
      "24/37: f'http://{lang}.wikipedia.org/w/api.php?action=query&titles={title}&prop=links|extlinks'\n",
      "24/38:\n",
      "ll = zuk_he.langlinks\n",
      "for k in sorted(ll.keys()):\n",
      "    v = ll[k]\n",
      "    print(requests.get(f'http://{k}.wikipedia.org/w/api.php?action=query&titles={v.title}&prop=links|extlinks').response)\n",
      "24/39:\n",
      "ll = zuk_he.langlinks\n",
      "for k in sorted(ll.keys()):\n",
      "    v = ll[k]\n",
      "    print(requests.get(f'http://{k}.wikipedia.org/w/api.php?action=query&titles={v.title}&prop=links|extlinks').text)\n",
      "24/40:\n",
      "ll = zuk_he.langlinks\n",
      "for k in sorted(ll.keys()):\n",
      "    v = ll[k]\n",
      "    print(requests.get(f'http://{k}.wikipedia.org/w/api.php?action=query&titles={v.title}&prop=extlinks').text)\n",
      "24/41:\n",
      "ll = zuk_he.langlinks\n",
      "ext_links = \n",
      "for k in sorted(ll.keys()):\n",
      "    v = ll[k]\n",
      "    print(requests.get(f'http://{k}.wikipedia.org/w/api.php?action=query&titles={v.title}&prop=extlinks&format=json').text)\n",
      "24/42:\n",
      "ll = zuk_he.langlinks\n",
      "ext_links = {}\n",
      "for k in sorted(ll.keys()):\n",
      "    v = ll[k]\n",
      "    print(requests.get(f'http://{k}.wikipedia.org/w/api.php?action=query&titles={v.title}&prop=extlinks&format=json').text)\n",
      "24/43:\n",
      "ll = zuk_he.langlinks\n",
      "ext_links = {}\n",
      "for k in sorted(ll.keys()):\n",
      "    v = ll[k]\n",
      "    print(requests.get(f'http://{k}.wikipedia.org/w/api.php?action=query&titles={v.title}&prop=extlinks&format=json&pllimit=max').json)\n",
      "24/44:\n",
      "ll = zuk_he.langlinks\n",
      "ext_links = {}\n",
      "for k in sorted(ll.keys()):\n",
      "    v = ll[k]\n",
      "    ext_links[k] = requests.get(f'http://{k}.wikipedia.org/w/api.php?action=query&titles={v.title}&prop=extlinks&format=json&pllimit=max').json\n",
      "24/45: len(ext_links)\n",
      "24/46: ext_links['ai']\n",
      "24/47: ext_links['ar']\n",
      "24/48: dir(ext_links['ar'])\n",
      "24/49:\n",
      "ll = zuk_he.langlinks\n",
      "ext_links = {}\n",
      "for k in sorted(ll.keys()):\n",
      "    v = ll[k]\n",
      "    ext_links[k] = requests.get(f'http://{k}.wikipedia.org/w/api.php?action=query&titles={v.title}&prop=extlinks&format=json&pllimit=max').json()\n",
      "24/50: dir(ext_links['ar'])\n",
      "24/51: ext_links['ar']\n",
      "24/52:\n",
      "ll = zuk_he.langlinks\n",
      "ext_links = {}\n",
      "for k in sorted(ll.keys()):\n",
      "    v = ll[k]\n",
      "    ext_links[k] = requests.get(f'http://{k}.wikipedia.org/w/api.php?action=query&titles={v.title}&prop=extlinks&format=json&ellimit=max').json()\n",
      "24/53: ext_links['ar']\n",
      "24/54:\n",
      "for lang in ext_links:\n",
      "    print(f'{lang}: {len(ext_links[lang])}')\n",
      "24/55:\n",
      "for lang in ext_links:\n",
      "    print(f'{lang}: {len(ext_links[lang][\\'query\\'][\\'extlinks\\'])}')\n",
      "24/56:\n",
      "for lang in ext_links:\n",
      "    num = len(ext_links[lang]['query']['extlinks'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    print(f'{lang}: {}')\n",
      "24/57:\n",
      "for lang in ext_links:\n",
      "    num = len(ext_links[lang]['query']['extlinks'])\n",
      "    print(f'{lang}: {num}')\n",
      "24/58:\n",
      "for lang in ext_links:\n",
      "    for page in ext_links[lang]['query']['pages']:\n",
      "        num = len(ext_links[lang]['query'][page]['ext_links'])\n",
      "        print(f'{lang}: {num}')\n",
      "24/59:\n",
      "for lang in ext_links:\n",
      "    for page in ext_links[lang]['query']['pages']:\n",
      "        num = len(ext_links[lang]['query']['pages'][page]['ext_links'])\n",
      "        print(f'{lang}: {num}')\n",
      "24/60:\n",
      "for lang in ext_links:\n",
      "    for page in ext_links[lang]['query']['pages']:\n",
      "        num = len(ext_links[lang]['query']['pages'][page]['extlinks'])\n",
      "        print(f'{lang}: {num}')\n",
      "24/61:\n",
      "for lang in ext_links:\n",
      "    for page in ext_links[lang]['query']['pages']:\n",
      "        if 'ext_links' in ext_links[lang]['query']['pages'][page]:\n",
      "            num = len(ext_links[lang]['query']['pages'][page]['extlinks'])\n",
      "        else: num=0\n",
      "        print(f'{lang}: {num}')\n",
      "24/62:\n",
      "for lang in ext_links:\n",
      "    for page in ext_links[lang]['query']['pages']:\n",
      "        if 'extlinks' in ext_links[lang]['query']['pages'][page]:\n",
      "            num = len(ext_links[lang]['query']['pages'][page]['extlinks'])\n",
      "        else: num=0\n",
      "        print(f'{lang}: {num}')\n",
      "24/63:\n",
      "for lang in ext_links:\n",
      "    for page in ext_links[lang]['query']['pages']:\n",
      "        if 'extlinks' in ext_links[lang]['query']['pages'][page]:\n",
      "            num = len(ext_links[lang]['query']['pages'][page]['extlinks'])\n",
      "        else: num=0\n",
      "        title = ext_links[lang]['query']['pages'][page]['title']\n",
      "        print(f'{lang}: extlinks={num}, title: {title}')\n",
      "24/64: ext_links['en']\n",
      "24/65:\n",
      "ll = zuk_he.langlinks\n",
      "ext_links = {}\n",
      "for k in sorted(ll.keys()):\n",
      "    v = ll[k]\n",
      "    resp = requests.get(f'http://{k}.wikipedia.org/w/api.php?action=query&titles={v.title}&prop=extlinks&format=json&ellimit=max').json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        eloffset = resp['continue']['eloffset']\n",
      "        resp = requests.get(f'http://{k}.wikipedia.org/w/api.php?action=query&titles={v.title}&prop=extlinks&format=json&ellimit=max&elcontinue={eloffset}').json()\n",
      "        final_resp.append(resp)\n",
      "    ext_links[k] = final_resp\n",
      "24/66: final_resp\n",
      "24/67: len(final_resp)\n",
      "24/68: ext_links['en']\n",
      "24/69:\n",
      "ll = zuk_he.langlinks\n",
      "ext_links = {}\n",
      "for k in sorted(ll.keys()):\n",
      "    v = ll[k]\n",
      "    resp = requests.get(f'http://{k}.wikipedia.org/w/api.php?action=query&titles={v.title}&prop=extlinks&format=json&ellimit=max').json()\n",
      "    final_resp = [resp, ]\n",
      "    print (resp)\n",
      "    while 'continue' in resp:\n",
      "        eloffset = resp['continue']['eloffset']\n",
      "        resp = requests.get(f'http://{k}.wikipedia.org/w/api.php?action=query&titles={v.title}&prop=extlinks&format=json&ellimit=max&elcontinue={eloffset}').json()\n",
      "        final_resp.append(resp)\n",
      "    ext_links[k] = final_resp\n",
      "24/70:\n",
      "ll = zuk_he.langlinks\n",
      "ext_links = {}\n",
      "for k in sorted(ll.keys()):\n",
      "    v = ll[k]\n",
      "    resp = requests.get(f'http://{k}.wikipedia.org/w/api.php?action=query&titles={v.title}&prop=extlinks&format=json&ellimit=max').json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        print (resp)\n",
      "        eloffset = resp['continue']['eloffset']\n",
      "        resp = requests.get(f'http://{k}.wikipedia.org/w/api.php?action=query&titles={v.title}&prop=extlinks&format=json&ellimit=max&elcontinue={eloffset}').json()\n",
      "        final_resp.append(resp)\n",
      "    ext_links[k] = final_resp\n",
      "24/71:\n",
      "ll = zuk_he.langlinks\n",
      "ext_links = {}\n",
      "for k in sorted(ll.keys()):\n",
      "    v = ll[k]\n",
      "    resp = requests.get(f'http://{k}.wikipedia.org/w/api.php?action=query&titles={v.title}&prop=extlinks&format=json&ellimit=max').json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        print (resp['continue'])\n",
      "        eloffset = resp['continue']['eloffset']\n",
      "        resp = requests.get(f'http://{k}.wikipedia.org/w/api.php?action=query&titles={v.title}&prop=extlinks&format=json&ellimit=max&elcontinue={eloffset}').json()\n",
      "        final_resp.append(resp)\n",
      "    ext_links[k] = final_resp\n",
      "24/72:\n",
      "ll = zuk_he.langlinks\n",
      "ext_links = {}\n",
      "\n",
      "req = {'action': 'query',\n",
      "       'prop': 'extlinks',\n",
      "       'format': 'json',\n",
      "       'ellimit': 'max',}\n",
      "\n",
      "for k in sorted(ll.keys()):\n",
      "    v = ll[k]\n",
      "    params = req.copy()\n",
      "    params['title'] = v.title\n",
      "    resp = requests.get(f'http://{k}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        eloffset = resp['continue']['eloffset']\n",
      "        resp = requests.get(f'http://{k}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    ext_links[k] = final_resp\n",
      "24/73: len(final_resp)\n",
      "24/74: ext_links['en']\n",
      "24/75:\n",
      "ll = zuk_he.langlinks\n",
      "ext_links = {}\n",
      "\n",
      "req = {'action': 'query',\n",
      "       'prop': 'extlinks',\n",
      "       'format': 'json',\n",
      "       'ellimit': 'max',}\n",
      "\n",
      "for k in sorted(ll.keys()):\n",
      "    v = ll[k]\n",
      "    params = req.copy()\n",
      "    params['titles'] = v.title\n",
      "    resp = requests.get(f'http://{k}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        eloffset = resp['continue']['eloffset']\n",
      "        resp = requests.get(f'http://{k}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    ext_links[k] = final_resp\n",
      "24/76: len(final_resp)\n",
      "24/77: ext_links['en']\n",
      "24/78:\n",
      "for lang in ext_links:\n",
      "    for resp in ext_links[lang]:\n",
      "        for page in resp['query']['pages']:\n",
      "            if 'extlinks' in resp['query']['pages'][page]:\n",
      "                num = len(resp['query']['pages'][page]['extlinks'])\n",
      "            else: num=0\n",
      "            title = resp['query']['pages'][page]['title']\n",
      "            print(f'{lang}: extlinks={num}, title: {title}')\n",
      "24/79:\n",
      "clean_links = {}\n",
      "for lang in ext_links:\n",
      "    for resp in ext_links[lang]:\n",
      "        for page in resp['query']['pages']:\n",
      "            if 'extlinks' in resp['query']['pages'][page]:\n",
      "                links = resp['query']['pages'][page]['extlinks']\n",
      "                num = len(links)\n",
      "                clean_links[lang] = links.values()\n",
      "            else: \n",
      "                clean_links = []\n",
      "                num=0\n",
      "            title = resp['query']['pages'][page]['title']\n",
      "\n",
      "            print(f'{lang}: extlinks={num}, title: {title}')\n",
      "24/80:\n",
      "clean_links = {}\n",
      "for lang in ext_links:\n",
      "    for resp in ext_links[lang]:\n",
      "        for page in resp['query']['pages']:\n",
      "            if 'extlinks' in resp['query']['pages'][page]:\n",
      "                links = resp['query']['pages'][page]['extlinks']\n",
      "                num = len(links)\n",
      "                clean_links[lang] = [l.values() for l in links]\n",
      "            else: \n",
      "                clean_links = []\n",
      "                num=0\n",
      "            title = resp['query']['pages'][page]['title']\n",
      "\n",
      "            print(f'{lang}: extlinks={num}, title: {title}')\n",
      "24/81:\n",
      "clean_links = {}\n",
      "for lang in ext_links:\n",
      "    for resp in ext_links[lang]:\n",
      "        for page in resp['query']['pages']:\n",
      "            if 'extlinks' in resp['query']['pages'][page]:\n",
      "                links = resp['query']['pages'][page]['extlinks']\n",
      "                num = len(links)\n",
      "                clean_links[lang] = [l.values() for l in links]\n",
      "            else: \n",
      "                clean_links[lang] = []\n",
      "                num=0\n",
      "                \n",
      "            title = resp['query']['pages'][page]['title']\n",
      "\n",
      "            print(f'{lang}: extlinks={num}, title: {title}')\n",
      "24/82: clean_links['en']\n",
      "24/83:\n",
      "flatten = lambda l: [item for sublist in l for item in sublist]\n",
      "\n",
      "clean_links = {}\n",
      "for lang in ext_links:\n",
      "    for resp in ext_links[lang]:\n",
      "        for page in resp['query']['pages']:\n",
      "            if 'extlinks' in resp['query']['pages'][page]:\n",
      "                links = resp['query']['pages'][page]['extlinks']\n",
      "                num = len(links)\n",
      "                clean_links[lang] = [flatten(l.values()) for l in links]\n",
      "            else: \n",
      "                clean_links[lang] = []\n",
      "                num=0\n",
      "                \n",
      "            title = resp['query']['pages'][page]['title']\n",
      "\n",
      "            print(f'{lang}: extlinks={num}, title: {title}')\n",
      "24/84: clean_links['en']\n",
      "24/85:\n",
      "flatten = lambda l: [item for sublist in l for item in sublist]\n",
      "\n",
      "clean_links = {}\n",
      "for lang in ext_links:\n",
      "    for resp in ext_links[lang]:\n",
      "        for page in resp['query']['pages']:\n",
      "            if 'extlinks' in resp['query']['pages'][page]:\n",
      "                links = resp['query']['pages'][page]['extlinks']\n",
      "                num = len(links)\n",
      "                clean_links[lang] = [flatten(list(l.values())) for l in links]\n",
      "            else: \n",
      "                clean_links[lang] = []\n",
      "                num=0\n",
      "                \n",
      "            title = resp['query']['pages'][page]['title']\n",
      "\n",
      "            print(f'{lang}: extlinks={num}, title: {title}')\n",
      "24/86: clean_links['en']\n",
      "24/87:\n",
      "flatten = lambda l: [item for sublist in l for item in sublist]\n",
      "\n",
      "clean_links = {}\n",
      "for lang in ext_links:\n",
      "    for resp in ext_links[lang]:\n",
      "        for page in resp['query']['pages']:\n",
      "            if 'extlinks' in resp['query']['pages'][page]:\n",
      "                links = resp['query']['pages'][page]['extlinks']\n",
      "                num = len(links)\n",
      "                clean_links[lang] = flatten([list(l.values()) for l in links])\n",
      "            else: \n",
      "                clean_links[lang] = []\n",
      "                num=0\n",
      "                \n",
      "            title = resp['query']['pages'][page]['title']\n",
      "\n",
      "            print(f'{lang}: extlinks={num}, title: {title}')\n",
      "24/88: clean_links['en']\n",
      "24/89:\n",
      "from collections import defaultdict\n",
      "\n",
      "flatten = lambda l: [item for sublist in l for item in sublist]\n",
      "\n",
      "clean_links = defaultdict(list)\n",
      "for lang in ext_links:\n",
      "    for resp in ext_links[lang]:\n",
      "        for page in resp['query']['pages']:\n",
      "            if 'extlinks' in resp['query']['pages'][page]:\n",
      "                links = resp['query']['pages'][page]['extlinks']\n",
      "                num = len(links)\n",
      "                clean_links[lang].extend(flatten([list(l.values()) for l in links]))\n",
      "            else: \n",
      "                clean_links[lang].extend([])\n",
      "                num=0\n",
      "                \n",
      "            title = resp['query']['pages'][page]['title']\n",
      "\n",
      "            print(f'{lang}: extlinks={num}, title: {title}')\n",
      "24/90: clean_links['en']\n",
      "24/91: len(clean_links['en'])\n",
      "24/92: clean_links['en']\n",
      "24/93:\n",
      "from collections import Counter\n",
      "\n",
      "lc = Counter(flatten(l for l in clean_links.values()))\n",
      "24/94: lc\n",
      "24/95: lc.most_common()\n",
      "25/1: ll['ar'].langlinks\n",
      "25/2:\n",
      "x = ll['ar'].langlinks\n",
      "x\n",
      "25/3: import wikipediaapi as wa\n",
      "25/4: wiki = wa.Wikipedia('he')\n",
      "25/5: zuk_he = wiki.page('מבצע_צוק_איתן')\n",
      "25/6:\n",
      "def print_langlinks(page):\n",
      "    langlinks = page.langlinks\n",
      "    for k in sorted(langlinks.keys()):\n",
      "        v = langlinks[k]\n",
      "        print(\"%s: %s - %s: %s\" % (k, v.language, v.title, v.fullurl))\n",
      "\n",
      "print_langlinks(zuk_he)\n",
      "25/7:\n",
      "def print_sections(sections, level=0):\n",
      "        for s in sections:\n",
      "                print(\"%s: %s - %s\" % (\"*\" * (level + 1), s.title, s.text[0:40]))\n",
      "                print_sections(s.sections, level + 1)\n",
      "print_sections(zuk_he.sections)\n",
      "25/8: zuk_he.section_by_title('קישורים חיצוניים').sections[0]\n",
      "25/9:\n",
      "def print_sections(sections, level=0):\n",
      "        for s in sections:\n",
      "                print(\"%s: %s - %s\" % (\"*\" * (level + 1), s.title, s.text[0:40]))\n",
      "                print_sections(s.sections, level + 1)\n",
      "#print_sections(zuk_he.sections)\n",
      "25/10: zuk_he.section_by_title('קישורים חיצוניים').sections[0]\n",
      "25/11: zuks = {key: wa.Wikipedia(key) for key in }\n",
      "25/12: zuk_he.links\n",
      "25/13: import requests\n",
      "25/14: import requests\n",
      "25/15:\n",
      "ll = zuk_he.langlinks\n",
      "ext_links = {}\n",
      "\n",
      "req = {'action': 'query',\n",
      "       'prop': 'extlinks',\n",
      "       'format': 'json',\n",
      "       'ellimit': 'max',}\n",
      "\n",
      "for k in sorted(ll.keys()):\n",
      "    v = ll[k]\n",
      "    params = req.copy()\n",
      "    params['titles'] = v.title\n",
      "    resp = requests.get(f'http://{k}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        eloffset = resp['continue']['eloffset']\n",
      "        resp = requests.get(f'http://{k}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    ext_links[k] = final_resp\n",
      "25/16:\n",
      "x = ll['ar'].langlinks\n",
      "x\n",
      "25/17:\n",
      "x = ll['ar'].langlinks['he']\n",
      "x\n",
      "25/18: import requests\n",
      "25/19:\n",
      "ll = zuk_he.langlinks\n",
      "ll['he'] = ll['ar'].langlinks['he']\n",
      "\n",
      "ext_links = {}\n",
      "\n",
      "req = {'action': 'query',\n",
      "       'prop': 'extlinks',\n",
      "       'format': 'json',\n",
      "       'ellimit': 'max',}\n",
      "\n",
      "for k in sorted(ll.keys()):\n",
      "    v = ll[k]\n",
      "    params = req.copy()\n",
      "    params['titles'] = v.title\n",
      "    resp = requests.get(f'http://{k}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        eloffset = resp['continue']['eloffset']\n",
      "        resp = requests.get(f'http://{k}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    ext_links[k] = final_resp\n",
      "25/20: len(final_resp)\n",
      "25/21: ext_links['en']\n",
      "25/22:\n",
      "from collections import defaultdict\n",
      "\n",
      "flatten = lambda l: [item for sublist in l for item in sublist]\n",
      "\n",
      "clean_links = defaultdict(list)\n",
      "for lang in ext_links:\n",
      "    for resp in ext_links[lang]:\n",
      "        for page in resp['query']['pages']:\n",
      "            if 'extlinks' in resp['query']['pages'][page]:\n",
      "                links = resp['query']['pages'][page]['extlinks']\n",
      "                num = len(links)\n",
      "                clean_links[lang].extend(flatten([list(l.values()) for l in links]))\n",
      "            else: \n",
      "                clean_links[lang].extend([])\n",
      "                num=0\n",
      "                \n",
      "            title = resp['query']['pages'][page]['title']\n",
      "\n",
      "            print(f'{lang}: extlinks={num}, title: {title}')\n",
      "25/23: clean_links['en']\n",
      "25/24:\n",
      "from collections import Counter\n",
      "\n",
      "lc = Counter(flatten(l for l in clean_links.values()))\n",
      "25/25: lc.most_common()\n",
      "25/26: ll['he'].categories\n",
      "25/27:\n",
      "categories = {}\n",
      "for k in ll.keys():\n",
      "    categories[k] = ll[k].categories\n",
      "25/28:\n",
      "from collections import Counter\n",
      "print({k: len(v) for categories.items()})\n",
      "25/29:\n",
      "from collections import Counter\n",
      "print({k: len(v) for k,v in  categories.items()})\n",
      "25/30:\n",
      "categories = {}\n",
      "for k in sorted(ll.keys()):\n",
      "    categories[k] = ll[k].categories\n",
      "25/31:\n",
      "from collections import Counter\n",
      "print({k: len(v) for k,v in  categories.items()})\n",
      "25/32: zuk_he.categories\n",
      "25/33:\n",
      "import pandas as pd\n",
      "pd.DataFrame(clean_links)\n",
      "25/34:\n",
      "import pandas as pd\n",
      "cl_for_df = []\n",
      "\n",
      "for lang in clean_links:\n",
      "    for link in clean_links[lang]:\n",
      "        cl_for_df.append({'lang': lang, 'link': link})\n",
      "\n",
      "link_df = pd.DataFrame(cl_for_df)\n",
      "25/35:\n",
      "import pandas as pd\n",
      "cl_for_df = []\n",
      "\n",
      "for lang in clean_links:\n",
      "    for link in clean_links[lang]:\n",
      "        cl_for_df.append({'lang': lang, 'link': link})\n",
      "\n",
      "link_df = pd.DataFrame(cl_for_df)\n",
      "link_df.head()\n",
      "25/36:\n",
      "import pandas as pd\n",
      "cl_for_df = []\n",
      "\n",
      "for lang in clean_links:\n",
      "    for link in clean_links[lang]:\n",
      "        cl_for_df.append({'lang': lang, 'link': link})\n",
      "\n",
      "link_df = pd.DataFrame(cl_for_df)\n",
      "link_df.unstack()\n",
      "25/37:\n",
      "import pandas as pd\n",
      "import networkx nx\n",
      "25/38:\n",
      "import pandas as pd\n",
      "import networkx as nx\n",
      "25/39:\n",
      "cl_for_df = []\n",
      "\n",
      "for lang in clean_links:\n",
      "    for link in clean_links[lang]:\n",
      "        cl_for_df.append({'lang': lang, 'link': link})\n",
      "\n",
      "link_df = pd.DataFrame(cl_for_df)\n",
      "\n",
      "g = nx.from_pandas_edgelist(link_df, source='lang', target='link')\n",
      "\n",
      "g.write_gexf('data/extlinks.gexf')\n",
      "25/40:\n",
      "cl_for_df = []\n",
      "\n",
      "for lang in clean_links:\n",
      "    for link in clean_links[lang]:\n",
      "        cl_for_df.append({'lang': lang, 'link': link})\n",
      "\n",
      "link_df = pd.DataFrame(cl_for_df)\n",
      "\n",
      "g = nx.from_pandas_edgelist(link_df, source='lang', target='link')\n",
      "\n",
      "nx.write_gexf(g, 'data/extlinks.gexf')\n",
      "25/41:\n",
      "cl_for_df = []\n",
      "\n",
      "for lang in clean_links:\n",
      "    for link in clean_links[lang]:\n",
      "        cl_for_df.append({'lang': lang, 'link': link})\n",
      "\n",
      "link_df = pd.DataFrame(cl_for_df)\n",
      "link_df.head()\n",
      "25/42:\n",
      "cl_for_df = []\n",
      "\n",
      "for lang in clean_links:\n",
      "    for link in clean_links[lang]:\n",
      "        cl_for_df.append({'source': lang, 'target': link})\n",
      "\n",
      "link_df = pd.DataFrame(cl_for_df)\n",
      "link_df.head()\n",
      "25/43:\n",
      "g = nx.from_pandas_edgelist(link_df)\n",
      "\n",
      "nx.write_gexf(g, 'data/extlinks.gexf')\n",
      "25/44: link_df.target.value_counts()\n",
      "25/45: ldf[ldf.lang=='he']\n",
      "25/46:\n",
      "cl_for_df = []\n",
      "\n",
      "for lang in clean_links:\n",
      "    for link in clean_links[lang]:\n",
      "        cl_for_df.append({'lang': lang, 'link': link})\n",
      "\n",
      "ldf = pd.DataFrame(cl_for_df)\n",
      "ldf.head()\n",
      "25/47: ldf[ldf.lang=='he']\n",
      "25/48: link_df.source.value_counts()\n",
      "25/49:\n",
      "g = nx.from_pandas_edgelist(link_df, 'lang', 'link')\n",
      "\n",
      "nx.write_gexf(g, 'data/extlinks.gexf')\n",
      "25/50:\n",
      "g = nx.from_pandas_edgelist(ldf, 'lang', 'link')\n",
      "\n",
      "nx.write_gexf(g, 'data/extlinks.gexf')\n",
      "25/51: ll\n",
      "25/52:\n",
      "for_cdf = []\n",
      "for lang in ll:\n",
      "    page = ll[lang]\n",
      "    for_cdf.append({'lang': lang, 'full_url': page.fullurl, 'title': page.title, 'summary': page.summary})\n",
      "    \n",
      "cdf = pd.DataFrame(for_cdf)\n",
      "25/53:\n",
      "for_cdf = []\n",
      "for lang in ll:\n",
      "    page = ll[lang]\n",
      "    for_cdf.append({'lang': lang, 'full_url': page.fullurl, 'title': page.title, 'summary': page.summary})\n",
      "    \n",
      "cdf = pd.DataFrame(for_cdf)\n",
      "cdf.head()\n",
      "25/54: cdf.to_csv('data/zuk_wiki_pages.csv')\n",
      "25/55: cdf.to_csv('data/zuk_wiki_pages.csv', index=False)\n",
      "25/56: ldf.to_csv('data/lang_extlinks.csv', index=False)\n",
      "25/57: link_df.lang.value_counts()\n",
      "25/58: ldf.lang.value_counts()\n",
      "25/59:\n",
      "g = nx.from_pandas_edgelist(ldf, 'lang', 'link', create_using=nx.DiGraph())\n",
      "\n",
      "nx.write_gexf(g, 'data/extlinks.gexf')\n",
      "25/60:\n",
      "g = nx.from_pandas_edgelist(ldf, 'lang', 'link', create_using=nx.Graph())\n",
      "\n",
      "nx.write_gexf(g, 'data/extlinks.gexf')\n",
      "25/61:\n",
      "g = nx.from_pandas_edgelist(ldf, 'lang', 'link', create_using=nx.DiGraph())\n",
      "\n",
      "nx.write_gexf(g, 'data/extlinks.gexf')\n",
      "25/62: from wikidata.client import Client\n",
      "25/63:\n",
      "from wikidata.client import Client\n",
      "from wikidata.client import Client\n",
      "client = Client()  # doctest: +SKIP\n",
      "entity = client.get('Q20145', load=True)\n",
      "entity\n",
      "25/64:\n",
      "from wikidata.client import Client\n",
      "client = Client()  # doctest: +SKIP\n",
      "entity = client.get('Q17324420', load=True)\n",
      "entity\n",
      "25/65: entity.description\n",
      "25/66: dir(entity)\n",
      "25/67: entity.label\n",
      "25/68: dir(entity)\n",
      "25/69: entity.lists\n",
      "25/70: print(entity.lists)\n",
      "25/71: entity.listvalues\n",
      "25/72: entity.list()\n",
      "25/73: entity.list|\n",
      "25/74: entity.lists()\n",
      "25/75: entity.listsvalues()\n",
      "25/76: entity.listvalues()\n",
      "25/77: entity.data\n",
      "25/78: entity.data['labels']\n",
      "25/79: entity.data['aliases']\n",
      "25/80:\n",
      "\n",
      "[{'lang': v['language'], label: v['value']} for k, v in entity.data['labels'].items()]\n",
      "25/81:\n",
      "\n",
      "[{'lang': v['language'], 'label': v['value']} for k, v in entity.data['labels'].items()]\n",
      "25/82:\n",
      "labels = pd.DataFrame([{'lang': v['language'], 'label': v['value']} for k, v in entity.data['labels'].items()])\n",
      "labels.head()\n",
      "25/83: aliases = pd.DataFrame([{'lang': v['language'], 'alias': v['value']} for v in l for k, l in entity.data['aliases'].items()])\n",
      "25/84: aliases = pd.DataFrame([{'lang': v['language'], 'alias': v['value']} for v in v for k, l in entity.data['aliases'].items()])\n",
      "25/85: entity.data['aliases']\n",
      "25/86: aliases = pd.DataFrame([{'lang': v['language'], 'alias': v['value']} for v in l for k, l in entity.data['aliases'].items()])\n",
      "25/87: aliases = pd.DataFrame([[{'lang': v['language'], 'alias': v['value']} for v in l] for k, l in entity.data['aliases'].items()])\n",
      "25/88:\n",
      "aliases = pd.DataFrame([[{'lang': v['language'], 'alias': v['value']} for v in l] for k, l in entity.data['aliases'].items()])\n",
      "aliases.head()\n",
      "25/89: [[{'lang': v['language'], 'alias': v['value']} for v in l] for k, l in entity.data['aliases'].items()]\n",
      "25/90: flatten([[{'lang': v['language'], 'alias': v['value']} for v in l] for k, l in entity.data['aliases'].items()])\n",
      "25/91:\n",
      "aliases = pd.DataFrame(flatten([[{'lang': v['language'], 'alias': v['value']} for v in l] for k, l in entity.data['aliases'].items()]))\n",
      "aliases.head()\n",
      "25/92: labels.to_csv('data/wikidata_labels.csv', index=False)\n",
      "25/93: labels.to_csv('data/wikidata_aliases.csv', index=False)\n",
      "25/94: aliases.to_csv('data/wikidata_aliases.csv', index=False)\n",
      "25/95: entity.data\n",
      "26/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "26/2:\n",
      "%%time\n",
      "df = pd.read_pickle('turf/data_df.pkl.gz', compression='gzip')\n",
      "26/3: df.dtypes\n",
      "26/4:\n",
      "sdf = pd.read_pickle('turf/shared_df.pkl.gz', compression='gzip')\n",
      "sdf.dtypes\n",
      "26/5: st = pd.read_pickle('turf/total_stats_no_multiindex.pkl.gz', compression='gzip')\n",
      "26/6:\n",
      "# OLD STUFF\n",
      "#save(st, 'turf/total_stats.pkl.gz')\n",
      "#st = pd.read_pickle( 'turf/total_stats.pkl.gz', compression='gzip')\n",
      "26/7: st.head()\n",
      "26/8: st.shape\n",
      "26/9: df[df.is_post==0].shape\n",
      "26/10: print ('Total number of comments: %d' % st.id.sum())\n",
      "26/11: print ('Total number of distinct users (commenters): %d' % st.from_id.nunique())\n",
      "26/12:\n",
      "counts = st.groupby('from_id').id.sum()\n",
      "one_timers = counts[counts==1]\n",
      "\n",
      "print ('Total number of one-timers: %d' % one_timers.shape[0])\n",
      "print ('Ratio of one timers from all users: %.2f' % (one_timers.shape[0] / st.from_id.nunique()))\n",
      "26/13: mat = st.pivot(index='from_id', columns='name', values='id')\n",
      "26/14: mat[(mat.count(axis=1)==1)].shape[0]\n",
      "26/15: mat.loc[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu==1), 'Benjamin_Netanyahu'].shape[0]\n",
      "26/16:\n",
      "print ('Ratio of Bibi from one-timers: %.2f' % (mat[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu==1)].shape[0] \n",
      " / counts[counts==1].shape[0]))\n",
      "26/17: print ('Ratio of users that commented 10 times and up: %.2f' % (counts[counts>=10].shape[0]/st.from_id.nunique()))\n",
      "26/18: (st.reset_index().groupby('from_id').name.nunique().value_counts().head(20)/st.reset_index().from_id.nunique()).plot(kind='bar')\n",
      "26/19: (st.reset_index().groupby('from_id').id.sum().value_counts().head(20)/st.reset_index().from_id.nunique()).plot(kind='bar')\n",
      "26/20:\n",
      "fans_users = (st.reset_index()\n",
      " .loc[st.groupby('from_id').name.transform('nunique')==1, ['from_id', 'name', 'party']])\n",
      "26/21: fans = ( fans_users.name.value_counts())\n",
      "26/22: fans.head(10)\n",
      "26/23:\n",
      "print ('Total number of fans: %d' % fans.sum())\n",
      "print ('Ratio of fans from all users: %.2f' % (fans.sum() / st.from_id.nunique()))\n",
      "print ('Ratio of one-timers from fans: %.2f' % (counts[counts==1].shape[0] / fans.sum()))\n",
      "26/24:\n",
      "(pd.concat([fans, \n",
      "            fans/st.from_id.nunique(), \n",
      "            fans/st.groupby('name').from_id.nunique()],\n",
      "            axis=1, keys=['num_uniques', 'ratio_from_all_users', 'ratio_from_own_users'])\n",
      "             .sort_values(by='ratio_from_own_users', ascending=False)).head(10)\n",
      "26/25:\n",
      "mkdet = pd.read_csv('turf/mk_details.csv', sep='\\t')\n",
      "mkdet.head()\n",
      "26/26: df['party'] = df['name'].map(mkdet.set_index('folder_name')['party'])\n",
      "26/27: df.groupby(['party', 'langdetect']).size().unstack()\n",
      "26/28: st['party'] = st['name'].map(mkdet.set_index('folder_name')['party'])\n",
      "26/29:\n",
      "two_mks_plus = (st.reset_index()\n",
      " .loc[st.groupby('from_id').name.transform('nunique')>1, ['from_id', 'name', 'party']])\n",
      "26/30: two_mks_plus.head()\n",
      "26/31: two_mks_plus.from_id.nunique()\n",
      "26/32: two_mks_plus.party.value_counts()\n",
      "26/33: two_mks_plus.groupby('party').from_id.nunique()\n",
      "26/34: base = two_mks_plus[two_mks_plus.groupby('from_id').party.transform('nunique')==1]\n",
      "26/35:\n",
      "print ('Total number of basers: %d' % base.from_id.nunique())\n",
      "print ('Ratio of basers from all users: %.2f' % (base.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of basers from two-mks-plus: %.2f' % (base.from_id.nunique() / two_mks_plus.from_id.nunique()))\n",
      "26/36: st.party.unique()\n",
      "26/37:\n",
      "camps = { 'מרצ': 'opposition',\n",
      "          'יש עתיד': 'opposition',\n",
      "          'ליכוד': 'coalition',\n",
      "          'הבית היהודי בראשות נפתלי בנט': 'coalition',\n",
      "          'המחנה הציוני': 'opposition',\n",
      "          'הרשימה המשותפת (חד”ש, רע”מ, בל”ד, תע”ל)': 'opposition',\n",
      "          'כולנו בראשות משה כחלון': 'coalition',\n",
      "          'ישראל ביתנו': 'coalition' }\n",
      "26/38: st['camp'] = st['party'].map(camps)\n",
      "26/39: two_mks_plus['camp'] = two_mks_plus['party'].map(camps)\n",
      "26/40: two_mks_plus.groupby('camp').from_id.nunique()\n",
      "26/41: campers = two_mks_plus[two_mks_plus.groupby('from_id').camp.transform('nunique')==1]\n",
      "26/42:\n",
      "print ('Total number of campers: %d' % campers.from_id.nunique())\n",
      "print ('Ratio of campers from all users: %.2f' % (campers.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of campers from two_mks_plus: %.2f' % (campers.from_id.nunique() / two_mks_plus.from_id.nunique()))\n",
      "print ('Coalition | Opposition ratio from campers: %.2f | %.2f' % \n",
      "       ((campers[campers.camp=='coalition'].from_id.nunique() / campers.from_id.nunique()),\n",
      "       (campers[campers.camp=='opposition'].from_id.nunique() / campers.from_id.nunique())))\n",
      "cg = st.groupby('camp')\n",
      "print ('Coalition | Opposition ratios from total users: %.2f | %.2f' % \n",
      "       ((cg.from_id.nunique()['coalition'] / st.from_id.nunique()),\n",
      "       ((cg.from_id.nunique()['opposition'] / st.from_id.nunique()))))\n",
      "\n",
      "print ('Coalition | Opposition total comments: %.2f | %.2f' % \n",
      "       ((cg.id.sum()['coalition'] / st.id.sum()),\n",
      "       ((cg.id.sum()['opposition'] / st.id.sum()))))\n",
      "26/43: top_commented = list(st.groupby('name').id.sum().sort_values(ascending=False).index)\n",
      "26/44:\n",
      "top_10_commented = top_commented[:10]\n",
      "top_10_commented\n",
      "26/45:\n",
      "#celebz =  two_mks_plus[two_mks_plus.groupby('from_id').name.transform(lambda x: x.isin(top_10_commented).all())]\n",
      "celebz = two_mks_plus[two_mks_plus.from_id.isin(mat[mat.loc[:, top_commented[10:]].isna().all(axis=1)].index)]\n",
      "26/46: num_celebiez_no_base_camp = len(set(celebz.from_id.drop_duplicates().values).difference(set(campers.from_id.drop_duplicates().values)).difference(set(base.from_id.drop_duplicates().values)))\n",
      "26/47:\n",
      "print ('Total number of celeb followers (celebies): %d' % celebz.from_id.nunique())\n",
      "print ('Ratio of celebies from all users: %.2f' % (celebz.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of celebies from two_mks_plus: %.2f' % (celebz.from_id.nunique() / two_mks_plus.from_id.nunique()))\n",
      "print ('Remove campers and base from celebies: %d' % num_celebiez_no_base_camp)\n",
      "26/48: top_commented[:10]\n",
      "26/49: fans.reindex(top_commented).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "26/50:\n",
      "only_bibi = mat.loc[:, top_commented[1:]].isna().all(axis=1)\n",
      "only_bibi[only_bibi].shape\n",
      "26/51:\n",
      "def get_accum_df(mat, order):\n",
      "    onlys = []\n",
      "    \n",
      "    for i, name in enumerate(order):\n",
      "        only = mat.loc[:, order[i+1:]].isna().all(axis=1)\n",
      "        onlys.append(only[only].shape[0])\n",
      "        \n",
      "    user_accum_df = pd.DataFrame({'mk': order, 'user_accum': onlys})\n",
      "    user_accum_df['party'] = user_accum_df['mk'].map(mkdet.set_index('folder_name')['party'])\n",
      "    user_accum_df['camp'] = user_accum_df['party'].map(camps)\n",
      "    user_accum_df['delta'] = user_accum_df.user_accum.diff()\n",
      "    unique_delta_ratio = (fans.reindex(top_commented) / user_accum_df.set_index('mk').delta).drop('Benjamin_Netanyahu')\n",
      "    user_accum_df['unique_delta_ratio'] = user_accum_df['mk'].map(unique_delta_ratio)\n",
      "    user_accum_df['delta_n'] = user_accum_df['delta'] / user_accum_df['delta'].max()\n",
      "    \n",
      "    return user_accum_df\n",
      "26/52:\n",
      "user_accum_df = get_accum_df(mat, top_commented)\n",
      "user_accum_df.head()\n",
      "26/53: user_accum_df.set_index('mk').user_accum.plot(kind='bar', figsize=(15,10), rot=75)\n",
      "26/54:\n",
      "import altair as alt\n",
      "alt.renderers.enable('notebook')\n",
      "26/55:\n",
      "def chart_user_accum(accum_df, order, order_name):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('user_accum',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'user_accum']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = f'User accumulation by {order_name} order - left to right, only users that comment inside the group until the current column')\n",
      "\n",
      "\n",
      "def chart_user_accum_delta(accum_df, order, order_name):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('delta',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'delta']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = f'User accumulation delta by {order_name} order - left to right, only users that comment inside the group until the current column')\n",
      "\n",
      "\n",
      "def chart_user_accum_unique_delta_ratio(accum_df, order, order_name):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('unique_delta_ratio',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'unique_delta_ratio']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = f'Page unique count divided by user_accum delta by {order_name} order (to measure how much a page is part of the group)')\n",
      "26/56:\n",
      "(chart_user_accum(user_accum_df, top_commented, 'Top Commented') \n",
      " & chart_user_accum_delta(user_accum_df, top_commented, 'Top Commented')\n",
      " & chart_user_accum_unique_delta_ratio(user_accum_df, top_commented, 'Top Commented')).properties(title='Top Commented')\n",
      "26/57: user_accum_df[['delta_n', 'unique_delta_ratio']].div(user_accum_df[['delta_n', 'unique_delta_ratio']].sum(axis=1), axis=0).plot(kind='bar', stacked=True, figsize=(15,10), rot=75, width=1.0)\n",
      "26/58:\n",
      "print ('First celebiez group: ', top_commented[:6])\n",
      "print ('Second celebiez group: ', top_commented[:15])\n",
      "26/59: top_mean_commented = list(df.loc[df.is_post==1].groupby('name').comment_count.mean().sort_values(ascending=False).index)\n",
      "26/60: fans.reindex(top_mean_commented).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "26/61: mean_com_accum_df = get_accum_df(mat, top_mean_commented)\n",
      "26/62:\n",
      "(chart_user_accum(mean_com_accum_df, top_mean_commented, 'Top Mean Commented') \n",
      " & chart_user_accum_delta(mean_com_accum_df, top_mean_commented, 'Top Mean Commented') \n",
      " & chart_user_accum_unique_delta_ratio(mean_com_accum_df, top_mean_commented, 'Top Mean Commented')).properties(title='Top Mean Commented')\n",
      "26/63: top_mean_commented.head()\n",
      "26/64: df.loc[df.is_post==1].groupby('name').comment_count.mean().sort_values(ascending=False).head()\n",
      "26/65: user_counts = st.groupby('from_id').id.sum()\n",
      "26/66:\n",
      "perc = pd.qcut(user_counts, 20, duplicates='drop')\n",
      "perc.value_counts(sort=False)\n",
      "26/67:\n",
      "perc_bah = pd.cut(user_counts, [0,1,2,3,4,5,7,11,24,9359])\n",
      "(perc_bah.value_counts(sort=False))\n",
      "26/68: (perc_bah.value_counts(sort=False)/ user_counts.shape[0])\n",
      "26/69: (perc_bah.value_counts(sort=False)/ user_counts.shape[0]).plot(kind='bar',figsize=(10,7))\n",
      "26/70:\n",
      "dedicated = st[st.from_id.isin(user_counts[user_counts>24].index)]\n",
      "dedicated.from_id.nunique()\n",
      "26/71: st[st.from_id.isin(user_counts[user_counts>24].index)].id.sum() / st.id.sum()\n",
      "26/72: st[st.from_id.isin(user_counts[user_counts>11].index)].from_id.nunique() / st.from_id.nunique()\n",
      "26/73: st[st.from_id.isin(user_counts[user_counts>11].index)].id.sum() / st.id.sum()\n",
      "26/74:\n",
      "campers = campers[(~campers.from_id.isin(base.from_id))& (~campers.from_id.isin(dedicated.from_id))]\n",
      "campers.from_id.nunique()\n",
      "26/75:\n",
      "base = base[(~base.from_id.isin(dedicated.from_id))]\n",
      "base.from_id.nunique()\n",
      "26/76:\n",
      "two_mks_plus = two_mks_plus[(~two_mks_plus.from_id.isin(base.from_id)) \n",
      "                            & (~two_mks_plus.from_id.isin(campers.from_id))\n",
      "                            & (~two_mks_plus.from_id.isin(dedicated.from_id))]\n",
      "two_mks_plus.from_id.nunique()\n",
      "26/77: campers.from_id.nunique() + base.from_id.nunique() + two_mks_plus.from_id.nunique()\n",
      "26/78:\n",
      "fans_users = fans_users[~fans_users.from_id.isin(one_timers.index)\n",
      "                        & (~fans_users.from_id.isin(dedicated.from_id))]\n",
      "fans_users.from_id.nunique()\n",
      "26/79:\n",
      "ded_fans_users = (dedicated.loc[st.groupby('from_id').name.transform('nunique')==1, ['from_id', 'name', 'party']])\n",
      "ded_fans_users.from_id.nunique(), dedicated.from_id.nunique()\n",
      "26/80:\n",
      "ded_two_mks_plus = (dedicated.loc[st.groupby('from_id').name.transform('nunique')>1, ['from_id', 'name', 'party', 'camp']])\n",
      "ded_base = ded_two_mks_plus[ded_two_mks_plus.groupby('from_id').party.transform('nunique')==1]\n",
      "ded_campers = ded_two_mks_plus[ded_two_mks_plus.groupby('from_id').camp.transform('nunique')==1]\n",
      "ded_celebies = ded_two_mks_plus[ded_two_mks_plus.from_id.isin(mat[mat.loc[:, list(all_scores_div.sort_values(by='total_score', ascending=False).index)[12:]].isna().all(axis=1)].index)]\n",
      "dedicated.from_id.nunique(), ded_two_mks_plus.from_id.nunique(), ded_base.from_id.nunique(), ded_campers.from_id.nunique(), ded_celebies.from_id.nunique()\n",
      "26/81:\n",
      "gfans = st[st.from_id.isin(fans_users.from_id)].groupby('from_id')\n",
      "gfans.id.sum().mean(), gfans.post_id.sum().mean(), gfans.id.sum().mean()/ gfans.post_id.sum().mean()\n",
      "26/82:\n",
      "fans_page = st[st.from_id.isin(fans_users.from_id)].groupby('name').size()\n",
      "fans_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "26/83:\n",
      "gbase = st[st.from_id.isin(base.from_id)].groupby('from_id')\n",
      "gbase.id.sum().mean(), gbase.post_id.sum().mean(), gbase.id.sum().mean()/ gbase.post_id.sum().mean()\n",
      "26/84:\n",
      "base_page = st[st.from_id.isin(base.from_id)].groupby('name').size()\n",
      "base_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "26/85:\n",
      "gcamp = st[st.from_id.isin(campers.from_id)].groupby('from_id')\n",
      "gcamp.id.sum().mean(), gcamp.post_id.sum().mean(), gcamp.id.sum().mean()/ gcamp.post_id.sum().mean()\n",
      "26/86: st[st.from_id.isin(campers.from_id)].id.sum()\n",
      "26/87:\n",
      "camp_page = st[st.from_id.isin(campers.from_id)].groupby('name').size()\n",
      "camp_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "26/88: st[(st.from_id.isin(two_mks_plus.from_id))].id.sum()\n",
      "26/89:\n",
      "gtwo = st[(st.from_id.isin(two_mks_plus.from_id))].groupby('from_id')\n",
      "gtwo.id.sum().mean(), gtwo.post_id.sum().mean(), gtwo.id.sum().mean()/gtwo.post_id.sum().mean()\n",
      "26/90:\n",
      "twomk_page = st[st.from_id.isin(two_mks_plus.from_id)].groupby('name').size()\n",
      "twomk_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "26/91:\n",
      "gded = dedicated.groupby('from_id')\n",
      "gded.id.sum().mean(), gded.post_id.sum().mean(), gded.id.sum().mean()/ gded.post_id.sum().mean()\n",
      "26/92:\n",
      "ded_page = st[st.from_id.isin(dedicated.from_id)].groupby('name').size()\n",
      "ded_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "26/93:\n",
      "page_cats = pd.concat([one_timers_page, fans_page, base_page, camp_page, twomk_page, ded_page], axis=1,\n",
      "          keys=['One-timers', 'Fans', 'Basers', 'Campers', 'Two-MK-Plus', 'Dedicated'])\n",
      "26/94: fans.reindex(top_mean_commented).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "26/95: mean_com_accum_df = get_accum_df(mat, top_mean_commented)\n",
      "26/96:\n",
      "(chart_user_accum(mean_com_accum_df, top_mean_commented, 'Top Mean Commented') \n",
      " & chart_user_accum_delta(mean_com_accum_df, top_mean_commented, 'Top Mean Commented') \n",
      " & chart_user_accum_unique_delta_ratio(mean_com_accum_df, top_mean_commented, 'Top Mean Commented')).properties(title='Top Mean Commented')\n",
      "26/97: top_liked = list(df.loc[df.is_post==1].groupby('name').like_count.mean().sort_values(ascending=False).index)\n",
      "26/98: fans.reindex(top_liked).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "26/99: like_accum_df = get_accum_df(mat, top_liked)\n",
      "26/100:\n",
      "(chart_user_accum(like_accum_df, top_liked, 'Top Liked') \n",
      " & chart_user_accum_delta(like_accum_df, top_liked, 'Top Liked') \n",
      " & chart_user_accum_unique_delta_ratio(like_accum_df, top_liked, 'Top Liked')).properties(title='Top Liked')\n",
      "26/101: top_shared = list(df.loc[df.is_post==1].groupby('name').share_count.mean().sort_values(ascending=False).index)\n",
      "26/102: fans.reindex(top_shared).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "26/103:\n",
      "share_accum_df = get_accum_df(mat, top_shared)\n",
      "share_accum_df.head()\n",
      "26/104:\n",
      "(chart_user_accum(share_accum_df, top_shared, 'Top Shared') \n",
      " & chart_user_accum_delta(share_accum_df, top_shared, 'Top Shared') \n",
      " & chart_user_accum_unique_delta_ratio(share_accum_df, top_shared, 'Top Shared')).properties(title='Top Shared')\n",
      "26/105:\n",
      "ts = (df.loc[df.is_post==1].groupby('name').share_count.mean().sort_values(ascending=False))\n",
      "tmc = (df.loc[df.is_post==1].groupby('name').comment_count.mean().sort_values(ascending=False))\n",
      "tl = (df.loc[df.is_post==1].groupby('name').like_count.mean().sort_values(ascending=False))\n",
      "all_scores = pd.concat([tmc, tl, ts], axis=1)\n",
      "all_scores_div = all_scores.div(all_scores.sum(axis=0), axis=1)\n",
      "all_scores['total_score'] = all_scores['like_count'] + 1.1*all_scores['comment_count'] + 1.2*all_scores['share_count']\n",
      "all_scores_div['total_score'] = all_scores_div['like_count'] + 1.1*all_scores_div['comment_count'] + 1.2*all_scores_div['share_count']\n",
      "all_scores.sort_values(by='total_score', ascending=False).head(10)\n",
      "26/106: all_scores_div.sort_values(by='total_score', ascending=False).head(20)\n",
      "26/107:\n",
      "all_accum_df = get_accum_df(mat, list(all_scores_div.sort_values(by='total_score', ascending=False).index))\n",
      "(chart_user_accum(all_accum_df, list(all_scores_div.sort_values(by='total_score', ascending=False).index), 'All Score') \n",
      " & chart_user_accum_delta(all_accum_df, list(all_scores_div.sort_values(by='total_score', ascending=False).index), 'All Score') \n",
      " & chart_user_accum_unique_delta_ratio(all_accum_df, list(all_scores_div.sort_values(by='total_score', ascending=False).index), 'All Score')).properties(title='All Score')\n",
      "26/108: new_celebies = two_mks_plus[two_mks_plus.from_id.isin(mat[mat.loc[:, list(all_scores_div.sort_values(by='total_score', ascending=False).index)[12:]].isna().all(axis=1)].index)]\n",
      "26/109: new_celebies.from_id.nunique(), two_mks_plus.from_id.nunique()\n",
      "26/110: user_counts = st.groupby('from_id').id.sum()\n",
      "26/111:\n",
      "perc = pd.qcut(user_counts, 20, duplicates='drop')\n",
      "perc.value_counts(sort=False)\n",
      "26/112:\n",
      "perc_bah = pd.cut(user_counts, [0,1,2,3,4,5,7,11,24,9359])\n",
      "(perc_bah.value_counts(sort=False))\n",
      "26/113: (perc_bah.value_counts(sort=False)/ user_counts.shape[0])\n",
      "26/114: (perc_bah.value_counts(sort=False)/ user_counts.shape[0]).plot(kind='bar',figsize=(10,7))\n",
      "26/115:\n",
      "dedicated = st[st.from_id.isin(user_counts[user_counts>24].index)]\n",
      "dedicated.from_id.nunique()\n",
      "26/116: st[st.from_id.isin(user_counts[user_counts>24].index)].id.sum() / st.id.sum()\n",
      "26/117: st[st.from_id.isin(user_counts[user_counts>11].index)].from_id.nunique() / st.from_id.nunique()\n",
      "26/118: st[st.from_id.isin(user_counts[user_counts>11].index)].id.sum() / st.id.sum()\n",
      "26/119:\n",
      "campers = campers[(~campers.from_id.isin(base.from_id))& (~campers.from_id.isin(dedicated.from_id))]\n",
      "campers.from_id.nunique()\n",
      "26/120:\n",
      "base = base[(~base.from_id.isin(dedicated.from_id))]\n",
      "base.from_id.nunique()\n",
      "26/121:\n",
      "two_mks_plus = two_mks_plus[(~two_mks_plus.from_id.isin(base.from_id)) \n",
      "                            & (~two_mks_plus.from_id.isin(campers.from_id))\n",
      "                            & (~two_mks_plus.from_id.isin(dedicated.from_id))]\n",
      "two_mks_plus.from_id.nunique()\n",
      "26/122: campers.from_id.nunique() + base.from_id.nunique() + two_mks_plus.from_id.nunique()\n",
      "26/123:\n",
      "fans_users = fans_users[~fans_users.from_id.isin(one_timers.index)\n",
      "                        & (~fans_users.from_id.isin(dedicated.from_id))]\n",
      "fans_users.from_id.nunique()\n",
      "26/124:\n",
      "ded_fans_users = (dedicated.loc[st.groupby('from_id').name.transform('nunique')==1, ['from_id', 'name', 'party']])\n",
      "ded_fans_users.from_id.nunique(), dedicated.from_id.nunique()\n",
      "26/125:\n",
      "ded_two_mks_plus = (dedicated.loc[st.groupby('from_id').name.transform('nunique')>1, ['from_id', 'name', 'party', 'camp']])\n",
      "ded_base = ded_two_mks_plus[ded_two_mks_plus.groupby('from_id').party.transform('nunique')==1]\n",
      "ded_campers = ded_two_mks_plus[ded_two_mks_plus.groupby('from_id').camp.transform('nunique')==1]\n",
      "ded_celebies = ded_two_mks_plus[ded_two_mks_plus.from_id.isin(mat[mat.loc[:, list(all_scores_div.sort_values(by='total_score', ascending=False).index)[12:]].isna().all(axis=1)].index)]\n",
      "dedicated.from_id.nunique(), ded_two_mks_plus.from_id.nunique(), ded_base.from_id.nunique(), ded_campers.from_id.nunique(), ded_celebies.from_id.nunique()\n",
      "26/126:\n",
      "labels = ('All', 'One Timers', 'Fans', 'two_mks_plus', 'Basers', 'Campers')\n",
      "sets = [set(st.from_id.values), set(one_timers.index), set(fans_users.from_id.values), set(two_mks_plus.from_id.values), set(base.from_id.values), set(campers.from_id.values)]\n",
      "\n",
      "set_dict = dict(zip(labels, sets))\n",
      "26/127:\n",
      "inter_set_dict = {}\n",
      "for i in set_dict:\n",
      "    inter_set_dict[i] = {}\n",
      "    for j in set_dict:\n",
      "        inter_set_dict[i][j] = len(set_dict[i].intersection(set_dict[j]))\n",
      "26/128: pd.DataFrame(inter_set_dict).reindex(labels)\n",
      "26/129: pd.DataFrame(inter_set_dict).reindex(labels).to_csv('turf/group_intersections.csv')\n",
      "26/130: from matplotlib_venn import venn3\n",
      "26/131:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(one_timers.index), set(fans_users.from_id.values)], set_labels = ('All', 'One Timers', 'Fans', ), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.25)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.20)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.20)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.09)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]-0.19)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.19)\n",
      "26/132:\n",
      "labels = ('All', 'One Timers', 'Fans', 'two_mks_plus', 'Basers', 'Campers')\n",
      "sets = [set(st.from_id.values), set(one_timers.index), set(fans_users.from_id.values), set(two_mks_plus.from_id.values), set(base.from_id.values), set(campers.from_id.values)]\n",
      "\n",
      "set_dict = dict(zip(labels, sets))\n",
      "26/133:\n",
      "inter_set_dict = {}\n",
      "for i in set_dict:\n",
      "    inter_set_dict[i] = {}\n",
      "    for j in set_dict:\n",
      "        inter_set_dict[i][j] = len(set_dict[i].intersection(set_dict[j]))\n",
      "26/134: pd.DataFrame(inter_set_dict).reindex(labels)\n",
      "26/135: pd.DataFrame(inter_set_dict).reindex(labels).to_csv('turf/group_intersections.csv')\n",
      "26/136: from matplotlib_venn import venn3\n",
      "26/137:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(one_timers.index), set(fans_users.from_id.values)], set_labels = ('All', 'One Timers', 'Fans', ), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.25)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.20)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.20)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.09)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]-0.19)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.19)\n",
      "26/138: ax.get_figure().savefig('turf/venn_one_timers_fans.png')\n",
      "26/139:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(base.from_id.values), set(campers.from_id.values)], set_labels = ('All', 'Basers', 'Campers'), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.15)\n",
      "v.get_label_by_id('111').set_y(v.get_label_by_id('111').get_position()[1]-0.05)\n",
      "v.get_patch_by_id('101').set_color('blue')\n",
      "v.get_patch_by_id('111').set_color('green')\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.20)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.35)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.15)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]+0.25)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.1)\n",
      "26/140: ax.get_figure().savefig('turf/venn_basers_campers.png')\n",
      "26/141:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(two_mks_plus.from_id.values), set(base.from_id.values), set(campers.from_id.values)], set_labels = ('two_mks_plus', 'Basers', 'Campers'), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.15)\n",
      "v.get_label_by_id('111').set_y(v.get_label_by_id('111').get_position()[1]-0.05)\n",
      "v.get_patch_by_id('101').set_color('blue')\n",
      "v.get_patch_by_id('111').set_color('green')\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.35)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.25)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.15)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]+0.05)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.1)\n",
      "26/142: df[df.is_post==1].shape[0], df[df.is_post==1].comment_count.mean()\n",
      "26/143: df[df.is_post==1].groupby('name').comment_count.mean().sort_values().plot(kind='barh', figsize=(10,15))\n",
      "26/144: one_timers.shape\n",
      "26/145:\n",
      "one_timers_page = st[st.from_id.isin(one_timers.index)].groupby('name').size()\n",
      "one_timers_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "26/146:\n",
      "gfans = st[st.from_id.isin(fans_users.from_id)].groupby('from_id')\n",
      "gfans.id.sum().mean(), gfans.post_id.sum().mean(), gfans.id.sum().mean()/ gfans.post_id.sum().mean()\n",
      "26/147:\n",
      "fans_page = st[st.from_id.isin(fans_users.from_id)].groupby('name').size()\n",
      "fans_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "26/148:\n",
      "gbase = st[st.from_id.isin(base.from_id)].groupby('from_id')\n",
      "gbase.id.sum().mean(), gbase.post_id.sum().mean(), gbase.id.sum().mean()/ gbase.post_id.sum().mean()\n",
      "26/149:\n",
      "base_page = st[st.from_id.isin(base.from_id)].groupby('name').size()\n",
      "base_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "26/150:\n",
      "gcamp = st[st.from_id.isin(campers.from_id)].groupby('from_id')\n",
      "gcamp.id.sum().mean(), gcamp.post_id.sum().mean(), gcamp.id.sum().mean()/ gcamp.post_id.sum().mean()\n",
      "26/151: st[st.from_id.isin(campers.from_id)].id.sum()\n",
      "26/152:\n",
      "camp_page = st[st.from_id.isin(campers.from_id)].groupby('name').size()\n",
      "camp_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "26/153: st[(st.from_id.isin(two_mks_plus.from_id))].id.sum()\n",
      "26/154:\n",
      "gtwo = st[(st.from_id.isin(two_mks_plus.from_id))].groupby('from_id')\n",
      "gtwo.id.sum().mean(), gtwo.post_id.sum().mean(), gtwo.id.sum().mean()/gtwo.post_id.sum().mean()\n",
      "26/155:\n",
      "twomk_page = st[st.from_id.isin(two_mks_plus.from_id)].groupby('name').size()\n",
      "twomk_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "26/156:\n",
      "gded = dedicated.groupby('from_id')\n",
      "gded.id.sum().mean(), gded.post_id.sum().mean(), gded.id.sum().mean()/ gded.post_id.sum().mean()\n",
      "26/157:\n",
      "ded_page = st[st.from_id.isin(dedicated.from_id)].groupby('name').size()\n",
      "ded_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "26/158:\n",
      "page_cats = pd.concat([one_timers_page, fans_page, base_page, camp_page, twomk_page, ded_page], axis=1,\n",
      "          keys=['One-timers', 'Fans', 'Basers', 'Campers', 'Two-MK-Plus', 'Dedicated'])\n",
      "26/159: page_cats = page_cats.stack().reset_index().rename(columns={'level_1': 'Category', 0: 'user_count'})\n",
      "26/160:\n",
      "labels = ('One Timers', 'Fans', 'Two-MKs-Plus', 'Basers', 'Campers', 'Dedicated')\n",
      "sets = [set(one_timers.index), \n",
      "        set(fans_users.from_id.values), \n",
      "        set(two_mks_plus.from_id.values), \n",
      "        set(base.from_id.values), \n",
      "        set(campers.from_id.values), \n",
      "        set(dedicated.from_id.values)]\n",
      "#df = df.drop('group_cat', axis=1)\n",
      "for l, s in zip(labels, sets):\n",
      "    df.loc[df.from_id.isin(s), 'group_cat'] = l\n",
      "\n",
      "df['group_cat'] = pd.Categorical(df.group_cat)\n",
      "df.group_cat.value_counts()/df.shape[0]\n",
      "26/161: df.group_cat.value_counts()/df.shape[0]\n",
      "26/162:\n",
      "group_lang = df.groupby(['name', 'group_cat', 'langdetect']).size().reset_index().rename(columns = {0: 'comment_count'})\n",
      "group_lang['langdetect'] = group_lang.langdetect.cat.add_categories(['other'])\n",
      "group_lang.loc[group_lang.langdetect=='cy', 'langdetect'] = 'ru'\n",
      "top_lang = group_lang.groupby('langdetect').comment_count.sum().sort_values(ascending=False).head(9).index\n",
      "group_lang.loc[~group_lang.langdetect.isin(top_lang), 'langdetect'] = 'other'\n",
      "group_lang = group_lang.groupby(['name', 'group_cat', 'langdetect']).comment_count.sum().reset_index()\n",
      "\n",
      "group_lang.langdetect.value_counts()\n",
      "26/163: alt.data_transformers.enable('default', max_rows=None)\n",
      "26/164:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#a6cee3','#1f78b4','#ff7f00','#fdbf6f','#e31a1c','#cab2d6','#33a02c'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['name', 'group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(width=150)\n",
      "\n",
      "(normalized).facet(column='group_cat').properties(title='Comment Count by MK, User Group Category and Language Detected')\n",
      "26/165:\n",
      "regular = alt.Chart(group_lang.groupby(['group_cat', 'langdetect']).comment_count.sum().reset_index()).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#a6cee3','#1f78b4','#ff7f00','#fdbf6f','#e31a1c','#cab2d6','#33a02c'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ")\n",
      "\n",
      "(normalized).properties(title='Comment Count by User Group Category and Language Detected')\n",
      "26/166:\n",
      "regular = alt.Chart(group_lang[group_lang.name!='Benjamin_Netanyahu'].groupby(['group_cat', 'langdetect']).comment_count.sum().reset_index()).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#a6cee3','#1f78b4','#ff7f00','#fdbf6f','#e31a1c','#cab2d6','#33a02c'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ")\n",
      "\n",
      "(normalized).properties(title='Comment Count by User Group Category and Language Detected (No Netanyahu)')\n",
      "26/167: post_lang = df[df.is_post==0].groupby(['name', 'group_cat', 'langdetect', 'post_id']).size().reset_index().rename(columns = {0: 'comment_count'})\n",
      "26/168:\n",
      "post_lang = df[df.is_post==0].groupby(['name', 'group_cat', 'langdetect', 'post_id']).size().reset_index().rename(columns = {0: 'comment_count'})\n",
      "post_lang[post_lang.name.str.startswith('Benj')]\n",
      "26/169:\n",
      "post_lang = df[df.is_post==0].groupby(['name', 'group_cat', 'langdetect', 'post_id']).size().reset_index().rename(columns = {0: 'comment_count'})\n",
      "post_lang[post_lang.name =='Benjamin_Netanyahu')]\n",
      "26/170:\n",
      "post_lang = df[df.is_post==0].groupby(['name', 'group_cat', 'langdetect', 'post_id']).size().reset_index().rename(columns = {0: 'comment_count'})\n",
      "post_lang[post_lang.name =='Benjamin_Netanyahu']\n",
      "26/171:\n",
      "post_lang = df[df.is_post==0].groupby(['name', 'group_cat', 'langdetect', 'post_id']).size().reset_index().rename(columns = {0: 'comment_count'})\n",
      "post_lang['langdetect'] = post_lang.langdetect.cat.add_categories(['other'])\n",
      "post_lang.loc[post_lang.langdetect=='cy', 'langdetect'] = 'ru'\n",
      "top_post_lang = post_lang.groupby('langdetect').comment_count.sum().sort_values(ascending=False).head(9).index\n",
      "post_lang.loc[~post_lang.langdetect.isin(top_post_lang), 'langdetect'] = 'other'\n",
      "post_lang = post_lang.groupby(['name', 'group_cat', 'langdetect']).comment_count.sum().reset_index()\n",
      "post_lang[post_lang.name =='Benjamin_Netanyahu']\n",
      "26/172:\n",
      "post_lang = df[df.is_post==0].groupby(['name', 'group_cat', 'langdetect', 'post_id']).size().reset_index().rename(columns = {0: 'comment_count'})\n",
      "post_lang['langdetect'] = post_lang.langdetect.cat.add_categories(['other'])\n",
      "post_lang.loc[post_lang.langdetect=='cy', 'langdetect'] = 'ru'\n",
      "top_post_lang = post_lang.groupby('langdetect').comment_count.sum().sort_values(ascending=False).head(9).index\n",
      "post_lang.loc[~post_lang.langdetect.isin(top_post_lang), 'langdetect'] = 'other'\n",
      "post_lang = post_lang.groupby(['name', 'group_cat', 'langdetect', 'post_id']).comment_count.sum().reset_index()\n",
      "post_lang[post_lang.name =='Benjamin_Netanyahu']\n",
      "26/173:\n",
      "post_lang = df[df.is_post==0].groupby(['name', 'group_cat', 'langdetect', 'post_id']).size().reset_index().rename(columns = {0: 'comment_count'})\n",
      "post_lang['langdetect'] = post_lang.langdetect.cat.add_categories(['other'])\n",
      "post_lang.loc[post_lang.langdetect=='cy', 'langdetect'] = 'ru'\n",
      "top_post_lang = post_lang.groupby('langdetect').comment_count.sum().sort_values(ascending=False).head(9).index\n",
      "post_lang.loc[~post_lang.langdetect.isin(top_post_lang), 'langdetect'] = 'other'\n",
      "post_lang = post_lang.groupby(['name', 'group_cat', 'langdetect', 'post_id']).comment_count.sum().reset_index()\n",
      "post_lang[post_lang.name =='Benjamin_Netanyahu'].groupby(['group_cat', 'langdetect']).comment_count.median()\n",
      "26/174: sns.boxplot(post_lang[post_lang.name =='Benjamin_Netanyahu'], x='group_cat', y='comment_count')\n",
      "26/175: sns.boxplot(data=post_lang[post_lang.name =='Benjamin_Netanyahu'], x='group_cat', y='comment_count')\n",
      "26/176:\n",
      "fig, ax = plt.subplots(figsize=(10, 7))\n",
      "sns.boxplot(data=post_lang[post_lang.name =='Benjamin_Netanyahu'], x='group_cat', y='comment_count', ax=ax)\n",
      "26/177:\n",
      "fig, ax = plt.subplots(figsize=(10, 7))\n",
      "sns.boxplot(data=post_lang[(post_lang.name =='Benjamin_Netanyahu') & (post_lang.comment_count<2000)], x='group_cat', y='comment_count', ax=ax)\n",
      "26/178:\n",
      "fig, ax = plt.subplots(figsize=(10, 7))\n",
      "sns.boxplot(data=post_lang[(post_lang.name =='Benjamin_Netanyahu')], x='langdetect', y='comment_count', ax=ax)\n",
      "26/179:\n",
      "post_lang = df[df.is_post==0].groupby(['name', 'group_cat', 'langdetect', 'post_id']).size().reset_index().rename(columns = {0: 'comment_count'})\n",
      "post_lang['langdetect'] = post_lang.langdetect.cat.add_categories(['other'])\n",
      "post_lang.loc[post_lang.langdetect=='cy', 'langdetect'] = 'ru'\n",
      "top_post_lang = post_lang.groupby('langdetect').comment_count.sum().sort_values(ascending=False).head(9).index\n",
      "post_lang.loc[~post_lang.langdetect.isin(top_post_lang), 'langdetect'] = 'other'\n",
      "post_lang['langdetect'] = post_lang.langdetect.cat.remove_unused_categories()\n",
      "post_lang = post_lang.groupby(['name', 'group_cat', 'langdetect', 'post_id']).comment_count.sum().reset_index()\n",
      "post_lang[post_lang.name =='Benjamin_Netanyahu'].groupby(['group_cat', 'langdetect']).comment_count.median()\n",
      "26/180:\n",
      "fig, ax = plt.subplots(figsize=(10, 7))\n",
      "sns.boxplot(data=post_lang[(post_lang.name =='Benjamin_Netanyahu')], x='langdetect', y='comment_count', ax=ax)\n",
      "26/181:\n",
      "fig, ax = plt.subplots(figsize=(10, 7))\n",
      "sns.boxplot(data=post_lang[(post_lang.name =='Benjamin_Netanyahu')], x='langdetect', y='comment_count', hue='langdetect', ax=ax)\n",
      "26/182:\n",
      "fig, ax = plt.subplots(figsize=(10, 7))\n",
      "sns.boxplot(data=post_lang[(post_lang.name =='Benjamin_Netanyahu')], x='langdetect', y='comment_count', hue='group_cat', ax=ax)\n",
      "26/183:\n",
      "fig, ax = plt.subplots(figsize=(10, 7))\n",
      "sns.boxplot(data=post_lang[(post_lang.name =='Benjamin_Netanyahu')], x='langdetect', y='comment_count', row='group_cat', ax=ax)\n",
      "26/184:\n",
      "fig, ax = plt.subplots(figsize=(10, 7))\n",
      "sns.catplot(data=post_lang[(post_lang.name =='Benjamin_Netanyahu')], x='langdetect', y='comment_count', row='group_cat', ax=ax)\n",
      "26/185:\n",
      "fig, ax = plt.subplots(figsize=(10, 7))\n",
      "sns.boxplot(data=post_lang[(post_lang.name =='Benjamin_Netanyahu')], x='langdetect', y='comment_count', row='group_cat', ax=ax)\n",
      "26/186:\n",
      "fig, ax = plt.subplots(figsize=(10, 7))\n",
      "sns.boxplot(data=post_lang[(post_lang.name =='Benjamin_Netanyahu')], x='langdetect', y='comment_count', hue='group_cat', ax=ax)\n",
      "26/187:\n",
      "fig, ax = plt.subplots(figsize=(10, 7))\n",
      "sns.boxplot(data=post_lang[(post_lang.name =='Benjamin_Netanyahu')], hue='langdetect', y='comment_count', x='group_cat', ax=ax)\n",
      "26/188:\n",
      "fig, ax = plt.subplots(figsize=(10, 7))\n",
      "sns.boxplot(data=post_lang[(post_lang.name =='Benjamin_Netanyahu')], x='langdetect', y='comment_count', hue='group_cat', ax=ax)\n",
      "26/189:\n",
      "fig, ax = plt.subplots(figsize=(20, 15))\n",
      "sns.boxplot(data=post_lang[(post_lang.name =='Benjamin_Netanyahu')], x='langdetect', y='comment_count', hue='group_cat', ax=ax)\n",
      "26/190: post_lang[(post_lang.name =='Benjamin_Netanyahu') & (post_lang.comment_count>2000)]\n",
      "26/191: df[(df.is_post==1) & (df.post_id.isin(high.post_id))\n",
      "26/192: df[(df.is_post==1) & (df.post_id.isin(high.post_id))]\n",
      "26/193:\n",
      "high = post_lang[(post_lang.name =='Benjamin_Netanyahu') & (post_lang.comment_count>2000)].post_id\n",
      "high\n",
      "26/194: df[(df.is_post==1) & (df.post_id.isin(high.post_id))]\n",
      "26/195:\n",
      "high = post_lang[(post_lang.name =='Benjamin_Netanyahu') & (post_lang.comment_count>2000)]\n",
      "high\n",
      "26/196: df[(df.is_post==1) & (df.post_id.isin(high.post_id))]\n",
      "26/197:\n",
      "high = post_lang[(post_lang.name =='Benjamin_Netanyahu') & (post_lang.comment_count>1000)]\n",
      "high\n",
      "26/198: df[(df.is_post==1) & (df.post_id.isin(high.post_id))]\n",
      "26/199:\n",
      "high = post_lang[(post_lang.name =='Benjamin_Netanyahu') & (post_lang.comment_count>1000) & (post_lang.langdetect=='en')]\n",
      "high\n",
      "26/200: df[(df.is_post==1) & (df.post_id.isin(high.post_id))]\n",
      "26/201: df[(df.is_post==1) & (df.post_id.isin(high.post_id))].text\n",
      "26/202: df[(df.is_post==1) & (df.post_id.isin(high.post_id))].text.to_list()\n",
      "26/203: df[(df.is_post==1) & (df.post_id.isin(high.post_id))].text.tolist()\n",
      "26/204:\n",
      "high = post_lang[(post_lang.name =='Benjamin_Netanyahu') & (post_lang.comment_count>200) & (post_lang.langdetect=='en')]\n",
      "high\n",
      "26/205: df[(df.is_post==1) & (df.post_id.isin(high.post_id))].text.tolist()\n",
      "26/206: df[(df.is_post==1) & (df.post_id.isin(high.post_id))].langdetect\n",
      "26/207: df[(df.is_post==1) & (df.post_id.isin(high.post_id))][['comment_count'], 'langdetect']\n",
      "26/208: df[(df.is_post==1) & (df.post_id.isin(high.post_id))][['comment_count', 'langdetect']]\n",
      "26/209:\n",
      "high = post_lang[(post_lang.name =='Benjamin_Netanyahu') & (post_lang.comment_count>1000) & (post_lang.langdetect=='en')]\n",
      "high\n",
      "26/210: df[(df.is_post==1) & (df.post_id.isin(high.post_id))][['comment_count', 'langdetect']]\n",
      "26/211:\n",
      "regular = alt.Chart(post_lang[(post_lang.name =='Benjamin_Netanyahu')]).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('post_id:N', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#a6cee3','#1f78b4','#ff7f00','#fdbf6f','#e31a1c','#cab2d6','#33a02c'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['post_id', 'group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(width=150)\n",
      "\n",
      "(normalized).facet(column='group_cat').properties(title='Comment Count by MK, User Group Category and Language Detected')\n",
      "26/212: normalized\n",
      "26/213:\n",
      "regular = alt.Chart(post_lang[(post_lang.name =='Benjamin_Netanyahu')]).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('post_id:N', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#a6cee3','#1f78b4','#ff7f00','#fdbf6f','#e31a1c','#cab2d6','#33a02c'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['post_id', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(width=150)\n",
      "\n",
      "(normalized).facet(column='group_cat').properties(title='Comment Count by MK, User Group Category and Language Detected')\n",
      "26/214: normalized\n",
      "26/215:\n",
      "regular = alt.Chart(post_lang[(post_lang.name =='Benjamin_Netanyahu')]).mark_bar().encode(\n",
      "    alt.X('sum(comment_count):Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('post_id:N', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#a6cee3','#1f78b4','#ff7f00','#fdbf6f','#e31a1c','#cab2d6','#33a02c'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['post_id', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(width=150)\n",
      "\n",
      "(normalized).facet(column='group_cat').properties(title='Comment Count by Post ID, User Group Category and Language Detected')\n",
      "26/216: normalized\n",
      "26/217:\n",
      "regular = alt.Chart(post_lang[(post_lang.name =='Benjamin_Netanyahu')]).mark_bar().encode(\n",
      "    alt.X('sum(comment_count):Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('post_id:N', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#a6cee3','#1f78b4','#ff7f00','#fdbf6f','#e31a1c','#cab2d6','#33a02c'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['post_id', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('sum(comment_count):Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(width=150)\n",
      "\n",
      "(normalized).facet(column='group_cat').properties(title='Comment Count by Post ID, User Group Category and Language Detected')\n",
      "26/218: normalized\n",
      "26/219: regular\n",
      "26/220: normalized | regular\n",
      "26/221: df[df.post_id=='268108602075_10153041947152076']\n",
      "26/222: df[df.post_id=='268108602075_10153717170732076']\n",
      "27/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "27/2:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "27/3:\n",
      "%%time\n",
      "df = pd.read_pickle('turf/data_df.pkl.gz', compression='gzip')\n",
      "27/4: df.dtypes\n",
      "27/5:\n",
      "sdf = pd.read_pickle('turf/shared_df.pkl.gz', compression='gzip')\n",
      "sdf.dtypes\n",
      "27/6:\n",
      "def percentile(n):\n",
      "    def percentile_(x):\n",
      "        return np.percentile(x, n)\n",
      "    percentile_.__name__ = 'percentile_%s' % n\n",
      "    return percentile_\n",
      "27/7: times = pd.from_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "27/8: st = pd.read_pickle('turf/total_stats_no_multiindex.pkl.gz', compression='gzip')\n",
      "27/9: times.period_length.hist(bins=30)\n",
      "27/10:\n",
      "times = pd.read_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "st = pd.read_pickle('turf/total_stats_no_multiindex.pkl.gz', compression='gzip')\n",
      "27/11: times.period_length.hist(bins=30)\n",
      "27/12: times.head()\n",
      "27/13:\n",
      "day_in_seconds = 60*60*24\n",
      "day_in_seconds\n",
      "27/14: day_in_seconds = 60*60*24\n",
      "27/15: (times.period_length)/.hist(bins=30)\n",
      "27/16: (times.period_length/day_in_seconds).hist(bins=30)\n",
      "27/17: (times.period_length/day_in_seconds).hist(bins=50)\n",
      "27/18: (times.period_length/day_in_seconds).hist(bins=100)\n",
      "27/19: st.dtypes\n",
      "26/223: st.dtypes\n",
      "26/224: st.dtypes)_\n",
      "26/225: st.dtypes()\n",
      "26/226: st.dtypes\n",
      "26/227:\n",
      "for l, s in zip(labels, sets):\n",
      "    st.loc[df.from_id.isin(s), 'group_cat'] = l\n",
      "st['group_cat'] = pd.Categorical(st.group_cat)\n",
      "st.groupby('group_cat').id.sum()/df.shape[0]\n",
      "26/228: st.to_pickle('turf/total_stats_no_multiindex_groups.pkl.gz', compression='gzip')\n",
      "27/20:\n",
      "times = pd.read_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "st = pd.read_pickle('turf/total_stats_no_multiindex_groups.pkl.gz', compression='gzip')\n",
      "27/21:\n",
      "times = pd.read_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "st = pd.read_pickle('turf/total_stats_no_multiindex_groups.pkl.gz', compression='gzip')\n",
      "27/22: st.dtypes\n",
      "27/23: times.head()\n",
      "27/24: times['group_cat'] = times.merge(st[['from_id', 'group_cat']], how='left', on='from_id'))\n",
      "27/25: times['group_cat'] = times.merge(st[['from_id', 'group_cat']], how='left', on='from_id')\n",
      "27/26: from itertools import combinations\n",
      "27/27:\n",
      "columns = ['from_id', 'name', 'elapsed_time',  'created_time', 'created_month', 'created_day', 'created_day_of_week', 'created_hour']\n",
      "stats = ['nunique', 'min', 'max']\n",
      "['_'.join(x, y) for x,y in combinations(columns, stats)\n",
      "#times.columns =\n",
      "27/28:\n",
      "columns = ['from_id', 'name', 'elapsed_time',  'created_time', 'created_month', 'created_day', 'created_day_of_week', 'created_hour']\n",
      "stats = ['nunique', 'min', 'max']\n",
      "['_'.join(x, y) for x,y in combinations(columns, stats)]\n",
      "#times.columns =\n",
      "27/29: from itertools import product\n",
      "27/30:\n",
      "columns = ['from_id', 'name', 'elapsed_time',  'created_time', 'created_month', 'created_day', 'created_day_of_week', 'created_hour']\n",
      "stats = ['nunique', 'min', 'max']\n",
      "['_'.join(x, y) for x,y in product(columns, stats)]\n",
      "#times.columns =\n",
      "27/31:\n",
      "columns = ['from_id', 'name', 'elapsed_time',  'created_time', 'created_month', 'created_day', 'created_day_of_week', 'created_hour']\n",
      "stats = ['nunique', 'min', 'max']\n",
      "['_'.join(x) for x in product(columns, stats)]\n",
      "#times.columns =\n",
      "27/32:\n",
      "columns = ['from_id', 'name', 'elapsed_time',  'created_time', 'created_month', 'created_day', 'created_day_of_week', 'created_hour']\n",
      "stats = ['nunique', 'min', 'max']\n",
      "new_cols = ['_'.join(x) for x in product(columns, stats)]\n",
      "times.columns = new_cols\n",
      "27/33: times.dtypes\n",
      "27/34:\n",
      "columns = ['from_id', 'name', 'elapsed_time',  'created_time', 'created_month', 'created_day', 'created_day_of_week', 'created_hour']\n",
      "stats = ['nunique', 'min', 'max']\n",
      "new_cols = 'from_id' + ['_'.join(x) for x in product(columns, stats)] + 'period_length'\n",
      "new_cols\n",
      "27/35:\n",
      "columns = ['from_id', 'name', 'elapsed_time',  'created_time', 'created_month', 'created_day', 'created_day_of_week', 'created_hour']\n",
      "stats = ['nunique', 'min', 'max']\n",
      "new_cols = ['from_id'] + ['_'.join(x) for x in product(columns, stats)] + ['period_length']\n",
      "new_cols\n",
      "27/36:\n",
      "columns = ['from_id', 'name', 'elapsed_time',  'created_time', 'created_month', 'created_day']\n",
      "stats = ['nunique', 'min', 'max']\n",
      "new_cols = ['from_id'] + ['_'.join(x) for x in product(columns, stats)] + ['period_length']\n",
      "new_cols\n",
      "27/37: times.columns = new_cols\n",
      "27/38:\n",
      "columns = ['from_id', 'name', 'elapsed_time',  'created_time', 'created_month', 'created_day']\n",
      "stats = ['nunique', 'min', 'max']\n",
      "new_cols = ['from_id'] + ['_'.join(x) for x in product(columns, stats)] + ['period_length']\n",
      "new_cols\n",
      "27/39:\n",
      "columns = ['from_id', 'elapsed_time',  'created_time', 'created_month', 'created_day']\n",
      "stats = ['nunique', 'min', 'max']\n",
      "new_cols = ['from_id'] + ['_'.join(x) for x in product(columns, stats)] + ['period_length']\n",
      "new_cols\n",
      "27/40: times.columns = new_cols\n",
      "27/41:\n",
      "columns = ['elapsed_time',  'created_time', 'created_month', 'created_day']\n",
      "stats = ['nunique', 'min', 'max']\n",
      "new_cols = ['from_id'] + ['_'.join(x) for x in product(columns, stats)] + ['period_length']\n",
      "new_cols\n",
      "27/42: times.columns = new_cols\n",
      "27/43: times['group_cat'] = times.merge(st[['from_id', 'group_cat']], how='left', on='from_id')\n",
      "27/44: st[['from_id', 'group_cat']].head()\n",
      "26/229:\n",
      "for l, s in zip(labels, sets):\n",
      "    st.loc[st.from_id.isin(s), 'group_cat'] = l\n",
      "st['group_cat'] = pd.Categorical(st.group_cat)\n",
      "st.groupby('group_cat').id.sum()/df.shape[0]\n",
      "26/230: st.to_pickle('turf/total_stats_no_multiindex_groups.pkl.gz', compression='gzip')\n",
      "27/45:\n",
      "times = pd.read_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "st = pd.read_pickle('turf/total_stats_no_multiindex_groups.pkl.gz', compression='gzip')\n",
      "27/46:\n",
      "times = pd.read_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "st = pd.read_pickle('turf/total_stats_no_multiindex_groups.pkl.gz', compression='gzip')\n",
      "27/47: st.dtypes\n",
      "27/48: from itertools import product\n",
      "27/49: times.dtypes\n",
      "27/50:\n",
      "columns = ['elapsed_time',  'created_time', 'created_month', 'created_day']\n",
      "stats = ['nunique', 'min', 'max']\n",
      "new_cols = ['from_id'] + ['_'.join(x) for x in product(columns, stats)] + ['period_length']\n",
      "new_cols\n",
      "27/51: times.columns = new_cols\n",
      "27/52: st[['from_id', 'group_cat']].head()\n",
      "27/53: times['group_cat'] = times.merge(st[['from_id', 'group_cat']], how='left', on='from_id')\n",
      "27/54: times['group_cat'] = times.merge(st[['from_id', 'group_cat']].drop_duplicates(), how='left', on='from_id')\n",
      "27/55: times = times.merge(st[['from_id', 'group_cat']].drop_duplicates(), how='left', on='from_id')\n",
      "27/56: day_in_seconds = 60*60*24\n",
      "27/57: (times.period_length/day_in_seconds).hist(bins=100, )\n",
      "27/58: (times.period_length/day_in_seconds).hist(bins=100, by='group_cat')\n",
      "27/59: times.head()\n",
      "27/60: times.groupby('group_cat').period_length.median()/day_in_seconds\n",
      "27/61: times.hist(bins=100, column='period_length', by='group_cat')\n",
      "27/62: times['period_length'] = times.period_length/day_in_seconds\n",
      "27/63: times.hist(bins=100, column='period_length', by='group_cat')\n",
      "27/64: times.hist(bins=100, column='period_length', by='group_cat', layout=(6,1))\n",
      "27/65: times.hist(bins=100, column='period_length', by='group_cat', layout=(6,1), sharex=True, sharey=True)\n",
      "27/66: times.hist(bins=100, column='period_length', by='group_cat', layout=(6,1), sharex=True)\n",
      "27/67: times.hist(bins=100, column='period_length', by='group_cat', layout=(6,1), sharex=True, figsize=(15,20))\n",
      "27/68: times.hist(bins=100, column='period_length', by='group_cat', layout=(6,1), sharex=True, figsize=(15,20), grid=True)\n",
      "27/69: times.hist(bins=100, column='period_length', by='group_cat', layout=(6,1), sharex=True, figsize=(10,15), grid=True)\n",
      "27/70: times.hist(bins=30, column='period_length', by='group_cat', layout=(6,1), sharex=True, figsize=(10,15), grid=True)\n",
      "27/71: times[times.period_length<850).hist(bins=30, column='period_length', by='group_cat', layout=(6,1), sharex=True, figsize=(10,15), grid=True)\n",
      "27/72: times[times.period_length<850].hist(bins=30, column='period_length', by='group_cat', layout=(6,1), sharex=True, figsize=(10,15), grid=True)\n",
      "27/73: times[times.period_length<850].hist(bins=30, column='period_length', by='group_cat', layout=(6,1), figsize=(10,15), grid=True)\n",
      "27/74: times[times.period_length<850].hist(bins=30, column='period_length', by='group_cat', layout=(6,1), sharex=True, figsize=(10,15), grid=True)\n",
      "26/231:\n",
      "ded_campers = ded_campers[(~ded_campers.from_id.isin(ded_base.from_id))]\n",
      "ded_campers.from_id.nunique()\n",
      "26/232:\n",
      "ded_two_mks_plus = ded_two_mks_plus[(~ded_two_mks_plus.from_id.isin(ded_base.from_id)) \n",
      "                            & (~ded_two_mks_plus.from_id.isin(ded_campers.from_id))]\n",
      "ded_two_mks_plus.from_id.nunique()\n",
      "26/233: ded_campers.from_id.nunique() + ded_base.from_id.nunique() + ded_two_mks_plus.from_id.nunique()\n",
      "26/234: ded_campers.from_id.nunique() + ded_base.from_id.nunique() + ded_two_mks_plus.from_id.nunique() + ded_fans_users.nunique()\n",
      "26/235: ded_campers.from_id.nunique() + ded_base.from_id.nunique() + ded_two_mks_plus.from_id.nunique() + ded_fans_users.from_id.nunique()\n",
      "26/236:\n",
      "ded_two_mks_plus = ded_two_mks_plus[(~ded_two_mks_plus.from_id.isin(ded_base.from_id)) \n",
      "                                    & (~ded_two_mks_plus.from_id.isin(ded_campers.from_id))\n",
      "                                    & (~ded_two_mks_plus.from_id.isin(ded_celebies.from_id))]\n",
      "ded_two_mks_plus.from_id.nunique()\n",
      "26/237:\n",
      "ded_fans_users = (dedicated.loc[st.groupby('from_id').name.transform('nunique')==1, ['from_id', 'name', 'party']])\n",
      "ded_fans_users.from_id.nunique(), dedicated.from_id.nunique()\n",
      "26/238:\n",
      "ded_two_mks_plus = (dedicated.loc[st.groupby('from_id').name.transform('nunique')>1, ['from_id', 'name', 'party', 'camp']])\n",
      "ded_base = ded_two_mks_plus[ded_two_mks_plus.groupby('from_id').party.transform('nunique')==1]\n",
      "ded_campers = ded_two_mks_plus[ded_two_mks_plus.groupby('from_id').camp.transform('nunique')==1]\n",
      "ded_celebies = ded_two_mks_plus[ded_two_mks_plus.from_id.isin(mat[mat.loc[:, list(all_scores_div.sort_values(by='total_score', ascending=False).index)[12:]].isna().all(axis=1)].index)]\n",
      "dedicated.from_id.nunique(), ded_two_mks_plus.from_id.nunique(), ded_base.from_id.nunique(), ded_campers.from_id.nunique(), ded_celebies.from_id.nunique()\n",
      "26/239:\n",
      "ded_campers = ded_campers[(~ded_campers.from_id.isin(ded_base.from_id))]\n",
      "ded_campers.from_id.nunique()\n",
      "26/240:\n",
      "ded_two_mks_plus = ded_two_mks_plus[(~ded_two_mks_plus.from_id.isin(ded_base.from_id)) \n",
      "                                    & (~ded_two_mks_plus.from_id.isin(ded_campers.from_id))\n",
      "                                    & (~ded_two_mks_plus.from_id.isin(ded_celebies.from_id))]\n",
      "ded_two_mks_plus.from_id.nunique()\n",
      "26/241: ded_campers.from_id.nunique() + ded_base.from_id.nunique() + ded_two_mks_plus.from_id.nunique() + ded_fans_users.from_id.nunique()\n",
      "26/242:\n",
      "ded_celebies = ded_celebies[(~ded_celebies.from_id.isin(ded_two_mks_plus.from_id)) \n",
      "                                    & (~ded_celebies.from_id.isin(ded_campers.from_id))\n",
      "                                    & (~ded_celebies.from_id.isin(ded_base.from_id))]\n",
      "ded_celebies.from_id.nunique()\n",
      "26/243:\n",
      "ded_celebies = ded_celebies[(~ded_celebies.from_id.isin(ded_two_mks_plus.from_id)) \n",
      "                                    & (~ded_celebies.from_id.isin(ded_campers.from_id))\n",
      "                                    & (~ded_celebies.from_id.isin(ded_base.from_id))]\n",
      "ded_celebies.from_id.nunique()\n",
      "26/244: ded_campers.from_id.nunique() + ded_base.from_id.nunique() + ded_two_mks_plus.from_id.nunique() + ded_fans_users.from_id.nunique()\n",
      "26/245: ded_campers.from_id.nunique() + ded_base.from_id.nunique() + ded_two_mks_plus.from_id.nunique() + ded_fans_users.from_id.nunique() + ded_celebies.from_id.nunique()\n",
      "26/246:\n",
      "labels = ('Dedicated Fans', 'Dedicated Basers', 'Dedicated 2MK', 'Campers', 'Dedicated')\n",
      "sets = [set(ded_fans_users.from_id.values), \n",
      "        set(ded_base.from_id.values), \n",
      "        set(ded_two_mks_plus.from_id.values), \n",
      "        set(ded_campers.from_id.values)\n",
      "        set(ded_celebies.from_id.values)]\n",
      "\n",
      "for l, s in zip(labels, sets):\n",
      "    st.loc[st.from_id.isin(s), 'group_cat'] = l\n",
      "st['group_cat'] = pd.Categorical(st.group_cat)\n",
      "st.groupby('group_cat').id.sum()/df.shape[0]\n",
      "26/247:\n",
      "labels = ('Dedicated Fans', 'Dedicated Basers', 'Dedicated 2MK', 'Campers', 'Dedicated')\n",
      "sets = [set(ded_fans_users.from_id.values), \n",
      "        set(ded_base.from_id.values), \n",
      "        set(ded_two_mks_plus.from_id.values), \n",
      "        set(ded_campers.from_id.values),\n",
      "        set(ded_celebies.from_id.values),]\n",
      "\n",
      "for l, s in zip(labels, sets):\n",
      "    st.loc[st.from_id.isin(s), 'group_cat'] = l\n",
      "    \n",
      "st['group_cat'] = pd.Categorical(st.group_cat)\n",
      "st.groupby('group_cat').id.sum()/df.shape[0]\n",
      "26/248:\n",
      "labels = ('Dedicated Fans', 'Dedicated Basers', 'Dedicated 2MK', 'Campers', 'Dedicated Celebies')\n",
      "sets = [set(ded_fans_users.from_id.values), \n",
      "        set(ded_base.from_id.values), \n",
      "        set(ded_two_mks_plus.from_id.values), \n",
      "        set(ded_campers.from_id.values),\n",
      "        set(ded_celebies.from_id.values),]\n",
      "\n",
      "for l, s in zip(labels, sets):\n",
      "    st.loc[st.from_id.isin(s), 'group_cat'] = l\n",
      "    \n",
      "st['group_cat'] = pd.Categorical(st.group_cat)\n",
      "st.groupby('group_cat').id.sum()/df.shape[0]\n",
      "26/249:\n",
      "labels = ('Dedicated Fans', 'Dedicated Basers', 'Dedicated 2MK', 'Campers', 'Dedicated Celebies')\n",
      "sets = [set(ded_fans_users.from_id.values), \n",
      "        set(ded_base.from_id.values), \n",
      "        set(ded_two_mks_plus.from_id.values), \n",
      "        set(ded_campers.from_id.values),\n",
      "        set(ded_celebies.from_id.values),]\n",
      "\n",
      "for l, s in zip(labels, sets):\n",
      "    st.loc[st.from_id.isin(s), 'new_group_cat'] = l\n",
      "    \n",
      "st['group_cat'] = pd.Categorical(st.group_cat)\n",
      "st.groupby('group_cat').id.sum()/df.shape[0]\n",
      "26/250:\n",
      "labels = ('Dedicated Fans', 'Dedicated Basers', 'Dedicated 2MK', 'Campers', 'Dedicated Celebies')\n",
      "sets = [set(ded_fans_users.from_id.values), \n",
      "        set(ded_base.from_id.values), \n",
      "        set(ded_two_mks_plus.from_id.values), \n",
      "        set(ded_campers.from_id.values),\n",
      "        set(ded_celebies.from_id.values),]\n",
      "\n",
      "for l, s in zip(labels, sets):\n",
      "    st.loc[st.from_id.isin(s), 'new_group_cat'] = l\n",
      "    \n",
      "st['new_group_cat'] = pd.Categorical(st.group_cat)\n",
      "st.groupby('group_cat').id.sum()/df.shape[0]\n",
      "26/251:\n",
      "labels = ('Dedicated Fans', 'Dedicated Basers', 'Dedicated 2MK', 'Campers', 'Dedicated Celebies')\n",
      "sets = [set(ded_fans_users.from_id.values), \n",
      "        set(ded_base.from_id.values), \n",
      "        set(ded_two_mks_plus.from_id.values), \n",
      "        set(ded_campers.from_id.values),\n",
      "        set(ded_celebies.from_id.values),]\n",
      "\n",
      "for l, s in zip(labels, sets):\n",
      "    st.loc[st.from_id.isin(s), 'new_group_cat'] = l\n",
      "    \n",
      "st['new_group_cat'] = pd.Categorical(st.group_cat)\n",
      "st.groupby('new_group_cat').id.sum()/df.shape[0]\n",
      "26/252:\n",
      "labels = ('Dedicated Fans', 'Dedicated Basers', 'Dedicated 2MK', 'Campers', 'Dedicated Celebies')\n",
      "sets = [set(ded_fans_users.from_id.values), \n",
      "        set(ded_base.from_id.values), \n",
      "        set(ded_two_mks_plus.from_id.values), \n",
      "        set(ded_campers.from_id.values),\n",
      "        set(ded_celebies.from_id.values),]\n",
      "\n",
      "for l, s in zip(labels, sets):\n",
      "    st.loc[st.from_id.isin(s), 'new_group_cat'] = l\n",
      "    \n",
      "st['new_group_cat'] = pd.Categorical(st.new_group_cat)\n",
      "st.groupby('new_group_cat').id.sum()/df.shape[0]\n",
      "26/253: st = st.drop('new_group_cat', axis=1)\n",
      "26/254:\n",
      "labels = ('Dedicated Fans', 'Dedicated Basers', 'Dedicated 2MK', 'Campers', 'Dedicated Celebies')\n",
      "sets = [set(ded_fans_users.from_id.values), \n",
      "        set(ded_base.from_id.values), \n",
      "        set(ded_two_mks_plus.from_id.values), \n",
      "        set(ded_campers.from_id.values),\n",
      "        set(ded_celebies.from_id.values),]\n",
      "\n",
      "for l, s in zip(labels, sets):\n",
      "    st.loc[st.from_id.isin(s), 'new_group_cat'] = l\n",
      "    \n",
      "st['new_group_cat'] = pd.Categorical(st.new_group_cat)\n",
      "st.groupby('new_group_cat').id.sum()/df.shape[0]\n",
      "26/255:\n",
      "labels = ('Dedicated Fans', 'Dedicated Basers', 'Dedicated 2MK', 'Dedicated Campers', 'Dedicated Celebies')\n",
      "sets = [set(ded_fans_users.from_id.values), \n",
      "        set(ded_base.from_id.values), \n",
      "        set(ded_two_mks_plus.from_id.values), \n",
      "        set(ded_campers.from_id.values),\n",
      "        set(ded_celebies.from_id.values),]\n",
      "\n",
      "for l, s in zip(labels, sets):\n",
      "    st.loc[st.from_id.isin(s), 'new_group_cat'] = l\n",
      "    \n",
      "st['new_group_cat'] = pd.Categorical(st.new_group_cat)\n",
      "st.groupby('new_group_cat').id.sum()/df.shape[0]\n",
      "26/256: st = st.drop('new_group_cat', axis=1)\n",
      "26/257:\n",
      "labels = ('Dedicated Fans', 'Dedicated Basers', 'Dedicated 2MK', 'Dedicated Campers', 'Dedicated Celebies')\n",
      "sets = [set(ded_fans_users.from_id.values), \n",
      "        set(ded_base.from_id.values), \n",
      "        set(ded_two_mks_plus.from_id.values), \n",
      "        set(ded_campers.from_id.values),\n",
      "        set(ded_celebies.from_id.values),]\n",
      "\n",
      "for l, s in zip(labels, sets):\n",
      "    st.loc[st.from_id.isin(s), 'new_group_cat'] = l\n",
      "    \n",
      "st['new_group_cat'] = pd.Categorical(st.new_group_cat)\n",
      "st.groupby('new_group_cat').id.sum()/df.shape[0]\n",
      "26/258:\n",
      "st.group_cat = st.new_group_cat.combine_first(st.group_cat)\n",
      "st.groupby('new_group_cat').id.sum()/df.shape[0]\n",
      "26/259:\n",
      "st.group_cat = st.new_group_cat.combine_first(st.group_cat)\n",
      "st.groupby('group_cat').id.sum()/df.shape[0]\n",
      "26/260: st.to_pickle('turf/total_stats_no_multiindex_groups_ded_cuts.pkl.gz', compression='gzip')\n",
      "27/75:\n",
      "times = pd.read_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "st = pd.read_pickle('turf/total_stats_no_multiindex_groups_ded_cuts.pkl.gz', compression='gzip')\n",
      "27/76:\n",
      "times = pd.read_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "st = pd.read_pickle('turf/total_stats_no_multiindex_groups_ded_cuts.pkl.gz', compression='gzip')\n",
      "27/77: st.dtypes\n",
      "27/78: from itertools import product\n",
      "27/79: times.dtypes\n",
      "27/80:\n",
      "columns = ['elapsed_time',  'created_time', 'created_month', 'created_day']\n",
      "stats = ['nunique', 'min', 'max']\n",
      "new_cols = ['from_id'] + ['_'.join(x) for x in product(columns, stats)] + ['period_length']\n",
      "new_cols\n",
      "27/81: times.columns = new_cols\n",
      "27/82: st[['from_id', 'group_cat']].head()\n",
      "27/83: times = times.merge(st[['from_id', 'group_cat']].drop_duplicates(), how='left', on='from_id')\n",
      "27/84: day_in_seconds = 60*60*24\n",
      "27/85: times.groupby('group_cat').period_length.median()/day_in_seconds\n",
      "27/86: times['period_length'] = times.period_length/day_in_seconds\n",
      "27/87: times[times.period_length<850].hist(bins=30, column='period_length', by='group_cat', layout=(6,1), sharex=True, figsize=(10,15), grid=True)\n",
      "27/88: times[times.period_length<850].hist(bins=30, column='period_length', by='group_cat', layout=(11,1), sharex=True, figsize=(10,15), grid=True)\n",
      "27/89: times[times.period_length<850].hist(bins=30, column='period_length', by='group_cat', layout=(11,1), sharex=True, sharey=True, figsize=(10,15), grid=True)\n",
      "27/90: times[times.period_length<850].hist(bins=30, column='period_length', by='group_cat', layout=(11,1), sharex=True, figsize=(10,15), grid=True)\n",
      "26/261: dedicated.from_id.nunique()\n",
      "26/262: ded_count = dedicated.from_id.nunique()\n",
      "26/263:\n",
      "ded_fans_users = (dedicated.loc[st.groupby('from_id').name.transform('nunique')==1, ['from_id', 'name', 'party']])\n",
      "ded_fans_users.from_id.nunique()/ded_count, dedicated.from_id.nunique()/ded_count\n",
      "26/264:\n",
      "ded_two_mks_plus = (dedicated.loc[st.groupby('from_id').name.transform('nunique')>1, ['from_id', 'name', 'party', 'camp']])\n",
      "ded_base = ded_two_mks_plus[ded_two_mks_plus.groupby('from_id').party.transform('nunique')==1]\n",
      "ded_campers = ded_two_mks_plus[ded_two_mks_plus.groupby('from_id').camp.transform('nunique')==1]\n",
      "ded_celebies = ded_two_mks_plus[ded_two_mks_plus.from_id.isin(mat[mat.loc[:, list(all_scores_div.sort_values(by='total_score', ascending=False).index)[12:]].isna().all(axis=1)].index)]\n",
      "dedicated.from_id.nunique(), ded_two_mks_plus.from_id.nunique()/ded_count, ded_base.from_id.nunique()/ded_count, ded_campers.from_id.nunique()/ded_count, ded_celebies.from_id.nunique()/ded_count\n",
      "26/265:\n",
      "ded_campers = ded_campers[(~ded_campers.from_id.isin(ded_base.from_id))]\n",
      "ded_campers.from_id.nunique()\n",
      "26/266:\n",
      "ded_two_mks_plus = ded_two_mks_plus[(~ded_two_mks_plus.from_id.isin(ded_base.from_id)) \n",
      "                                    & (~ded_two_mks_plus.from_id.isin(ded_campers.from_id))\n",
      "                                    & (~ded_two_mks_plus.from_id.isin(ded_celebies.from_id))]\n",
      "ded_two_mks_plus.from_id.nunique()\n",
      "26/267:\n",
      "ded_celebies = ded_celebies[(~ded_celebies.from_id.isin(ded_two_mks_plus.from_id)) \n",
      "                                    & (~ded_celebies.from_id.isin(ded_campers.from_id))\n",
      "                                    & (~ded_celebies.from_id.isin(ded_base.from_id))]\n",
      "ded_celebies.from_id.nunique()\n",
      "26/268: ded_campers.from_id.nunique() + ded_base.from_id.nunique() + ded_two_mks_plus.from_id.nunique() + ded_fans_users.from_id.nunique() + ded_celebies.from_id.nunique()\n",
      "26/269: dedicated.from_id.nunique(), ded_two_mks_plus.from_id.nunique()/ded_count, ded_base.from_id.nunique()/ded_count, ded_campers.from_id.nunique()/ded_count, ded_celebies.from_id.nunique()/ded_count\n",
      "26/270: (dedicated.from_id.nunique(), ded_two_mks_plus.from_id.nunique()/ded_count, ded_base.from_id.nunique()/ded_count, ded_campers.from_id.nunique()/ded_count, ded_celebies.from_id.nunique()/ded_count, ded_fans_users.from_id.nunique()/ded_count)\n",
      "26/271: (ded_two_mks_plus.from_id.nunique()/ded_count, ded_base.from_id.nunique()/ded_count, ded_campers.from_id.nunique()/ded_count, ded_celebies.from_id.nunique()/ded_count, ded_fans_users.from_id.nunique()/ded_count)\n",
      "26/272: sum(ded_two_mks_plus.from_id.nunique()/ded_count, ded_base.from_id.nunique()/ded_count, ded_campers.from_id.nunique()/ded_count, ded_celebies.from_id.nunique()/ded_count, ded_fans_users.from_id.nunique()/ded_count)\n",
      "26/273: sum((ded_two_mks_plus.from_id.nunique()/ded_count, ded_base.from_id.nunique()/ded_count, ded_campers.from_id.nunique()/ded_count, ded_celebies.from_id.nunique()/ded_count, ded_fans_users.from_id.nunique()/ded_count))\n",
      "26/274: (ded_two_mks_plus.from_id.nunique()/ded_count, ded_base.from_id.nunique()/ded_count, ded_campers.from_id.nunique()/ded_count, ded_celebies.from_id.nunique()/ded_count, ded_fans_users.from_id.nunique()/ded_count)\n",
      "26/275:\n",
      "labels = ('Dedicated Fans', 'Dedicated Basers', 'Dedicated Campers', 'Dedicated Celebies', 'Dedicated 2MK')\n",
      "(ded_fans_users.from_id.nunique()/ded_count, ded_base.from_id.nunique()/ded_count, ded_campers.from_id.nunique()/ded_count, ded_celebies.from_id.nunique()/ded_count, ded_two_mks_plus.from_id.nunique()/ded_count)\n",
      "26/276:\n",
      "labels = ('Dedicated Fans', 'Dedicated Basers', 'Dedicated Campers', 'Dedicated Celebies', 'Dedicated 2MK')\n",
      "ratios = (ded_fans_users.from_id.nunique()/ded_count, ded_base.from_id.nunique()/ded_count, ded_campers.from_id.nunique()/ded_count, ded_celebies.from_id.nunique()/ded_count, ded_two_mks_plus.from_id.nunique()/ded_count)\n",
      "zip(labels,ratios)\n",
      "26/277:\n",
      "labels = ('Dedicated Fans', 'Dedicated Basers', 'Dedicated Campers', 'Dedicated Celebies', 'Dedicated 2MK')\n",
      "ratios = (ded_fans_users.from_id.nunique()/ded_count, ded_base.from_id.nunique()/ded_count, ded_campers.from_id.nunique()/ded_count, ded_celebies.from_id.nunique()/ded_count, ded_two_mks_plus.from_id.nunique()/ded_count)\n",
      "list(zip(labels,ratios))\n",
      "26/278:\n",
      "labels = ('Dedicated Fans', 'Dedicated Basers', 'Dedicated Campers', 'Dedicated Celebies', 'Dedicated 2MK')\n",
      "ratios = (ded_fans_users.from_id.nunique()/ded_count, ded_base.from_id.nunique()/ded_count, ded_campers.from_id.nunique()/ded_count, ded_celebies.from_id.nunique()/ded_count, ded_two_mks_plus.from_id.nunique()/ded_count)\n",
      "dict(zip(labels,ratios))\n",
      "26/279:\n",
      "labels = ('Dedicated Fans', 'Dedicated Basers', 'Dedicated Campers', 'Dedicated Celebies', 'Dedicated 2MK')\n",
      "ratios = (ded_fans_users.from_id.nunique()/ded_count, ded_base.from_id.nunique()/ded_count, ded_campers.from_id.nunique()/ded_count, ded_celebies.from_id.nunique()/ded_count, ded_two_mks_plus.from_id.nunique()/ded_count)\n",
      "dict(zip(labels,[round(x, 2) for x in ratios]))\n",
      "27/91: tn.head()\n",
      "27/92:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "27/93:\n",
      "%%time\n",
      "df = pd.read_pickle('turf/data_df.pkl.gz', compression='gzip')\n",
      "27/94: df.dtypes\n",
      "27/95:\n",
      "sdf = pd.read_pickle('turf/shared_df.pkl.gz', compression='gzip')\n",
      "sdf.dtypes\n",
      "27/96:\n",
      "def percentile(n):\n",
      "    def percentile_(x):\n",
      "        return np.percentile(x, n)\n",
      "    percentile_.__name__ = 'percentile_%s' % n\n",
      "    return percentile_\n",
      "27/97:\n",
      "times = pd.read_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "st = pd.read_pickle('turf/total_stats_no_multiindex_groups_ded_cuts.pkl.gz', compression='gzip')\n",
      "27/98: st.dtypes\n",
      "27/99: from itertools import product\n",
      "27/100: times.dtypes\n",
      "27/101:\n",
      "columns = ['elapsed_time',  'created_time', 'created_month', 'created_day']\n",
      "stats = ['nunique', 'min', 'max']\n",
      "new_cols = ['from_id'] + ['_'.join(x) for x in product(columns, stats)] + ['period_length']\n",
      "new_cols\n",
      "27/102: times.columns = new_cols\n",
      "27/103: st[['from_id', 'group_cat']].head()\n",
      "27/104: times = times.merge(st[['from_id', 'group_cat']].drop_duplicates(), how='left', on='from_id')\n",
      "27/105: day_in_seconds = 60*60*24\n",
      "27/106: times.groupby('group_cat').period_length.median()/day_in_seconds\n",
      "27/107: times['period_length'] = times.period_length/day_in_seconds\n",
      "27/108: times[times.period_length<850].hist(bins=30, column='period_length', by='group_cat', layout=(11,1), sharex=True, figsize=(10,15), grid=True)\n",
      "27/109:\n",
      "stats = ['nunique', 'min', 'max']\n",
      "\n",
      "tn = (df.loc[df.is_post==0, ['from_id', 'name', 'elapsed_time',  'created_time', 'created_month', 'created_day', 'created_day_of_week', 'created_hour',]]\n",
      "      #.dropna(subset=['post_id'])\n",
      "      .groupby(by=['from_id', 'name'])\n",
      "      .agg(stats)\n",
      "      .assign(period_length = lambda x: (x['created_time']['max'] - x['created_time']['min']).dt.total_seconds())\n",
      "      .reset_index()\n",
      "     )\n",
      "\n",
      "columns = ['elapsed_time',  'created_time', 'created_month', 'created_day']\n",
      "stats = ['nunique', 'min', 'max']\n",
      "new_cols = ['from_id', 'name'] + ['_'.join(x) for x in product(columns, stats)] + ['period_length']\n",
      "tn.columns = new_cols\n",
      "tn.head()\n",
      "27/110:\n",
      "tn = tn.merge(st[['from_id', 'group_cat']].drop_duplicates(), how='left', on='from_id')\n",
      "tn.head()\n",
      "27/111: tn.head()\n",
      "27/112: tn.to_pickle('turf/times_names.pkl.gz', compression='gzip')\n",
      "27/113: tn.to_pickle('turf/times_names.pkl.gz', compression='gzip')\n",
      "27/114: tn.groupby(['name', 'group_cat']).period_length.agg.(['mean', 'median'])\n",
      "27/115: tn.groupby(['name', 'group_cat']).period_length.agg(['mean', 'median'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/1: import wikipediaapi as wa\n",
      "28/2: wiki = wa.Wikipedia('he')\n",
      "28/3: zuk_he = wiki.page('מבצע_צוק_איתן')\n",
      "28/4:\n",
      "def print_langlinks(page):\n",
      "    langlinks = page.langlinks\n",
      "    for k in sorted(langlinks.keys()):\n",
      "        v = langlinks[k]\n",
      "        print(\"%s: %s - %s: %s\" % (k, v.language, v.title, v.fullurl))\n",
      "\n",
      "print_langlinks(zuk_he)\n",
      "28/5:\n",
      "def print_sections(sections, level=0):\n",
      "        for s in sections:\n",
      "                print(\"%s: %s - %s\" % (\"*\" * (level + 1), s.title, s.text[0:40]))\n",
      "                print_sections(s.sections, level + 1)\n",
      "#print_sections(zuk_he.sections)\n",
      "28/6: zuk_he.section_by_title('קישורים חיצוניים').sections[0]\n",
      "28/7: zuks = {key: wa.Wikipedia(key) for key in }\n",
      "28/8:\n",
      "from collections import defaultdict\n",
      "\n",
      "flatten = lambda l: [item for sublist in l for item in sublist]\n",
      "\n",
      "clean_links = defaultdict(list)\n",
      "for lang in ext_links:\n",
      "    for resp in ext_links[lang]:\n",
      "        for page in resp['query']['pages']:\n",
      "            if 'extlinks' in resp['query']['pages'][page]:\n",
      "                links = resp['query']['pages'][page]['extlinks']\n",
      "                num = len(links)\n",
      "                clean_links[lang].extend(flatten([list(l.values()) for l in links]))\n",
      "            else: \n",
      "                clean_links[lang].extend([])\n",
      "                num=0\n",
      "                \n",
      "            title = resp['query']['pages'][page]['title']\n",
      "\n",
      "            print(f'{lang}: extlinks={num}, title: {title}')\n",
      "28/9: lc = Counter(flatten(l for l in clean_links.values()))\n",
      "28/10: lc.most_common()\n",
      "28/11:\n",
      "for_cdf = []\n",
      "for lang in ll:\n",
      "    page = ll[lang]\n",
      "    for_cdf.append({'lang': lang, 'full_url': page.fullurl, 'title': page.title, 'summary': page.summary})\n",
      "    \n",
      "cdf = pd.DataFrame(for_cdf)\n",
      "cdf.head()\n",
      "28/12: zuk_he.links\n",
      "28/13:\n",
      "ll = zuk_he.langlinks\n",
      "ll['he'] = ll['ar'].langlinks['he']\n",
      "28/14: zuk_he.categories\n",
      "28/15:\n",
      "categories = {}\n",
      "for k in sorted(ll.keys()):\n",
      "    categories[k] = ll[k].categories\n",
      "28/16:\n",
      "from collections import Counter\n",
      "print({k: len(v) for k,v in  categories.items()})\n",
      "28/17: import requests\n",
      "28/18:\n",
      "ext_links = {}\n",
      "\n",
      "req = {'action': 'query',\n",
      "       'prop': 'extlinks',\n",
      "       'format': 'json',\n",
      "       'ellimit': 'max',}\n",
      "\n",
      "for k in sorted(ll.keys()):\n",
      "    v = ll[k]\n",
      "    params = req.copy()\n",
      "    params['titles'] = v.title\n",
      "    resp = requests.get(f'http://{k}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        eloffset = resp['continue']['eloffset']\n",
      "        resp = requests.get(f'http://{k}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    ext_links[k] = final_resp\n",
      "28/19: len(final_resp)\n",
      "28/20: ext_links['en']\n",
      "28/21:\n",
      "from collections import defaultdict\n",
      "\n",
      "flatten = lambda l: [item for sublist in l for item in sublist]\n",
      "\n",
      "clean_links = defaultdict(list)\n",
      "for lang in ext_links:\n",
      "    for resp in ext_links[lang]:\n",
      "        for page in resp['query']['pages']:\n",
      "            if 'extlinks' in resp['query']['pages'][page]:\n",
      "                links = resp['query']['pages'][page]['extlinks']\n",
      "                num = len(links)\n",
      "                clean_links[lang].extend(flatten([list(l.values()) for l in links]))\n",
      "            else: \n",
      "                clean_links[lang].extend([])\n",
      "                num=0\n",
      "                \n",
      "            title = resp['query']['pages'][page]['title']\n",
      "\n",
      "            print(f'{lang}: extlinks={num}, title: {title}')\n",
      "28/22: clean_links['en']\n",
      "28/23: lc = Counter(flatten(l for l in clean_links.values()))\n",
      "28/24: lc.most_common()\n",
      "28/25:\n",
      "for_cdf = []\n",
      "for lang in ll:\n",
      "    page = ll[lang]\n",
      "    for_cdf.append({'lang': lang, 'full_url': page.fullurl, 'title': page.title, 'summary': page.summary})\n",
      "    \n",
      "cdf = pd.DataFrame(for_cdf)\n",
      "cdf.head()\n",
      "28/26:\n",
      "import wikipediaapi as wa\n",
      "import pandas as pd\n",
      "import networkx as nx\n",
      "28/27:\n",
      "for_cdf = []\n",
      "for lang in ll:\n",
      "    page = ll[lang]\n",
      "    for_cdf.append({'lang': lang, 'full_url': page.fullurl, 'title': page.title, 'summary': page.summary})\n",
      "    \n",
      "cdf = pd.DataFrame(for_cdf)\n",
      "cdf.head()\n",
      "28/28: cdf.to_csv('data/zuk_wiki_pages.csv', index=False)\n",
      "28/29:\n",
      "cl_for_df = []\n",
      "\n",
      "for lang in clean_links:\n",
      "    for link in clean_links[lang]:\n",
      "        cl_for_df.append({'lang': lang, 'link': link})\n",
      "\n",
      "ldf = pd.DataFrame(cl_for_df)\n",
      "ldf.head()\n",
      "28/30: ldf.to_csv('data/lang_extlinks.csv', index=False)\n",
      "28/31: ldf.lang.value_counts()\n",
      "28/32:\n",
      "g = nx.from_pandas_edgelist(ldf, 'lang', 'link', create_using=nx.DiGraph())\n",
      "\n",
      "nx.write_gexf(g, 'data/extlinks.gexf')\n",
      "28/33:\n",
      "from wikidata.client import Client\n",
      "client = Client()  # doctest: +SKIP\n",
      "entity = client.get('Q17324420', load=True)\n",
      "entity\n",
      "28/34: entity.description\n",
      "28/35: entity.data\n",
      "28/36: entity.data['labels']\n",
      "28/37:\n",
      "labels = pd.DataFrame([{'lang': v['language'], 'label': v['value']} for k, v in entity.data['labels'].items()])\n",
      "labels.head()\n",
      "28/38: labels.to_csv('data/wikidata_labels.csv', index=False)\n",
      "28/39: entity.data['aliases']\n",
      "28/40: flatten([[{'lang': v['language'], 'alias': v['value']} for v in l] for k, l in entity.data['aliases'].items()])\n",
      "28/41:\n",
      "aliases = pd.DataFrame(flatten([[{'lang': v['language'], 'alias': v['value']} for v in l] for k, l in entity.data['aliases'].items()]))\n",
      "aliases.head()\n",
      "28/42: aliases.to_csv('data/wikidata_aliases.csv', index=False)\n",
      "28/43:\n",
      "revisions = {}\n",
      "\n",
      "req = {'action': 'query',\n",
      "       'prop': 'revisions',\n",
      "       'format': 'json',\n",
      "       'rvlimit': 'max',\n",
      "       'rvprop': 'timestamp|user|comment|size|tags',}\n",
      "\n",
      "for k in sorted(ll.keys()):\n",
      "    v = ll[k]\n",
      "    params = req.copy()\n",
      "    params['titles'] = v.title\n",
      "    resp = requests.get(f'http://{k}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        eloffset = resp['continue']['rvoffset']\n",
      "        resp = requests.get(f'http://{k}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    revisions[k] = final_resp\n",
      "28/44:\n",
      "revisions = {}\n",
      "\n",
      "req = {'action': 'query',\n",
      "       'prop': 'revisions',\n",
      "       'format': 'json',\n",
      "       'rvlimit': 'max',\n",
      "       'rvprop': 'timestamp|user|comment|size|tags',}\n",
      "\n",
      "for k in sorted(ll.keys()):\n",
      "    v = ll[k]\n",
      "    params = req.copy()\n",
      "    params['titles'] = v.title\n",
      "    resp = requests.get(f'http://{k}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        eloffset = resp['continue']['rvcontinue']\n",
      "        resp = requests.get(f'http://{k}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    revisions[k] = final_resp\n",
      "28/45: revisions.keys\n",
      "28/46: revisions.keys()\n",
      "28/47: revisions['he']\n",
      "28/48: revisions['he'][0]\n",
      "28/49: revisions['he'][-1]\n",
      "28/50: len(revisions['he'])\n",
      "28/51: flatten(x for x in revisions)\n",
      "28/52: revf = {k: flatten(v) for k, v in revisions.items()}\n",
      "28/53:\n",
      "revf = {k: flatten(v) for k, v in revisions.items()}\n",
      "rev_for_df = []\n",
      "for lang, revs in revf.items():\n",
      "    rl = []\n",
      "    for r in revs:\n",
      "        r['lang'] = lang\n",
      "        rl.append(r)\n",
      "    revs_for_df.extend(rl)\n",
      "    \n",
      "rdf = pd.DataFrame(revs_for_df)\n",
      "rdf.head()\n",
      "28/54:\n",
      "revf = {k: flatten(v) for k, v in revisions.items()}\n",
      "rev_for_df = []\n",
      "for lang, revs in revf.items():\n",
      "    rl = []\n",
      "    print revs\n",
      "    for r in revs:\n",
      "        r['lang'] = lang\n",
      "        rl.append(r)\n",
      "    revs_for_df.extend(rl)\n",
      "    \n",
      "rdf = pd.DataFrame(revs_for_df)\n",
      "rdf.head()\n",
      "28/55:\n",
      "revf = {k: flatten(v) for k, v in revisions.items()}\n",
      "rev_for_df = []\n",
      "for lang, revs in revf.items():\n",
      "    rl = []\n",
      "    print (revs)\n",
      "    for r in revs:\n",
      "        r['lang'] = lang\n",
      "        rl.append(r)\n",
      "    revs_for_df.extend(rl)\n",
      "    \n",
      "rdf = pd.DataFrame(revs_for_df)\n",
      "rdf.head()\n",
      "28/56:\n",
      "revf = {k: flatten(v) for k, v in revisions.items()}\n",
      "revf[1]\n",
      "28/57:\n",
      "revf = {k: flatten(v) for k, v in revisions.items()}\n",
      "revf['he']\n",
      "28/58: revisions[0]\n",
      "28/59: revisions['he']\n",
      "28/60: revisions['he'][0]\n",
      "28/61:\n",
      "clean_revs = defaultdict(list)\n",
      "for lang in revisions:\n",
      "    for resp in revisions[lang]:\n",
      "        for page in resp['query']['pages']:\n",
      "            if 'revisions' in resp['query']['pages'][page]:\n",
      "                revs = resp['query']['pages'][page]['revisions']\n",
      "                num = len(revs)\n",
      "                clean_revs[lang].extend(flatten([list(l.values()) for l in revs]))\n",
      "            else: \n",
      "                clean_revs[lang].extend([])\n",
      "                num=0\n",
      "                \n",
      "            title = resp['query']['pages'][page]['title']\n",
      "\n",
      "            print(f'{lang}: extlinks={num}, title: {title}')\n",
      "28/62:\n",
      "clean_revs = defaultdict(list)\n",
      "for lang in revisions:\n",
      "    for resp in revisions[lang]:\n",
      "        for page in resp['query']['pages']:\n",
      "            if 'revisions' in resp['query']['pages'][page]:\n",
      "                revs = resp['query']['pages'][page]['revisions']\n",
      "                num = len(revs)\n",
      "                clean_revs[lang].extend(flatten([list(l.values()) for l in revs]))\n",
      "            else: \n",
      "                clean_revs[lang].extend([])\n",
      "                num=0\n",
      "                \n",
      "            title = resp['query']['pages'][page]['title']\n",
      "\n",
      "            print(f'{lang}: revs={num}, title: {title}')\n",
      "28/63:\n",
      "cr_for_df = []\n",
      "\n",
      "for lang in clean_links:\n",
      "    for link in clean_links[lang]:\n",
      "        link['lang'] = lang\n",
      "        cl_for_df.append(link)\n",
      "\n",
      "rdf = pd.DataFrame(cr_for_df)\n",
      "rdf.head()\n",
      "28/64:\n",
      "cr_for_df = []\n",
      "\n",
      "for lang in clean_links:\n",
      "    for link in clean_links[lang]:\n",
      "        print(link)\n",
      "        link['lang'] = lang\n",
      "        cl_for_df.append(link)\n",
      "\n",
      "rdf = pd.DataFrame(cr_for_df)\n",
      "rdf.head()\n",
      "28/65:\n",
      "cr_for_df = []\n",
      "\n",
      "for lang in clean_revs:\n",
      "    for rev in clean_revs[lang]:\n",
      "        rev['lang'] = lang\n",
      "        cl_for_df.append(rev)\n",
      "\n",
      "rdf = pd.DataFrame(cr_for_df)\n",
      "rdf.head()\n",
      "28/66:\n",
      "cr_for_df = []\n",
      "\n",
      "for lang in clean_revs:\n",
      "    for rev in clean_revs[lang]:\n",
      "        print(rev)\n",
      "        rev['lang'] = lang\n",
      "        cl_for_df.append(rev)\n",
      "\n",
      "rdf = pd.DataFrame(cr_for_df)\n",
      "rdf.head()\n",
      "28/67:\n",
      "cr_for_df = []\n",
      "\n",
      "for lang in clean_revs:\n",
      "    print(clean_revs[lang])\n",
      "    for rev in clean_revs[lang]:\n",
      "        print(rev)\n",
      "        rev['lang'] = lang\n",
      "        cl_for_df.append(rev)\n",
      "\n",
      "rdf = pd.DataFrame(cr_for_df)\n",
      "rdf.head()\n",
      "28/68:\n",
      "clean_revs = defaultdict(list)\n",
      "for lang in revisions:\n",
      "    for resp in revisions[lang]:\n",
      "        for page in resp['query']['pages']:\n",
      "            if 'revisions' in resp['query']['pages'][page]:\n",
      "                revs = resp['query']['pages'][page]['revisions']\n",
      "                num = len(revs)\n",
      "                clean_revs[lang].extend(revs)\n",
      "            else: \n",
      "                clean_revs[lang].extend([])\n",
      "                num=0\n",
      "                \n",
      "            title = resp['query']['pages'][page]['title']\n",
      "\n",
      "            print(f'{lang}: revs={num}, title: {title}')\n",
      "28/69:\n",
      "cr_for_df = []\n",
      "\n",
      "for lang in clean_revs:\n",
      "    print(clean_revs[lang])\n",
      "    for rev in clean_revs[lang]:\n",
      "        print(rev)\n",
      "        rev['lang'] = lang\n",
      "        cl_for_df.append(rev)\n",
      "\n",
      "rdf = pd.DataFrame(cr_for_df)\n",
      "rdf.head()\n",
      "28/70:\n",
      "cr_for_df = []\n",
      "\n",
      "for lang in clean_revs:\n",
      "    for rev in clean_revs[lang]:\n",
      "        print(rev)\n",
      "        rev['lang'] = lang\n",
      "        cl_for_df.append(rev)\n",
      "\n",
      "rdf = pd.DataFrame(cr_for_df)\n",
      "rdf.head()\n",
      "28/71:\n",
      "cr_for_df = []\n",
      "\n",
      "for lang in clean_revs:\n",
      "    for rev in clean_revs[lang]:\n",
      "        rev['lang'] = lang\n",
      "        cl_for_df.append(rev)\n",
      "\n",
      "rdf = pd.DataFrame(cr_for_df)\n",
      "rdf.head()\n",
      "28/72:\n",
      "cr_for_df = []\n",
      "\n",
      "for lang in clean_revs:\n",
      "    for rev in clean_revs[lang]:\n",
      "        rev['lang'] = lang\n",
      "        cl_for_df.append(rev)\n",
      "\n",
      "rdf = pd.DataFrame(cr_for_df)\n",
      "rdf.head()\n",
      "28/73:\n",
      "cr_for_df = []\n",
      "\n",
      "for lang in clean_revs:\n",
      "    for rev in clean_revs[lang]:\n",
      "        rev['lang'] = lang\n",
      "        print(rev)\n",
      "        cl_for_df.append(rev)\n",
      "\n",
      "rdf = pd.DataFrame(cr_for_df)\n",
      "rdf.head()\n",
      "28/74:\n",
      "cr_for_df = []\n",
      "\n",
      "for lang in clean_revs:\n",
      "    for rev in clean_revs[lang]:\n",
      "        rev['lang'] = lang\n",
      "        rev['tags'] = ','.join(rev['tags'])\n",
      "        print(rev)\n",
      "        cl_for_df.append(rev)\n",
      "\n",
      "rdf = pd.DataFrame(cr_for_df)\n",
      "rdf.head()\n",
      "28/75:\n",
      "cr_for_df = []\n",
      "\n",
      "for lang in clean_revs:\n",
      "    for rev in clean_revs[lang]:\n",
      "        rev['lang'] = lang\n",
      "        rev['tags'] = ','.join(rev['tags'])\n",
      "        cl_for_df.append(rev)\n",
      "\n",
      "rdf = pd.DataFrame(cr_for_df)\n",
      "rdf.head()\n",
      "28/76:\n",
      "cr_for_df = []\n",
      "\n",
      "for lang in clean_revs:\n",
      "    for rev in clean_revs[lang]:\n",
      "        rev['lang'] = lang\n",
      "        rev['tags'] = ','.join(rev['tags'])\n",
      "        cl_for_df.append(rev)\n",
      "\n",
      "rdf = pd.DataFrame(cr_for_df)\n",
      "rdf.shape\n",
      "28/77:\n",
      "cr_for_df = []\n",
      "\n",
      "for lang in clean_revs:\n",
      "    for rev in clean_revs[lang]:\n",
      "        rev['lang'] = lang\n",
      "        rev['tags'] = ','.join(rev['tags'])\n",
      "        cr_for_df.append(rev)\n",
      "\n",
      "rdf = pd.DataFrame(cr_for_df)\n",
      "rdf.shape\n",
      "28/78:\n",
      "cr_for_df = []\n",
      "\n",
      "for lang in clean_revs:\n",
      "    for rev in clean_revs[lang]:\n",
      "        rev['lang'] = lang\n",
      "        rev['tags'] = ','.join(rev['tags'])\n",
      "        cr_for_df.append(rev)\n",
      "\n",
      "rdf = pd.DataFrame(cr_for_df)\n",
      "rdf.head()\n",
      "28/79: rdf.tags.value_counts()\n",
      "28/80:\n",
      "cr_for_df = []\n",
      "\n",
      "for lang in clean_revs:\n",
      "    for rev in clean_revs[lang]:\n",
      "        rev['lang'] = lang\n",
      "        #rev['tags'] = ','.join(rev['tags'])\n",
      "        cr_for_df.append(rev)\n",
      "\n",
      "rdf = pd.DataFrame(cr_for_df)\n",
      "rdf.head()\n",
      "28/81:\n",
      "clean_revs = defaultdict(list)\n",
      "for lang in revisions:\n",
      "    for resp in revisions[lang]:\n",
      "        for page in resp['query']['pages']:\n",
      "            if 'revisions' in resp['query']['pages'][page]:\n",
      "                revs = resp['query']['pages'][page]['revisions']\n",
      "                num = len(revs)\n",
      "                clean_revs[lang].extend(revs)\n",
      "            else: \n",
      "                clean_revs[lang].extend([])\n",
      "                num=0\n",
      "                \n",
      "            title = resp['query']['pages'][page]['title']\n",
      "\n",
      "            print(f'{lang}: revs={num}, title: {title}')\n",
      "28/82:\n",
      "cr_for_df = []\n",
      "\n",
      "for lang in clean_revs:\n",
      "    for rev in clean_revs[lang]:\n",
      "        rev['lang'] = lang\n",
      "        #rev['tags'] = ','.join(rev['tags'])\n",
      "        cr_for_df.append(rev)\n",
      "\n",
      "rdf = pd.DataFrame(cr_for_df)\n",
      "rdf.head()\n",
      "28/83:\n",
      "revisions = {}\n",
      "\n",
      "req = {'action': 'query',\n",
      "       'prop': 'revisions',\n",
      "       'format': 'json',\n",
      "       'rvlimit': 'max',\n",
      "       'rvprop': 'timestamp|user|comment|size|tags',}\n",
      "\n",
      "for k in sorted(ll.keys()):\n",
      "    v = ll[k]\n",
      "    params = req.copy()\n",
      "    params['titles'] = v.title\n",
      "    resp = requests.get(f'http://{k}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        eloffset = resp['continue']['rvcontinue']\n",
      "        resp = requests.get(f'http://{k}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    revisions[k] = final_resp\n",
      "28/84: revisions['he'][0]\n",
      "28/85:\n",
      "clean_revs = defaultdict(list)\n",
      "for lang in revisions:\n",
      "    for resp in revisions[lang]:\n",
      "        for page in resp['query']['pages']:\n",
      "            if 'revisions' in resp['query']['pages'][page]:\n",
      "                revs = resp['query']['pages'][page]['revisions']\n",
      "                num = len(revs)\n",
      "                clean_revs[lang].extend(revs)\n",
      "            else: \n",
      "                clean_revs[lang].extend([])\n",
      "                num=0\n",
      "                \n",
      "            title = resp['query']['pages'][page]['title']\n",
      "\n",
      "            print(f'{lang}: revs={num}, title: {title}')\n",
      "28/86:\n",
      "cr_for_df = []\n",
      "\n",
      "for lang in clean_revs:\n",
      "    for rev in clean_revs[lang]:\n",
      "        tmp = rev.copy()\n",
      "        tmp['lang'] = lang\n",
      "        #rev['tags'] = ','.join(rev['tags'])\n",
      "        cr_for_df.append(tmp)\n",
      "\n",
      "rdf = pd.DataFrame(cr_for_df)\n",
      "rdf.head()\n",
      "28/87: rdf.tags.value_counts()\n",
      "28/88: rdf.tags.apply(len)\n",
      "28/89: rdf[rdf.tags.apply(len)>1]\n",
      "28/90:\n",
      "cr_for_df = []\n",
      "\n",
      "for lang in clean_revs:\n",
      "    for rev in clean_revs[lang]:\n",
      "        tmp = rev.copy()\n",
      "        tmp['lang'] = lang\n",
      "        tmp['tags'] = ','.join(rev['tags'])\n",
      "        cr_for_df.append(tmp)\n",
      "\n",
      "rdf = pd.DataFrame(cr_for_df)\n",
      "rdf.head()\n",
      "28/91: rdf[rdf.tags.apply(len)>1]\n",
      "28/92:\n",
      "cr_for_df = []\n",
      "\n",
      "for lang in clean_revs:\n",
      "    for rev in clean_revs[lang]:\n",
      "        tmp = rev.copy()\n",
      "        tmp['lang'] = lang\n",
      "        tmp['tags'] = ','.join(rev['tags'])\n",
      "        cr_for_df.append(tmp)\n",
      "\n",
      "rdf = pd.DataFrame(cr_for_df)\n",
      "\n",
      "rdf.dtypes\n",
      "28/93:\n",
      "cr_for_df = []\n",
      "\n",
      "for lang in clean_revs:\n",
      "    for rev in clean_revs[lang]:\n",
      "        tmp = rev.copy()\n",
      "        tmp['lang'] = lang\n",
      "        tmp['tags'] = ','.join(rev['tags'])\n",
      "        cr_for_df.append(tmp)\n",
      "\n",
      "rdf = (pd.DataFrame(cr_for_df)\n",
      "       .assign(timestamp = lambda x: pd.to_datetime(x.timestamp))\n",
      "      )\n",
      "\n",
      "rdf.dtypes\n",
      "28/94: rdf[rdf.tags.apply(len)>1]\n",
      "28/95: rdf[rdf.timestamp==rdf.timestamp.transform(min)]\n",
      "28/96: rdf.timestamp.transform(min)\n",
      "28/97:\n",
      "import numpy as np\n",
      "rdf.timestamp.transform(np.min)\n",
      "28/98: rdf.groupby('timestamp').transform(min)\n",
      "28/99: rdf.groupby('lang').timestamp.transform(min)\n",
      "28/100: rdf[rdf.timestamp==rdf.groupby('lang').timestamp.transform(min)]\n",
      "28/101: rdf[rdf.timestamp==rdf.groupby('lang').timestamp.transform(min)][['lang'],' timestamp'].sort_values('timestamp')\n",
      "28/102: rdf[rdf.timestamp==rdf.groupby('lang').timestamp.transform(min)][['lang', 'timestamp'].sort_values('timestamp')\n",
      "28/103: rdf[rdf.timestamp==rdf.groupby('lang').timestamp.transform(min)][['lang', 'timestamp']].sort_values('timestamp')\n",
      "27/116: tn['period_length'] = tn.period_length/day_in_seconds\n",
      "27/117: tn.groupby(['name', 'group_cat']).period_length.agg(['mean', 'median'])\n",
      "27/118: tn.groupby(['name', 'group_cat']).period_length.agg(['mean', 'median']).unstack()\n",
      "27/119: tn.groupby(['name', 'group_cat']).period_length.agg(['mean', 'median']).style.background_gradient(cmap='summer')\n",
      "27/120: tn.groupby(['name', 'group_cat']).period_length.agg(['mean', 'median']).unstack().style.background_gradient(cmap='summer')\n",
      "27/121: tn.groupby(['name', 'group_cat']).period_length.agg(['mean', 'median']).unstack().style.background_gradient(cmap='summer')[['mean']]\n",
      "27/122: tn.groupby(['name', 'group_cat']).period_length.agg(['mean', 'median']).unstack()[['mean']]style.background_gradient(cmap='summer')\n",
      "27/123: tn.groupby(['name', 'group_cat']).period_length.agg(['mean', 'median']).unstack()[['mean']].style.background_gradient(cmap='summer')\n",
      "27/124: tn.groupby(['name', 'group_cat']).period_length.agg(['mean', 'median']).unstack()[['median']].style.background_gradient(cmap='summer')\n",
      "27/125: tn.groupby(['name', 'group_cat']).period_length.agg(['mean', 'median']).unstack()[['mean']].style.background_gradient(cmap='magma')\n",
      "28/104: rdf[rdf.lang='hu']\n",
      "28/105: rdf[rdf.lang=='hu']\n",
      "27/126: tn.groupby(['name', 'group_cat']).period_length.agg(['mean', 'median']).unstack()[['mean']].style.background_gradient(cmap='magma').render()\n",
      "27/127: html_heatmap = tn.groupby(['name', 'group_cat']).period_length.agg(['mean', 'median']).unstack()[['mean']].style.background_gradient(cmap='magma').render()\n",
      "27/128: tn.groupby(['name', 'group_cat']).period_length.agg(['mean', 'median']).unstack()[['mean']].style.background_gradient(cmap='magma')\n",
      "27/129: open('turf/mean_period_length.html', 'r').write(html_heatmap)\n",
      "27/130: open('turf/mean_period_length.html', 'w').write(html_heatmap)\n",
      "27/131:\n",
      "import os\n",
      "os.mkdir('outputs')\n",
      "27/132: open('outputs/mean_period_length.html', 'w').write(html_heatmap)\n",
      "27/133: mean_heatmap.to_csv('outputs/mean_period_length.csv')\n",
      "27/134:\n",
      "mean_heatmap = tn.groupby(['name', 'group_cat']).period_length.agg(['mean', 'median']).unstack()[['mean']]\n",
      "html_heatmap = mean_heatmap.style.background_gradient(cmap='magma').render()\n",
      "27/135: mean_heatmap.to_csv('outputs/mean_period_length.csv')\n",
      "27/136: tn.groupby(['name', 'group_cat']).period_length.agg(['mean', 'median']).unstack()[['mean']].style.background_gradient(cmap='magma').highlight_null('red'))\n",
      "27/137: tn.groupby(['name', 'group_cat']).period_length.agg(['mean', 'median']).unstack()[['mean']].style.background_gradient(cmap='magma').highlight_null('red')\n",
      "27/138: tn.groupby(['name', 'group_cat']).period_length.agg(['mean', 'median']).unstack()[['mean']].style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "27/139:\n",
      "mean_heatmap = tn.groupby(['name', 'group_cat']).period_length.agg(['mean', 'median']).unstack()[['mean']]\n",
      "mean_heatmap.columns = mean_heatmap.columns.droplevel(0)\n",
      "mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "27/140: mean_mk = tn.groupby(['name']).period_length.mean()\n",
      "27/141:\n",
      "mean_mk = tn.groupby(['name']).period_length.mean()\n",
      "mean_mk\n",
      "27/142: mean_mk = tn.groupby(['name']).period_length.mean()\n",
      "27/143:\n",
      "mean_heatmap = (tn.groupby(['name', 'group_cat'])\n",
      "                .period_length.agg(['mean', 'median'])\n",
      "                .unstack()[['mean']])\n",
      "mean_heatmap.columns = mean_heatmap.columns.droplevel(0)\n",
      "mean_heatmap['mean'] = mean_mk\n",
      "mean_heatmap = mean_heatmap.sort_values('mean', ascending=False)\n",
      "mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "27/144: html_heatmap = mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "27/145: open('outputs/mean_period_length.html', 'w').write(html_heatmap)\n",
      "27/146: html_heatmap = mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red').render()\n",
      "27/147: open('outputs/mean_period_length.html', 'w').write(html_heatmap)\n",
      "27/148:\n",
      "mean_heatmap = (tn.groupby(['name', 'group_cat'])\n",
      "                .period_length.agg(['mean', 'median'])\n",
      "                .unstack()[['mean']])\n",
      "mean_heatmap.columns = mean_heatmap.columns.droplevel(0)\n",
      "mean_heatmap['mean'] = mean_mk\n",
      "mean_heatmap = mean_heatmap.sort_values('mean', ascending=False)\n",
      "mean_heatmap.loc['mean', :] = tn.groupby(['group_cat']).period_length.mean()\n",
      "mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "27/149: tn.groupby(['group_cat']).period_length.mean()\n",
      "27/150:\n",
      "mean_mk = tn.groupby(['name']).period_length.mean()\n",
      "mean_group_cat = tn.groupby(['group_cat']).period_length.mean().unstack()\n",
      "27/151:\n",
      "mean_mk = tn.groupby(['name']).period_length.mean()\n",
      "mean_group_cat = tn.groupby(['group_cat']).period_length.mean()\n",
      "27/152: tn.groupby(['group_cat']).period_length.mean().unstack()\n",
      "27/153: tn.groupby(['group_cat']).period_length.mean().index\n",
      "27/154: tn.groupby(['group_cat']).period_length.mean().unstack()\n",
      "27/155: tn.groupby(['group_cat']).period_length.mean()\n",
      "27/156: tn.groupby(['group_cat']).period_length.mean().T\n",
      "27/157: tn.groupby(['group_cat']).period_length.mean().to_frame()\n",
      "27/158: tn.groupby(['group_cat']).period_length.mean().to_frame().unstack()\n",
      "27/159: tn.groupby(['group_cat']).period_length.mean().to_frame().T\n",
      "27/160:\n",
      "mean_heatmap = (tn.groupby(['name', 'group_cat'])\n",
      "                .period_length.agg(['mean', 'median'])\n",
      "                .unstack()[['mean']])\n",
      "mean_heatmap.columns = mean_heatmap.columns.droplevel(0)\n",
      "mean_heatmap['mean'] = mean_mk\n",
      "mean_heatmap = mean_heatmap.sort_values('mean', ascending=False)\n",
      "mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "27/161:\n",
      "mean_heatmap = (tn.groupby(['name', 'group_cat'])\n",
      "                .period_length.agg(['mean', 'median'])\n",
      "                .unstack()[['mean']])\n",
      "mean_heatmap.columns = mean_heatmap.columns.droplevel(0)\n",
      "mean_heatmap['mean'] = mean_mk\n",
      "mean_heatmap = mean_heatmap.sort_values('mean', ascending=False)\n",
      "mean_heatmap = mean_heatmap.append(mean_group_cat)\n",
      "mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "27/162:\n",
      "mean_mk = tn.groupby(['name']).period_length.mean()\n",
      "mean_group_cat = tn.groupby(['group_cat']).period_length.mean().to_frame().T\n",
      "mean_group_cat.index=['mean']\n",
      "27/163:\n",
      "mean_mk = tn.groupby(['name']).period_length.mean()\n",
      "mean_group_cat = tn.groupby(['group_cat']).period_length.mean().to_frame().T\n",
      "mean_group_cat.index=['mean']\n",
      "mean_group_cat\n",
      "27/164:\n",
      "mean_heatmap = (tn.groupby(['name', 'group_cat'])\n",
      "                .period_length.agg(['mean', 'median'])\n",
      "                .unstack()[['mean']])\n",
      "mean_heatmap.columns = mean_heatmap.columns.droplevel(0)\n",
      "mean_heatmap['mean'] = mean_mk\n",
      "mean_heatmap = mean_heatmap.sort_values('mean', ascending=False)\n",
      "mean_heatmap.index = mean_heatmap.index.add_categories('mean')\n",
      "mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "27/165:\n",
      "mean_heatmap = (tn.groupby(['name', 'group_cat'])\n",
      "                .period_length.agg(['mean', 'median'])\n",
      "                .unstack()[['mean']])\n",
      "mean_heatmap.columns = mean_heatmap.columns.droplevel(0)\n",
      "mean_heatmap['mean'] = mean_mk\n",
      "mean_heatmap = mean_heatmap.sort_values('mean', ascending=False)\n",
      "mean_heatmap.index = mean_heatmap.index.add_categories('mean')\n",
      "mean_heatmap = mean_heatmap.append(mean_group_cat)\n",
      "mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "27/166:\n",
      "mean_mk = tn.groupby(['name']).period_length.mean()\n",
      "mean_group_cat = tn.groupby(['group_cat']).period_length.mean().to_frame().T\n",
      "mean_group_cat.index=['mean']\n",
      "mean_group_cat['mean'] = tn.period_length.mean()\n",
      "27/167:\n",
      "mean_heatmap = (tn.groupby(['name', 'group_cat'])\n",
      "                .period_length.agg(['mean', 'median'])\n",
      "                .unstack()[['mean']])\n",
      "mean_heatmap.columns = mean_heatmap.columns.droplevel(0)\n",
      "mean_heatmap['mean'] = mean_mk\n",
      "mean_heatmap = mean_heatmap.sort_values('mean', ascending=False)\n",
      "mean_heatmap.index = mean_heatmap.index.add_categories('mean')\n",
      "mean_heatmap = mean_heatmap.append(mean_group_cat)\n",
      "mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "27/168: html_heatmap = mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red').render()\n",
      "27/169: open('outputs/mean_period_length.html', 'w').write(html_heatmap)\n",
      "27/170: mean_heatmap.to_csv('outputs/mean_period_length.csv')\n",
      "27/171: mean_heatmap.columns\n",
      "27/172: mean_heatmap = mean_heatmap.rename({'Dedicated Polemicists','Polemicists'})\n",
      "27/173: mean_heatmap = mean_heatmap.rename(columns = {'Dedicated 2MK': 'Dedicated Polemicists', 'Two-MKs-Plus': 'Polemicists'})\n",
      "27/174:\n",
      "mean_heatmap = mean_heatmap.rename(columns = {'Dedicated 2MK': 'Dedicated Polemicists', 'Two-MKs-Plus': 'Polemicists'})\n",
      "mean_heatmap.columns\n",
      "27/175:\n",
      "mean_heatmap = (tn.groupby(['name', 'group_cat'])\n",
      "                .period_length.agg(['mean', 'median'])\n",
      "                .unstack()[['mean']])\n",
      "mean_heatmap.columns = mean_heatmap.columns.droplevel(0)\n",
      "mean_heatmap['mean'] = mean_mk\n",
      "mean_heatmap = mean_heatmap.sort_values('mean', ascending=False)\n",
      "mean_heatmap.index = mean_heatmap.index.add_categories('mean')\n",
      "mean_heatmap = mean_heatmap.append(mean_group_cat)\n",
      "mean_heatmap = mean_heatmap.rename(columns = {'Dedicated 2MK': 'Dedicated Polemicists', 'Two-MKs-Plus': 'Polemicists'})\n",
      "mean_heatmap = mean_heatmap(['One Timers', 'Fans', 'Basers', 'Campers', 'Polemicists', 'Dedicated Fans', 'Dedicated Basers',\n",
      "       'Dedicated Campers', 'Dedicated Polemicists', 'Dedicated Celebies', 'mean'])\n",
      "mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "27/176:\n",
      "mean_heatmap = (tn.groupby(['name', 'group_cat'])\n",
      "                .period_length.agg(['mean', 'median'])\n",
      "                .unstack()[['mean']])\n",
      "mean_heatmap.columns = mean_heatmap.columns.droplevel(0)\n",
      "mean_heatmap['mean'] = mean_mk\n",
      "mean_heatmap = mean_heatmap.sort_values('mean', ascending=False)\n",
      "mean_heatmap.index = mean_heatmap.index.add_categories('mean')\n",
      "mean_heatmap = mean_heatmap.append(mean_group_cat)\n",
      "mean_heatmap = mean_heatmap.rename(columns = {'Dedicated 2MK': 'Dedicated Polemicists', 'Two-MKs-Plus': 'Polemicists'})\n",
      "mean_heatmap = mean_heatmap[['One Timers', 'Fans', 'Basers', 'Campers', 'Polemicists', 'Dedicated Fans', 'Dedicated Basers',\n",
      "       'Dedicated Campers', 'Dedicated Polemicists', 'Dedicated Celebies', 'mean']]\n",
      "mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "27/177: sns.heatmap(mean_heatmap)\n",
      "27/178: sns.heatmap(mean_heatmap, figsize=(20,15))\n",
      "27/179:\n",
      "fig,ax = plt.subplots(figsize=(20,15))\n",
      "sns.heatmap(mean_heatmap, ax=ax)\n",
      "27/180:\n",
      "mean_heatmap = (tn.groupby(['name', 'group_cat'])\n",
      "                .period_length.agg(['mean', 'median'])\n",
      "                .unstack()[['mean']])\n",
      "mean_heatmap.columns = mean_heatmap.columns.droplevel(0)\n",
      "mean_heatmap['mean'] = mean_mk\n",
      "mean_heatmap = mean_heatmap.sort_values('mean', ascending=False)\n",
      "mean_heatmap.index = mean_heatmap.index.add_categories('mean')\n",
      "mean_heatmap = mean_heatmap.append(mean_group_cat)\n",
      "mean_heatmap = mean_heatmap.rename(columns = {'Dedicated 2MK': 'Dedicated Polemicists', 'Two-MKs-Plus': 'Polemicists'})\n",
      "mean_heatmap = mean_heatmap[['One Timers', 'Fans', 'Basers', 'Campers', 'Polemicists', 'Dedicated Fans', 'Dedicated Basers',\n",
      "       'Dedicated Campers', 'Dedicated Polemicists', 'Dedicated Celebies', 'mean']]\n",
      "mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "27/181: html_heatmap = mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red').render()\n",
      "27/182: open('outputs/mean_period_length.html', 'w').write(html_heatmap)\n",
      "29/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "29/2: seaborn.__version__\n",
      "29/3: sns.__version__\n",
      "30/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "30/2: sns.__version__\n",
      "30/3:\n",
      "%%time\n",
      "df = pd.read_pickle('turf/data_df.pkl.gz', compression='gzip')\n",
      "30/4: df.dtypes\n",
      "30/5:\n",
      "sdf = pd.read_pickle('turf/shared_df.pkl.gz', compression='gzip')\n",
      "sdf.dtypes\n",
      "30/6:\n",
      "def percentile(n):\n",
      "    def percentile_(x):\n",
      "        return np.percentile(x, n)\n",
      "    percentile_.__name__ = 'percentile_%s' % n\n",
      "    return percentile_\n",
      "30/7:\n",
      "times = pd.read_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "st = pd.read_pickle('turf/total_stats_no_multiindex_groups_ded_cuts.pkl.gz', compression='gzip')\n",
      "30/8: st.dtypes\n",
      "30/9: from itertools import product\n",
      "30/10: times.dtypes\n",
      "30/11:\n",
      "columns = ['elapsed_time',  'created_time', 'created_month', 'created_day']\n",
      "stats = ['nunique', 'min', 'max']\n",
      "new_cols = ['from_id'] + ['_'.join(x) for x in product(columns, stats)] + ['period_length']\n",
      "new_cols\n",
      "30/12: times.columns = new_cols\n",
      "30/13: st[['from_id', 'group_cat']].head()\n",
      "30/14: times = times.merge(st[['from_id', 'group_cat']].drop_duplicates(), how='left', on='from_id')\n",
      "30/15: day_in_seconds = 60*60*24\n",
      "30/16: times.groupby('group_cat').period_length.median()/day_in_seconds\n",
      "30/17: times['period_length'] = times.period_length/day_in_seconds\n",
      "30/18: times[times.period_length<850].hist(bins=30, column='period_length', by='group_cat', layout=(11,1), sharex=True, figsize=(10,15), grid=True)\n",
      "30/19: tn = pd.read_pickle('turf/times_names.pkl.gz', compression='gzip')\n",
      "30/20: tn['period_length'] = tn.period_length/day_in_seconds\n",
      "30/21:\n",
      "mean_mk = tn.groupby(['name']).period_length.mean()\n",
      "mean_group_cat = tn.groupby(['group_cat']).period_length.mean().to_frame().T\n",
      "mean_group_cat.index=['mean']\n",
      "mean_group_cat['mean'] = tn.period_length.mean()\n",
      "30/22:\n",
      "mean_heatmap = (tn.groupby(['name', 'group_cat'])\n",
      "                .period_length.agg(['mean', 'median'])\n",
      "                .unstack()[['mean']])\n",
      "mean_heatmap.columns = mean_heatmap.columns.droplevel(0)\n",
      "mean_heatmap['mean'] = mean_mk\n",
      "mean_heatmap = mean_heatmap.sort_values('mean', ascending=False)\n",
      "mean_heatmap.index = mean_heatmap.index.add_categories('mean')\n",
      "mean_heatmap = mean_heatmap.append(mean_group_cat)\n",
      "mean_heatmap = mean_heatmap.rename(columns = {'Dedicated 2MK': 'Dedicated Polemicists', 'Two-MKs-Plus': 'Polemicists'})\n",
      "mean_heatmap = mean_heatmap[['One Timers', 'Fans', 'Basers', 'Campers', 'Polemicists', 'Dedicated Fans', 'Dedicated Basers',\n",
      "       'Dedicated Campers', 'Dedicated Polemicists', 'Dedicated Celebies', 'mean']]\n",
      "mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "30/23: html_heatmap = mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red').render()\n",
      "30/24: open('outputs/mean_period_length.html', 'w').write(html_heatmap)\n",
      "30/25: mean_heatmap.to_csv('outputs/mean_period_length.csv')\n",
      "30/26:\n",
      ">>> g = sns.catplot(x=\"sex\", y=\"period_length\",\n",
      "...                 hue=\"group_cat\", row=\"name\",\n",
      "...                 data=tn, kind=\"box\",\n",
      "...                 height=4, aspect=.7);\n",
      "30/27:\n",
      ">>> g = sns.catplot(x=\"group_cat\", y=\"period_length\",\n",
      "...                 hue=\"group_cat\", row=\"name\",\n",
      "...                 data=tn, kind=\"box\",\n",
      "...                 height=4, aspect=.7);\n",
      "30/28:\n",
      ">>> g = sns.catplot(x=\"group_cat\", y=\"period_length\",\n",
      "...                 hue=\"group_cat\", row=\"name\",\n",
      "...                 data=tn[tn.name=='Yoav_Kisch'], kind=\"box\",\n",
      "...                 height=4, aspect=.7);\n",
      "30/29:\n",
      "tn_c = tn.copy()\n",
      "tn_c['group_cat'] = tn_c.group_cat.astype(str)\n",
      "tn_c['name'] = tn_c.name.astype(str)\n",
      "30/30:\n",
      ">>> g = sns.catplot(x=\"group_cat\", y=\"period_length\",\n",
      "...                 hue=\"group_cat\", row=\"name\",\n",
      "...                 data=tn_c, kind=\"box\",\n",
      "...                 height=4, aspect=.7);\n",
      "30/31:\n",
      ">>> g = sns.catplot(x=\"group_cat\", y=\"period_length\",\n",
      "...                 #hue=\"group_cat\", row=\"name\",\n",
      "...                 data=tn_c, kind=\"box\",\n",
      "...                 height=4, aspect=.7);\n",
      "30/32:\n",
      ">>> g = sns.catplot(x=\"group_cat\", y=\"period_length\",\n",
      "...                 #hue=\"group_cat\", row=\"name\",\n",
      "...                 data=tn_c, kind=\"box\",\n",
      "...                 height=4, aspect=5);\n",
      "30/33:\n",
      ">>> g = sns.catplot(x=\"group_cat\", y=\"period_length\",\n",
      "...                 hue=\"group_cat\", row=\"name\",\n",
      "...                 data=tn_c, kind=\"box\",\n",
      "...                 height=4, aspect=5);\n",
      "30/34:\n",
      ">>> g = sns.catplot(x=\"group_cat\", y=\"period_length\",\n",
      "...                 hue=\"group_cat\", row=\"name\",\n",
      "...                 data=tn_c, kind=\"boxקמ\",\n",
      "...                 height=4, aspect=5);\n",
      "30/35:\n",
      ">>> g = sns.catplot(x=\"group_cat\", y=\"period_length\",\n",
      "...                 hue=\"group_cat\", row=\"name\",\n",
      "...                 data=tn_c, kind=\"boxen\",\n",
      "...                 height=4, aspect=5);\n",
      "31/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "31/2: sns.__version__\n",
      "31/3: tn = pd.read_pickle('turf/times_names.pkl.gz', compression='gzip')\n",
      "31/4: tn['period_length'] = tn.period_length/day_in_seconds\n",
      "31/5: day_in_seconds = 60*60*24\n",
      "31/6: tn['period_length'] = tn.period_length/day_in_seconds\n",
      "31/7:\n",
      "mean_mk = tn.groupby(['name']).period_length.mean()\n",
      "mean_group_cat = tn.groupby(['group_cat']).period_length.mean().to_frame().T\n",
      "mean_group_cat.index=['mean']\n",
      "mean_group_cat['mean'] = tn.period_length.mean()\n",
      "31/8:\n",
      "mean_heatmap = (tn.groupby(['name', 'group_cat'])\n",
      "                .period_length.agg(['mean', 'median'])\n",
      "                .unstack()[['mean']])\n",
      "mean_heatmap.columns = mean_heatmap.columns.droplevel(0)\n",
      "mean_heatmap['mean'] = mean_mk\n",
      "mean_heatmap = mean_heatmap.sort_values('mean', ascending=False)\n",
      "mean_heatmap.index = mean_heatmap.index.add_categories('mean')\n",
      "mean_heatmap = mean_heatmap.append(mean_group_cat)\n",
      "mean_heatmap = mean_heatmap.rename(columns = {'Dedicated 2MK': 'Dedicated Polemicists', 'Two-MKs-Plus': 'Polemicists'})\n",
      "mean_heatmap = mean_heatmap[['One Timers', 'Fans', 'Basers', 'Campers', 'Polemicists', 'Dedicated Fans', 'Dedicated Basers',\n",
      "       'Dedicated Campers', 'Dedicated Polemicists', 'Dedicated Celebies', 'mean']]\n",
      "mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "31/9: html_heatmap = mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red').render()\n",
      "31/10:\n",
      ">>> g = sns.catplot(x=\"group_cat\", y=\"period_length\",\n",
      "...                 hue=\"group_cat\", row=\"name\", row_order= list(mean_mk.sort_values(ascending=False).index),\n",
      "...                 data=tn, kind=\"boxen\",\n",
      "...                 height=4, aspect=5);\n",
      "31/11:\n",
      ">>> g = sns.catplot(x=\"group_cat\", y=\"period_length\",\n",
      "...                 #hue=\"group_cat\", \n",
      "                    row=\"name\", row_order= list(mean_mk.sort_values(ascending=False).index),\n",
      "...                 data=tn, kind=\"box\",\n",
      "...                 height=4, aspect=5);\n",
      "31/12:\n",
      ">>> g = sns.catplot(x=\"group_cat\", y=\"period_length\",\n",
      "...                 #hue=\"group_cat\", \n",
      "                    row=\"name\", row_order= list(mean_mk.sort_values(ascending=False).index),\n",
      "...                 data=tn[tn.period_length<850], kind=\"box\",\n",
      "...                 height=4, aspect=5);\n",
      "31/13:\n",
      "order = ['One Timers', 'Fans', 'Basers', 'Campers', 'Two-MKs-Plus', 'Dedicated Fans', 'Dedicated Basers',\n",
      "       'Dedicated Campers', 'Dedicated 2MK', 'Dedicated Celebies'\n",
      "31/14:\n",
      "order = ['One Timers', 'Fans', 'Basers', 'Campers', 'Two-MKs-Plus', 'Dedicated Fans', 'Dedicated Basers',\n",
      "       'Dedicated Campers', 'Dedicated 2MK', 'Dedicated Celebies']\n",
      "31/15:\n",
      "order = ['One Timers', 'Fans', 'Basers', 'Campers', 'Two-MKs-Plus', 'Dedicated Fans', 'Dedicated Basers',\n",
      "       'Dedicated Campers', 'Dedicated 2MK', 'Dedicated Celebies']\n",
      "31/16:\n",
      ">>> g = sns.catplot(x=\"group_cat\", y=\"period_length\",\n",
      "...                 #hue=\"group_cat\", \n",
      "                    order = order,\n",
      "                    row=\"name\", row_order= list(mean_mk.sort_values(ascending=False).index),\n",
      "...                 data=tn[tn.period_length<850], kind=\"box\",\n",
      "...                 height=4, aspect=5);\n",
      "31/17: tn.period_length.mean()\n",
      "31/18: mean_group_cat\n",
      "31/19: tn.groupby(['group_cat']).period_length.median().to_frame().T\n",
      "31/20:\n",
      "def get_heatmap(kind):\n",
      "    mean_heatmap = (tn.groupby(['name', 'group_cat'])\n",
      "                    .period_length.agg(['mean', 'median'])\n",
      "                    .unstack()[[kind]])\n",
      "    mean_heatmap.columns = mean_heatmap.columns.droplevel(0)\n",
      "    mean_heatmap['mean'] = mean_mk\n",
      "    mean_heatmap = mean_heatmap.sort_values('mean', ascending=False)\n",
      "    mean_heatmap.index = mean_heatmap.index.add_categories('mean')\n",
      "    mean_heatmap = mean_heatmap.append(mean_group_cat)\n",
      "    mean_heatmap = mean_heatmap.rename(columns = {'Dedicated 2MK': 'Dedicated Polemicists', 'Two-MKs-Plus': 'Polemicists'})\n",
      "    mean_heatmap = mean_heatmap[['One Timers', 'Fans', 'Basers', 'Campers', 'Polemicists', 'Dedicated Fans', 'Dedicated Basers',\n",
      "           'Dedicated Campers', 'Dedicated Polemicists', 'Dedicated Celebies', 'mean']]\n",
      "mean_heatmap = get_heatmap('mean')\n",
      "median_heatmap = get_heatmap('median')\n",
      "mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "31/21:\n",
      "def get_heatmap(kind):\n",
      "    mean_heatmap = (tn.groupby(['name', 'group_cat'])\n",
      "                    .period_length.agg(['mean', 'median'])\n",
      "                    .unstack()[[kind]])\n",
      "    mean_heatmap.columns = mean_heatmap.columns.droplevel(0)\n",
      "    mean_heatmap['mean'] = mean_mk\n",
      "    mean_heatmap = mean_heatmap.sort_values('mean', ascending=False)\n",
      "    mean_heatmap.index = mean_heatmap.index.add_categories('mean')\n",
      "    mean_heatmap = mean_heatmap.append(mean_group_cat)\n",
      "    mean_heatmap = mean_heatmap.rename(columns = {'Dedicated 2MK': 'Dedicated Polemicists', 'Two-MKs-Plus': 'Polemicists'})\n",
      "    mean_heatmap = mean_heatmap[['One Timers', 'Fans', 'Basers', 'Campers', 'Polemicists', 'Dedicated Fans', 'Dedicated Basers',\n",
      "           'Dedicated Campers', 'Dedicated Polemicists', 'Dedicated Celebies', 'mean']]\n",
      "    return mean_heatmap\n",
      "mean_heatmap = get_heatmap('mean')\n",
      "median_heatmap = get_heatmap('median')\n",
      "mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "31/22:\n",
      "def get_heatmap(kind):\n",
      "    mean_mk = tn.groupby(['name']).period_length.mean()\n",
      "    mean_group_cat = tn.groupby(['group_cat']).period_length.agg(kind).to_frame().T\n",
      "    mean_group_cat.index=[kind]\n",
      "    mean_group_cat[kind] = tn.period_length.agg(kind)\n",
      "    mean_heatmap = (tn.groupby(['name', 'group_cat'])\n",
      "                    .period_length.agg(kind)\n",
      "                    .unstack()[[kind]])\n",
      "    mean_heatmap.columns = mean_heatmap.columns.droplevel(0)\n",
      "    mean_heatmap[kind] = mean_mk\n",
      "    mean_heatmap = mean_heatmap.sort_values(kind, ascending=False)\n",
      "    mean_heatmap.index = mean_heatmap.index.add_categories(kind)\n",
      "    mean_heatmap = mean_heatmap.append(mean_group_cat)\n",
      "    mean_heatmap = mean_heatmap.rename(columns = {'Dedicated 2MK': 'Dedicated Polemicists', 'Two-MKs-Plus': 'Polemicists'})\n",
      "    mean_heatmap = mean_heatmap[['One Timers', 'Fans', 'Basers', 'Campers', 'Polemicists', 'Dedicated Fans', 'Dedicated Basers',\n",
      "           'Dedicated Campers', 'Dedicated Polemicists', 'Dedicated Celebies', kind]]\n",
      "    return mean_heatmap\n",
      "mean_heatmap = get_heatmap('mean')\n",
      "median_heatmap = get_heatmap('median')\n",
      "mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "31/23:\n",
      "def get_heatmap(kind):\n",
      "    mean_mk = tn.groupby(['name']).period_length.agg(kind)\n",
      "    mean_group_cat = tn.groupby(['group_cat']).period_length.agg(kind).to_frame().T\n",
      "    mean_group_cat.index=[kind]\n",
      "    mean_group_cat[kind] = tn.period_length.agg(kind)\n",
      "    mean_heatmap = (tn.groupby(['name', 'group_cat'])\n",
      "                    .period_length.agg(kind)\n",
      "                    .unstack()[[kind]])\n",
      "    mean_heatmap.columns = mean_heatmap.columns.droplevel(0)\n",
      "    mean_heatmap[kind] = mean_mk\n",
      "    mean_heatmap = mean_heatmap.sort_values(kind, ascending=False)\n",
      "    mean_heatmap.index = mean_heatmap.index.add_categories(kind)\n",
      "    mean_heatmap = mean_heatmap.append(mean_group_cat)\n",
      "    mean_heatmap = mean_heatmap.rename(columns = {'Dedicated 2MK': 'Dedicated Polemicists', 'Two-MKs-Plus': 'Polemicists'})\n",
      "    mean_heatmap = mean_heatmap[['One Timers', 'Fans', 'Basers', 'Campers', 'Polemicists', 'Dedicated Fans', 'Dedicated Basers',\n",
      "           'Dedicated Campers', 'Dedicated Polemicists', 'Dedicated Celebies', kind]]\n",
      "    return mean_heatmap\n",
      "mean_heatmap = get_heatmap('mean')\n",
      "median_heatmap = get_heatmap('median')\n",
      "mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "31/24: tn.groupby(['name']).period_length.agg('mean')\n",
      "31/25:\n",
      "def get_heatmap(kind):\n",
      "    mean_mk = tn.groupby(['name']).period_length.agg(kind)\n",
      "    mean_group_cat = tn.groupby(['group_cat']).period_length.agg(kind).to_frame().T\n",
      "    mean_group_cat.index=[kind]\n",
      "    mean_group_cat[kind] = tn.period_length.agg(kind)\n",
      "    mean_heatmap = (tn.groupby(['name', 'group_cat'])\n",
      "                    .period_length.agg(kind)\n",
      "                    .unstack())\n",
      "    mean_heatmap.columns = mean_heatmap.columns.droplevel(0)\n",
      "    mean_heatmap[kind] = mean_mk\n",
      "    mean_heatmap = mean_heatmap.sort_values(kind, ascending=False)\n",
      "    mean_heatmap.index = mean_heatmap.index.add_categories(kind)\n",
      "    mean_heatmap = mean_heatmap.append(mean_group_cat)\n",
      "    mean_heatmap = mean_heatmap.rename(columns = {'Dedicated 2MK': 'Dedicated Polemicists', 'Two-MKs-Plus': 'Polemicists'})\n",
      "    mean_heatmap = mean_heatmap[['One Timers', 'Fans', 'Basers', 'Campers', 'Polemicists', 'Dedicated Fans', 'Dedicated Basers',\n",
      "           'Dedicated Campers', 'Dedicated Polemicists', 'Dedicated Celebies', kind]]\n",
      "    return mean_heatmap\n",
      "mean_heatmap = get_heatmap('mean')\n",
      "median_heatmap = get_heatmap('median')\n",
      "mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "31/26:\n",
      "def get_heatmap(kind):\n",
      "    mean_mk = tn.groupby(['name']).period_length.agg(kind)\n",
      "    mean_group_cat = tn.groupby(['group_cat']).period_length.agg(kind).to_frame().T\n",
      "    mean_group_cat.index=[kind]\n",
      "    mean_group_cat[kind] = tn.period_length.agg(kind)\n",
      "    mean_heatmap = (tn.groupby(['name', 'group_cat'])\n",
      "                    .period_length.agg(kind)\n",
      "                    .unstack())\n",
      "    mean_heatmap[kind] = mean_mk\n",
      "    mean_heatmap = mean_heatmap.sort_values(kind, ascending=False)\n",
      "    mean_heatmap.index = mean_heatmap.index.add_categories(kind)\n",
      "    mean_heatmap = mean_heatmap.append(mean_group_cat)\n",
      "    mean_heatmap = mean_heatmap.rename(columns = {'Dedicated 2MK': 'Dedicated Polemicists', 'Two-MKs-Plus': 'Polemicists'})\n",
      "    mean_heatmap = mean_heatmap[['One Timers', 'Fans', 'Basers', 'Campers', 'Polemicists', 'Dedicated Fans', 'Dedicated Basers',\n",
      "           'Dedicated Campers', 'Dedicated Polemicists', 'Dedicated Celebies', kind]]\n",
      "    return mean_heatmap\n",
      "mean_heatmap = get_heatmap('mean')\n",
      "median_heatmap = get_heatmap('median')\n",
      "mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "31/27: median_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "31/28:\n",
      "html_heatmap = median_heatmap.style.background_gradient(cmap='viridis').highlight_null('red').render()\n",
      "open('outputs/mean_period_length.html', 'w').write(html_heatmap)\n",
      "31/29: open('outputs/mean_period_length.html', 'w').write(html_heatmap)\n",
      "31/30:\n",
      "html_heatmap = median_heatmap.style.background_gradient(cmap='viridis').highlight_null('red').render()\n",
      "open('outputs/median_period_length.html', 'w').write(html_heatmap)\n",
      "31/31:\n",
      "def get_heatmap(kind, field='period_length'):\n",
      "    mean_mk = tn.groupby(['name'])[field].agg(kind)\n",
      "    mean_group_cat = tn.groupby(['group_cat'])[field].agg(kind).to_frame().T\n",
      "    mean_group_cat.index=[kind]\n",
      "    mean_group_cat[kind] = tn[field].agg(kind)\n",
      "    mean_heatmap = (tn.groupby(['name', 'group_cat'])[field].agg(kind)\n",
      "                    .unstack())\n",
      "    mean_heatmap[kind] = mean_mk\n",
      "    mean_heatmap = mean_heatmap.sort_values(kind, ascending=False)\n",
      "    mean_heatmap.index = mean_heatmap.index.add_categories(kind)\n",
      "    mean_heatmap = mean_heatmap.append(mean_group_cat)\n",
      "    mean_heatmap = mean_heatmap.rename(columns = {'Dedicated 2MK': 'Dedicated Polemicists', 'Two-MKs-Plus': 'Polemicists'})\n",
      "    mean_heatmap = mean_heatmap[['One Timers', 'Fans', 'Basers', 'Campers', 'Polemicists', 'Dedicated Fans', 'Dedicated Basers',\n",
      "           'Dedicated Campers', 'Dedicated Polemicists', 'Dedicated Celebies', kind]]\n",
      "    return mean_heatmap\n",
      "mean_heatmap = get_heatmap('mean')\n",
      "median_heatmap = get_heatmap('median')\n",
      "mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "31/32: tn.dtypes\n",
      "31/33:\n",
      "days_heatmap = get_heatmap('median', 'created_day_nunique')\n",
      "days_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "31/34:\n",
      "days_heatmap = get_heatmap('mean', 'created_day_nunique')\n",
      "days_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "31/35:\n",
      "days_heatmap = get_heatmap('median', 'created_day_nunique')\n",
      "days_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "31/36:\n",
      "days_heatmap = get_heatmap('mean', 'created_day_nunique')\n",
      "days_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "31/37:\n",
      "html_heatmap = days_heatmap.style.background_gradient(cmap='viridis').highlight_null('red').render()\n",
      "open('outputs/mean_created_day_nunique.html', 'w').write(html_heatmap)\n",
      "31/38:\n",
      "month_heatmap = get_heatmap('mean', 'created_month_nunique')\n",
      "month_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "28/106: creation_dates = rdf[rdf.timestamp==rdf.groupby('lang').timestamp.transform(min)][['lang', 'timestamp']].sort_values('timestamp')\n",
      "28/107:\n",
      "creation_dates = rdf[rdf.timestamp==rdf.groupby('lang').timestamp.transform(min)][['lang', 'timestamp']].sort_values('timestamp')\n",
      "creation_dates\n",
      "28/108: creation_dates.to_csv('data/pages_creation_dates.csv')\n",
      "28/109: rdf.to_csv('data/all_revisions_by_page_lang.csv')\n",
      "28/110: creation_dates.to_csv('data/pages_creation_dates.csv', index=False)\n",
      "28/111: rdf.to_csv('data/all_revisions_by_page_lang.csv', index=False)\n",
      "28/112:\n",
      "revisions = {}\n",
      "\n",
      "req = {'action': 'query',\n",
      "       'prop': 'revisions',\n",
      "       'format': 'json',\n",
      "       'rvlimit': 'max',\n",
      "       'rvprop': 'timestamp|user|comment|size|tags|content',}\n",
      "\n",
      "#for k in sorted(ll.keys()):\n",
      "for k in sorted(['he']):\n",
      "    v = ll[k]\n",
      "    params = req.copy()\n",
      "    params['titles'] = v.title\n",
      "    resp = requests.get(f'http://{k}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        eloffset = resp['continue']['rvcontinue']\n",
      "        resp = requests.get(f'http://{k}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    revisions[k] = final_resp\n",
      "28/113:\n",
      "rev_cont = {}\n",
      "\n",
      "req = {'action': 'query',\n",
      "       'prop': 'revisions',\n",
      "       'format': 'json',\n",
      "       'rvlimit': 'max',\n",
      "       'rvprop': 'timestamp|user|comment|size|tags|content',}\n",
      "\n",
      "#for k in sorted(ll.keys()):\n",
      "for k in sorted(['he']):\n",
      "    v = ll[k]\n",
      "    params = req.copy()\n",
      "    params['titles'] = v.title\n",
      "    resp = requests.get(f'http://{k}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        eloffset = resp['continue']['rvcontinue']\n",
      "        resp = requests.get(f'http://{k}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    rev_cont[k] = final_resp\n",
      "28/114: len(rev_cont['he'])\n",
      "28/115: rev_cont['he'][0]\n",
      "28/116: len(rev_cont['he'][0])\n",
      "28/117: type(rev_cont['he'][0])\n",
      "28/118: rev_cont['he'][0].keys\n",
      "28/119: rev_cont['he'][0].keys()\n",
      "28/120: rev_cont['he'][0]['query'].keys()\n",
      "28/121: rev_cont['he'][0]['query']['pages'].keys()\n",
      "28/122: rev_cont['he'][0]['query']['pages']['1201545'].keys()\n",
      "28/123: rev_cont['he'][0]['query']['pages']['1201545']['revisions'].keys()\n",
      "28/124: rev_cont['he'][0]['query']['pages']['1201545']['revisions'][0]\n",
      "28/125:\n",
      "rev_slots = {}\n",
      "\n",
      "req = {'action': 'query',\n",
      "       'prop': 'revisions',\n",
      "       'format': 'json',\n",
      "       'rvlimit': 'max',\n",
      "       'rvprop': 'timestamp|user|comment|size|tags|content',\n",
      "       'rvslots': 'extlinks',}\n",
      "\n",
      "#for k in sorted(ll.keys()):\n",
      "for k in sorted(['he']):\n",
      "    v = ll[k]\n",
      "    params = req.copy()\n",
      "    params['titles'] = v.title\n",
      "    resp = requests.get(f'http://{k}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        eloffset = resp['continue']['rvcontinue']\n",
      "        resp = requests.get(f'http://{k}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    rev_slots[k] = final_resp\n",
      "28/126: rev_cont['he'][0]['query']['pages']['1201545']['revisions'][0]\n",
      "28/127:\n",
      "revisions = {}\n",
      "\n",
      "req = {'action': 'query',\n",
      "       'prop': 'revisions',\n",
      "       'format': 'json',\n",
      "       'rvlimit': 'max',\n",
      "       'rvprop': 'timestamp|user|comment|size|tags|ids|roles',}\n",
      "\n",
      "for k in sorted(ll.keys()):\n",
      "    v = ll[k]\n",
      "    params = req.copy()\n",
      "    params['titles'] = v.title\n",
      "    resp = requests.get(f'http://{k}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        eloffset = resp['continue']['rvcontinue']\n",
      "        resp = requests.get(f'http://{k}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    revisions[k] = final_resp\n",
      "28/128: revisions['he'][0]\n",
      "28/129:\n",
      "clean_revs = defaultdict(list)\n",
      "for lang in revisions:\n",
      "    for resp in revisions[lang]:\n",
      "        for page in resp['query']['pages']:\n",
      "            if 'revisions' in resp['query']['pages'][page]:\n",
      "                revs = resp['query']['pages'][page]['revisions']\n",
      "                num = len(revs)\n",
      "                clean_revs[lang].extend(revs)\n",
      "            else: \n",
      "                clean_revs[lang].extend([])\n",
      "                num=0\n",
      "                \n",
      "            title = resp['query']['pages'][page]['title']\n",
      "\n",
      "            print(f'{lang}: revs={num}, title: {title}')\n",
      "28/130:\n",
      "cr_for_df = []\n",
      "\n",
      "for lang in clean_revs:\n",
      "    for rev in clean_revs[lang]:\n",
      "        tmp = rev.copy()\n",
      "        tmp['lang'] = lang\n",
      "        tmp['tags'] = ','.join(rev['tags'])\n",
      "        cr_for_df.append(tmp)\n",
      "\n",
      "rdf = (pd.DataFrame(cr_for_df)\n",
      "       .assign(timestamp = lambda x: pd.to_datetime(x.timestamp))\n",
      "      )\n",
      "\n",
      "rdf.dtypes\n",
      "28/131:\n",
      "cr_for_df = []\n",
      "\n",
      "for lang in clean_revs:\n",
      "    for rev in clean_revs[lang]:\n",
      "        tmp = rev.copy()\n",
      "        tmp['lang'] = lang\n",
      "        tmp['tags'] = ','.join(rev['tags'])\n",
      "        cr_for_df.append(tmp)\n",
      "\n",
      "rdf = (pd.DataFrame(cr_for_df)\n",
      "       .assign(timestamp = lambda x: pd.to_datetime(x.timestamp))\n",
      "      )\n",
      "\n",
      "rdf.head()\n",
      "28/132:\n",
      "cr_for_df = []\n",
      "\n",
      "for lang in clean_revs:\n",
      "    for rev in clean_revs[lang]:\n",
      "        tmp = rev.copy()\n",
      "        tmp['lang'] = lang\n",
      "        tmp['tags'] = ','.join(rev['tags'])\n",
      "        tmp['roles'] = ','.join(rev['roles'])\n",
      "        cr_for_df.append(tmp)\n",
      "\n",
      "rdf = (pd.DataFrame(cr_for_df)\n",
      "       .assign(timestamp = lambda x: pd.to_datetime(x.timestamp))\n",
      "      )\n",
      "\n",
      "rdf.head()\n",
      "28/133: rdf.roles.value_counts()\n",
      "28/134: rdf.to_csv('data/all_revisions_by_page_lang.csv', index=False)\n",
      "28/135:\n",
      "creation_dates = rdf[rdf.timestamp==rdf.groupby('lang').timestamp.transform(min)][['lang', 'timestamp']].sort_values('timestamp')\n",
      "creation_dates\n",
      "28/136: creation_dates.to_csv('data/pages_creation_dates.csv', index=False)\n",
      "28/137: rdf[rdf.lang=='hu']\n",
      "28/138:\n",
      "cr_for_df = []\n",
      "\n",
      "for lang in clean_revs:\n",
      "    for rev in clean_revs[lang]:\n",
      "        tmp = rev.copy()\n",
      "        tmp['lang'] = lang\n",
      "        tmp['title'] = ll[lang].title\n",
      "        tmp['tags'] = ','.join(rev['tags'])\n",
      "        tmp['roles'] = ','.join(rev['roles'])\n",
      "        cr_for_df.append(tmp)\n",
      "\n",
      "rdf = (pd.DataFrame(cr_for_df)\n",
      "       .assign(timestamp = lambda x: pd.to_datetime(x.timestamp))\n",
      "      )\n",
      "\n",
      "rdf.head()\n",
      "28/139:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'extlinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for revid, title in rdf.loc[rdf.lang=='uk', ['title', 'revid']].itertuples():\n",
      "    params = req.copy()\n",
      "    params['titles'] = title\n",
      "    params['revid'] = revid\n",
      "    resp = requests.get(f'http://{k}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        eloffset = resp['continue']['rvcontinue']\n",
      "        resp = requests.get(f'http://{k}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    rev_ext[k] = final_resp\n",
      "28/140:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'extlinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for x in rdf.loc[rdf.lang=='uk', ['title', 'revid']].itertuples():\n",
      "    print(x)\n",
      "    params = req.copy()\n",
      "    params['titles'] = title\n",
      "    params['revid'] = revid\n",
      "    resp = requests.get(f'http://{k}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        eloffset = resp['continue']['rvcontinue']\n",
      "        resp = requests.get(f'http://{k}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    rev_ext[k] = final_resp\n",
      "28/141:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'extlinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, revid, title in rdf.loc[rdf.lang=='uk', ['title', 'revid']].itertuples():\n",
      "    params = req.copy()\n",
      "    params['titles'] = title\n",
      "    params['revid'] = revid\n",
      "    resp = requests.get(f'http://{k}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        eloffset = resp['continue']['rvcontinue']\n",
      "        resp = requests.get(f'http://{k}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    rev_ext[k] = final_resp\n",
      "28/142: len(rev_ext)\n",
      "28/143: len(rev_ext['uk'])\n",
      "28/144:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'extlinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, revid, title in rdf.loc[rdf.lang=='uk', ['lang', 'title', 'revid']].itertuples():\n",
      "    params = req.copy()\n",
      "    params['titles'] = title\n",
      "    params['revid'] = revid\n",
      "    resp = requests.get(f'http://{k}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        eloffset = resp['continue']['rvcontinue']\n",
      "        resp = requests.get(f'http://{k}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    rev_ext[lang] = final_resp\n",
      "28/145: len(rev_ext['uk'])\n",
      "28/146: rev_ext['uk'].keys()\n",
      "28/147: rev_ext['uk'][0]\n",
      "28/148:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'extlinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, revid, title in rdf.loc[rdf.lang=='uk', ['lang', 'title', 'revid']].itertuples():\n",
      "    print (idx, lang, revid, title)\n",
      "    params = req.copy()\n",
      "    params['titles'] = title\n",
      "    params['revid'] = revid\n",
      "    resp = requests.get(f'http://{k}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        eloffset = resp['continue']['rvcontinue']\n",
      "        resp = requests.get(f'http://{k}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    rev_ext[lang] = final_resp\n",
      "28/149: rev_ext['uk'][0]\n",
      "28/150:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'extlinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, revid, title in rdf.loc[rdf.lang=='uk', ['lang', 'title', 'revid']].itertuples():\n",
      "    print (idx, lang, revid, title)\n",
      "    params = req.copy()\n",
      "    params['titles'] = title\n",
      "    params['revid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        eloffset = resp['continue']['rvcontinue']\n",
      "        resp = requests.get(f'http://{k}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    rev_ext[lang] = final_resp\n",
      "28/151:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'extlinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, revid, title in rdf.loc[rdf.lang=='uk', ['lang', 'title', 'revid']].itertuples():\n",
      "    print (idx, lang, revid, title)\n",
      "    params = req.copy()\n",
      "    params['titles'] = title\n",
      "    params['revid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        eloffset = resp['continue']['rvcontinue']\n",
      "        resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    rev_ext[lang] = final_resp\n",
      "28/152: rev_ext['uk'][0]\n",
      "28/153:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'extlinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, revid, title in rdf.loc[rdf.lang=='uk', ['lang', 'title', 'revid']].itertuples():\n",
      "    print revid\n",
      "    params = req.copy()\n",
      "    params['titles'] = title\n",
      "    params['revid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        eloffset = resp['continue']['rvcontinue']\n",
      "        resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    rev_ext[lang] = final_resp\n",
      "28/154:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'extlinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, revid, title in rdf.loc[rdf.lang=='uk', ['lang', 'title', 'revid']].itertuples():\n",
      "    print( revid)\n",
      "    params = req.copy()\n",
      "    params['titles'] = title\n",
      "    params['revid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        eloffset = resp['continue']['rvcontinue']\n",
      "        resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    rev_ext[lang] = final_resp\n",
      "28/155:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'extlinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid in rdf.loc[rdf.lang=='uk', ['lang', 'title', 'revid']].itertuples():\n",
      "    params = req.copy()\n",
      "    params['titles'] = title\n",
      "    params['revid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        eloffset = resp['continue']['rvcontinue']\n",
      "        resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    rev_ext[lang] = final_resp\n",
      "28/156: rev_ext['uk'][0]\n",
      "28/157:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'extlinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid in rdf.loc[rdf.lang=='uk', ['lang', 'title', 'revid']].itertuples():\n",
      "    params = req.copy()\n",
      "    params['title'] = title\n",
      "    params['revid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        eloffset = resp['continue']['rvcontinue']\n",
      "        resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    rev_ext[lang] = final_resp\n",
      "28/158: rev_ext['uk'][0]\n",
      "28/159:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'externallinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid in rdf.loc[rdf.lang=='uk', ['lang', 'title', 'revid']].itertuples():\n",
      "    params = req.copy()\n",
      "    params['title'] = title\n",
      "    params['revid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        eloffset = resp['continue']['rvcontinue']\n",
      "        resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    rev_ext[lang] = final_resp\n",
      "28/160: rev_ext['uk'][0]\n",
      "28/161:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'externallinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid in rdf.loc[rdf.lang=='uk', ['lang', 'title', 'revid']].itertuples():\n",
      "    params = req.copy()\n",
      "    params['page'] = title\n",
      "    params['revid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        eloffset = resp['continue']['rvcontinue']\n",
      "        resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    rev_ext[lang] = final_resp\n",
      "28/162: rev_ext['uk'][0]\n",
      "28/163:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'externallinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid in rdf.loc[rdf.lang=='uk', ['lang', 'title', 'revid']].itertuples():\n",
      "    params = req.copy()\n",
      "    params['title'] = title\n",
      "    params['revid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        eloffset = resp['continue']['rvcontinue']\n",
      "        resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    rev_ext[lang] = final_resp\n",
      "28/164: rev_ext['uk'][0]\n",
      "28/165:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'externallinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid in rdf.loc[rdf.lang=='uk', ['lang', 'title', 'revid']].itertuples():\n",
      "    params = req.copy()\n",
      "    params['title'] = title\n",
      "    params['oldid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        eloffset = resp['continue']['rvcontinue']\n",
      "        resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    rev_ext[lang] = final_resp\n",
      "28/166: rev_ext['uk'][0]\n",
      "28/167:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'externallinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid in rdf.loc[rdf.lang=='uk', ['lang', 'title', 'revid']].itertuples():\n",
      "    params = req.copy()\n",
      "    params['title'] = title\n",
      "    params['revid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        eloffset = resp['continue']['rvcontinue']\n",
      "        resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    rev_ext[lang] = final_resp\n",
      "28/168: rev_ext['uk'][0]\n",
      "28/169:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'externallinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid in rdf.loc[rdf.lang=='uk', ['lang', 'title', 'revid']].itertuples():\n",
      "    params = req.copy()\n",
      "    params['text'] = title\n",
      "    params['revid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        eloffset = resp['continue']['rvcontinue']\n",
      "        resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    rev_ext[lang] = final_resp\n",
      "28/170: rev_ext['uk'][0]\n",
      "28/171: rev_ext['uk'][1]\n",
      "28/172:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'externallinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid in rdf.loc[:,['lang', 'title', 'revid']].itertuples():\n",
      "    params = req.copy()\n",
      "    params['text'] = title\n",
      "    params['revid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        eloffset = resp['continue']['rvcontinue']\n",
      "        resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    rev_ext[lang] = final_resp\n",
      "28/173:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'externallinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid in rdf.loc['ar',['lang', 'title', 'revid']].itertuples():\n",
      "    params = req.copy()\n",
      "    params['text'] = title\n",
      "    params['revid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        eloffset = resp['continue']['rvcontinue']\n",
      "        resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    rev_ext[lang] = final_resp\n",
      "28/174:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'externallinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid in rdf.loc[rdf.lang=='ar', ['lang', 'title', 'revid']].itertuples():\n",
      "    params = req.copy()\n",
      "    params['text'] = title\n",
      "    params['revid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        eloffset = resp['continue']['rvcontinue']\n",
      "        resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    rev_ext[lang] = final_resp\n",
      "28/175:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'externallinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid in rdf.loc[rdf.lang=='de', ['lang', 'title', 'revid']].itertuples():\n",
      "    params = req.copy()\n",
      "    params['text'] = title\n",
      "    params['revid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    rev_ext[lang] = final_resp\n",
      "28/176: rdf.loc[rdf.lang=='de', ['lang', 'title', 'revid']].shape\n",
      "28/177:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'externallinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid in rdf.loc[rdf.lang=='de', ['lang', 'title', 'revid']].itertuples():\n",
      "    print(lang, revid)\n",
      "    params = req.copy()\n",
      "    params['text'] = title\n",
      "    params['revid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    rev_ext[lang] = final_resp\n",
      "28/178:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'externallinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid in rdf.loc[rdf.lang=='de', ['lang', 'title', 'revid']].itertuples():\n",
      "    params = req.copy()\n",
      "    params['text'] = title\n",
      "    params['revid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    print (resp)\n",
      "    final_resp = [resp, ]\n",
      "    rev_ext[lang] = final_resp\n",
      "28/179:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       #'prop': 'externallinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid in rdf.loc[rdf.lang=='de', ['lang', 'title', 'revid']].itertuples():\n",
      "    params = req.copy()\n",
      "    params['text'] = title\n",
      "    params['revid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    print (resp)\n",
      "    final_resp = [resp, ]\n",
      "    rev_ext[lang] = final_resp\n",
      "28/180:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       #'prop': 'externallinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid in rdf.loc[rdf.lang=='de', ['lang', 'title', 'revid']].itertuples():\n",
      "    params = req.copy()\n",
      "    params['text'] = title\n",
      "    params['revid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    print (resp)\n",
      "    final_resp = [resp, ]\n",
      "    rev_ext[lang] = final_resp\n",
      "    break\n",
      "28/181: rev_ext['de'][1]\n",
      "28/182: rev_ext['de'][0]\n",
      "28/183:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       #'prop': 'externallinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid in rdf.loc[rdf.lang=='ur', ['lang', 'title', 'revid']].itertuples():\n",
      "    params = req.copy()\n",
      "    params['text'] = title\n",
      "    params['revid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    rev_ext[lang] = final_resp\n",
      "28/184: rev_ext['de'][0]\n",
      "28/185: rev_ext['ur'][0]\n",
      "28/186:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'externallinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid in rdf.loc[rdf.lang=='ur', ['lang', 'title', 'revid']].itertuples():\n",
      "    params = req.copy()\n",
      "    params['text'] = title\n",
      "    params['revid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    rev_ext[lang] = final_resp\n",
      "28/187: rev_ext['ur'][0]\n",
      "28/188: rev_ext['ur']\n",
      "28/189:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'externallinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid in rdf.loc[rdf.lang=='he', ['lang', 'title', 'revid']].itertuples():\n",
      "    params = req.copy()\n",
      "    params['text'] = title\n",
      "    params['revid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    rev_ext[lang] = final_resp\n",
      "28/190:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'externallinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid in rdf.loc[rdf.lang=='he', ['lang', 'title', 'revid']].itertuples():\n",
      "    params = req.copy()\n",
      "    params['text'] = title\n",
      "    params['revid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    print(resp)\n",
      "    rev_ext[lang] = final_resp\n",
      "28/191:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'revid|externallinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid in rdf.loc[rdf.lang=='he', ['lang', 'title', 'revid']].itertuples():\n",
      "    params = req.copy()\n",
      "    params['text'] = title\n",
      "    params['revid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    print(resp)\n",
      "    rev_ext[lang] = final_resp\n",
      "28/192:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'revid|externallinks|text',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid in rdf.loc[rdf.lang=='he', ['lang', 'title', 'revid']].itertuples():\n",
      "    params = req.copy()\n",
      "    params['text'] = title\n",
      "    params['revid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    print(resp)\n",
      "    rev_ext[lang] = final_resp\n",
      "28/193:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'revid|externallinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid in rdf.loc[rdf.lang=='he', ['lang', 'title', 'revid']].itertuples():\n",
      "    params = req.copy()\n",
      "    params['text'] = title\n",
      "    params['revid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    print(resp)\n",
      "    rev_ext[lang] = final_resp\n",
      "28/194:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'revid',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid in rdf.loc[rdf.lang=='he', ['lang', 'title', 'revid']].itertuples():\n",
      "    params = req.copy()\n",
      "    params['text'] = title\n",
      "    params['revid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    print(resp)\n",
      "    rev_ext[lang] = final_resp\n",
      "28/195:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'revid',\n",
      "       'format': 'json',\n",
      "       'rvprop':'content',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid in rdf.loc[rdf.lang=='he', ['lang', 'title', 'revid']].itertuples():\n",
      "    params = req.copy()\n",
      "    params['text'] = title\n",
      "    params['revid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    print(resp)\n",
      "    rev_ext[lang] = final_resp\n",
      "28/196:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'revid',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid in rdf.loc[rdf.lang=='he', ['lang', 'title', 'revid']].itertuples():\n",
      "    params = req.copy()\n",
      "    params['title'] = title\n",
      "    params['text'] = title\n",
      "    params['revid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    print(resp)\n",
      "    rev_ext[lang] = final_resp\n",
      "28/197:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'externallinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid in rdf.loc[rdf.lang=='he', ['lang', 'title', 'revid']].itertuples():\n",
      "    params = req.copy()\n",
      "    params['title'] = title\n",
      "    params['text'] = title\n",
      "    params['revid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    print(resp)\n",
      "    rev_ext[lang] = final_resp\n",
      "28/198:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'extlinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid in rdf.loc[rdf.lang=='he', ['lang', 'title', 'revid']].itertuples():\n",
      "    params = req.copy()\n",
      "    params['title'] = title\n",
      "    params['text'] = title\n",
      "    params['revid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    print(resp)\n",
      "    rev_ext[lang] = final_resp\n",
      "28/199:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'externallinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid in rdf.loc[rdf.lang=='he', ['lang', 'title', 'revid']].itertuples():\n",
      "    params = req.copy()\n",
      "    params['title'] = title\n",
      "    params['text'] = title\n",
      "    params['revid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    print(resp)\n",
      "    rev_ext[lang] = final_resp\n",
      "28/200:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       #'prop': 'content',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid in rdf.loc[rdf.lang=='he', ['lang', 'title', 'revid']].itertuples():\n",
      "    params = req.copy()\n",
      "    params['title'] = title\n",
      "    params['text'] = title\n",
      "    params['revid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    print(resp)\n",
      "    rev_ext[lang] = final_resp\n",
      "28/201:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'text',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid in rdf.loc[rdf.lang=='he', ['lang', 'title', 'revid']].itertuples():\n",
      "    params = req.copy()\n",
      "    params['title'] = title\n",
      "    params['text'] = title\n",
      "    params['revid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    print(resp)\n",
      "    rev_ext[lang] = final_resp\n",
      "28/202: rev_cont[0]\n",
      "28/203: rev_cont['he']\n",
      "28/204: rev_cont['he'].keys()\n",
      "28/205: rev_cont['he'][0].keys()\n",
      "28/206: rev_cont['he'][0]['query'].keys()\n",
      "28/207: rev_cont['he'][0]['query']['pages']['1201545'].keys()\n",
      "28/208: rev_cont['he'][0]['query']['pages']['1201545']['revisions']\n",
      "28/209: rev_cont['he'][0]['query']['pages']['1201545']['revisions'][0]\n",
      "28/210: len(rev_cont['he'][0]['query']['pages']['1201545']['revisions'])\n",
      "28/211: len(rev_cont['he'][0]['query']['pages']['1201545']['revisions'][0].keys()\n",
      "28/212: len(rev_cont['he'][0]['query']['pages']['1201545']['revisions'][0].keys()\n",
      "28/213: len(rev_cont['he'][0]['query']['pages']['1201545']['revisions'][0].keys())\n",
      "28/214: rev_cont['he'][0]['query']['pages']['1201545']['revisions'][0].keys()\n",
      "28/215: rev_cont['he'][0]['query']['pages']['1201545'].keys()\n",
      "28/216: rev_cont['he'][0]['query']['pages']['1201545']['revisions'][0].keys()\n",
      "28/217: rev_cont['he'][0]['query']['pages']['1201545']['revisions'][0]['*'].keys()\n",
      "28/218: rev_cont['he'][0]['query']['pages']['1201545']['revisions'][0]['*']\n",
      "28/219:\n",
      "text = rev_cont['he'][0]['query']['pages']['1201545']['revisions'][0]['*']\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'externallinks',\n",
      "       'format': 'json',\n",
      "       'text': text,\n",
      "       'title': 'מבצע צוק איתן',\n",
      "      }\n",
      "resp = requests.get(f'http://he.wikipedia.org/w/api.php', params = params).json()\n",
      "28/220:\n",
      "text = rev_cont['he'][0]['query']['pages']['1201545']['revisions'][0]['*']\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'externallinks',\n",
      "       'format': 'json',\n",
      "       'text': text,\n",
      "       'title': 'מבצע צוק איתן',\n",
      "      }\n",
      "resp = requests.get(f'http://he.wikipedia.org/w/api.php', params = params).json()\n",
      "resp\n",
      "28/221:\n",
      "text = rev_cont['he'][0]['query']['pages']['1201545']['revisions'][0]['*']\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'externallinks',\n",
      "       'format': 'json',\n",
      "       'text': text,\n",
      "      }\n",
      "resp = requests.get(f'http://he.wikipedia.org/w/api.php', params = params).json()\n",
      "resp\n",
      "28/222:\n",
      "text = rev_cont['he'][0]['query']['pages']['1201545']['revisions'][0]['*']\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'externallinks',\n",
      "       'format': 'json',\n",
      "       'text': text,\n",
      "      }\n",
      "resp = requests.get(f'http://he.wikipedia.org/w/api.php', params = req).json()\n",
      "resp\n",
      "28/223:\n",
      "text = rev_cont['he'][0]['query']['pages']['1201545']['revisions'][0]['*']\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'externallinks',\n",
      "       'format': 'json',\n",
      "       'text': text,\n",
      "      }\n",
      "resp = requests.get(f'http://he.wikipedia.org/w/api.php', params = req).json()\n",
      "resp\n",
      "28/224:\n",
      "text = rev_cont['he'][0]['query']['pages']['1201545']['revisions'][0]['*']\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'format': 'json',\n",
      "       'text': text,\n",
      "      }\n",
      "resp = requests.get(f'http://he.wikipedia.org/w/api.php', params = req).json()\n",
      "resp\n",
      "28/225:\n",
      "text = rev_cont['he'][0]['query']['pages']['1201545']['revisions'][0]['*']\n",
      "text\n",
      "28/226:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'text',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid in rdf.loc[rdf.lang=='he', ['lang', 'title', 'revid']].itertuples():\n",
      "    params = req.copy()\n",
      "    params['title'] = title\n",
      "    params['text'] = title\n",
      "    params['revid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    print(resp)\n",
      "    rev_ext[lang] = final_resp\n",
      "28/227:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'text',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid in rdf.loc[rdf.lang=='he', ['lang', 'title', 'revid']].itertuples():\n",
      "    print(revid)\n",
      "    params = req.copy()\n",
      "    params['title'] = title\n",
      "    params['text'] = title\n",
      "    params['revid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    rev_ext[lang] = final_resp\n",
      "28/228:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'text',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid in rdf.loc[rdf.lang=='he', ['lang', 'title', 'revid']].itertuples():\n",
      "    print(revid)\n",
      "    params = req.copy()\n",
      "    #params['title'] = title\n",
      "    #params['text'] = title\n",
      "    params['old'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    rev_ext[lang] = final_resp\n",
      "28/229:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'externallinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid in rdf.loc[rdf.lang=='he', ['lang', 'title', 'revid']].itertuples():\n",
      "    print(revid)\n",
      "    params = req.copy()\n",
      "    #params['title'] = title\n",
      "    #params['text'] = title\n",
      "    params['old'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    rev_ext[lang] = final_resp\n",
      "28/230:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'externallinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid in rdf.loc[rdf.lang=='ur', ['lang', 'title', 'revid']].itertuples():\n",
      "    print(revid)\n",
      "    params = req.copy()\n",
      "    #params['title'] = title\n",
      "    #params['text'] = title\n",
      "    params['old'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    rev_ext[lang] = final_resp\n",
      "28/231: rev_ext['ur']\n",
      "28/232:\n",
      "rev_ext = {}\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'externallinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid in rdf.loc[rdf.lang=='ur', ['lang', 'title', 'revid']].itertuples():\n",
      "    print(revid)\n",
      "    params = req.copy()\n",
      "    #params['title'] = title\n",
      "    #params['text'] = title\n",
      "    params['oldid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    rev_ext[lang] = final_resp\n",
      "28/233: rev_ext['ur']\n",
      "28/234:\n",
      "rev_ext = defaultdict(list)\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'externallinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid in rdf.loc[rdf.lang=='ur', ['lang', 'title', 'revid']].itertuples():\n",
      "    params = req.copy()\n",
      "    #params['title'] = title\n",
      "    #params['text'] = title\n",
      "    params['oldid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    rev_ext[lang].append({'lang': lang, 'revid': revid, 'extlinks': resp})\n",
      "28/235: rev_ext['ur']\n",
      "28/236:\n",
      "rev_ext = defaultdict(list)\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'externallinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid in rdf.loc[rdf.lang=='ur', ['lang', 'title', 'revid']].itertuples():\n",
      "    params = req.copy()\n",
      "    #params['title'] = title\n",
      "    #params['text'] = title\n",
      "    params['oldid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    resp['lang'] = lang\n",
      "    rev_ext[lang].append(resp)\n",
      "28/237: rev_ext['ur']\n",
      "28/238:\n",
      "rev_ext = defaultdict(list)\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'externallinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid in rdf.loc[:, ['lang', 'title', 'revid']].itertuples():\n",
      "    params = req.copy()\n",
      "    #params['title'] = title\n",
      "    #params['text'] = title\n",
      "    params['oldid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    resp['lang'] = lang\n",
      "    rev_ext[lang].append(resp)\n",
      "28/239:\n",
      "rev_ext = defaultdict(list)\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'externallinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid in rdf.loc[rdf.lang=='he', ['lang', 'title', 'revid']].itertuples():\n",
      "    params = req.copy()\n",
      "    #params['title'] = title\n",
      "    #params['text'] = title\n",
      "    params['oldid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    resp['lang'] = lang\n",
      "    rev_ext[lang].append(resp)\n",
      "28/240:\n",
      "rev_ext = defaultdict(list)\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'externallinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid in rdf.loc[rdf.lang=='ur', ['lang', 'title', 'revid']].sort_values('timestamp').itertuples():\n",
      "    params = req.copy()\n",
      "    #params['title'] = title\n",
      "    #params['text'] = title\n",
      "    params['oldid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    resp['lang'] = lang\n",
      "    rev_ext[lang].append(resp)\n",
      "28/241:\n",
      "rev_ext = defaultdict(list)\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'externallinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid, timestamp in rdf.loc[rdf.lang=='ur', ['lang', 'title', 'revid', 'timestamp']].sort_values('timestamp').itertuples():\n",
      "    params = req.copy()\n",
      "    #params['title'] = title\n",
      "    #params['text'] = title\n",
      "    params['oldid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    resp['lang'] = lang\n",
      "    rev_ext[lang].append(resp)\n",
      "28/242:\n",
      "rev_ext = defaultdict(list)\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'externallinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid, timestamp in rdf.loc[rdf.lang=='ur', ['lang', 'title', 'revid', 'timestamp']].sort_values('timestamp').itertuples():\n",
      "    params = req.copy()\n",
      "    #params['title'] = title\n",
      "    #params['text'] = title\n",
      "    params['oldid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    rev_ext[lang].append(resp)\n",
      "28/243:\n",
      "link_first_rev = {}\n",
      "for resp in rev_ext['he']:\n",
      "    for link in resp['parse']['externallinks']:\n",
      "        if link not in link_first_rev:\n",
      "            link_first_rev = resp['parse']['revid']\n",
      "28/244:\n",
      "link_first_rev = {}\n",
      "for resp in rev_ext['he']:\n",
      "    for link in resp['parse']['externallinks']:\n",
      "        if link not in link_first_rev:\n",
      "            link_first_rev = resp['parse']['revid']\n",
      "link_first_rev\n",
      "28/245:\n",
      "link_first_rev = {}\n",
      "for resp in rev_ext['ur']:\n",
      "    for link in resp['parse']['externallinks']:\n",
      "        if link not in link_first_rev:\n",
      "            link_first_rev = resp['parse']['revid']\n",
      "link_first_rev\n",
      "28/246:\n",
      "link_first_rev = {}\n",
      "for resp in rev_ext['ur']:\n",
      "    for link in resp['parse']['externallinks']:\n",
      "        if link not in link_first_rev:\n",
      "            link_first_rev = resp['parse']['revid']\n",
      "link_first_rev\n",
      "28/247:\n",
      "link_first_rev = {}\n",
      "\n",
      "for resp in rev_ext['ur']:\n",
      "    for link in resp['parse']['externallinks']:\n",
      "        print (link)\n",
      "        if link not in link_first_rev:\n",
      "            link_first_rev = resp['parse']['revid']\n",
      "link_first_rev\n",
      "28/248:\n",
      "link_first_rev = {}\n",
      "\n",
      "for resp in rev_ext['ur']:\n",
      "    for link in resp['parse']['externallinks']:\n",
      "        print (link)\n",
      "        if link not in link_first_rev:\n",
      "            link_first_rev['link'] = resp['parse']['revid']\n",
      "link_first_rev\n",
      "28/249:\n",
      "link_first_rev = {}\n",
      "\n",
      "for resp in rev_ext['ur']:\n",
      "    for link in resp['parse']['externallinks']:\n",
      "        print (link)\n",
      "        if link not in link_first_rev:\n",
      "            link_first_rev['link'] = resp['parse']['revid']\n",
      "link_first_rev\n",
      "28/250:\n",
      "link_first_rev = {}\n",
      "\n",
      "for resp in rev_ext['ur']:\n",
      "    for link in resp['parse']['externallinks']:\n",
      "        print (link)\n",
      "        if link not in link_first_rev:\n",
      "            link_first_rev['link'] = resp['parse']['revid']\n",
      "link_first_rev.keys()\n",
      "28/251:\n",
      "link_first_rev = {}\n",
      "\n",
      "for resp in rev_ext['ur']:\n",
      "    for link in resp['parse']['externallinks']:\n",
      "        print (link)\n",
      "        if link not in link_first_rev:\n",
      "            link_first_rev['link'] = resp['parse']['revid']\n",
      "link_first_rev.values\n",
      "28/252:\n",
      "link_first_rev = {}\n",
      "\n",
      "for resp in rev_ext['ur']:\n",
      "    for link in resp['parse']['externallinks']:\n",
      "\n",
      "        if link not in link_first_rev:\n",
      "            link_first_rev['link'] = resp['parse']['revid']\n",
      "link_first_rev\n",
      "28/253:\n",
      "link_first_rev = {}\n",
      "\n",
      "for resp in rev_ext['ur']:\n",
      "    for link in resp['parse']['externallinks']:\n",
      "\n",
      "        if link not in link_first_rev:\n",
      "            link_first_rev[link] = resp['parse']['revid']\n",
      "link_first_rev\n",
      "28/254:\n",
      "link_first_rev = defaultdict{dict}\n",
      "\n",
      "for lang in rev_ext\n",
      "    for resp in rev_ext[lang]:\n",
      "        for link in resp['parse']['externallinks']:\n",
      "            if link not in link_first_rev:\n",
      "                link_first_rev[lang][link] = resp['parse']['revid']\n",
      "link_first_rev\n",
      "28/255:\n",
      "link_first_rev = defaultdict{dict}\n",
      "\n",
      "for lang in rev_ext:\n",
      "    for resp in rev_ext[lang]:\n",
      "        for link in resp['parse']['externallinks']:\n",
      "            if link not in link_first_rev:\n",
      "                link_first_rev[lang][link] = resp['parse']['revid']\n",
      "link_first_rev\n",
      "28/256:\n",
      "link_first_rev = defaultdict(dict)\n",
      "\n",
      "for lang in rev_ext:\n",
      "    for resp in rev_ext[lang]:\n",
      "        for link in resp['parse']['externallinks']:\n",
      "            if link not in link_first_rev:\n",
      "                link_first_rev[lang][link] = resp['parse']['revid']\n",
      "link_first_rev\n",
      "28/257:\n",
      "rev_ext = defaultdict(list)\n",
      "\n",
      "req = {'action': 'parse',\n",
      "       'prop': 'externallinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "\n",
      "for idx, lang, title, revid, timestamp in rdf.loc[:, ['lang', 'title', 'revid', 'timestamp']].sort_values('timestamp').itertuples():\n",
      "    params = req.copy()\n",
      "    #params['title'] = title\n",
      "    #params['text'] = title\n",
      "    params['oldid'] = revid\n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    rev_ext[lang].append(resp)\n",
      "28/258: [l for l in ll]\n",
      "28/259:\n",
      "from functools import wraps\n",
      "\n",
      "def retry(times):\n",
      "\n",
      "    def wrapper_fn(f):\n",
      "\n",
      "        @wraps(f)\n",
      "        def new_wrapper(*args,**kwargs):\n",
      "            for i in range(times):\n",
      "                try:\n",
      "                    #print 'try %s' % (i + 1)\n",
      "                    return f(*args,**kwargs)\n",
      "                except Exception as e:\n",
      "                    error = e\n",
      "            raise error\n",
      "\n",
      "        return new_wrapper\n",
      "\n",
      "    return wrapper_fn\n",
      "\n",
      "@retry(3)\n",
      "def get_revisions_external_links(rev_ext, lang):\n",
      "    req = {'action': 'parse',\n",
      "       'prop': 'externallinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "    for idx, title, revid, timestamp in rdf.loc[rdf.lang==lang, ['title', 'revid', 'timestamp']].sort_values('timestamp').itertuples():\n",
      "        params = req.copy()\n",
      "        #params['title'] = title\n",
      "        #params['text'] = title\n",
      "        params['oldid'] = revid\n",
      "        resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "        rev_ext.append(resp)\n",
      "        \n",
      "import concurrent.futures\n",
      "\n",
      "rev_ext = defaultdict(list)\n",
      "\n",
      "for lang in ll:\n",
      "    with concurrent.futures.ThreadPoolExecutor(max_workers=40) as executor:\n",
      "        executor.submit(get_revisions_external_links, rev_ext[lang], lang)\n",
      "28/260:\n",
      "from functools import wraps\n",
      "\n",
      "def retry(times):\n",
      "\n",
      "    def wrapper_fn(f):\n",
      "\n",
      "        @wraps(f)\n",
      "        def new_wrapper(*args,**kwargs):\n",
      "            for i in range(times):\n",
      "                try:\n",
      "                    #print 'try %s' % (i + 1)\n",
      "                    return f(*args,**kwargs)\n",
      "                except Exception as e:\n",
      "                    error = e\n",
      "            raise error\n",
      "\n",
      "        return new_wrapper\n",
      "\n",
      "    return wrapper_fn\n",
      "\n",
      "@retry(3)\n",
      "def get_revisions_external_links(rev_ext, lang):\n",
      "    req = {'action': 'parse',\n",
      "       'prop': 'externallinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "    for idx, title, revid, timestamp in rdf.loc[rdf.lang==lang, ['title', 'revid', 'timestamp']].sort_values('timestamp').itertuples():\n",
      "        params = req.copy()\n",
      "        #params['title'] = title\n",
      "        #params['text'] = title\n",
      "        params['oldid'] = revid\n",
      "        resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "        rev_ext.append(resp)\n",
      "        \n",
      "import concurrent.futures\n",
      "\n",
      "rev_ext = defaultdict(list)\n",
      "\n",
      "for lang in ['ur', 'uk']:\n",
      "    with concurrent.futures.ThreadPoolExecutor(max_workers=40) as executor:\n",
      "        executor.submit(get_revisions_external_links, rev_ext[lang], lang)\n",
      "32/1:\n",
      "import wikipediaapi as wa\n",
      "import pandas as pd\n",
      "import networkx as nx\n",
      "32/2: wiki = wa.Wikipedia('he')\n",
      "32/3: zuk_he = wiki.page('מבצע_צוק_איתן')\n",
      "32/4:\n",
      "def print_langlinks(page):\n",
      "    langlinks = page.langlinks\n",
      "    for k in sorted(langlinks.keys()):\n",
      "        v = langlinks[k]\n",
      "        print(\"%s: %s - %s: %s\" % (k, v.language, v.title, v.fullurl))\n",
      "\n",
      "print_langlinks(zuk_he)\n",
      "32/5:\n",
      "def print_sections(sections, level=0):\n",
      "        for s in sections:\n",
      "                print(\"%s: %s - %s\" % (\"*\" * (level + 1), s.title, s.text[0:40]))\n",
      "                print_sections(s.sections, level + 1)\n",
      "#print_sections(zuk_he.sections)\n",
      "32/6: zuk_he.section_by_title('קישורים חיצוניים').sections[0]\n",
      "32/7:\n",
      "categories = {}\n",
      "for k in sorted(ll.keys()):\n",
      "    categories[k] = ll[k].categories\n",
      "32/8: zuk_he.links\n",
      "32/9:\n",
      "ll = zuk_he.langlinks\n",
      "ll['he'] = ll['ar'].langlinks['he']\n",
      "32/10: zuk_he.categories\n",
      "32/11:\n",
      "categories = {}\n",
      "for k in sorted(ll.keys()):\n",
      "    categories[k] = ll[k].categories\n",
      "32/12:\n",
      "from collections import Counter\n",
      "print({k: len(v) for k,v in  categories.items()})\n",
      "32/13: import requests\n",
      "32/14:\n",
      "ext_links = {}\n",
      "\n",
      "req = {'action': 'query',\n",
      "       'prop': 'extlinks',\n",
      "       'format': 'json',\n",
      "       'ellimit': 'max',}\n",
      "\n",
      "for k in sorted(ll.keys()):\n",
      "    v = ll[k]\n",
      "    params = req.copy()\n",
      "    params['titles'] = v.title\n",
      "    resp = requests.get(f'https://{k}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        eloffset = resp['continue']['eloffset']\n",
      "        resp = requests.get(f'https://{k}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    ext_links[k] = final_resp\n",
      "32/15: len(final_resp)\n",
      "32/16: ext_links['en']\n",
      "32/17:\n",
      "from collections import defaultdict\n",
      "\n",
      "flatten = lambda l: [item for sublist in l for item in sublist]\n",
      "\n",
      "clean_links = defaultdict(list)\n",
      "for lang in ext_links:\n",
      "    for resp in ext_links[lang]:\n",
      "        for page in resp['query']['pages']:\n",
      "            if 'extlinks' in resp['query']['pages'][page]:\n",
      "                links = resp['query']['pages'][page]['extlinks']\n",
      "                num = len(links)\n",
      "                clean_links[lang].extend(flatten([list(l.values()) for l in links]))\n",
      "            else: \n",
      "                clean_links[lang].extend([])\n",
      "                num=0\n",
      "                \n",
      "            title = resp['query']['pages'][page]['title']\n",
      "\n",
      "            print(f'{lang}: extlinks={num}, title: {title}')\n",
      "32/18: clean_links['en']\n",
      "32/19: lc = Counter(flatten(l for l in clean_links.values()))\n",
      "32/20: lc.most_common()\n",
      "32/21:\n",
      "for_cdf = []\n",
      "for lang in ll:\n",
      "    page = ll[lang]\n",
      "    for_cdf.append({'lang': lang, 'full_url': page.fullurl, 'title': page.title, 'summary': page.summary})\n",
      "    \n",
      "cdf = pd.DataFrame(for_cdf)\n",
      "cdf.head()\n",
      "32/22: cdf.to_csv('data/zuk_wiki_pages.csv', index=False)\n",
      "32/23:\n",
      "cl_for_df = []\n",
      "\n",
      "for lang in clean_links:\n",
      "    for link in clean_links[lang]:\n",
      "        cl_for_df.append({'lang': lang, 'link': link})\n",
      "\n",
      "ldf = pd.DataFrame(cl_for_df)\n",
      "ldf.head()\n",
      "32/24: ldf.to_csv('data/lang_extlinks.csv', index=False)\n",
      "32/25: ldf.lang.value_counts()\n",
      "32/26:\n",
      "g = nx.from_pandas_edgelist(ldf, 'lang', 'link', create_using=nx.DiGraph())\n",
      "\n",
      "nx.write_gexf(g, 'data/extlinks.gexf')\n",
      "32/27:\n",
      "from wikidata.client import Client\n",
      "client = Client()  # doctest: +SKIP\n",
      "entity = client.get('Q17324420', load=True)\n",
      "entity\n",
      "32/28: entity.description\n",
      "32/29: entity.data\n",
      "32/30: entity.data['labels']\n",
      "32/31:\n",
      "labels = pd.DataFrame([{'lang': v['language'], 'label': v['value']} for k, v in entity.data['labels'].items()])\n",
      "labels.head()\n",
      "32/32: labels.to_csv('data/wikidata_labels.csv', index=False)\n",
      "32/33: entity.data['aliases']\n",
      "32/34: flatten([[{'lang': v['language'], 'alias': v['value']} for v in l] for k, l in entity.data['aliases'].items()])\n",
      "32/35:\n",
      "aliases = pd.DataFrame(flatten([[{'lang': v['language'], 'alias': v['value']} for v in l] for k, l in entity.data['aliases'].items()]))\n",
      "aliases.head()\n",
      "32/36: aliases.to_csv('data/wikidata_aliases.csv', index=False)\n",
      "32/37:\n",
      "revisions = {}\n",
      "\n",
      "req = {'action': 'query',\n",
      "       'prop': 'revisions',\n",
      "       'format': 'json',\n",
      "       'rvlimit': 'max',\n",
      "       'rvprop': 'timestamp|user|comment|size|tags|flags|ids|roles',}\n",
      "\n",
      "for k in sorted(ll.keys()):\n",
      "    v = ll[k]\n",
      "    params = req.copy()\n",
      "    params['titles'] = v.title\n",
      "    resp = requests.get(f'https://{k}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        eloffset = resp['continue']['rvcontinue']\n",
      "        resp = requests.get(f'https://{k}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    revisions[k] = final_resp\n",
      "32/38: revisions['he'][0]\n",
      "32/39:\n",
      "clean_revs = defaultdict(list)\n",
      "for lang in revisions:\n",
      "    for resp in revisions[lang]:\n",
      "        for page in resp['query']['pages']:\n",
      "            if 'revisions' in resp['query']['pages'][page]:\n",
      "                revs = resp['query']['pages'][page]['revisions']\n",
      "                num = len(revs)\n",
      "                clean_revs[lang].extend(revs)\n",
      "            else: \n",
      "                clean_revs[lang].extend([])\n",
      "                num=0\n",
      "                \n",
      "            title = resp['query']['pages'][page]['title']\n",
      "\n",
      "            print(f'{lang}: revs={num}, title: {title}')\n",
      "32/40:\n",
      "cr_for_df = []\n",
      "\n",
      "for lang in clean_revs:\n",
      "    for rev in clean_revs[lang]:\n",
      "        tmp = rev.copy()\n",
      "        tmp['lang'] = lang\n",
      "        tmp['title'] = ll[lang].title\n",
      "        tmp['tags'] = ','.join(rev['tags'])\n",
      "        tmp['roles'] = ','.join(rev['roles'])\n",
      "        cr_for_df.append(tmp)\n",
      "\n",
      "rdf = (pd.DataFrame(cr_for_df)\n",
      "       .assign(timestamp = lambda x: pd.to_datetime(x.timestamp))\n",
      "      )\n",
      "\n",
      "rdf.head()\n",
      "32/41: rdf.to_csv('data/all_revisions_by_page_lang.csv', index=False)\n",
      "32/42:\n",
      "creation_dates = rdf[rdf.timestamp==rdf.groupby('lang').timestamp.transform(min)][['lang', 'timestamp']].sort_values('timestamp')\n",
      "creation_dates\n",
      "32/43: creation_dates.to_csv('data/pages_creation_dates.csv', index=False)\n",
      "32/44: rdf[rdf.lang=='hu']\n",
      "32/45:\n",
      "rev_cont = {}\n",
      "\n",
      "req = {'action': 'query',\n",
      "       'prop': 'revisions',\n",
      "       'format': 'json',\n",
      "       'rvlimit': 'max',\n",
      "       'rvprop': 'timestamp|user|comment|size|tags|content',}\n",
      "\n",
      "#for k in sorted(ll.keys()):\n",
      "for k in sorted(['he']):\n",
      "    v = ll[k]\n",
      "    params = req.copy()\n",
      "    params['titles'] = v.title\n",
      "    resp = requests.get(f'https://{k}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        eloffset = resp['continue']['rvcontinue']\n",
      "        resp = requests.get(f'https://{k}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    rev_cont[k] = final_resp\n",
      "32/46:\n",
      "from functools import wraps\n",
      "\n",
      "def retry(times):\n",
      "\n",
      "    def wrapper_fn(f):\n",
      "\n",
      "        @wraps(f)\n",
      "        def new_wrapper(*args,**kwargs):\n",
      "            for i in range(times):\n",
      "                try:\n",
      "                    #print 'try %s' % (i + 1)\n",
      "                    return f(*args,**kwargs)\n",
      "                except Exception as e:\n",
      "                    error = e\n",
      "            raise error\n",
      "\n",
      "        return new_wrapper\n",
      "\n",
      "    return wrapper_fn\n",
      "\n",
      "@retry(3)\n",
      "def get_revisions_external_links(rev_ext, lang):\n",
      "    req = {'action': 'parse',\n",
      "       'prop': 'externallinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "    for idx, title, revid, timestamp in rdf.loc[rdf.lang==lang, ['title', 'revid', 'timestamp']].sort_values('timestamp').itertuples():\n",
      "        params = req.copy()\n",
      "        #params['title'] = title\n",
      "        #params['text'] = title\n",
      "        params['oldid'] = revid\n",
      "        resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "        rev_ext.append(resp)\n",
      "        \n",
      "import concurrent.futures\n",
      "\n",
      "rev_ext = defaultdict(list)\n",
      "\n",
      "for lang in ['ur', 'uk']:\n",
      "    with concurrent.futures.ThreadPoolExecutor(max_workers=40) as executor:\n",
      "        executor.submit(get_revisions_external_links, rev_ext[lang], lang)\n",
      "32/47:\n",
      "from functools import wraps\n",
      "\n",
      "def retry(times):\n",
      "\n",
      "    def wrapper_fn(f):\n",
      "\n",
      "        @wraps(f)\n",
      "        def new_wrapper(*args,**kwargs):\n",
      "            for i in range(times):\n",
      "                try:\n",
      "                    #print 'try %s' % (i + 1)\n",
      "                    return f(*args,**kwargs)\n",
      "                except Exception as e:\n",
      "                    error = e\n",
      "            raise error\n",
      "\n",
      "        return new_wrapper\n",
      "\n",
      "    return wrapper_fn\n",
      "\n",
      "@retry(3)\n",
      "def get_revisions_external_links(rev_ext, lang):\n",
      "    req = {'action': 'parse',\n",
      "       'prop': 'externallinks',\n",
      "       'format': 'json',\n",
      "      }\n",
      "    for idx, title, revid, timestamp in rdf.loc[rdf.lang==lang, ['title', 'revid', 'timestamp']].sort_values('timestamp').itertuples():\n",
      "        params = req.copy()\n",
      "        #params['title'] = title\n",
      "        #params['text'] = title\n",
      "        params['oldid'] = revid\n",
      "        resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "        rev_ext.append(resp)\n",
      "        \n",
      "import concurrent.futures\n",
      "\n",
      "rev_ext = defaultdict(list)\n",
      "\n",
      "for lang in ll:\n",
      "    with concurrent.futures.ThreadPoolExecutor(max_workers=40) as executor:\n",
      "        executor.submit(get_revisions_external_links, rev_ext[lang], lang)\n",
      "32/48:\n",
      "@retry(3)\n",
      "def get_rev_external_links(lang, revid):\n",
      "    req = {'action': 'parse',\n",
      "       'prop': 'externallinks',\n",
      "       'format': 'json',\n",
      "       'oldid': revid\n",
      "      }\n",
      "    params = req.copy()\n",
      "    #params['title'] = title\n",
      "    #params['text'] = title\n",
      "    params['oldid'] = revid\n",
      "        \n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    return resp\n",
      "rev_ext = defaultdict(list)\n",
      "\n",
      "# We can use a with statement to ensure threads are cleaned up promptly\n",
      "with concurrent.futures.ThreadPoolExecutor(max_workers=30) as executor:\n",
      "    # Start the load operations and mark each future with its URL\n",
      "    future_to_parse = {executor.submit(get_rev_external_links, lang, revid): lang, revid, timestamp for idx, lang, title, revid, timestamp in rdf.loc[:, ['lang', 'title', 'revid', 'timestamp']].sort_values('timestamp').itertuples()}\n",
      "    for future in concurrent.futures.as_completed(future_to_parse):\n",
      "        lang, revid, timestamp = future_to_url[future]\n",
      "        rev_ext[lang].append(future)\n",
      "32/49:\n",
      "@retry(3)\n",
      "def get_rev_external_links(lang, revid):\n",
      "    req = {'action': 'parse',\n",
      "       'prop': 'externallinks',\n",
      "       'format': 'json',\n",
      "       'oldid': revid\n",
      "      }\n",
      "    params = req.copy()\n",
      "    #params['title'] = title\n",
      "    #params['text'] = title\n",
      "    params['oldid'] = revid\n",
      "        \n",
      "    resp = requests.get(f'http://{lang}.wikipedia.org/w/api.php', params = params).json()\n",
      "    return resp\n",
      "rev_ext = defaultdict(list)\n",
      "\n",
      "# We can use a with statement to ensure threads are cleaned up promptly\n",
      "with concurrent.futures.ThreadPoolExecutor(max_workers=30) as executor:\n",
      "    # Start the load operations and mark each future with its URL\n",
      "    future_to_parse = {executor.submit(get_rev_external_links, lang, revid): (lang, revid, timestamp) for idx, lang, title, revid, timestamp in rdf.loc[:, ['lang', 'title', 'revid', 'timestamp']].sort_values('timestamp').itertuples()}\n",
      "    for future in concurrent.futures.as_completed(future_to_parse):\n",
      "        lang, revid, timestamp = future_to_url[future]\n",
      "        rev_ext[lang].append(future)\n",
      "32/50: future_to_parse\n",
      "32/51:\n",
      "for future in concurrent.futures.as_completed(future_to_parse):\n",
      "    lang, revid, timestamp = future_to_parse[future]\n",
      "    rev_ext[lang].append(future)\n",
      "32/52: rev_ext.keys()\n",
      "32/53: len(rev_ext['he'])\n",
      "32/54:\n",
      "import gzip, pickle\n",
      "fp=gzip.open('data/revision_external_links.pkl','wb')\n",
      "pickle.dump(rev_ext,fp)\n",
      "fp.close()\n",
      "32/55:\n",
      "for future in concurrent.futures.as_completed(future_to_parse):\n",
      "    lang, revid, timestamp = future_to_parse[future]\n",
      "    rev_ext[lang].append(future.result())\n",
      "32/56:\n",
      "import gzip, pickle\n",
      "fp=gzip.open('data/revision_external_links.pkl','wb')\n",
      "pickle.dump(rev_ext,fp)\n",
      "fp.close()\n",
      "32/57: rev_ext['he'][0]\n",
      "32/58: dir(rev_ext['he'][0])\n",
      "32/59:\n",
      "for future in concurrent.futures.as_completed(future_to_parse):\n",
      "    lang, revid, timestamp = future_to_parse[future]\n",
      "    rev_ext[lang].append(future.result())\n",
      "32/60: dir(rev_ext['he'][0])\n",
      "32/61: rev_ext = defaultdict(list)\n",
      "32/62:\n",
      "for future in concurrent.futures.as_completed(future_to_parse):\n",
      "    lang, revid, timestamp = future_to_parse[future]\n",
      "    rev_ext[lang].append(future.result())\n",
      "32/63: dir(rev_ext['he'][0])\n",
      "32/64:\n",
      "import gzip, pickle\n",
      "fp=gzip.open('data/revision_external_links.pkl','wb')\n",
      "pickle.dump(rev_ext,fp)\n",
      "fp.close()\n",
      "32/65: rev_ext['he'][0]\n",
      "32/66: sorted_rev_ext = {lang: sorted(list_to_be_sorted, key=lambda k: k['parse']['revid']) for lang, srt in rev_ext.items()}\n",
      "32/67: sorted_rev_ext = {lang: sorted(srt, key=lambda k: k['parse']['revid']) for lang, srt in rev_ext.items()}\n",
      "32/68: sorted_rev_ext = {lang: srt for lang, srt in rev_ext.items()}\n",
      "32/69: rev_ext['he'][2]\n",
      "32/70: [srt.keys() for srt in rev_ext.values()]\n",
      "32/71: sorted_rev_ext = {lang: sorted(revs, key=lambda k: k['parse']['revid']) for lang, revs in rev_ext.items()}\n",
      "32/72:\n",
      "for lang, revs in rev_ext.items():\n",
      "    for rev in revs:\n",
      "        print rev.keys()\n",
      "32/73:\n",
      "for lang, revs in rev_ext.items():\n",
      "    for rev in revs:\n",
      "        print( rev.keys())\n",
      "32/74:\n",
      "for lang, revs in rev_ext.items():\n",
      "    for rev in revs:\n",
      "        if 'parse' not in rev.keys():\n",
      "            print rev['error']\n",
      "32/75:\n",
      "for lang, revs in rev_ext.items():\n",
      "    for rev in revs:\n",
      "        if 'parse' not in rev.keys():\n",
      "            print (rev['error'])\n",
      "32/76: sorted_rev_ext = {lang: sorted([rev for rev in revs if 'parse' in rev], key=lambda k: k['parse']['revid']) for lang, revs in rev_ext.items()}\n",
      "32/77:\n",
      "# this relies on the revisions being sorted beforehand \n",
      "link_first_rev = defaultdict(dict)\n",
      "\n",
      "for lang in rev_ext:\n",
      "    for resp in rev_ext[lang]:\n",
      "        for link in resp['parse']['externallinks']:\n",
      "            if link not in link_first_rev:\n",
      "                link_first_rev[lang][link] = resp['parse']['revid']\n",
      "                \n",
      "for_df = []\n",
      "for lang in link_first_rev:\n",
      "    for link, revid in link_first_rev[lang].items():\n",
      "        for_df.append({'lang': lang, 'link': link, 'revid': revid})\n",
      "        \n",
      "rev_link_df = pd.DataFrame(for_df)\n",
      "32/78:\n",
      "# this relies on the revisions being sorted beforehand \n",
      "link_first_rev = defaultdict(dict)\n",
      "\n",
      "for lang in rev_ext:\n",
      "    for resp in rev_ext[lang]:\n",
      "        if 'parse' in resp:\n",
      "            for link in resp['parse']['externallinks']:\n",
      "                if link not in link_first_rev:\n",
      "                    link_first_rev[lang][link] = resp['parse']['revid']\n",
      "                \n",
      "for_df = []\n",
      "for lang in link_first_rev:\n",
      "    for link, revid in link_first_rev[lang].items():\n",
      "        for_df.append({'lang': lang, 'link': link, 'revid': revid})\n",
      "        \n",
      "rev_link_df = pd.DataFrame(for_df)\n",
      "32/79: rev_link_df.head()\n",
      "32/80:\n",
      "# this relies on the revisions being sorted beforehand \n",
      "link_first_rev = defaultdict(dict)\n",
      "\n",
      "for lang in rev_ext:\n",
      "    for resp in sorted_rev_ext[lang]:\n",
      "        if 'parse' in resp:\n",
      "            for link in resp['parse']['externallinks']:\n",
      "                if link not in link_first_rev:\n",
      "                    link_first_rev[lang][link] = resp['parse']['revid']\n",
      "                \n",
      "for_df = []\n",
      "for lang in link_first_rev:\n",
      "    for link, revid in link_first_rev[lang].items():\n",
      "        for_df.append({'lang': lang, 'link': link, 'revid': revid})\n",
      "        \n",
      "rev_link_df = pd.DataFrame(for_df)\n",
      "32/81:\n",
      "# this relies on the revisions being sorted beforehand \n",
      "link_first_rev = defaultdict(dict)\n",
      "\n",
      "for lang in sorted_rev_ext:\n",
      "    for resp in sorted_rev_ext[lang]:\n",
      "        if 'parse' in resp:\n",
      "            for link in resp['parse']['externallinks']:\n",
      "                if link not in link_first_rev:\n",
      "                    link_first_rev[lang][link] = resp['parse']['revid']\n",
      "                \n",
      "for_df = []\n",
      "for lang in link_first_rev:\n",
      "    for link, revid in link_first_rev[lang].items():\n",
      "        for_df.append({'lang': lang, 'link': link, 'revid': revid})\n",
      "        \n",
      "rev_link_df = pd.DataFrame(for_df)\n",
      "32/82: rev_link_df.head()\n",
      "32/83: rev_link_df = rev_link_df.merge(rdf, how='left')\n",
      "32/84:\n",
      "rev_link_df = rev_link_df.merge(rdf, how='left')\n",
      "rev_link_df.head(20)\n",
      "32/85: rev_link_df.to_csv('data/rev_link_df.csv')\n",
      "32/86: rev_link_df.groupby('link').timestamp.min()\n",
      "32/87: rev_link_df.groupby('link').timestamp.min().to_df()\n",
      "32/88: rev_link_df.groupby('link').timestamp.min().todf()\n",
      "32/89: rev_link_df.groupby('link').timestamp.min().as_df()\n",
      "32/90: rev_link_df.groupby('link').timestamp.min().to_dataframe()\n",
      "32/91: rev_link_df.groupby('link').timestamp.min().to_frame()\n",
      "32/92: rev_link_df.groupby('link').timestamp.min().to_frame().sort_values(ascending=False)\n",
      "32/93: rev_link_df.groupby('link').timestamp.min().sort_values(ascending=False).to_frame()\n",
      "32/94: rev_link_df.to_pickle('data/rev_link_df.pkl.gz', compression='gzip')\n",
      "32/95: rev_link_df.groupby('link').timestamp.min().sort_values(ascending=False).to_frame().shape\n",
      "32/96: rev_link_df.groupby('link').timestamp.min().sort_values(ascending=False).to_frame()\n",
      "32/97: rev_link_df.groupby('link').timestamp.min().sort_values(ascending=False).to_frame().timestamp.hist()\n",
      "32/98:\n",
      "import wikipediaapi as wa\n",
      "import pandas as pd\n",
      "import networkx as nx\n",
      "import matplotlib.pyplot as plt\n",
      "32/99: rev_link_df.groupby('link').timestamp.min().sort_values(ascending=False).to_frame().timestamp.hist()\n",
      "32/100: rev_link_df.groupby('link').timestamp.min().sort_values(ascending=False).to_frame().timestamp.hist(bins=30)\n",
      "32/101: rev_link_df.groupby(['lang', 'link']).timestamp.min().sort_values(ascending=False).to_frame().timestamp.hist(bins=30)\n",
      "32/102: rev_link_df.groupby(['lang', 'link']).timestamp.min().sort_values(ascending=False).to_frame()\n",
      "32/103: rev_link_df[rev_link_df.lang=='fr'].groupby('link').timestamp.min().sort_values(ascending=False).to_frame()\n",
      "32/104: rev_link_df[rev_link_df.lang=='fr']\n",
      "32/105: rev_link_df[rev_link_df.revid==151919524]\n",
      "32/106: rev_link_df[rev_link_df.revid==151919524].shape\n",
      "32/107: rev_link_df[(rev_link_df.lang=='fr')&(rev_link_df.revid==151919524)].shape\n",
      "32/108: rev_link_df[(rev_link_df.lang=='fr')&(rev_link_df.revid==151919524)]\n",
      "32/109: rev_link_df[(rev_link_df.lang=='fr')].shape\n",
      "32/110: rev_link_df[(rev_link_df.lang=='fr')]\n",
      "32/111: rev_link_df[(rev_link_df.lang=='fr')].link.nunique()\n",
      "32/112: rev_link_df[(rev_link_df.lang=='fr')]\n",
      "32/113: rev_link_df.groupby('lang').link.nunique()\n",
      "32/114:\n",
      "# this relies on the revisions being sorted beforehand \n",
      "link_first_rev = defaultdict(dict)\n",
      "\n",
      "for lang in sorted_rev_ext:\n",
      "    for resp in sorted_rev_ext[lang]:\n",
      "        if 'parse' in resp:\n",
      "            print(resp['parse']['revid'])\n",
      "            for link in resp['parse']['externallinks']:\n",
      "                if link not in link_first_rev:\n",
      "                    link_first_rev[lang][link] = resp['parse']['revid']\n",
      "                \n",
      "for_df = []\n",
      "for lang in link_first_rev:\n",
      "    for link, revid in link_first_rev[lang].items():\n",
      "        for_df.append({'lang': lang, 'link': link, 'revid': revid})\n",
      "        \n",
      "rev_link_df = pd.DataFrame(for_df)\n",
      "32/115:\n",
      "# this relies on the revisions being sorted beforehand \n",
      "link_first_rev = defaultdict(dict)\n",
      "\n",
      "for lang in sorted_rev_ext:\n",
      "    for resp in sorted_rev_ext[lang]:\n",
      "        if 'parse' in resp:\n",
      "            for link in resp['parse']['externallinks']:\n",
      "                if link not in link_first_rev:\n",
      "                    link_first_rev[lang][link] = resp['parse']['revid']\n",
      "                \n",
      "for_df = []\n",
      "for lang in link_first_rev:\n",
      "    for link, revid in link_first_rev[lang].items():\n",
      "        for_df.append({'lang': lang, 'link': link, 'revid': revid})\n",
      "        \n",
      "rev_link_df = pd.DataFrame(for_df)\n",
      "32/116:\n",
      "rev_link_df = rev_link_df.merge(rdf, how='left')\n",
      "rev_link_df.head(20)\n",
      "32/117:\n",
      "full_df = []\n",
      "for lang in sorted_rev_ext:\n",
      "    for resp in sorted_rev_ext[lang]:\n",
      "        if 'parse' in resp:\n",
      "            for link in resp['parse']['externallinks']:\n",
      "                full_df.append({'lang': lang, 'link': link, 'revid': resp['parse']['revid']})\n",
      "        \n",
      "full_rev_link_df = pd.DataFrame(full_df)\n",
      "32/118: full_rev_link_df.shape\n",
      "32/119:\n",
      "frl = full_rev_link_df\n",
      "frl[frl.lang=='he'].groupby('link').timestamp.min()\n",
      "32/120:\n",
      "frl = full_rev_link_df.merge(rdf[['lang', 'revid', 'timestamp']], how='left')\n",
      "frl[frl.lang=='he'].groupby('link').timestamp.min()\n",
      "32/121: frl[frl.lang=='he'].groupby('link').timestamp.min().sort_values(ascending=False).to_df()\n",
      "32/122: frl[frl.lang=='he'].groupby('link').timestamp.min().sort_values(ascending=False).to_frame()\n",
      "32/123: frl[frl.lang=='cy'].groupby('link').timestamp.min().sort_values(ascending=False).to_frame()\n",
      "32/124: frl[frl.lang=='en'].groupby('link').timestamp.min().sort_values(ascending=False).to_frame()\n",
      "32/125: frl[frl.lang=='fr'].groupby('link').timestamp.min().sort_values(ascending=False).to_frame()\n",
      "32/126: frl[frl.lang=='fr'].groupby('link').timestamp.min().sort_values(ascending=False).hist(bins=30)\n",
      "32/127: frl.to_pickle('data/rev_link_df.pkl.gz', compression='gzip')\n",
      "32/128: frl.to_csv('data/rev_link_df.csv.gz', compression='gzip')\n",
      "32/129: frl.groupby('link').timestamp.min().sort_values(ascending=False).hist(bins=30)\n",
      "32/130: frl.groupby(['lang', 'link']).timestamp.min().sort_values(by=['lang', 'creation_date'], ascending=False)\n",
      "32/131: frl.groupby(['lang', 'link']).timestamp.min().reset_index().sort_values(by=['lang', 'timestamp'], ascending=False)\n",
      "32/132: frl.groupby(['lang', 'link']).timestamp.min().reset_index().sort_values(by=['lang', 'timestamp'], ascending=False).to_csv('data/links_creation_date.csv', index=False)\n",
      "32/133:\n",
      "(frl\n",
      " .assign(timestamp=lambda x: x.timestamp.astype(int))\n",
      " .groupby(['lang', 'link'])\n",
      " .timestamp.min().reset_index().sort_values(by=['lang', 'timestamp'], ascending=False).to_csv('data/links_creation_date_int.csv', index=False)\n",
      "32/134:\n",
      "(frl\n",
      " .assign(timestamp=lambda x: x.timestamp.astype(int))\n",
      " .groupby(['lang', 'link'])\n",
      " .timestamp.min()\n",
      " .reset_index()\n",
      " .sort_values(by=['lang', 'timestamp'], ascending=False)\n",
      " .to_csv('data/links_creation_date_int.csv', index=False))\n",
      "32/135: frl.groupby('link').timestamp.min().resample(freq='month').plot(kind='bar')\n",
      "32/136: frl.groupby('link').timestamp.min().resample('month').plot(kind='bar')\n",
      "32/137: frl.groupby('link').timestamp.min().to_frame().resample('month', on='timestamp').plot(kind='bar')\n",
      "32/138: frl.groupby('link').timestamp.min().to_frame().resample('M', on='timestamp').plot(kind='bar')\n",
      "32/139: frl.groupby('link').timestamp.min().to_frame().resample('M', on='timestamp')\n",
      "32/140: frl.groupby('link').timestamp.min().to_frame().resample('M', on='timestamp').size()\n",
      "32/141: frl.groupby('link').timestamp.min().to_frame().resample('M', on='timestamp').size().plot(kind='bar')\n",
      "32/142:\n",
      "import wikipediaapi as wa\n",
      "import pandas as pd\n",
      "import networkx as nx\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "import seaborn as sns\n",
      "32/143:\n",
      "import wikipediaapi as wa\n",
      "import pandas as pd\n",
      "import networkx as nx\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "import seaborn as sns\n",
      "sns.set(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
      "sns.set_context(\"talk\")\n",
      "sns.set_palette('Set2', 10)\n",
      "32/144: frl.groupby('link').timestamp.min().to_frame().resample('M', on='timestamp').size().plot(kind='bar')\n",
      "32/145: frl.groupby('link').timestamp.min().to_frame().resample('M', on='timestamp').size().plot(kind='bar', figsize=(10,7))\n",
      "32/146:\n",
      "fig, ax = plt.subplots(figsize=(10,7))\n",
      "frl.groupby('link').timestamp.min().to_frame().resample('M', on='timestamp').size().plot(kind='bar', ax=ax)\n",
      "\n",
      "import matplotlib.dates as mdates\n",
      "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
      "32/147:\n",
      "fig, ax = plt.subplots(figsize=(10,7))\n",
      "frl.groupby('link').timestamp.min().to_frame().resample('M', on='timestamp').size().plot(kind='bar', ax=ax)\n",
      "\n",
      "import matplotlib.dates as mdates\n",
      "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
      "fig.autofmt_xdate()\n",
      "32/148:\n",
      "fig, ax = plt.subplots(figsize=(10,7))\n",
      "frl.groupby('link').timestamp.min().to_frame().resample('M', on='timestamp').size().plot(kind='bar', ax=ax)\n",
      "32/149:\n",
      "fig, ax = plt.subplots(figsize=(10,7))\n",
      "frl.groupby('link').timestamp.min().to_frame().resample('M', on='timestamp').size().plot(kind='bar', ax=ax)\n",
      "\n",
      "import matplotlib.dates as mdates\n",
      "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
      "fig.autofmt_xdate()\n",
      "32/150:\n",
      "fig, ax = plt.subplots(figsize=(10,7))\n",
      "frl.groupby('link').timestamp.min().to_frame().resample('M', on='timestamp').size().plot(kind='bar', ax=ax)\n",
      "\n",
      "import matplotlib.dates as mdates\n",
      "ax.xaxis_date()\n",
      "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
      "\n",
      "fig.autofmt_xdate()\n",
      "32/151:\n",
      "fig, ax = plt.subplots(figsize=(10,7))\n",
      "frl.groupby('link').timestamp.min().to_frame().resample('M', on='timestamp').size().plot(kind='bar', ax=ax)\n",
      "\n",
      "import matplotlib.dates as mdates\n",
      "#set ticks every week\n",
      "ax.xaxis.set_major_locator(mdates.WeekdayLocator())\n",
      "#set major ticks format\n",
      "ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))\n",
      "32/152:\n",
      "fig, ax = plt.subplots(figsize=(10,7))\n",
      "frl.groupby('link').timestamp.min().to_frame().resample('M', on='timestamp').size().plot(kind='bar', ax=ax)\n",
      "\n",
      "import matplotlib.dates as mdates\n",
      "#set ticks every week\n",
      "ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
      "#set major ticks format\n",
      "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
      "32/153:\n",
      "fig, ax = plt.subplots(figsize=(10,7))\n",
      "frl.groupby('link').timestamp.min().to_frame().resample('M', on='timestamp').size().to_frame()plot(kind='bar', ax=ax)\n",
      "\n",
      "import matplotlib.dates as mdates\n",
      "#set ticks every week\n",
      "ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
      "#set major ticks format\n",
      "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
      "32/154:\n",
      "fig, ax = plt.subplots(figsize=(10,7))\n",
      "frl.groupby('link').timestamp.min().to_frame().resample('M', on='timestamp').size().to_frame().plot(kind='bar', ax=ax)\n",
      "\n",
      "import matplotlib.dates as mdates\n",
      "#set ticks every week\n",
      "ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
      "#set major ticks format\n",
      "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/155:\n",
      "fig, ax = plt.subplots(figsize=(10,7))\n",
      "frl.groupby('link').timestamp.min().to_frame().resample('M', on='timestamp').size().to_frame().plot(kind='bar', ax=ax, legend=False)\n",
      "\n",
      "import matplotlib.dates as mdates\n",
      "#set ticks every week\n",
      "ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
      "#set major ticks format\n",
      "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
      "32/156:\n",
      "fig, ax = plt.subplots(figsize=(10,7))\n",
      "frl.groupby('link').timestamp.min().to_frame().resample('M', on='timestamp').size().to_frame().plot(kind='bar', ax=ax, legend=False)\n",
      "\n",
      "import matplotlib.dates as mdates\n",
      "#set ticks every week\n",
      "ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
      "#set major ticks format\n",
      "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
      "32/157:\n",
      "fig, ax = plt.subplots(figsize=(10,7))\n",
      "frl.groupby('link').timestamp.min().to_frame().resample('M', on='timestamp').size().to_frame().plot(kind='bar', ax=ax, legend=False)\n",
      "\n",
      "import matplotlib.dates as mdates\n",
      "#set ticks every month\n",
      "ax.xaxis.set_major_locator(mdates.YearLocator())\n",
      "#set major ticks format\n",
      "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
      "32/158:\n",
      "fig, ax = plt.subplots(figsize=(10,7))\n",
      "frl.groupby('link').timestamp.min().to_frame().resample('M', on='timestamp').size().to_frame()\n",
      "32/159:\n",
      "fig, ax = plt.subplots(figsize=(10,7))\n",
      "frl.groupby('link').timestamp.min().to_frame().resample('M', on='timestamp').size().to_frame().index\n",
      "32/160:\n",
      "fig, ax = plt.subplots(figsize=(10,7))\n",
      "frl.groupby('link').timestamp.min().to_frame().resample('M', on='timestamp').size().to_frame().plot(kind='bar', ax=ax, legend=False)\n",
      "\n",
      "import matplotlib.dates as mdates\n",
      "#set ticks every month\n",
      "ax.xaxis.set_major_locator(mdates.YearLocator())\n",
      "#set major ticks format\n",
      "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
      "32/161:\n",
      "import altair as alt\n",
      "\n",
      "\n",
      "link_appear = frl.groupby('link').timestamp.min().to_frame().resample('M', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "\n",
      "alt.Chart(link_appear).mark_bar().encode(\n",
      "    x='timestamp:T',\n",
      "    y='links:O'\n",
      ")\n",
      "32/162:\n",
      "import altair as alt\n",
      "alt.renderers('notebook')\n",
      "\n",
      "link_appear = frl.groupby('link').timestamp.min().to_frame().resample('M', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "\n",
      "alt.Chart(link_appear).mark_bar().encode(\n",
      "    x='timestamp:T',\n",
      "    y='links:O'\n",
      ")\n",
      "32/163:\n",
      "import altair as alt\n",
      "alt.renderers.enable('notebook')\n",
      "\n",
      "link_appear = frl.groupby('link').timestamp.min().to_frame().resample('M', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "\n",
      "alt.Chart(link_appear).mark_bar().encode(\n",
      "    x='timestamp:T',\n",
      "    y='links:O'\n",
      ")\n",
      "32/164:\n",
      "import altair as alt\n",
      "alt.renderers.enable('notebook')\n",
      "\n",
      "link_appear = frl.groupby('link').timestamp.min().to_frame().resample('M', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "\n",
      "alt.Chart(link_appear).mark_bar().encode(\n",
      "    x='timestamp:T',\n",
      "    y='links:Q'\n",
      ")\n",
      "32/165:\n",
      "first_link_appear = frl.groupby('link').timestamp.min().to_frame().resample('D', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "alt.Chart(link_appear[(first_link_appear.timestamp>'2014-07-05') & (first_link_appear.timestamp<'2014-09-01')]).mark_bar().encode(\n",
      "    x='timestamp:T',\n",
      "    y='links:Q'\n",
      ").properties\n",
      "32/166:\n",
      "first_link_appear = frl.groupby('link').timestamp.min().to_frame().resample('D', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "alt.Chart(first_link_appear[(first_link_appear.timestamp>'2014-07-05') & (first_link_appear.timestamp<'2014-09-01')]).mark_bar().encode(\n",
      "    x='timestamp:T',\n",
      "    y='links:Q'\n",
      ").properties\n",
      "32/167:\n",
      "first_link_appear = frl.groupby('link').timestamp.min().to_frame().resample('D', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "alt.Chart(first_link_appear[(first_link_appear.timestamp>'2014-07-05') & (first_link_appear.timestamp<'2014-09-01')]).mark_bar().encode(\n",
      "    x='timestamp:T',\n",
      "    y='links:Q'\n",
      ")\n",
      "32/168:\n",
      "first_link_appear = frl.groupby('link').timestamp.min().to_frame().resample('D', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "alt.Chart(first_link_appear[(first_link_appear.timestamp>'2014-07-05') & (first_link_appear.timestamp<'2014-09-01')]).mark_bar().encode(\n",
      "    x='timestamp:T',\n",
      "    y='links:Q'\n",
      ").properties(width=800, height=600)\n",
      "32/169:\n",
      "creation_dates_day = creation_dates.resample('D', on='timestamp').size().reset_index()\n",
      "creation_dates_day\n",
      "32/170:\n",
      "creation_dates_day = creation_dates.resample('D', on='timestamp').size().reset_index().rename(columns={0:'pages'})\n",
      "creation_dates_day\n",
      "32/171:\n",
      "creation_dates_day = creation_dates.resample('D', on='timestamp').size().reset_index().rename(columns={0:'pages_created'})\n",
      "creation_dates_day['pages'] = creation_dates_day.pages_created.cumsum()\n",
      "32/172:\n",
      "creation_dates_day = creation_dates.resample('D', on='timestamp').size().reset_index().rename(columns={0:'pages_created'})\n",
      "creation_dates_day['pages'] = creation_dates_day.pages_created.cumsum()\n",
      "creation_dates_day\n",
      "32/173:\n",
      "first_link_appear = frl.groupby('link').timestamp.min().to_frame().resample('D', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "bar = alt.Chart(first_link_appear[(first_link_appear.timestamp>'2014-07-05') & (first_link_appear.timestamp<'2014-09-01')]).mark_bar().encode(\n",
      "    x='timestamp:T',\n",
      "    y='links:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "line = alt.Chart(creation_dates_day[(creation_dates_day.timestamp>'2014-07-05') & (creation_dates_day.timestamp<'2014-09-01')]).mark_line().encode(\n",
      "    x='timestamp:T',\n",
      "    y='pages:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "alt.layer(\n",
      "    bar,\n",
      "    line\n",
      ").resolve_scale(\n",
      "    y='independent'\n",
      ")\n",
      "32/174:\n",
      "first_link_appear = frl.groupby('link').timestamp.min().to_frame().resample('D', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "bar = alt.Chart(first_link_appear[(first_link_appear.timestamp>'2014-07-05') & (first_link_appear.timestamp<'2014-09-01')]).mark_bar().encode(\n",
      "    x='timestamp:T',\n",
      "    y='links:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "line = alt.Chart(creation_dates_day[(creation_dates_day.timestamp>'2014-07-05') & (creation_dates_day.timestamp<'2014-09-01')]).mark_line(color='red').encode(\n",
      "    x='timestamp:T',\n",
      "    y='pages:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "alt.layer(\n",
      "    bar,\n",
      "    line\n",
      ").resolve_scale(\n",
      "    y='independent'\n",
      ")\n",
      "32/175:\n",
      "first_link_appear = frl.groupby('link').timestamp.min().to_frame().resample('D', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "bar = alt.Chart(first_link_appear[(first_link_appear.timestamp>'2014-07-05') & (first_link_appear.timestamp<'2014-09-01')]).mark_bar().encode(\n",
      "    x='timestamp:T',\n",
      "    y='links:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "line = alt.Chart(creation_dates_day[(creation_dates_day.timestamp>'2014-07-05') & (creation_dates_day.timestamp<'2014-09-01')]).mark_line(color='red').encode(\n",
      "    x='timestamp:T',\n",
      "    y='pages_created:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "alt.layer(\n",
      "    bar,\n",
      "    line\n",
      ").resolve_scale(\n",
      "    y='independent'\n",
      ")\n",
      "32/176:\n",
      "first_link_appear = frl.groupby('link').timestamp.min().to_frame().resample('D', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "bar = alt.Chart(first_link_appear[(first_link_appear.timestamp>'2014-07-05') & (first_link_appear.timestamp<'2014-09-01')]).mark_bar().encode(\n",
      "    x='timestamp:T',\n",
      "    y='links:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "line = alt.Chart(creation_dates_day[(creation_dates_day.timestamp>'2014-07-05') & (creation_dates_day.timestamp<'2014-09-01')]).mark_line(color='red').encode(\n",
      "    x='timestamp:T',\n",
      "    y='pages:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "alt.layer(\n",
      "    bar,\n",
      "    line\n",
      ").resolve_scale(\n",
      "    y='independent'\n",
      ")\n",
      "32/177:\n",
      "first_link_appear = frl.groupby('link').timestamp.min().to_frame().resample('D', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "bar = alt.Chart(first_link_appear[(first_link_appear.timestamp>'2014-07-05') & (first_link_appear.timestamp<'2014-09-01')]).mark_bar().encode(\n",
      "    x='timestamp:T',\n",
      "    y='links:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "line = alt.Chart(creation_dates_day[(creation_dates_day.timestamp>'2014-07-05') & (creation_dates_day.timestamp<'2014-09-01')]).mark_line(color='red').encode(\n",
      "    x='timestamp:T',\n",
      "    y='pages:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "alt.layer(\n",
      "    bar,\n",
      "    line\n",
      ").resolve_scale(\n",
      "    y='independent'\n",
      ").properties(title='Protective Edge Wikipedia pages count and external link apperances')\n",
      "32/178:\n",
      "first_link_appear = frl.groupby('link').timestamp.min().to_frame().resample('D', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "bar = alt.Chart(first_link_appear[(first_link_appear.timestamp>='2014-07-05') & (first_link_appear.timestamp<'2014-09-01')]).mark_bar().encode(\n",
      "    x='timestamp:T',\n",
      "    y='links:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "line = alt.Chart(creation_dates_day[(creation_dates_day.timestamp>='2014-07-05') & (creation_dates_day.timestamp<'2014-09-01')]).mark_line(color='red').encode(\n",
      "    x='timestamp:T',\n",
      "    y='pages:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "alt.layer(\n",
      "    bar,\n",
      "    line\n",
      ").resolve_scale(\n",
      "    y='independent'\n",
      ").properties(title='Protective Edge Wikipedia pages count and external link apperances')\n",
      "32/179:\n",
      "first_link_appear = frl.groupby('link').timestamp.min().to_frame().resample('D', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "bar = alt.Chart(first_link_appear[(first_link_appear.timestamp>='2014-07-05') & (first_link_appear.timestamp<'2014-10-01')]).mark_bar().encode(\n",
      "    x='timestamp:T',\n",
      "    y='links:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "line = alt.Chart(creation_dates_day[(creation_dates_day.timestamp>='2014-07-05') & (creation_dates_day.timestamp<'2014-10-01')]).mark_line(color='red').encode(\n",
      "    x='timestamp:T',\n",
      "    y='pages:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "alt.layer(\n",
      "    bar,\n",
      "    line\n",
      ").resolve_scale(\n",
      "    y='independent'\n",
      ").properties(title='Protective Edge Wikipedia pages count and external link apperances')\n",
      "32/180:\n",
      "first_link_appear = frl.groupby('link').timestamp.min().to_frame().resample('D', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "first_link_appear['link_accum'] = first_link_appear.link.cumsum()\n",
      "\n",
      "bar = alt.Chart(first_link_appear[(first_link_appear.timestamp>='2014-07-05') & (first_link_appear.timestamp<'2014-10-01')]).mark_bar().encode(\n",
      "    x='timestamp:T',\n",
      "    y='link_accum:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "line = alt.Chart(creation_dates_day[(creation_dates_day.timestamp>='2014-07-05') & (creation_dates_day.timestamp<'2014-10-01')]).mark_line(color='red').encode(\n",
      "    x='timestamp:T',\n",
      "    y='pages:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "alt.layer(\n",
      "    bar,\n",
      "    line\n",
      ").resolve_scale(\n",
      "    y='independent'\n",
      ").properties(title='Protective Edge Wikipedia pages count and external link apperances')\n",
      "32/181:\n",
      "first_link_appear = frl.groupby('link').timestamp.min().to_frame().resample('D', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "first_link_appear['link_accum'] = first_link_appear.links.cumsum()\n",
      "\n",
      "bar = alt.Chart(first_link_appear[(first_link_appear.timestamp>='2014-07-05') & (first_link_appear.timestamp<'2014-10-01')]).mark_bar().encode(\n",
      "    x='timestamp:T',\n",
      "    y='link_accum:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "line = alt.Chart(creation_dates_day[(creation_dates_day.timestamp>='2014-07-05') & (creation_dates_day.timestamp<'2014-10-01')]).mark_line(color='red').encode(\n",
      "    x='timestamp:T',\n",
      "    y='pages:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "alt.layer(\n",
      "    bar,\n",
      "    line\n",
      ").resolve_scale(\n",
      "    y='independent'\n",
      ").properties(title='Protective Edge Wikipedia pages count and external link apperances')\n",
      "32/182:\n",
      "first_link_appear = frl.groupby('link').timestamp.min().to_frame().resample('D', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "first_link_appear['link_accum'] = first_link_appear.links.cumsum()\n",
      "\n",
      "bar = alt.Chart(first_link_appear[(first_link_appear.timestamp>='2014-07-05') & (first_link_appear.timestamp<'2014-11-01')]).mark_bar().encode(\n",
      "    x='timestamp:T',\n",
      "    y='link_accum:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "line = alt.Chart(creation_dates_day[(creation_dates_day.timestamp>='2014-07-05') & (creation_dates_day.timestamp<'2014-11-01')]).mark_line(color='red').encode(\n",
      "    x='timestamp:T',\n",
      "    y='pages:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "alt.layer(\n",
      "    bar,\n",
      "    line\n",
      ").resolve_scale(\n",
      "    y='independent'\n",
      ").properties(title='Protective Edge Wikipedia pages count and external link apperances')\n",
      "32/183:\n",
      "first_link_appear = frl.groupby('link').timestamp.min().to_frame().resample('D', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "first_link_appear['link_accum'] = first_link_appear.links.cumsum()\n",
      "\n",
      "bar = alt.Chart(first_link_appear[(first_link_appear.timestamp>='2014-07-05') & (first_link_appear.timestamp<'2014-12-01')]).mark_bar().encode(\n",
      "    x='timestamp:T',\n",
      "    y='link_accum:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "line = alt.Chart(creation_dates_day[(creation_dates_day.timestamp>='2014-07-05') & (creation_dates_day.timestamp<'2014-12-01')]).mark_line(color='red').encode(\n",
      "    x='timestamp:T',\n",
      "    y='pages:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "alt.layer(\n",
      "    bar,\n",
      "    line\n",
      ").resolve_scale(\n",
      "    y='independent'\n",
      ").properties(title='Protective Edge Wikipedia pages count and external link apperances')\n",
      "32/184:\n",
      "first_link_appear = frl.groupby('link').timestamp.min().to_frame().resample('D', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "first_link_appear['link_accum'] = first_link_appear.links.cumsum()\n",
      "\n",
      "bar = alt.Chart(first_link_appear[(first_link_appear.timestamp>='2014-07-05') & (first_link_appear.timestamp<'2015-12-01')]).mark_bar().encode(\n",
      "    x='timestamp:T',\n",
      "    y='link_accum:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "line = alt.Chart(creation_dates_day[(creation_dates_day.timestamp>='2014-07-05') & (creation_dates_day.timestamp<'2015-12-01')]).mark_line(color='red').encode(\n",
      "    x='timestamp:T',\n",
      "    y='pages:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "alt.layer(\n",
      "    bar,\n",
      "    line\n",
      ").resolve_scale(\n",
      "    y='independent'\n",
      ").properties(title='Protective Edge Wikipedia pages count and external link apperances')\n",
      "32/185:\n",
      "first_link_appear = frl.groupby('link').timestamp.min().to_frame().resample('D', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "first_link_appear['link_accum'] = first_link_appear.links.cumsum()\n",
      "\n",
      "bar = alt.Chart(first_link_appear[(first_link_appear.timestamp>='2014-07-05') & (first_link_appear.timestamp<'2014-11-01')]).mark_bar().encode(\n",
      "    x='timestamp:T',\n",
      "    y='link_accum:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "line = alt.Chart(creation_dates_day[(creation_dates_day.timestamp>='2014-07-05') & (creation_dates_day.timestamp<'2014-11-01')]).mark_line(color='red').encode(\n",
      "    x='timestamp:T',\n",
      "    y='pages:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "alt.layer(\n",
      "    bar,\n",
      "    line\n",
      ").resolve_scale(\n",
      "    y='independent'\n",
      ").properties(title='Protective Edge Wikipedia pages count and external link apperances')\n",
      "32/186:\n",
      "creation_dates_day = creation_dates.resample('D', on='timestamp').size().reset_index().rename(columns={0:'pages_created'})\n",
      "creation_dates_day['pages'] = creation_dates_day.pages_created.cumsum()\n",
      "creation_dates_day['pages_pct_change'] = creation_dates_day.pages.pct_change()\n",
      "\n",
      "creation_dates_day\n",
      "32/187:\n",
      "creation_dates_day = creation_dates.resample('D', on='timestamp').size().reset_index().rename(columns={0:'pages_created'})\n",
      "creation_dates_day['pages'] = creation_dates_day.pages_created.cumsum()\n",
      "creation_dates_day['pages_pct_change'] = creation_dates_day.pages.pct_change()\n",
      "\n",
      "creation_dates_day\n",
      "32/188:\n",
      "creation_dates_day = creation_dates.resample('D', on='timestamp').size().reset_index().rename(columns={0:'pages_created'})\n",
      "creation_dates_day['pages'] = creation_dates_day.pages_created.cumsum()\n",
      "\n",
      "creation_dates_day\n",
      "32/189:\n",
      "creation_dates_day = creation_dates.resample('D', on='timestamp').size().reset_index().rename(columns={0:'pages_created'})\n",
      "creation_dates_day['pages'] = creation_dates_day.pages_created.cumsum()\n",
      "creation_dates_day['pages_pct_change'] = creation_dates_day.pages.pct_change()\n",
      "\n",
      "creation_dates_day\n",
      "32/190:\n",
      "first_link_appear = frl.groupby('link').timestamp.min().to_frame().resample('D', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "first_link_appear['link_accum'] = first_link_appear.links.cumsum()\n",
      "first_link_appear['link_pct_change'] = first_link_appear.link_accum.pct_change()\n",
      "\n",
      "bar = alt.Chart(first_link_appear[(first_link_appear.timestamp>='2014-07-05') & (first_link_appear.timestamp<'2014-11-01')]).mark_bar().encode(\n",
      "    x='timestamp:T',\n",
      "    y='link_pct_change:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "line = alt.Chart(creation_dates_day[(creation_dates_day.timestamp>='2014-07-05') & (creation_dates_day.timestamp<'2014-11-01')]).mark_line(color='red').encode(\n",
      "    x='timestamp:T',\n",
      "    y='pages_pct_change:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "alt.layer(\n",
      "    bar,\n",
      "    line\n",
      ").resolve_scale(\n",
      "    y='independent'\n",
      ").properties(title='Protective Edge Wikipedia pages count and external link apperances')\n",
      "32/191:\n",
      "first_link_appear = frl.groupby('link').timestamp.min().to_frame().resample('D', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "first_link_appear['link_accum'] = first_link_appear.links.cumsum()\n",
      "first_link_appear['link_pct'] = first_link_appear.link_accum / first_link_appear.link_accum.max()\n",
      "\n",
      "\n",
      "bar = alt.Chart(first_link_appear[(first_link_appear.timestamp>='2014-07-05') & (first_link_appear.timestamp<'2014-11-01')]).mark_bar().encode(\n",
      "    x='timestamp:T',\n",
      "    y='link_pct:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "line = alt.Chart(creation_dates_day[(creation_dates_day.timestamp>='2014-07-05') & (creation_dates_day.timestamp<'2014-11-01')]).mark_line(color='red').encode(\n",
      "    x='timestamp:T',\n",
      "    y='pages_pct:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "alt.layer(\n",
      "    bar,\n",
      "    line\n",
      ").resolve_scale(\n",
      "    y='independent'\n",
      ").properties(title='Protective Edge Wikipedia pages count and external link apperances')\n",
      "32/192:\n",
      "creation_dates_day = creation_dates.resample('D', on='timestamp').size().reset_index().rename(columns={0:'pages_created'})\n",
      "creation_dates_day['pages'] = creation_dates_day.pages_created.cumsum()\n",
      "creation_dates_day['pages_pct'] = creation_dates_day.pages / creations_dates_day.pages.max()\n",
      "\n",
      "creation_dates_day\n",
      "32/193:\n",
      "creation_dates_day = creation_dates.resample('D', on='timestamp').size().reset_index().rename(columns={0:'pages_created'})\n",
      "creation_dates_day['pages'] = creation_dates_day.pages_created.cumsum()\n",
      "creation_dates_day['pages_pct'] = creation_dates_day.pages / creation_dates_day.pages.max()\n",
      "\n",
      "creation_dates_day\n",
      "32/194:\n",
      "first_link_appear = frl.groupby('link').timestamp.min().to_frame().resample('D', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "first_link_appear['link_accum'] = first_link_appear.links.cumsum()\n",
      "first_link_appear['link_pct'] = first_link_appear.link_accum / first_link_appear.link_accum.max()\n",
      "\n",
      "\n",
      "bar = alt.Chart(first_link_appear[(first_link_appear.timestamp>='2014-07-05') & (first_link_appear.timestamp<'2014-11-01')]).mark_bar().encode(\n",
      "    x='timestamp:T',\n",
      "    y='link_pct:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "line = alt.Chart(creation_dates_day[(creation_dates_day.timestamp>='2014-07-05') & (creation_dates_day.timestamp<'2014-11-01')]).mark_line(color='red').encode(\n",
      "    x='timestamp:T',\n",
      "    y='pages_pct:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "alt.layer(\n",
      "    bar,\n",
      "    line\n",
      ").resolve_scale(\n",
      "    y='independent'\n",
      ").properties(title='Protective Edge Wikipedia pages count and external link apperances')\n",
      "32/195:\n",
      "first_link_appear = frl.groupby('link').timestamp.min().to_frame().resample('D', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "first_link_appear['link_accum'] = first_link_appear.links.cumsum()\n",
      "first_link_appear['link_pct'] = first_link_appear.link_accum / first_link_appear.link_accum.max()\n",
      "\n",
      "\n",
      "bar = alt.Chart(first_link_appear[(first_link_appear.timestamp>='2014-07-05') & (first_link_appear.timestamp<'2014-11-01')]).mark_bar().encode(\n",
      "    x='timestamp:T',\n",
      "    y='link_pct:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "line = alt.Chart(creation_dates_day[(creation_dates_day.timestamp>='2014-07-05') & (creation_dates_day.timestamp<'2014-11-01')]).mark_line(color='red').encode(\n",
      "    x='timestamp:T',\n",
      "    y='pages_pct:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "alt.layer(\n",
      "    bar,\n",
      "    line\n",
      ").properties(title='Protective Edge Wikipedia pages count and external link apperances')\n",
      "#.resolve_scale(\n",
      " #   y='independent'\n",
      "32/196:\n",
      "first_link_appear = frl.groupby('link').timestamp.min().to_frame().resample('D', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "first_link_appear['link_accum'] = first_link_appear.links.cumsum()\n",
      "first_link_appear['link_pct_change'] = first_link_appear.link_accum.pct_change()\n",
      "\n",
      "bar = alt.Chart(first_link_appear[(first_link_appear.timestamp>='2014-07-05') & (first_link_appear.timestamp<'2014-11-01')]).mark_bar().encode(\n",
      "    x='timestamp:T',\n",
      "    y='link_accum:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "line = alt.Chart(creation_dates_day[(creation_dates_day.timestamp>='2014-07-05') & (creation_dates_day.timestamp<'2014-11-01')]).mark_line(color='red').encode(\n",
      "    x='timestamp:T',\n",
      "    y='pages:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "alt.layer(\n",
      "    bar,\n",
      "    line\n",
      ").resolve_scale(\n",
      "    y='independent'\n",
      ").properties(title='Protective Edge Wikipedia pages count and external link apperances')\n",
      "32/197:\n",
      "first_link_appear = frl.groupby('link').timestamp.min().to_frame().resample('D', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "first_link_appear['link_accum'] = first_link_appear.links.cumsum()\n",
      "first_link_appear['link_pct'] = first_link_appear.link_accum / first_link_appear.link_accum.max()\n",
      "\n",
      "\n",
      "bar = alt.Chart(first_link_appear[(first_link_appear.timestamp>='2014-07-05') & (first_link_appear.timestamp<'2014-11-01')]).mark_line(color='blue').encode(\n",
      "    x='timestamp:T',\n",
      "    y='link_pct:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "line = alt.Chart(creation_dates_day[(creation_dates_day.timestamp>='2014-07-05') & (creation_dates_day.timestamp<'2014-11-01')]).mark_line(color='red').encode(\n",
      "    x='timestamp:T',\n",
      "    y='pages_pct:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "alt.layer(\n",
      "    bar,\n",
      "    line\n",
      ").properties(title='Protective Edge Wikipedia pages count and external link apperances')\n",
      "#.resolve_scale(\n",
      " #   y='independent'\n",
      "32/198:\n",
      "first_link_appear = frl.groupby('link').timestamp.min().to_frame().resample('D', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "first_link_appear['link_accum'] = first_link_appear.links.cumsum()\n",
      "first_link_appear['link_pct'] = first_link_appear.link_accum / first_link_appear.link_accum.max()\n",
      "\n",
      "\n",
      "bar = alt.Chart(first_link_appear[(first_link_appear.timestamp>='2014-07-05') & (first_link_appear.timestamp<'2014-11-01')]).mark_line(color='blue').encode(\n",
      "    x='timestamp:T',\n",
      "    y='link_pct:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "line = alt.Chart(creation_dates_day[(creation_dates_day.timestamp>='2014-07-05') & (creation_dates_day.timestamp<'2014-11-01')]).mark_line(color='red').encode(\n",
      "    x='timestamp:T',\n",
      "    y='pages_pct:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "alt.layer(\n",
      "    bar,\n",
      "    line\n",
      ").properties(title='Protective Edge Wikipedia pages count and external link apperances (pct from max number for each of them)')\n",
      "#.resolve_scale(\n",
      " #   y='independent'\n",
      "32/199:\n",
      "first_link_appear = frl.groupby('link').timestamp.min().to_frame().resample('D', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "first_link_appear['link_accum'] = first_link_appear.links.cumsum()\n",
      "first_link_appear['link_pct'] = first_link_appear.link_accum / first_link_appear.link_accum.max()\n",
      "\n",
      "\n",
      "bar = alt.Chart(first_link_appear[(first_link_appear.timestamp>='2014-07-05') & (first_link_appear.timestamp<'2018-11-01')]).mark_line(color='blue').encode(\n",
      "    x='timestamp:T',\n",
      "    y='link_pct:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "line = alt.Chart(creation_dates_day[(creation_dates_day.timestamp>='2014-07-05') & (creation_dates_day.timestamp<'2018-11-01')]).mark_line(color='red').encode(\n",
      "    x='timestamp:T',\n",
      "    y='pages_pct:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "alt.layer(\n",
      "    bar,\n",
      "    line\n",
      ").properties(title='Protective Edge Wikipedia pages count and external link apperances (pct from max number for each of them)')\n",
      "32/200:\n",
      "first_link_appear = frl.groupby('link').timestamp.min().to_frame().resample('D', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "first_link_appear['link_accum'] = first_link_appear.links.cumsum()\n",
      "first_link_appear['link_pct'] = first_link_appear.link_accum / first_link_appear.link_accum.max()\n",
      "\n",
      "\n",
      "bar = alt.Chart(first_link_appear[(first_link_appear.timestamp>='2014-07-05') & (first_link_appear.timestamp<'2018-11-01')]).mark_line(color='blue').encode(\n",
      "    x='timestamp:T',\n",
      "    y='link_pct:Q',\n",
      "    tooltip = 'timestamp:T'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "line = alt.Chart(creation_dates_day[(creation_dates_day.timestamp>='2014-07-05') & (creation_dates_day.timestamp<'2018-11-01')]).mark_line(color='red').encode(\n",
      "    x='timestamp:T',\n",
      "    y='pages_pct:Q',\n",
      "    tooltip = 'timestamp:T'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "alt.layer(\n",
      "    bar,\n",
      "    line\n",
      ").properties(title='Protective Edge Wikipedia pages count and external link apperances (pct from max number for each of them)')\n",
      "32/201:\n",
      "first_link_appear = frl.groupby('link').timestamp.min().to_frame().resample('D', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "first_link_appear['link_accum'] = first_link_appear.links.cumsum()\n",
      "first_link_appear['link_pct'] = first_link_appear.link_accum / first_link_appear.link_accum.max()\n",
      "\n",
      "\n",
      "bar = alt.Chart(first_link_appear[(first_link_appear.timestamp>='2014-07-05') & (first_link_appear.timestamp<'2018-11-01')]).mark_line(color='blue').encode(\n",
      "    x='timestamp:T',\n",
      "    y='link_pct:Q',\n",
      "    tooltip = 'timestamp'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "line = alt.Chart(creation_dates_day[(creation_dates_day.timestamp>='2014-07-05') & (creation_dates_day.timestamp<'2018-11-01')]).mark_line(color='red').encode(\n",
      "    x='timestamp:T',\n",
      "    y='pages_pct:Q',\n",
      "    tooltip = 'timestamp'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "alt.layer(\n",
      "    bar,\n",
      "    line\n",
      ").properties(title='Protective Edge Wikipedia pages count and external link apperances (pct from max number for each of them)')\n",
      "32/202:\n",
      "first_link_appear = frl.groupby('link').timestamp.min().to_frame().resample('D', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "first_link_appear['link_accum'] = first_link_appear.links.cumsum()\n",
      "first_link_appear['link_pct'] = first_link_appear.link_accum / first_link_appear.link_accum.max()\n",
      "\n",
      "\n",
      "bar = alt.Chart(first_link_appear[(first_link_appear.timestamp>='2014-07-05') & (first_link_appear.timestamp<'2018-11-01')]).mark_line(color='blue').encode(\n",
      "    x='timestamp:T',\n",
      "    y='link_pct:Q',\n",
      "    tooltip = 'timestamp'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "line = alt.Chart(creation_dates_day[(creation_dates_day.timestamp>='2014-07-05') & (creation_dates_day.timestamp<'2018-11-01')]).mark_line(color='red').encode(\n",
      "    x='timestamp:T',\n",
      "    y='pages_pct:Q',\n",
      "    tooltip = 'timestamp'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "alt.layer(\n",
      "    bar,\n",
      "    line\n",
      ").interactive().properties(title='Protective Edge Wikipedia pages count and external link apperances (pct from max number for each of them)')\n",
      "32/203: frl.head()\n",
      "32/204: frl[frl.str.contains('archive.org')]\n",
      "32/205: frl[frl.link.str.contains('archive.org')]\n",
      "32/206: frl[frl.link.str.contains('archive.org')].link\n",
      "32/207: frl[frl.link.str.contains('archive.org')].head()\n",
      "32/208:\n",
      "with pd.set_option('display.max_colwidth', -1):\n",
      "    frl[frl.link.str.contains('archive.org')].head()\n",
      "32/209:\n",
      "with pd.option_context('display.max_colwidth', -1):\n",
      "    frl[frl.link.str.contains('archive.org')].head()\n",
      "32/210:\n",
      "pd.set_option('display.max_colwidth', -1):\n",
      "frl[frl.link.str.contains('archive.org')].head()\n",
      "32/211:\n",
      "pd.set_option('display.max_colwidth', -1)\n",
      "frl[frl.link.str.contains('archive.org')].head()\n",
      "32/212: frl[frl.link.str.contains('1133-20871-he/Dover.aspx')]\n",
      "32/213:\n",
      "pd.set_option('display.max_colwidth', -1)\n",
      "first_link_appear[first_link_appear.link.str.contains('archive.org')].head()\n",
      "32/214: first_link_appear.head()\n",
      "32/215:\n",
      "pd.set_option('display.max_colwidth', -1)\n",
      "first_link_appear[first_link_appear.link.str.contains('archive.org')].drop_duplicate(subset=['lang', 'link'])\n",
      "32/216:\n",
      "pd.set_option('display.max_colwidth', -1)\n",
      "frl[frl.link.str.contains('archive.org')].drop_duplicate(subset=['lang', 'link'])\n",
      "32/217:\n",
      "pd.set_option('display.max_colwidth', -1)\n",
      "frl[frl.link.str.contains('archive.org')].drop_duplicates(subset=['lang', 'link'])\n",
      "32/218: first_link_appear[frl.link.str.contains('Dover.aspx')]\n",
      "32/219: frl[frl.link.str.contains('Dover.aspx')]\n",
      "32/220: frl[frl.link.str.contains('Dover.aspx')].drop_duplicates(subset=['lang', 'link'])\n",
      "32/221:\n",
      "first_link_appear = frl.groupby('link').timestamp.min().to_frame().resample('D', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "first_link_appear['link_accum'] = first_link_appear.links.cumsum()\n",
      "first_link_appear['link_pct'] = first_link_appear.link_accum / first_link_appear.link_accum.max()\n",
      "\n",
      "\n",
      "bar = alt.Chart(first_link_appear[(first_link_appear.timestamp>='2014-07-05') & (first_link_appear.timestamp<'2018-11-01')]).mark_line(color='blue').encode(\n",
      "    x='timestamp:T',\n",
      "    y='link_pct:Q',\n",
      "    tooltip = 'timestamp'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "line = alt.Chart(creation_dates_day[(creation_dates_day.timestamp>='2014-07-05') & (creation_dates_day.timestamp<'2018-11-01')]).mark_line(color='red').encode(\n",
      "    x='timestamp:T',\n",
      "    y='pages_pct:Q',\n",
      "    tooltip = 'timestamp'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "alt.layer(\n",
      "    bar,\n",
      "    line\n",
      ").interactive().properties(title='Protective Edge Wikipedia pages count and external link apperances (pct from max number for each of them)')\n",
      "33/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "33/2: sns.__version__\n",
      "33/3:\n",
      "times = pd.read_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "st = pd.read_pickle('turf/total_stats_no_multiindex_groups_ded_cuts.pkl.gz', compression='gzip')\n",
      "33/4: st.dtypes\n",
      "34/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "34/2: sns.__version__\n",
      "34/3:\n",
      "%%time\n",
      "df = pd.read_pickle('turf/data_df.pkl.gz', compression='gzip')\n",
      "34/4: df.dtypes\n",
      "34/5:\n",
      "sdf = pd.read_pickle('turf/shared_df.pkl.gz', compression='gzip')\n",
      "sdf.dtypes\n",
      "34/6:\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "tfidf  = TfidfVectorizer()\n",
      "stf = tfidf.fit_transform(sdf.text.values())\n",
      "34/7:\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "tfidf  = TfidfVectorizer()\n",
      "stf = tfidf.fit_transform(sdf.text)\n",
      "34/8: stf.head()\n",
      "34/9: stf\n",
      "34/10:\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "tfidf  = TfidfVectorizer()\n",
      "stf = tfidf.fit_transform(sdf.text.to_list())\n",
      "34/11:\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "tfidf  = TfidfVectorizer()\n",
      "stf = tfidf.fit_transform(sdf.text.tolist())\n",
      "34/12: stf\n",
      "34/13: dir(stf)\n",
      "34/14: stf.get_feature_names()\n",
      "34/15: stf.get_feature_names()\n",
      "34/16: sdf.text.tolist()\n",
      "34/17: stf.get_feature_names()\n",
      "34/18: dir(stf)\n",
      "34/19:\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "tfidf  = TfidfVectorizer()\n",
      "stf = tfidf.fit_transform(sdf.text)\n",
      "34/20: tf = tfidf.fit_transform(df.text.tolist())\n",
      "34/21: tf = tfidf.fit_transform(df.text)\n",
      "34/22: df.dtypes\n",
      "34/23: tf = tfidf.fit_transform(df.text.astype(str))\n",
      "34/24: 9\n",
      "34/25: tsne_results.head()\n",
      "34/26: tsne_results.head()\n",
      "34/27:\n",
      "from sklearn.manifold import TSNE\n",
      "from sklearn.decomposition import TruncatedSVD\n",
      "\n",
      "svd = TruncatedSVD(n_components=75, n_iter=7, random_state=42)\n",
      "docs75 = svd.fit_transform(tf)\n",
      "\n",
      "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=5000)\n",
      "tsne_results = tsne.fit_transform(docs75)\n",
      "36/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "36/2: sns.__version__\n",
      "36/3:\n",
      "%%time\n",
      "df = pd.read_pickle('turf/data_df.pkl.gz', compression='gzip')\n",
      "36/4: df.dtypes\n",
      "36/5:\n",
      "sdf = pd.read_pickle('turf/shared_df.pkl.gz', compression='gzip')\n",
      "sdf.dtypes\n",
      "36/6:\n",
      "def percentile(n):\n",
      "    def percentile_(x):\n",
      "        return np.percentile(x, n)\n",
      "    percentile_.__name__ = 'percentile_%s' % n\n",
      "    return percentile_\n",
      "36/7:\n",
      "times = pd.read_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "st = pd.read_pickle('turf/total_stats_no_multiindex_groups_ded_cuts.pkl.gz', compression='gzip')\n",
      "36/8: st.dtypes\n",
      "36/9: from itertools import product\n",
      "36/10: times.dtypes\n",
      "36/11:\n",
      "columns = ['elapsed_time',  'created_time', 'created_month', 'created_day']\n",
      "stats = ['nunique', 'min', 'max']\n",
      "new_cols = ['from_id'] + ['_'.join(x) for x in product(columns, stats)] + ['period_length']\n",
      "new_cols\n",
      "36/12: times.columns = new_cols\n",
      "36/13: st[['from_id', 'group_cat']].head()\n",
      "36/14: times = times.merge(st[['from_id', 'group_cat']].drop_duplicates(), how='left', on='from_id')\n",
      "36/15: day_in_seconds = 60*60*24\n",
      "36/16: times.groupby('group_cat').period_length.median()/day_in_seconds\n",
      "36/17: times['period_length'] = times.period_length/day_in_seconds\n",
      "36/18: times[times.period_length<850].hist(bins=30, column='period_length', by='group_cat', layout=(11,1), sharex=True, figsize=(10,15), grid=True)\n",
      "36/19: tn = pd.read_pickle('turf/times_names.pkl.gz', compression='gzip')\n",
      "36/20: tn['period_length'] = tn.period_length/day_in_seconds\n",
      "36/21: mean_group_cat\n",
      "36/22:\n",
      "%%time\n",
      "df = pd.read_pickle('turf/data_df.pkl.gz', compression='gzip')\n",
      "36/23: df.dtypes\n",
      "36/24:\n",
      "sdf = pd.read_pickle('turf/shared_df.pkl.gz', compression='gzip')\n",
      "sdf.dtypes\n",
      "36/25:\n",
      "times = pd.read_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "st = pd.read_pickle('turf/total_stats_no_multiindex_groups_ded_cuts.pkl.gz', compression='gzip')\n",
      "36/26: st.dtypes\n",
      "36/27: from itertools import product\n",
      "36/28: times.dtypes\n",
      "36/29:\n",
      "columns = ['elapsed_time',  'created_time', 'created_month', 'created_day']\n",
      "stats = ['nunique', 'min', 'max']\n",
      "new_cols = ['from_id'] + ['_'.join(x) for x in product(columns, stats)] + ['period_length']\n",
      "new_cols\n",
      "36/30: times.columns = new_cols\n",
      "36/31: st[['from_id', 'group_cat']].head()\n",
      "36/32: times = times.merge(st[['from_id', 'group_cat']].drop_duplicates(), how='left', on='from_id')\n",
      "36/33: day_in_seconds = 60*60*24\n",
      "36/34: times.groupby('group_cat').period_length.median()/day_in_seconds\n",
      "36/35: times['period_length'] = times.period_length/day_in_seconds\n",
      "36/36: times[times.period_length<850].hist(bins=30, column='period_length', by='group_cat', layout=(11,1), sharex=True, figsize=(10,15), grid=True)\n",
      "36/37: tn = pd.read_pickle('turf/times_names.pkl.gz', compression='gzip')\n",
      "36/38: tn['period_length'] = tn.period_length/day_in_seconds\n",
      "36/39: mean_group_cat\n",
      "36/40: tn.groupby(['group_cat']).period_length.median().to_frame().T\n",
      "36/41: tn.groupby(['name']).period_length.agg('mean')\n",
      "36/42:\n",
      "def get_heatmap(kind, field='period_length'):\n",
      "    mean_mk = tn.groupby(['name'])[field].agg(kind)\n",
      "    mean_group_cat = tn.groupby(['group_cat'])[field].agg(kind).to_frame().T\n",
      "    mean_group_cat.index=[kind]\n",
      "    mean_group_cat[kind] = tn[field].agg(kind)\n",
      "    mean_heatmap = (tn.groupby(['name', 'group_cat'])[field].agg(kind)\n",
      "                    .unstack())\n",
      "    mean_heatmap[kind] = mean_mk\n",
      "    mean_heatmap = mean_heatmap.sort_values(kind, ascending=False)\n",
      "    mean_heatmap.index = mean_heatmap.index.add_categories(kind)\n",
      "    mean_heatmap = mean_heatmap.append(mean_group_cat)\n",
      "    mean_heatmap = mean_heatmap.rename(columns = {'Dedicated 2MK': 'Dedicated Polemicists', 'Two-MKs-Plus': 'Polemicists'})\n",
      "    mean_heatmap = mean_heatmap[['One Timers', 'Fans', 'Basers', 'Campers', 'Polemicists', 'Dedicated Fans', 'Dedicated Basers',\n",
      "           'Dedicated Campers', 'Dedicated Polemicists', 'Dedicated Celebies', kind]]\n",
      "    return mean_heatmap\n",
      "mean_heatmap = get_heatmap('mean')\n",
      "median_heatmap = get_heatmap('median')\n",
      "mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "36/43: html_heatmap = mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red').render()\n",
      "36/44: open('outputs/mean_period_length.html', 'w').write(html_heatmap)\n",
      "36/45: mean_heatmap.to_csv('outputs/mean_period_length.csv')\n",
      "36/46:\n",
      "tn_c = tn.copy()\n",
      "tn_c['group_cat'] = tn_c.group_cat.astype(str)\n",
      "tn_c['name'] = tn_c.name.astype(str)\n",
      "36/47:\n",
      "order = ['One Timers', 'Fans', 'Basers', 'Campers', 'Two-MKs-Plus', 'Dedicated Fans', 'Dedicated Basers',\n",
      "       'Dedicated Campers', 'Dedicated 2MK', 'Dedicated Celebies']\n",
      "36/48: median_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "36/49:\n",
      "html_heatmap = median_heatmap.style.background_gradient(cmap='viridis').highlight_null('red').render()\n",
      "open('outputs/median_period_length.html', 'w').write(html_heatmap)\n",
      "36/50:\n",
      ">>> g = sns.catplot(x=\"group_cat\", y=\"period_length\",\n",
      "...                 #hue=\"group_cat\", \n",
      "                    order = order,\n",
      "                    row=\"name\", row_order= list(mean_mk.sort_values(ascending=False).index),\n",
      "...                 data=tn[tn.period_length<850], kind=\"box\",\n",
      "...                 height=4, aspect=5);\n",
      "36/51:\n",
      ">>> g = sns.catplot(x=\"group_cat\", y=\"period_length\",\n",
      "...                 #hue=\"group_cat\", \n",
      "                    order = order,\n",
      "                    row=\"name\", #row_order= list(mean_mk.sort_values(ascending=False).index),\n",
      "...                 data=tn[tn.period_length<850], kind=\"box\",\n",
      "...                 height=4, aspect=5);\n",
      "36/52:\n",
      "days_heatmap = get_heatmap('mean', 'created_day_nunique')\n",
      "days_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "36/53:\n",
      "html_heatmap = days_heatmap.style.background_gradient(cmap='viridis').highlight_null('red').render()\n",
      "open('outputs/mean_created_day_nunique.html', 'w').write(html_heatmap)\n",
      "36/54:\n",
      "month_heatmap = get_heatmap('mean', 'created_month_nunique')\n",
      "month_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "37/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "37/2:\n",
      "%%time\n",
      "df = pd.read_pickle('turf/data_df.pkl.gz', compression='gzip')\n",
      "37/3: df.dtypes\n",
      "37/4:\n",
      "sdf = pd.read_pickle('turf/shared_df.pkl.gz', compression='gzip')\n",
      "sdf.dtypes\n",
      "37/5: st = pd.read_pickle('turf/total_stats_no_multiindex.pkl.gz', compression='gzip')\n",
      "38/1: ### description of the dataset (no. posts, pages, comments, unique users)\n",
      "38/2: st.from_id.sum()\n",
      "40/1: st = pd.read_pickle('turf/total_stats_no_multiindex_groups_ded_cuts.pkl.gz', compression='gzip')\n",
      "40/2: st.dtypes\n",
      "40/3:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "40/4: st = pd.read_pickle('turf/total_stats_no_multiindex_groups_ded_cuts.pkl.gz', compression='gzip')\n",
      "40/5: st.dtypes\n",
      "40/6: ### description of the dataset (no. posts, pages, comments, unique users)\n",
      "40/7: st.from_id.max()\n",
      "40/8: st.id.max()\n",
      "40/9: st.id.sum()\n",
      "40/10: ##Load comments and shares dataframes\n",
      "40/11:\n",
      "df = pd.read_pickle('turf/data_df.pkl.gz', compression='gzip')\n",
      "df.dtypes\n",
      "40/12:\n",
      "sdf = pd.read_pickle('turf/shared_df.pkl.gz', compression='gzip')\n",
      "sdf.dtypes\n",
      "40/13:\n",
      "nposts = df.post_id.nunique()\n",
      "npages = st.name.nunique()\n",
      "ncomments = st.id.sum()\n",
      "nusers = st.from_id.nunique()\n",
      "\n",
      "pd.DataFrame({ 'No. Posts': nposts, 'No. Pages': npages, 'No. Comments': ncomments, 'No. Users': nusers)\n",
      "40/14:\n",
      "nposts = df.post_id.nunique()\n",
      "npages = st.name.nunique()\n",
      "ncomments = st.id.sum()\n",
      "nusers = st.from_id.nunique()\n",
      "\n",
      "pd.DataFrame({ 'No. Posts': nposts, 'No. Pages': npages, 'No. Comments': ncomments, 'No. Users': nusers})\n",
      "40/15:\n",
      "nposts = df.post_id.nunique()\n",
      "npages = st.name.nunique()\n",
      "ncomments = st.id.sum()\n",
      "nusers = st.from_id.nunique()\n",
      "\n",
      "pd.DataFrame([{ 'No. Posts': nposts, 'No. Pages': npages, 'No. Comments': ncomments, 'No. Users': nusers}])\n",
      "40/16:\n",
      "nposts = df.post_id.nunique()\n",
      "npages = st.name.nunique()\n",
      "ncomments = st.id.sum()\n",
      "nusers = st.from_id.nunique()\n",
      "\n",
      "pd.DataFrame([{ 'No. Posts': nposts, 'No. Pages': npages, 'No. Comments': ncomments, 'No. Users': nusers}]).T\n",
      "40/17:\n",
      "nposts = df.post_id.nunique()\n",
      "npages = st.name.nunique()\n",
      "ncomments = st.id.sum()\n",
      "nusers = st.from_id.nunique()\n",
      "\n",
      "pd.DataFrame([{ 'No. Posts': nposts, 'No. Pages': npages, 'No. Comments': ncomments, 'No. Users': nusers}]).T.to_html()\n",
      "40/18:\n",
      "nposts = df.post_id.nunique()\n",
      "npages = st.name.nunique()\n",
      "ncomments = st.id.sum()\n",
      "nusers = st.from_id.nunique()\n",
      "\n",
      "print(pd.DataFrame([{ 'No. Posts': nposts, 'No. Pages': npages, 'No. Comments': ncomments, ')No. Users': nusers}]).T.to_html()\n",
      "40/19:\n",
      "nposts = df.post_id.nunique()\n",
      "npages = st.name.nunique()\n",
      "ncomments = st.id.sum()\n",
      "nusers = st.from_id.nunique()\n",
      "\n",
      "print(pd.DataFrame([{ 'No. Posts': nposts, 'No. Pages': npages, 'No. Comments': ncomments, 'No. Users': nusers}]).T.to_html())\n",
      "40/20:\n",
      "nposts = df.post_id.nunique()\n",
      "npages = st.name.nunique()\n",
      "ncomments = st.id.sum()\n",
      "nusers = st.from_id.nunique()\n",
      "\n",
      "pd.DataFrame([{ 'No. Posts': nposts, 'No. Pages': npages, 'No. Comments': ncomments, 'No. Users': nusers}]).T\n",
      "40/21:\n",
      "nposts = df.post_id.nunique()\n",
      "npages = st.name.nunique()\n",
      "ncomments = st.id.sum()\n",
      "nusers = st.from_id.nunique()\n",
      "\n",
      "print(pd.DataFrame([{ 'No. Posts': nposts, 'No. Pages': npages, 'No. Comments': ncomments, 'No. Users': nusers}]).T.to_html())\n",
      "40/22:\n",
      "nposts = df.post_id.nunique()\n",
      "npages = st.name.nunique()\n",
      "ncomments = st.id.sum()\n",
      "nusers = st.from_id.nunique()\n",
      "\n",
      "pd.DataFrame([{ 'No. Posts': nposts, 'No. Pages': npages, 'No. Comments': ncomments, 'No. Users': nusers}])\n",
      "40/23:\n",
      "nposts = df.post_id.nunique()\n",
      "npages = st.name.nunique()\n",
      "ncomments = st.id.sum()\n",
      "nusers = st.from_id.nunique()\n",
      "\n",
      "pd.DataFrame([{ 'No. Posts': nposts, 'No. Pages': npages, 'No. Comments': ncomments, 'No. Users': nusers}]).T\n",
      "40/24: st.groupby('group_cat').from_name.nunique()\n",
      "40/25: st.groupby('group_cat').from_id.nunique()\n",
      "40/26: st.groupby('new_group_cat').from_id.nunique()\n",
      "40/27: st.groupby('group_cat').from_id.nunique()\n",
      "40/28: st.groupby('group_cat').from_id.nunique().to_frame()\n",
      "40/29: print(st.groupby('group_cat').from_id.nunique().to_frame().to_html())\n",
      "40/30:\n",
      "groups = st.groupby('group_cat').from_id.nunique().to_frame()\n",
      "groups(ratio = ngroups.group_cat / nusers)\n",
      "40/31:\n",
      "groups = st.groupby('group_cat').from_id.nunique().to_frame()\n",
      "groups['ratio'] = ngroups.group_cat / nusers)\n",
      "40/32:\n",
      "groups = st.groupby('group_cat').from_id.nunique().to_frame()\n",
      "groups['ratio'] = ngroups.group_cat / nusers\n",
      "40/33:\n",
      "groups = st.groupby('group_cat').from_id.nunique().to_frame()\n",
      "groups['ratio'] = groups.group_cat / nusers\n",
      "40/34: groups = st.groupby('group_cat').from_id.nunique().to_frame()\n",
      "40/35:\n",
      "groups = st.groupby('group_cat').from_id.nunique().to_frame()\n",
      "groups['ratio'] = groups.from_id / nusers\n",
      "40/36:\n",
      "groups = st.groupby('group_cat').from_id.nunique().to_frame()\n",
      "groups['ratio'] = groups.from_id / nusers\n",
      "groups\n",
      "40/37: print(groups.to_html())\n",
      "40/38:\n",
      "groups = (st.groupby('group_cat')\n",
      "          .agg({'from_id': 'nunique', 'id': 'sum'})\\\n",
      "          .rename(columns={'from_id': 'users', 'id': 'comments'})\n",
      "groups['users_ratio'] = groups.users / nusers\n",
      "groups['comments_ratio'] = groups.comments / nusers\n",
      "groups\n",
      "40/39:\n",
      "groups = (st.groupby('group_cat')\n",
      "          .agg({'from_id': 'nunique', 'id': 'sum'})\\\n",
      "          .rename(columns={'from_id': 'users', 'id': 'comments'}))\n",
      "groups['users_ratio'] = groups.users / nusers\n",
      "groups['comments_ratio'] = groups.comments / nusers\n",
      "groups\n",
      "40/40:\n",
      "groups = (st.groupby('group_cat')\n",
      "          .agg({'from_id': 'nunique', 'id': 'sum'})\\\n",
      "          .rename(columns={'from_id': 'users', 'id': 'comments'}))\n",
      "groups['users_ratio'] = groups.users / nusers\n",
      "groups['comments_ratio'] = groups.comments / nusers\n",
      "groups = groups[['users', 'users_ratio', 'comments', 'comments_ratio']]\n",
      "40/41:\n",
      "groups = (st.groupby('group_cat')\n",
      "          .agg({'from_id': 'nunique', 'id': 'sum'})\\\n",
      "          .rename(columns={'from_id': 'users', 'id': 'comments'}))\n",
      "groups['users_ratio'] = groups.users / nusers\n",
      "groups['comments_ratio'] = groups.comments / nusers\n",
      "groups = groups[['users', 'users_ratio', 'comments', 'comments_ratio']]\n",
      "groups\n",
      "40/42:\n",
      "groups = (st.groupby('group_cat')\n",
      "          .agg({'from_id': 'nunique', 'id': 'sum'})\\\n",
      "          .rename(columns={'from_id': 'users', 'id': 'comments'}))\n",
      "groups['users_ratio'] = groups.users / nusers\n",
      "groups['comments_ratio'] = groups.comments / ncomments\n",
      "groups = groups[['users', 'users_ratio', 'comments', 'comments_ratio']]\n",
      "groups\n",
      "40/43: print(groups.to_html())\n",
      "40/44:\n",
      "groups = (st.groupby('group_cat')\n",
      "          .agg({'from_id': 'nunique', 'id': 'sum'})\\\n",
      "          .rename(columns={'from_id': 'users', 'id': 'comments'}))\n",
      "groups['users_ratio'] = groups.users / nusers\n",
      "groups['comments_ratio'] = groups.comments / ncomments\n",
      "groups = groups[['users', 'users_ratio', 'comments', 'comments_ratio']]\n",
      "groups = groups.reindex(['One Timers', \n",
      "                         'Fans', 'Dedicated Fans', \n",
      "                         'Basers', 'Dedicated Basers', \n",
      "                         'Campers', 'Dedicated Campers',\n",
      "                         'Two-MKs-Plus', 'Dedicated 2MK',\n",
      "                         'Dedicated Celebies'])\n",
      "groups\n",
      "40/45: print(groups.to_html())\n",
      "40/46:\n",
      "groups = (st.groupby('group_cat')\n",
      "          .agg({'from_id': 'nunique', 'id': 'sum'})\\\n",
      "          .rename(columns={'from_id': 'users', 'id': 'comments'}))\n",
      "groups['users_ratio'] = groups.users / nusers\n",
      "groups['comments_ratio'] = groups.comments / ncomments\n",
      "groups['ratios_ratio'] = groups.comments / ncomments\n",
      "\n",
      "groups = groups[['users', 'users_ratio', 'comments', 'comments_ratio', 'comments_user']]\n",
      "groups = groups.reindex(['One Timers', \n",
      "                         'Fans', 'Dedicated Fans', \n",
      "                         'Basers', 'Dedicated Basers', \n",
      "                         'Campers', 'Dedicated Campers',\n",
      "                         'Two-MKs-Plus', 'Dedicated 2MK',\n",
      "                         'Dedicated Celebies'])\n",
      "groups\n",
      "40/47:\n",
      "groups = (st.groupby('group_cat')\n",
      "          .agg({'from_id': 'nunique', 'id': 'sum'})\\\n",
      "          .rename(columns={'from_id': 'users', 'id': 'comments'}))\n",
      "groups['users_ratio'] = groups.users / nusers\n",
      "groups['comments_ratio'] = groups.comments / ncomments\n",
      "groups['comments_per_user'] = groups.comments_ratio / groups.users_ratio\n",
      "\n",
      "groups = groups[['users', 'users_ratio', 'comments', 'comments_ratio', 'comments_user']]\n",
      "groups = groups.reindex(['One Timers', \n",
      "                         'Fans', 'Dedicated Fans', \n",
      "                         'Basers', 'Dedicated Basers', \n",
      "                         'Campers', 'Dedicated Campers',\n",
      "                         'Two-MKs-Plus', 'Dedicated 2MK',\n",
      "                         'Dedicated Celebies'])\n",
      "groups\n",
      "40/48:\n",
      "groups = (st.groupby('group_cat')\n",
      "          .agg({'from_id': 'nunique', 'id': 'sum'})\\\n",
      "          .rename(columns={'from_id': 'users', 'id': 'comments'}))\n",
      "groups['users_ratio'] = groups.users / nusers\n",
      "groups['comments_ratio'] = groups.comments / ncomments\n",
      "groups['comments_per_user'] = groups.comments_ratio / groups.users_ratio\n",
      "\n",
      "groups = groups[['users', 'users_ratio', 'comments', 'comments_ratio', 'comments_per_user']]\n",
      "groups = groups.reindex(['One Timers', \n",
      "                         'Fans', 'Dedicated Fans', \n",
      "                         'Basers', 'Dedicated Basers', \n",
      "                         'Campers', 'Dedicated Campers',\n",
      "                         'Two-MKs-Plus', 'Dedicated 2MK',\n",
      "                         'Dedicated Celebies'])\n",
      "groups\n",
      "40/49:\n",
      "groups = (st.groupby('group_cat')\n",
      "          .agg({'from_id': 'nunique', 'id': 'sum'})\\\n",
      "          .rename(columns={'from_id': 'users', 'id': 'comments'}))\n",
      "groups['users_ratio'] = groups.users / nusers\n",
      "groups['comments_ratio'] = groups.comments / ncomments\n",
      "groups['ratios_ratio'] = groups.comments_ratio / groups.users_ratio\n",
      "\n",
      "groups = groups[['users', 'users_ratio', 'comments', 'comments_ratio', 'ratios_ratio']]\n",
      "groups = groups.reindex(['One Timers', \n",
      "                         'Fans', 'Dedicated Fans', \n",
      "                         'Basers', 'Dedicated Basers', \n",
      "                         'Campers', 'Dedicated Campers',\n",
      "                         'Two-MKs-Plus', 'Dedicated 2MK',\n",
      "                         'Dedicated Celebies'])\n",
      "groups\n",
      "40/50: print(groups.to_html())\n",
      "41/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "41/2:\n",
      "%%time\n",
      "df = pd.read_pickle('turf/data_df.pkl.gz', compression='gzip')\n",
      "41/3: df.dtypes\n",
      "41/4:\n",
      "sdf = pd.read_pickle('turf/shared_df.pkl.gz', compression='gzip')\n",
      "sdf.dtypes\n",
      "41/5: st = pd.read_pickle('turf/total_stats_no_multiindex.pkl.gz', compression='gzip')\n",
      "41/6:\n",
      "# OLD STUFF\n",
      "#save(st, 'turf/total_stats.pkl.gz')\n",
      "#st = pd.read_pickle( 'turf/total_stats.pkl.gz', compression='gzip')\n",
      "41/7: st.head()\n",
      "41/8: st.shape\n",
      "41/9: df[df.is_post==0].shape\n",
      "41/10: print ('Total number of comments: %d' % st.id.sum())\n",
      "41/11: print ('Total number of distinct users (commenters): %d' % st.from_id.nunique())\n",
      "41/12:\n",
      "counts = st.groupby('from_id').id.sum()\n",
      "one_timers = counts[counts==1]\n",
      "\n",
      "print ('Total number of one-timers: %d' % one_timers.shape[0])\n",
      "print ('Ratio of one timers from all users: %.2f' % (one_timers.shape[0] / st.from_id.nunique()))\n",
      "41/13: mat = st.pivot(index='from_id', columns='name', values='id')\n",
      "41/14: mat[(mat.count(axis=1)==1)].shape[0]\n",
      "41/15: mat.loc[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu==1), 'Benjamin_Netanyahu'].shape[0]\n",
      "41/16:\n",
      "print ('Ratio of Bibi from one-timers: %.2f' % (mat[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu==1)].shape[0] \n",
      " / counts[counts==1].shape[0]))\n",
      "41/17: print ('Ratio of users that commented 10 times and up: %.2f' % (counts[counts>=10].shape[0]/st.from_id.nunique()))\n",
      "41/18: (st.reset_index().groupby('from_id').name.nunique().value_counts().head(20)/st.reset_index().from_id.nunique()).plot(kind='bar')\n",
      "41/19: (st.reset_index().groupby('from_id').id.sum().value_counts().head(20)/st.reset_index().from_id.nunique()).plot(kind='bar')\n",
      "41/20:\n",
      "fans_users = (st.reset_index()\n",
      " .loc[st.groupby('from_id').name.transform('nunique')==1, ['from_id', 'name', 'party']])\n",
      "41/21: fans = ( fans_users.name.value_counts())\n",
      "41/22: fans.head(10)\n",
      "41/23:\n",
      "print ('Total number of fans: %d' % fans.sum())\n",
      "print ('Ratio of fans from all users: %.2f' % (fans.sum() / st.from_id.nunique()))\n",
      "print ('Ratio of one-timers from fans: %.2f' % (counts[counts==1].shape[0] / fans.sum()))\n",
      "41/24:\n",
      "(pd.concat([fans, \n",
      "            fans/st.from_id.nunique(), \n",
      "            fans/st.groupby('name').from_id.nunique()],\n",
      "            axis=1, keys=['num_uniques', 'ratio_from_all_users', 'ratio_from_own_users'])\n",
      "             .sort_values(by='ratio_from_own_users', ascending=False)).head(10)\n",
      "41/25:\n",
      "mkdet = pd.read_csv('turf/mk_details.csv', sep='\\t')\n",
      "mkdet.head()\n",
      "41/26: df['party'] = df['name'].map(mkdet.set_index('folder_name')['party'])\n",
      "41/27: df.groupby(['party', 'langdetect']).size().unstack()\n",
      "41/28: st['party'] = st['name'].map(mkdet.set_index('folder_name')['party'])\n",
      "41/29:\n",
      "two_mks_plus = (st.reset_index()\n",
      " .loc[st.groupby('from_id').name.transform('nunique')>1, ['from_id', 'name', 'party']])\n",
      "41/30: two_mks_plus.head()\n",
      "41/31: two_mks_plus.from_id.nunique()\n",
      "41/32: two_mks_plus.party.value_counts()\n",
      "41/33: two_mks_plus.groupby('party').from_id.nunique()\n",
      "41/34: base = two_mks_plus[two_mks_plus.groupby('from_id').party.transform('nunique')==1]\n",
      "41/35:\n",
      "print ('Total number of basers: %d' % base.from_id.nunique())\n",
      "print ('Ratio of basers from all users: %.2f' % (base.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of basers from two-mks-plus: %.2f' % (base.from_id.nunique() / two_mks_plus.from_id.nunique()))\n",
      "41/36: st.party.unique()\n",
      "41/37:\n",
      "camps = { 'מרצ': 'opposition',\n",
      "          'יש עתיד': 'opposition',\n",
      "          'ליכוד': 'coalition',\n",
      "          'הבית היהודי בראשות נפתלי בנט': 'coalition',\n",
      "          'המחנה הציוני': 'opposition',\n",
      "          'הרשימה המשותפת (חד”ש, רע”מ, בל”ד, תע”ל)': 'opposition',\n",
      "          'כולנו בראשות משה כחלון': 'coalition',\n",
      "          'ישראל ביתנו': 'coalition' }\n",
      "41/38: st['camp'] = st['party'].map(camps)\n",
      "41/39: two_mks_plus['camp'] = two_mks_plus['party'].map(camps)\n",
      "41/40: two_mks_plus.groupby('camp').from_id.nunique()\n",
      "41/41: campers = two_mks_plus[two_mks_plus.groupby('from_id').camp.transform('nunique')==1]\n",
      "41/42:\n",
      "print ('Total number of campers: %d' % campers.from_id.nunique())\n",
      "print ('Ratio of campers from all users: %.2f' % (campers.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of campers from two_mks_plus: %.2f' % (campers.from_id.nunique() / two_mks_plus.from_id.nunique()))\n",
      "print ('Coalition | Opposition ratio from campers: %.2f | %.2f' % \n",
      "       ((campers[campers.camp=='coalition'].from_id.nunique() / campers.from_id.nunique()),\n",
      "       (campers[campers.camp=='opposition'].from_id.nunique() / campers.from_id.nunique())))\n",
      "cg = st.groupby('camp')\n",
      "print ('Coalition | Opposition ratios from total users: %.2f | %.2f' % \n",
      "       ((cg.from_id.nunique()['coalition'] / st.from_id.nunique()),\n",
      "       ((cg.from_id.nunique()['opposition'] / st.from_id.nunique()))))\n",
      "\n",
      "print ('Coalition | Opposition total comments: %.2f | %.2f' % \n",
      "       ((cg.id.sum()['coalition'] / st.id.sum()),\n",
      "       ((cg.id.sum()['opposition'] / st.id.sum()))))\n",
      "41/43: top_commented = list(st.groupby('name').id.sum().sort_values(ascending=False).index)\n",
      "41/44:\n",
      "top_10_commented = top_commented[:10]\n",
      "top_10_commented\n",
      "41/45:\n",
      "#celebz =  two_mks_plus[two_mks_plus.groupby('from_id').name.transform(lambda x: x.isin(top_10_commented).all())]\n",
      "celebz = two_mks_plus[two_mks_plus.from_id.isin(mat[mat.loc[:, top_commented[10:]].isna().all(axis=1)].index)]\n",
      "41/46: num_celebiez_no_base_camp = len(set(celebz.from_id.drop_duplicates().values).difference(set(campers.from_id.drop_duplicates().values)).difference(set(base.from_id.drop_duplicates().values)))\n",
      "41/47:\n",
      "print ('Total number of celeb followers (celebies): %d' % celebz.from_id.nunique())\n",
      "print ('Ratio of celebies from all users: %.2f' % (celebz.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of celebies from two_mks_plus: %.2f' % (celebz.from_id.nunique() / two_mks_plus.from_id.nunique()))\n",
      "print ('Remove campers and base from celebies: %d' % num_celebiez_no_base_camp)\n",
      "41/48: top_commented[:10]\n",
      "41/49: fans.reindex(top_commented).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "41/50:\n",
      "only_bibi = mat.loc[:, top_commented[1:]].isna().all(axis=1)\n",
      "only_bibi[only_bibi].shape\n",
      "41/51:\n",
      "def get_accum_df(mat, order):\n",
      "    onlys = []\n",
      "    \n",
      "    for i, name in enumerate(order):\n",
      "        only = mat.loc[:, order[i+1:]].isna().all(axis=1)\n",
      "        onlys.append(only[only].shape[0])\n",
      "        \n",
      "    user_accum_df = pd.DataFrame({'mk': order, 'user_accum': onlys})\n",
      "    user_accum_df['party'] = user_accum_df['mk'].map(mkdet.set_index('folder_name')['party'])\n",
      "    user_accum_df['camp'] = user_accum_df['party'].map(camps)\n",
      "    user_accum_df['delta'] = user_accum_df.user_accum.diff()\n",
      "    unique_delta_ratio = (fans.reindex(top_commented) / user_accum_df.set_index('mk').delta).drop('Benjamin_Netanyahu')\n",
      "    user_accum_df['unique_delta_ratio'] = user_accum_df['mk'].map(unique_delta_ratio)\n",
      "    user_accum_df['delta_n'] = user_accum_df['delta'] / user_accum_df['delta'].max()\n",
      "    \n",
      "    return user_accum_df\n",
      "41/52:\n",
      "user_accum_df = get_accum_df(mat, top_commented)\n",
      "user_accum_df.head()\n",
      "41/53: user_accum_df.set_index('mk').user_accum.plot(kind='bar', figsize=(15,10), rot=75)\n",
      "41/54:\n",
      "import altair as alt\n",
      "alt.renderers.enable('notebook')\n",
      "41/55:\n",
      "def chart_user_accum(accum_df, order, order_name):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('user_accum',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'user_accum']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = f'User accumulation by {order_name} order - left to right, only users that comment inside the group until the current column')\n",
      "\n",
      "\n",
      "def chart_user_accum_delta(accum_df, order, order_name):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('delta',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'delta']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = f'User accumulation delta by {order_name} order - left to right, only users that comment inside the group until the current column')\n",
      "\n",
      "\n",
      "def chart_user_accum_unique_delta_ratio(accum_df, order, order_name):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('unique_delta_ratio',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'unique_delta_ratio']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = f'Page unique count divided by user_accum delta by {order_name} order (to measure how much a page is part of the group)')\n",
      "41/56:\n",
      "(chart_user_accum(user_accum_df, top_commented, 'Top Commented') \n",
      " & chart_user_accum_delta(user_accum_df, top_commented, 'Top Commented')\n",
      " & chart_user_accum_unique_delta_ratio(user_accum_df, top_commented, 'Top Commented')).properties(title='Top Commented')\n",
      "41/57: user_accum_df[['delta_n', 'unique_delta_ratio']].div(user_accum_df[['delta_n', 'unique_delta_ratio']].sum(axis=1), axis=0).plot(kind='bar', stacked=True, figsize=(15,10), rot=75, width=1.0)\n",
      "41/58:\n",
      "print ('First celebiez group: ', top_commented[:6])\n",
      "print ('Second celebiez group: ', top_commented[:15])\n",
      "41/59: top_mean_commented = list(df.loc[df.is_post==1].groupby('name').comment_count.mean().sort_values(ascending=False).index)\n",
      "41/60: df.loc[df.is_post==1].groupby('name').comment_count.mean().sort_values(ascending=False).head()\n",
      "41/61: fans.reindex(top_mean_commented).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "41/62: mean_com_accum_df = get_accum_df(mat, top_mean_commented)\n",
      "41/63:\n",
      "(chart_user_accum(mean_com_accum_df, top_mean_commented, 'Top Mean Commented') \n",
      " & chart_user_accum_delta(mean_com_accum_df, top_mean_commented, 'Top Mean Commented') \n",
      " & chart_user_accum_unique_delta_ratio(mean_com_accum_df, top_mean_commented, 'Top Mean Commented')).properties(title='Top Mean Commented')\n",
      "41/64: top_liked = list(df.loc[df.is_post==1].groupby('name').like_count.mean().sort_values(ascending=False).index)\n",
      "41/65: fans.reindex(top_liked).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "41/66: like_accum_df = get_accum_df(mat, top_liked)\n",
      "41/67:\n",
      "(chart_user_accum(like_accum_df, top_liked, 'Top Liked') \n",
      " & chart_user_accum_delta(like_accum_df, top_liked, 'Top Liked') \n",
      " & chart_user_accum_unique_delta_ratio(like_accum_df, top_liked, 'Top Liked')).properties(title='Top Liked')\n",
      "41/68: top_shared = list(df.loc[df.is_post==1].groupby('name').share_count.mean().sort_values(ascending=False).index)\n",
      "41/69: fans.reindex(top_shared).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "41/70:\n",
      "share_accum_df = get_accum_df(mat, top_shared)\n",
      "share_accum_df.head()\n",
      "41/71:\n",
      "(chart_user_accum(share_accum_df, top_shared, 'Top Shared') \n",
      " & chart_user_accum_delta(share_accum_df, top_shared, 'Top Shared') \n",
      " & chart_user_accum_unique_delta_ratio(share_accum_df, top_shared, 'Top Shared')).properties(title='Top Shared')\n",
      "41/72:\n",
      "ts = (df.loc[df.is_post==1].groupby('name').share_count.mean().sort_values(ascending=False))\n",
      "tmc = (df.loc[df.is_post==1].groupby('name').comment_count.mean().sort_values(ascending=False))\n",
      "tl = (df.loc[df.is_post==1].groupby('name').like_count.mean().sort_values(ascending=False))\n",
      "all_scores = pd.concat([tmc, tl, ts], axis=1)\n",
      "all_scores_div = all_scores.div(all_scores.sum(axis=0), axis=1)\n",
      "all_scores['total_score'] = all_scores['like_count'] + 1.1*all_scores['comment_count'] + 1.2*all_scores['share_count']\n",
      "all_scores_div['total_score'] = all_scores_div['like_count'] + 1.1*all_scores_div['comment_count'] + 1.2*all_scores_div['share_count']\n",
      "all_scores.sort_values(by='total_score', ascending=False).head(10)\n",
      "41/73: all_scores_div.sort_values(by='total_score', ascending=False).head(20)\n",
      "41/74:\n",
      "all_accum_df = get_accum_df(mat, list(all_scores_div.sort_values(by='total_score', ascending=False).index))\n",
      "(chart_user_accum(all_accum_df, list(all_scores_div.sort_values(by='total_score', ascending=False).index), 'All Score') \n",
      " & chart_user_accum_delta(all_accum_df, list(all_scores_div.sort_values(by='total_score', ascending=False).index), 'All Score') \n",
      " & chart_user_accum_unique_delta_ratio(all_accum_df, list(all_scores_div.sort_values(by='total_score', ascending=False).index), 'All Score')).properties(title='All Score')\n",
      "41/75: new_celebies = two_mks_plus[two_mks_plus.from_id.isin(mat[mat.loc[:, list(all_scores_div.sort_values(by='total_score', ascending=False).index)[12:]].isna().all(axis=1)].index)]\n",
      "41/76: new_celebies.from_id.nunique(), two_mks_plus.from_id.nunique()\n",
      "41/77: user_counts = st.groupby('from_id').id.sum()\n",
      "41/78:\n",
      "perc = pd.qcut(user_counts, 20, duplicates='drop')\n",
      "perc.value_counts(sort=False)\n",
      "41/79:\n",
      "perc_bah = pd.cut(user_counts, [0,1,2,3,4,5,7,11,24,9359])\n",
      "(perc_bah.value_counts(sort=False))\n",
      "41/80: (perc_bah.value_counts(sort=False)/ user_counts.shape[0])\n",
      "41/81: (perc_bah.value_counts(sort=False)/ user_counts.shape[0]).plot(kind='bar',figsize=(10,7))\n",
      "41/82:\n",
      "dedicated = st[st.from_id.isin(user_counts[user_counts>24].index)]\n",
      "dedicated.from_id.nunique()\n",
      "41/83: st[st.from_id.isin(user_counts[user_counts>24].index)].id.sum() / st.id.sum()\n",
      "41/84: st[st.from_id.isin(user_counts[user_counts>11].index)].from_id.nunique() / st.from_id.nunique()\n",
      "41/85: st[st.from_id.isin(user_counts[user_counts>11].index)].id.sum() / st.id.sum()\n",
      "41/86:\n",
      "campers = campers[(~campers.from_id.isin(base.from_id))& (~campers.from_id.isin(dedicated.from_id))]\n",
      "campers.from_id.nunique()\n",
      "41/87:\n",
      "base = base[(~base.from_id.isin(dedicated.from_id))]\n",
      "base.from_id.nunique()\n",
      "41/88:\n",
      "two_mks_plus = two_mks_plus[(~two_mks_plus.from_id.isin(base.from_id)) \n",
      "                            & (~two_mks_plus.from_id.isin(campers.from_id))\n",
      "                            & (~two_mks_plus.from_id.isin(dedicated.from_id))]\n",
      "two_mks_plus.from_id.nunique()\n",
      "41/89: campers.from_id.nunique() + base.from_id.nunique() + two_mks_plus.from_id.nunique()\n",
      "41/90:\n",
      "fans_users = fans_users[~fans_users.from_id.isin(one_timers.index)\n",
      "                        & (~fans_users.from_id.isin(dedicated.from_id))]\n",
      "fans_users.from_id.nunique()\n",
      "41/91: ded_count = dedicated.from_id.nunique()\n",
      "41/92:\n",
      "ded_fans_users = (dedicated.loc[st.groupby('from_id').name.transform('nunique')==1, ['from_id', 'name', 'party']])\n",
      "ded_fans_users.from_id.nunique()/ded_count, dedicated.from_id.nunique()/ded_count\n",
      "41/93:\n",
      "ded_two_mks_plus = (dedicated.loc[st.groupby('from_id').name.transform('nunique')>1, ['from_id', 'name', 'party', 'camp']])\n",
      "ded_base = ded_two_mks_plus[ded_two_mks_plus.groupby('from_id').party.transform('nunique')==1]\n",
      "ded_campers = ded_two_mks_plus[ded_two_mks_plus.groupby('from_id').camp.transform('nunique')==1]\n",
      "ded_celebies = ded_two_mks_plus[ded_two_mks_plus.from_id.isin(mat[mat.loc[:, list(all_scores_div.sort_values(by='total_score', ascending=False).index)[12:]].isna().all(axis=1)].index)]\n",
      "dedicated.from_id.nunique(), ded_two_mks_plus.from_id.nunique()/ded_count, ded_base.from_id.nunique()/ded_count, ded_campers.from_id.nunique()/ded_count, ded_celebies.from_id.nunique()/ded_count\n",
      "41/94:\n",
      "ded_campers = ded_campers[(~ded_campers.from_id.isin(ded_base.from_id))]\n",
      "ded_campers.from_id.nunique()\n",
      "41/95:\n",
      "ded_two_mks_plus = ded_two_mks_plus[(~ded_two_mks_plus.from_id.isin(ded_base.from_id)) \n",
      "                                    & (~ded_two_mks_plus.from_id.isin(ded_campers.from_id))\n",
      "                                    & (~ded_two_mks_plus.from_id.isin(ded_celebies.from_id))]\n",
      "ded_two_mks_plus.from_id.nunique()\n",
      "41/96:\n",
      "ded_celebies = ded_celebies[(~ded_celebies.from_id.isin(ded_two_mks_plus.from_id)) \n",
      "                                    & (~ded_celebies.from_id.isin(ded_campers.from_id))\n",
      "                                    & (~ded_celebies.from_id.isin(ded_base.from_id))]\n",
      "ded_celebies.from_id.nunique()\n",
      "41/97: ded_campers.from_id.nunique() + ded_base.from_id.nunique() + ded_two_mks_plus.from_id.nunique() + ded_fans_users.from_id.nunique() + ded_celebies.from_id.nunique()\n",
      "41/98:\n",
      "labels = ('Dedicated Fans', 'Dedicated Basers', 'Dedicated Campers', 'Dedicated Celebies', 'Dedicated 2MK')\n",
      "ratios = (ded_fans_users.from_id.nunique()/ded_count, ded_base.from_id.nunique()/ded_count, ded_campers.from_id.nunique()/ded_count, ded_celebies.from_id.nunique()/ded_count, ded_two_mks_plus.from_id.nunique()/ded_count)\n",
      "dict(zip(labels,[round(x, 2) for x in ratios]))\n",
      "41/99:\n",
      "labels = ('All', 'One Timers', 'Fans', 'two_mks_plus', 'Basers', 'Campers')\n",
      "sets = [set(st.from_id.values), set(one_timers.index), set(fans_users.from_id.values), set(two_mks_plus.from_id.values), set(base.from_id.values), set(campers.from_id.values)]\n",
      "\n",
      "set_dict = dict(zip(labels, sets))\n",
      "41/100: df[df.is_post==1].shape[0], df[df.is_post==1].comment_count.mean()\n",
      "41/101: df[df.is_post==1].groupby('name').comment_count.mean().sort_values().plot(kind='barh', figsize=(10,15))\n",
      "41/102: one_timers.shape\n",
      "41/103:\n",
      "one_timers_page = st[st.from_id.isin(one_timers.index)].groupby('name').size()\n",
      "one_timers_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "41/104:\n",
      "gfans = st[st.from_id.isin(fans_users.from_id)].groupby('from_id')\n",
      "gfans.id.sum().mean(), gfans.post_id.sum().mean(), gfans.id.sum().mean()/ gfans.post_id.sum().mean()\n",
      "41/105:\n",
      "fans_page = st[st.from_id.isin(fans_users.from_id)].groupby('name').size()\n",
      "fans_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "41/106:\n",
      "gbase = st[st.from_id.isin(base.from_id)].groupby('from_id')\n",
      "gbase.id.sum().mean(), gbase.post_id.sum().mean(), gbase.id.sum().mean()/ gbase.post_id.sum().mean()\n",
      "41/107:\n",
      "base_page = st[st.from_id.isin(base.from_id)].groupby('name').size()\n",
      "base_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "41/108:\n",
      "gcamp = st[st.from_id.isin(campers.from_id)].groupby('from_id')\n",
      "gcamp.id.sum().mean(), gcamp.post_id.sum().mean(), gcamp.id.sum().mean()/ gcamp.post_id.sum().mean()\n",
      "41/109: st[st.from_id.isin(campers.from_id)].id.sum()\n",
      "41/110:\n",
      "camp_page = st[st.from_id.isin(campers.from_id)].groupby('name').size()\n",
      "camp_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "41/111: st[(st.from_id.isin(two_mks_plus.from_id))].id.sum()\n",
      "41/112:\n",
      "gtwo = st[(st.from_id.isin(two_mks_plus.from_id))].groupby('from_id')\n",
      "gtwo.id.sum().mean(), gtwo.post_id.sum().mean(), gtwo.id.sum().mean()/gtwo.post_id.sum().mean()\n",
      "41/113:\n",
      "twomk_page = st[st.from_id.isin(two_mks_plus.from_id)].groupby('name').size()\n",
      "twomk_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "41/114:\n",
      "gded = dedicated.groupby('from_id')\n",
      "gded.id.sum().mean(), gded.post_id.sum().mean(), gded.id.sum().mean()/ gded.post_id.sum().mean()\n",
      "41/115:\n",
      "ded_page = st[st.from_id.isin(dedicated.from_id)].groupby('name').size()\n",
      "ded_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "41/116:\n",
      "page_cats = pd.concat([one_timers_page, fans_page, base_page, camp_page, twomk_page, ded_page], axis=1,\n",
      "          keys=['One-timers', 'Fans', 'Basers', 'Campers', 'Two-MK-Plus', 'Dedicated'])\n",
      "41/117: page_cats = page_cats.stack().reset_index().rename(columns={'level_1': 'Category', 0: 'user_count'})\n",
      "41/118:\n",
      "regular = alt.Chart(page_cats).mark_bar().encode(\n",
      "    alt.X('user_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('Category:N'),\n",
      "    tooltip = ['name', 'Category','user_count']\n",
      ").properties(height=1200, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('user_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "41/119:\n",
      "labels = ('One Timers', 'Fans', 'Two-MKs-Plus', 'Basers', 'Campers', 'Dedicated')\n",
      "sets = [set(one_timers.index), \n",
      "        set(fans_users.from_id.values), \n",
      "        set(two_mks_plus.from_id.values), \n",
      "        set(base.from_id.values), \n",
      "        set(campers.from_id.values), \n",
      "        set(dedicated.from_id.values)]\n",
      "#df = df.drop('group_cat', axis=1)\n",
      "for l, s in zip(labels, sets):\n",
      "    df.loc[df.from_id.isin(s), 'group_cat'] = l\n",
      "\n",
      "df['group_cat'] = pd.Categorical(df.group_cat)\n",
      "df.group_cat.value_counts()/df.shape[0]\n",
      "41/120:\n",
      "for l, s in zip(labels, sets):\n",
      "    st.loc[st.from_id.isin(s), 'group_cat'] = l\n",
      "st['group_cat'] = pd.Categorical(st.group_cat)\n",
      "st.groupby('group_cat').id.sum()/df.shape[0]\n",
      "41/121: st.to_pickle('turf/total_stats_no_multiindex_groups.pkl.gz', compression='gzip')\n",
      "41/122: df.group_cat.value_counts()/df.shape[0]\n",
      "41/123:\n",
      "labels = ('Dedicated Fans', 'Dedicated Basers', 'Dedicated 2MK', 'Dedicated Campers', 'Dedicated Celebies')\n",
      "sets = [set(ded_fans_users.from_id.values), \n",
      "        set(ded_base.from_id.values), \n",
      "        set(ded_two_mks_plus.from_id.values), \n",
      "        set(ded_campers.from_id.values),\n",
      "        set(ded_celebies.from_id.values),]\n",
      "\n",
      "for l, s in zip(labels, sets):\n",
      "    st.loc[st.from_id.isin(s), 'new_group_cat'] = l\n",
      "    \n",
      "st['new_group_cat'] = pd.Categorical(st.new_group_cat)\n",
      "st.groupby('new_group_cat').id.sum()/df.shape[0]\n",
      "41/124:\n",
      "st.group_cat = st.new_group_cat.combine_first(st.group_cat)\n",
      "st.groupby('group_cat').id.sum()/df.shape[0]\n",
      "41/125: st.to_pickle('turf/total_stats_no_multiindex_groups_ded_cuts.pkl.gz', compression='gzip')\n",
      "41/126:\n",
      "group_lang = df.groupby(['name', 'group_cat', 'langdetect']).size().reset_index().rename(columns = {0: 'comment_count'})\n",
      "group_lang['langdetect'] = group_lang.langdetect.cat.add_categories(['other'])\n",
      "group_lang.loc[group_lang.langdetect=='cy', 'langdetect'] = 'ru'\n",
      "top_lang = group_lang.groupby('langdetect').comment_count.sum().sort_values(ascending=False).head(9).index\n",
      "group_lang.loc[~group_lang.langdetect.isin(top_lang), 'langdetect'] = 'other'\n",
      "group_lang = group_lang.groupby(['name', 'group_cat', 'langdetect']).comment_count.sum().reset_index()\n",
      "\n",
      "group_lang.langdetect.value_counts()\n",
      "41/127: alt.data_transformers.enable('default', max_rows=None)\n",
      "41/128:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#a6cee3','#1f78b4','#ff7f00','#fdbf6f','#e31a1c','#cab2d6','#33a02c'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['name', 'group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(width=150)\n",
      "\n",
      "(normalized).facet(column='group_cat').properties(title='Comment Count by MK, User Group Category and Language Detected')\n",
      "41/129:\n",
      "regular = alt.Chart(group_lang.groupby(['group_cat', 'langdetect']).comment_count.sum().reset_index()).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#a6cee3','#1f78b4','#ff7f00','#fdbf6f','#e31a1c','#cab2d6','#33a02c'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ")\n",
      "\n",
      "(normalized).properties(title='Comment Count by User Group Category and Language Detected')\n",
      "41/130:\n",
      "regular = alt.Chart(group_lang[group_lang.name!='Benjamin_Netanyahu'].groupby(['group_cat', 'langdetect']).comment_count.sum().reset_index()).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#a6cee3','#1f78b4','#ff7f00','#fdbf6f','#e31a1c','#cab2d6','#33a02c'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ")\n",
      "\n",
      "(normalized).properties(title='Comment Count by User Group Category and Language Detected (No Netanyahu)')\n",
      "41/131:\n",
      "post_lang = df[df.is_post==0].groupby(['name', 'group_cat', 'langdetect', 'post_id']).size().reset_index().rename(columns = {0: 'comment_count'})\n",
      "post_lang['langdetect'] = post_lang.langdetect.cat.add_categories(['other'])\n",
      "post_lang.loc[post_lang.langdetect=='cy', 'langdetect'] = 'ru'\n",
      "top_post_lang = post_lang.groupby('langdetect').comment_count.sum().sort_values(ascending=False).head(9).index\n",
      "post_lang.loc[~post_lang.langdetect.isin(top_post_lang), 'langdetect'] = 'other'\n",
      "post_lang['langdetect'] = post_lang.langdetect.cat.remove_unused_categories()\n",
      "post_lang = post_lang.groupby(['name', 'group_cat', 'langdetect', 'post_id']).comment_count.sum().reset_index()\n",
      "post_lang[post_lang.name =='Benjamin_Netanyahu'].groupby(['group_cat', 'langdetect']).comment_count.median()\n",
      "41/132:\n",
      "high = post_lang[(post_lang.name =='Benjamin_Netanyahu') & (post_lang.comment_count>1000) & (post_lang.langdetect=='en')]\n",
      "high\n",
      "41/133: df[(df.is_post==1) & (df.post_id.isin(high.post_id))][['comment_count', 'langdetect']]\n",
      "41/134:\n",
      "regular = alt.Chart(post_lang[(post_lang.name =='Benjamin_Netanyahu')]).mark_bar().encode(\n",
      "    alt.X('sum(comment_count):Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('post_id:N', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#a6cee3','#1f78b4','#ff7f00','#fdbf6f','#e31a1c','#cab2d6','#33a02c'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['post_id', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('sum(comment_count):Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(width=150)\n",
      "\n",
      "(normalized).facet(column='group_cat').properties(title='Comment Count by Post ID, User Group Category and Language Detected')\n",
      "41/135: normalized | regular\n",
      "41/136:\n",
      "regular = alt.Chart(page_cats).mark_bar().encode(\n",
      "    alt.X('user_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('Category:N'),\n",
      "    tooltip = ['name', 'Category','user_count']\n",
      ").properties(height=1500, width=400, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('user_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "41/137: page_cats.unstack()\n",
      "41/138: page_cats\n",
      "41/139: page_cats.unstack('Category')\n",
      "41/140:\n",
      "regular = alt.Chart(page_cats).mark_bar().encode(\n",
      "    alt.X('user_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('Category:N'),\n",
      "    tooltip = ['name', 'Category','user_count']\n",
      ").properties(height=1500, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('user_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "41/141: page_cats.unstack(-1)\n",
      "41/142: page_cats\n",
      "41/143: page_cats.pivot(index='name', columns='Category', value='user_count')\n",
      "41/144: page_cats.pivot(index='name', columns='Category', values='user_count')\n",
      "41/145:\n",
      "page_cats_table = page_cats.pivot(index='name', columns='Category', values='user_count')\n",
      "page_cats_table['total'] = page_cats_table.sum(axis=1)\n",
      "41/146:\n",
      "page_cats_table = page_cats.pivot(index='name', columns='Category', values='user_count')\n",
      "page_cats_table['total'] = page_cats_table.sum(axis=1)\n",
      "page_cats_table\n",
      "41/147:\n",
      "page_cats_table = page_cats.pivot(index='name', columns='Category', values='user_count')\n",
      "page_cats_table = page_cats_table / page_cats_table.sum(axis=1)\n",
      "page_cats_table\n",
      "41/148:\n",
      "page_cats_table = page_cats.pivot(index='name', columns='Category', values='user_count')\n",
      "#page_cats_table = page_cats_table / \n",
      "page_cats_table.sum(axis=1)\n",
      "41/149:\n",
      "page_cats_table = page_cats.pivot(index='name', columns='Category', values='user_count')\n",
      "page_cats_table = page_cats_table / page_cats_table.sum(axis=1)\n",
      "page_cats_table\n",
      "41/150:\n",
      "page_cats_table = page_cats.pivot(index='name', columns='Category', values='user_count')\n",
      "page_cats_table = page_cats_table.div(page_cats_table.sum(axis=1), axis=1)\n",
      "page_cats_table\n",
      "41/151:\n",
      "page_cats_table = page_cats.pivot(index='name', columns='Category', values='user_count')\n",
      "page_cats_table = page_cats_table.div(page_cats_table.sum(axis=1), axis=0)\n",
      "page_cats_table\n",
      "41/152:\n",
      "page_cats_table = page_cats.pivot(index='name', columns='Category', values='user_count')\n",
      "page_cats_table = page_cats_table.div(page_cats_table.sum(axis=1), axis=0).reindex(top_commented)\n",
      "page_cats_table\n",
      "41/153: page_cats_table.to_html()\n",
      "41/154: print(page_cats_table.to_html())\n",
      "41/155:\n",
      "page_cats_table = page_cats.pivot(index='name', columns='Category', values='user_count')\n",
      "page_cats_table = np.round(page_cats_table.div(page_cats_table.sum(axis=1), axis=0).reindex(top_commented), 2)\n",
      "page_cats_table\n",
      "41/156: print(page_cats_table.to_html())\n",
      "40/51: st.groupby('name', 'group_cat').id.sum()\n",
      "40/52: st.groupby(['name', 'group_cat']).id.sum()\n",
      "40/53: st.groupby(['name', 'group_cat']).id.sum().unstack()\n",
      "40/54: st.groupby(['name', 'group_cat']).id.sum().unstack().reindex(top_commented)\n",
      "40/55: top_commented = list(st.groupby('name').id.sum().sort_values(ascending=False).index)\n",
      "40/56: st.groupby(['name', 'group_cat']).id.sum().unstack().reindex(top_commented)\n",
      "40/57:\n",
      "import altair as alt\n",
      "alt.renderers.enable('notebook')\n",
      "40/58:\n",
      "page_groups = st.groupby(['name', 'group_cat']).id.sum().unstack().reindex(top_commented)\n",
      "page_groups = page_groups[['One Timers', \n",
      "                         'Fans', 'Dedicated Fans', \n",
      "                         'Basers', 'Dedicated Basers', \n",
      "                         'Campers', 'Dedicated Campers',\n",
      "                         'Two-MKs-Plus', 'Dedicated 2MK',\n",
      "                         'Dedicated Celebies']]\n",
      "40/59:\n",
      "page_groups = st.groupby(['name', 'group_cat']).id.sum().unstack().reindex(top_commented)\n",
      "page_groups = page_groups[['One Timers', \n",
      "                         'Fans', 'Dedicated Fans', \n",
      "                         'Basers', 'Dedicated Basers', \n",
      "                         'Campers', 'Dedicated Campers',\n",
      "                         'Two-MKs-Plus', 'Dedicated 2MK',\n",
      "                         'Dedicated Celebies']]\n",
      "page_groups()\n",
      "40/60:\n",
      "page_groups = st.groupby(['name', 'group_cat']).id.sum().unstack().reindex(top_commented)\n",
      "page_groups = page_groups[['One Timers', \n",
      "                         'Fans', 'Dedicated Fans', \n",
      "                         'Basers', 'Dedicated Basers', \n",
      "                         'Campers', 'Dedicated Campers',\n",
      "                         'Two-MKs-Plus', 'Dedicated 2MK',\n",
      "                         'Dedicated Celebies']]\n",
      "page_groups\n",
      "40/61:\n",
      "page_cats_alt = page_groups.stack().reset_index().rename(columns={'level_1': 'Category', 0: 'user_count'})\n",
      "regular = alt.Chart(page_groups).mark_bar().encode(\n",
      "    alt.X('user_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('Category:N'),\n",
      "    tooltip = ['name', 'Category','user_count']\n",
      ").properties(height=1500, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('user_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "40/62:\n",
      "page_cats_alt = page_groups.stack().reset_index().rename(columns={'level_1': 'Category', 0: 'user_count'})\n",
      "page_cats_alt\n",
      "40/63:\n",
      "page_cats_alt = page_groups.stack().reset_index().rename(columns={0: 'user_count'})\n",
      "regular = alt.Chart(page_groups).mark_bar().encode(\n",
      "    alt.X('user_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('Category:N'),\n",
      "    tooltip = ['name', 'group_cat','user_count']\n",
      ").properties(height=1500, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('user_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "40/64:\n",
      "page_cats_alt = page_groups.stack().reset_index().rename(columns={0: 'user_count'})\n",
      "regular = alt.Chart(page_cats_alt).mark_bar().encode(\n",
      "    alt.X('user_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('Category:N'),\n",
      "    tooltip = ['name', 'group_cat','user_count']\n",
      ").properties(height=1500, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('user_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "40/65:\n",
      "page_cats_alt = page_groups.stack().reset_index().rename(columns={0: 'user_count'})\n",
      "regular = alt.Chart(page_cats_alt).mark_bar().encode(\n",
      "    alt.X('user_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    tooltip = ['name', 'group_cat','user_count']\n",
      ").properties(height=1500, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('user_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "40/66:\n",
      "page_cats_alt = page_groups.stack().reset_index().rename(columns={0: 'user_count'}).fillna(0)\n",
      "regular = alt.Chart(page_cats_alt).mark_bar().encode(\n",
      "    alt.X('user_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    tooltip = ['name', 'group_cat','user_count']\n",
      ").properties(height=1500, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('user_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "40/67:\n",
      "page_groups = st.groupby(['name', 'group_cat']).id.sum().unstack().reindex(top_commented)\n",
      "page_groups = page_groups[['One Timers', \n",
      "                         'Fans', 'Dedicated Fans', \n",
      "                         'Basers', 'Dedicated Basers', \n",
      "                         'Campers', 'Dedicated Campers',\n",
      "                         'Two-MKs-Plus', 'Dedicated 2MK',\n",
      "                         'Dedicated Celebies']]\n",
      "page_groups = np.round(page_groups.div(page_groups.sum(axis=1), axis=0), 2)\n",
      "page_groups\n",
      "40/68: page_groups.to_html()\n",
      "40/69: print(page_groups.to_html())\n",
      "40/70: print(page_groups.fillna(0).to_html())\n",
      "40/71: groups[['users_ratio', 'comments_ratio']].plot(kind='bar')\n",
      "40/72: groups[['users_ratio', 'comments_ratio']].plot(kind='bar', figsize=(15, 12))\n",
      "40/73: groups[['users_ratio', 'comments_ratio']].plot(kind='bar', figsize=(15, 11))\n",
      "40/74: groups[['users_ratio', 'comments_ratio']].plot(kind='bar', figsize=(15, 11), rot=40)\n",
      "40/75: groups[['users_ratio', 'comments_ratio']].plot(kind='bar', figsize=(15, 11), rot=30)\n",
      "39/1: print(mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red').to_html())\n",
      "39/2:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "39/3: sns.__version__\n",
      "39/4:\n",
      "%%time\n",
      "df = pd.read_pickle('turf/data_df.pkl.gz', compression='gzip')\n",
      "39/5: df.dtypes\n",
      "39/6:\n",
      "sdf = pd.read_pickle('turf/shared_df.pkl.gz', compression='gzip')\n",
      "sdf.dtypes\n",
      "39/7:\n",
      "def percentile(n):\n",
      "    def percentile_(x):\n",
      "        return np.percentile(x, n)\n",
      "    percentile_.__name__ = 'percentile_%s' % n\n",
      "    return percentile_\n",
      "39/8:\n",
      "times = pd.read_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "st = pd.read_pickle('turf/total_stats_no_multiindex_groups_ded_cuts.pkl.gz', compression='gzip')\n",
      "39/9: st.dtypes\n",
      "39/10: from itertools import product\n",
      "39/11: times.dtypes\n",
      "39/12:\n",
      "columns = ['elapsed_time',  'created_time', 'created_month', 'created_day']\n",
      "stats = ['nunique', 'min', 'max']\n",
      "new_cols = ['from_id'] + ['_'.join(x) for x in product(columns, stats)] + ['period_length']\n",
      "new_cols\n",
      "39/13: times.columns = new_cols\n",
      "39/14: st[['from_id', 'group_cat']].head()\n",
      "39/15: times = times.merge(st[['from_id', 'group_cat']].drop_duplicates(), how='left', on='from_id')\n",
      "39/16: day_in_seconds = 60*60*24\n",
      "39/17: times.groupby('group_cat').period_length.median()/day_in_seconds\n",
      "39/18: times['period_length'] = times.period_length/day_in_seconds\n",
      "39/19: times[times.period_length<850].hist(bins=30, column='period_length', by='group_cat', layout=(11,1), sharex=True, figsize=(10,15), grid=True)\n",
      "39/20: tn = pd.read_pickle('turf/times_names.pkl.gz', compression='gzip')\n",
      "39/21: tn['period_length'] = tn.period_length/day_in_seconds\n",
      "39/22: tn.groupby(['group_cat']).period_length.median().to_frame().T\n",
      "39/23: tn.groupby(['name']).period_length.agg('mean')\n",
      "39/24:\n",
      "def get_heatmap(kind, field='period_length'):\n",
      "    mean_mk = tn.groupby(['name'])[field].agg(kind)\n",
      "    mean_group_cat = tn.groupby(['group_cat'])[field].agg(kind).to_frame().T\n",
      "    mean_group_cat.index=[kind]\n",
      "    mean_group_cat[kind] = tn[field].agg(kind)\n",
      "    mean_heatmap = (tn.groupby(['name', 'group_cat'])[field].agg(kind)\n",
      "                    .unstack())\n",
      "    mean_heatmap[kind] = mean_mk\n",
      "    mean_heatmap = mean_heatmap.sort_values(kind, ascending=False)\n",
      "    mean_heatmap.index = mean_heatmap.index.add_categories(kind)\n",
      "    mean_heatmap = mean_heatmap.append(mean_group_cat)\n",
      "    mean_heatmap = mean_heatmap.rename(columns = {'Dedicated 2MK': 'Dedicated Polemicists', 'Two-MKs-Plus': 'Polemicists'})\n",
      "    mean_heatmap = mean_heatmap[['One Timers', 'Fans', 'Basers', 'Campers', 'Polemicists', 'Dedicated Fans', 'Dedicated Basers',\n",
      "           'Dedicated Campers', 'Dedicated Polemicists', 'Dedicated Celebies', kind]]\n",
      "    return mean_heatmap\n",
      "mean_heatmap = get_heatmap('mean')\n",
      "median_heatmap = get_heatmap('median')\n",
      "mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "39/25: print(mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red').to_html())\n",
      "39/26: print(mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "39/27: mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "39/28: html_heatmap = mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red').render()\n",
      "39/29: open('outputs/mean_period_length.html', 'w').write(html_heatmap)\n",
      "39/30: mean_heatmap.to_csv('outputs/mean_period_length.csv')\n",
      "39/31:\n",
      "tn_c = tn.copy()\n",
      "tn_c['group_cat'] = tn_c.group_cat.astype(str)\n",
      "tn_c['name'] = tn_c.name.astype(str)\n",
      "39/32:\n",
      "order = ['One Timers', 'Fans', 'Basers', 'Campers', 'Two-MKs-Plus', 'Dedicated Fans', 'Dedicated Basers',\n",
      "       'Dedicated Campers', 'Dedicated 2MK', 'Dedicated Celebies']\n",
      "39/33: median_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "39/34:\n",
      "html_heatmap = median_heatmap.style.background_gradient(cmap='viridis').highlight_null('red').render()\n",
      "open('outputs/median_period_length.html', 'w').write(html_heatmap)\n",
      "40/76:\n",
      "import os \n",
      "import h5py\n",
      "\n",
      "if os.path.exists('turf/data.h5'):\n",
      "    with h5py.File('turf/data.h5', 'r') as hf:\n",
      "        tsne_results_40 = hf['tsne_results_per40_iter5000'][:]\n",
      "else:\n",
      "    tsne2 = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=5000)\n",
      "    tsne_results40 = tsne2.fit_transform(comms.iloc[:, 0:50].fillna(0).values)\n",
      "    \n",
      "    with h5py.File('turf/data.h5', 'w') as hf:\n",
      "        hf.create_dataset(\"tsne_results_per40_iter5000\",  data=tsne_results2)\n",
      "40/77: tsne_results_40.head()\n",
      "40/78: tsne_results_40\n",
      "40/79: from sklearn.manifold import TSNE\n",
      "40/80: mat = st.pivot(index='from_id', columns='name', values='id')\n",
      "40/81: mat.head()\n",
      "40/82: os.mkdir('turf/tsne')\n",
      "40/83:\n",
      "import os \n",
      "import h5py\n",
      "\n",
      "if os.path.exists('turf/tsne/data_40.h5'):\n",
      "    with h5py.File('turf/data.h5', 'r') as hf:\n",
      "        tsne_results_40 = hf['tsne_results_per40_iter5000'][:]\n",
      "else:\n",
      "    tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=5000)\n",
      "    tsne_results40 = tsne2.fit_transform(mat.values)\n",
      "    \n",
      "    with h5py.File('turf/data_40.h5', 'w') as hf:\n",
      "        hf.create_dataset(\"tsne_results_per40_iter5000\",  data=tsne_results40)\n",
      "40/84:\n",
      "import os \n",
      "import h5py\n",
      "\n",
      "if os.path.exists('turf/tsne/data_40.h5'):\n",
      "    with h5py.File('turf/data.h5', 'r') as hf:\n",
      "        tsne_results_40 = hf['tsne_results_per40_iter5000'][:]\n",
      "else:\n",
      "    tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=5000)\n",
      "    tsne_results40 = tsne.fit_transform(mat.values)\n",
      "    \n",
      "    with h5py.File('turf/data_40.h5', 'w') as hf:\n",
      "        hf.create_dataset(\"tsne_results_per40_iter5000\",  data=tsne_results40)\n",
      "40/85: mat = st.pivot(index='from_id', columns='name', values='id').fillna(0)\n",
      "40/86: mat.head()\n",
      "40/87: from sklearn.manifold import TSNE\n",
      "40/88: os.mkdir('turf/tsne')\n",
      "40/89:\n",
      "import os \n",
      "import h5py\n",
      "\n",
      "if os.path.exists('turf/tsne/data_40.h5'):\n",
      "    with h5py.File('turf/data.h5', 'r') as hf:\n",
      "        tsne_results_40 = hf['tsne_results_per40_iter5000'][:]\n",
      "else:\n",
      "    tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=5000)\n",
      "    tsne_results40 = tsne.fit_transform(mat.values)\n",
      "    \n",
      "    with h5py.File('turf/data_40.h5', 'w') as hf:\n",
      "        hf.create_dataset(\"tsne_results_per40_iter5000\",  data=tsne_results40)\n",
      "42/1:\n",
      "if os.path.exists('turf/data30.h5'):\n",
      "    with h5py.File('turf/data30.h5', 'r') as hf:\n",
      "        tsne_results30 = hf['tsne_results_per30_iter5000'][:]\n",
      "else:\n",
      "    tsne30 = TSNE(n_components=2, verbose=2, perplexity=30, n_iter=5000)\n",
      "    tsne_results30 = tsne30.fit_transform(comms.iloc[:, 0:50].fillna(0).values)\n",
      "    \n",
      "    with h5py.File('turf/data30.h5', 'w') as hf:\n",
      "        hf.create_dataset(\"tsne_results_per30_iter5000\",  data=tsne_results30)\n",
      "43/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "43/2:\n",
      "import altair as alt\n",
      "alt.renderers.enable('notebook')\n",
      "43/3: st = pd.read_pickle('turf/total_stats_no_multiindex_groups_ded_cuts.pkl.gz', compression='gzip')\n",
      "43/4: st.dtypes\n",
      "43/5: mat = st.pivot(index='from_id', columns='name', values='id').fillna(0)\n",
      "43/6: mat.head()\n",
      "43/7:\n",
      "import umap\n",
      "\n",
      "embedding = umap.UMAP().fit_transform(mat.values)\n",
      "43/8: st.head()\n",
      "43/9: st.head()\n",
      "43/10:\n",
      "import umap\n",
      "\n",
      "embedding = umap.UMAP().fit_transform(mat.values[st.set_index('from_id').group_cat.str.contains('MK')])\n",
      "43/11:\n",
      "import umap\n",
      "\n",
      "embedding = umap.UMAP().fit_transform(mat[st.set_index('from_id').group_cat.str.contains('MK')].values)\n",
      "43/12: st.set_index('from_id').group_cat.str.contains('MK')\n",
      "43/13:\n",
      "import umap\n",
      "\n",
      "embedding = umap.UMAP().fit_transform(mat.loc[st.set_index('from_id').group_cat.str.contains('MK')].values)\n",
      "43/14: mat.loc[st.set_index('from_id').group_cat.str.contains('MK')]\n",
      "43/15: st.from_id.value_counts()\n",
      "43/16: mat[st[st.set_index('from_id').group_cat.str.contains('MK')]]\n",
      "43/17: st[st.set_index('from_id').group_cat.str.contains('MK')]\n",
      "43/18: st[st.group_cat.str.contains('MK')]\n",
      "43/19: st[st.group_cat.str.contains('MK')].from_id.drop_duplicates()\n",
      "43/20:\n",
      "import umap\n",
      "\n",
      "embedding = umap.UMAP().fit_transform(mat[st[st.group_cat.str.contains('MK')].from_id.drop_duplicates().values].values)\n",
      "43/21: st[st.group_cat.str.contains('MK')].from_id.drop_duplicates().values\n",
      "43/22: mat.head()\n",
      "43/23:\n",
      "import umap\n",
      "\n",
      "embedding = umap.UMAP().fit_transform(mat.loc[st[st.group_cat.str.contains('MK')].from_id.drop_duplicates().values].values)\n",
      "43/24: mat.loc[st[st.group_cat.str.contains('MK')].from_id.drop_duplicates().values].shape\n",
      "43/25: mat.shape\n",
      "43/26: st.group_cat.value_counts()\n",
      "43/27: st.groupby('group_cat').from_id.nunique()\n",
      "43/28:\n",
      "import umap\n",
      "\n",
      "embedding = umap.UMAP().fit_transform(mat.loc[st[st.group_cat.str.contains('MK')].from_id.drop_duplicates().values].values)\n",
      "43/29: embedding\n",
      "43/30: plt.scatter(embedding)\n",
      "43/31: plt.scatter(embedding[:,0], embedding[1,:])\n",
      "43/32: plt.scatter(embedding[:,0], embedding[:,1])\n",
      "43/33: sns.set(style='white', rc={'figure.figsize':(12,8)})\n",
      "43/34: plt.scatter(embedding[:,0], embedding[:,1])\n",
      "43/35:\n",
      "import numpy as np\n",
      "import holoviews as hv\n",
      "import datashader as ds\n",
      "from holoviews.operation.datashader import datashade, shade, dynspread, rasterize\n",
      "from holoviews.operation import decimate\n",
      "hv.extension('bokeh','matplotlib')\n",
      "decimate.max_samples=1000\n",
      "dynspread.max_px=20\n",
      "dynspread.threshold=0.5\n",
      "43/36:\n",
      "import numpy as np\n",
      "import holoviews as hv\n",
      "import datashader as ds\n",
      "from holoviews.operation.datashader import datashade, shade, dynspread, rasterize\n",
      "from holoviews.operation import decimate\n",
      "hv.extension('bokeh','matplotlib')\n",
      "decimate.max_samples=1000\n",
      "dynspread.max_px=20\n",
      "dynspread.threshold=0.5\n",
      "43/37:\n",
      "import numpy as np\n",
      "import holoviews as hv\n",
      "import datashader as ds\n",
      "from holoviews.operation.datashader import datashade, shade, dynspread, rasterize\n",
      "from holoviews.operation import decimate\n",
      "hv.extension('bokeh','matplotlib')\n",
      "decimate.max_samples=1000\n",
      "dynspread.max_px=20\n",
      "dynspread.threshold=0.5\n",
      "43/38: ds.version\n",
      "43/39: ds.__ver__\n",
      "43/40: ds.__ver\n",
      "43/41: dir(ds)\n",
      "43/42: ds.__version__\n",
      "43/43:\n",
      "import numpy as np\n",
      "import holoviews as hv\n",
      "import datashader as ds\n",
      "from holoviews.operation.datashader import datashade, shade, dynspread, rasterize\n",
      "from holoviews.operation import decimate\n",
      "hv.extension('bokeh','matplotlib')\n",
      "decimate.max_samples=1000\n",
      "dynspread.max_px=20\n",
      "dynspread.threshold=0.5\n",
      "43/44:\n",
      "with h5py.File('turf/umap_mk.h5', 'w') as hf:\n",
      "    hf.create_dataset(\"umap_mk\",  data=embedding)\n",
      "43/45: import h5py\n",
      "43/46:\n",
      "with h5py.File('turf/umap_mk.h5', 'w') as hf:\n",
      "    hf.create_dataset(\"umap_mk\",  data=embedding)\n",
      "44/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "44/2:\n",
      "import altair as alt\n",
      "alt.renderers.enable('notebook')\n",
      "44/3: st = pd.read_pickle('turf/total_stats_no_multiindex_groups_ded_cuts.pkl.gz', compression='gzip')\n",
      "44/4: st.dtypes\n",
      "44/5: st.groupby('group_cat').from_id.nunique()\n",
      "44/6: mat.loc[st[st.group_cat.str.contains('MK')].from_id.drop_duplicates().values].shape\n",
      "44/7: mat = st.pivot(index='from_id', columns='name', values='id').fillna(0)\n",
      "44/8: mat.head()\n",
      "44/9: st.groupby('group_cat').from_id.nunique()\n",
      "44/10: mat.loc[st[st.group_cat.str.contains('MK')].from_id.drop_duplicates().values].shape\n",
      "44/11: import h5py\n",
      "44/12:\n",
      "import umap\n",
      "if os.path.exists('turf/umap_mk.h5'):\n",
      "    with h5py.File('turf/umap_mk.h5', 'r') as hf:\n",
      "        tsne_results40 = hf['umap_mk'][:]\n",
      "else:\n",
      "    embedding = umap.UMAP().fit_transform(mat.loc[st[st.group_cat.str.contains('MK')].from_id.drop_duplicates().values].values)\n",
      "    with h5py.File('turf/umap_mk.h5', 'w') as hf:\n",
      "        hf.create_dataset(\"umap_mk\",  data=embedding)\n",
      "44/13:\n",
      "import umap\n",
      "if os.path.exists('turf/umap_mk.h5'):\n",
      "    with h5py.File('turf/umap_mk.h5', 'r') as hf:\n",
      "        embedding = hf['umap_mk'][:]\n",
      "else:\n",
      "    embedding = umap.UMAP().fit_transform(mat.loc[st[st.group_cat.str.contains('MK')].from_id.drop_duplicates().values].values)\n",
      "    with h5py.File('turf/umap_mk.h5', 'w') as hf:\n",
      "        hf.create_dataset(\"umap_mk\",  data=embedding)\n",
      "44/14: import os\n",
      "44/15: import h5py\n",
      "44/16:\n",
      "import umap\n",
      "if os.path.exists('turf/umap_mk.h5'):\n",
      "    with h5py.File('turf/umap_mk.h5', 'r') as hf:\n",
      "        embedding = hf['umap_mk'][:]\n",
      "else:\n",
      "    embedding = umap.UMAP().fit_transform(mat.loc[st[st.group_cat.str.contains('MK')].from_id.drop_duplicates().values].values)\n",
      "    with h5py.File('turf/umap_mk.h5', 'w') as hf:\n",
      "        hf.create_dataset(\"umap_mk\",  data=embedding)\n",
      "44/17: sns.set(style='white', rc={'figure.figsize':(12,8)})\n",
      "44/18: plt.scatter(embedding[:,0], embedding[:,1])\n",
      "44/19:\n",
      "import numpy as np\n",
      "import holoviews as hv\n",
      "import datashader as ds\n",
      "from holoviews.operation.datashader import datashade, shade, dynspread, rasterize\n",
      "from holoviews.operation import decimate\n",
      "hv.extension('bokeh','matplotlib')\n",
      "decimate.max_samples=1000\n",
      "dynspread.max_px=20\n",
      "dynspread.threshold=0.5\n",
      "44/20: ds.__version__\n",
      "44/21: points = hv.Points(embedding, label=\"Points\")\n",
      "44/22: dynspread(datashade(points))\n",
      "44/23: dynspread(datashade(points)).opts(plot=dict(fig_size=200, aspect='square'))\n",
      "44/24: dynspread(datashade(points)).opts(plot=dict(fig_size=800, aspect='square'))\n",
      "44/25:\n",
      "%output size=200\n",
      "\n",
      "dynspread(datashade(points)).opts(plot=dict(fig_size=800, aspect='square'))\n",
      "44/26:\n",
      "%output size=200\n",
      "\n",
      "dynspread(datashade(points))\n",
      "44/27:\n",
      "%output size=600\n",
      "\n",
      "dynspread(datashade(points))\n",
      "44/28:\n",
      "%output size=300\n",
      "\n",
      "dynspread(datashade(points))\n",
      "44/29:\n",
      "%output size=300\n",
      "\n",
      "dynspread(datashade(points))(style=dict(cmap=\"fire\"))\n",
      "44/30:\n",
      "%output size=300\n",
      "\n",
      "dynspread(datashade(points)(style=dict(cmap=\"fire\")))\n",
      "44/31:\n",
      "%output size=300\n",
      "\n",
      "dynspread(datashade(points, cmap=\"fire\"))\n",
      "44/32:\n",
      "%output size=300\n",
      "from colorcet import fire\n",
      "datashade.cmap=fire[50:]\n",
      "\n",
      "dynspread(datashade(points, cmap=\"fire\"))\n",
      "44/33:\n",
      "%output size=300\n",
      "from colorcet import fire\n",
      "datashade.cmap=fire[50:]\n",
      "\n",
      "dynspread(datashade(points))\n",
      "44/34:\n",
      "%output size=300\n",
      "%opts RGB [bgcolor=\"black\" show_grid=False]\n",
      "from colorcet import fire\n",
      "datashade.cmap=fire[50:]\n",
      "\n",
      "dynspread(datashade(points))\n",
      "44/35:\n",
      "%output size=400\n",
      "%opts RGB [bgcolor=\"black\" show_grid=False]\n",
      "from colorcet import fire\n",
      "datashade.cmap=fire[50:]\n",
      "\n",
      "dynspread(datashade(points))\n",
      "44/36:\n",
      "%output size=300\n",
      "%opts RGB [bgcolor=\"black\" show_grid=False]\n",
      "from colorcet import fire\n",
      "datashade.cmap=fire[50:]\n",
      "\n",
      "dynspread(datashade(points))\n",
      "44/37: points = hv.Points(embedding, label=\"Polemicists & Dedicated UMAP embedding plot (datashaded)\")\n",
      "44/38:\n",
      "%output size=300\n",
      "%opts RGB [bgcolor=\"black\" show_grid=False]\n",
      "from colorcet import fire\n",
      "datashade.cmap=fire[50:]\n",
      "\n",
      "dynspread(datashade(points))\n",
      "44/39:\n",
      "%output size=300\n",
      "%opts RGB [bgcolor=\"black\" show_grid=False, show_axes=False]\n",
      "from colorcet import fire\n",
      "datashade.cmap=fire[50:]\n",
      "\n",
      "dynspread(datashade(points))\n",
      "44/40:\n",
      "%output size=300\n",
      "%opts RGB [bgcolor=\"black\" show_grid=False xaxis=None yaxis=None]\n",
      "from colorcet import fire\n",
      "datashade.cmap=fire[50:]\n",
      "\n",
      "dynspread(datashade(points))\n",
      "44/41:\n",
      "if os.path.exists('turf/umap_all.h5'):\n",
      "    with h5py.File('turf/umap_all.h5', 'r') as hf:\n",
      "        all_embedding = hf['umap_all'][:]\n",
      "else:\n",
      "    all_embedding = umap.UMAP().fit_transform(mat.values)\n",
      "    with h5py.File('turf/umap_all.h5', 'w') as hf:\n",
      "        hf.create_dataset(\"umap_all\",  data=embedding)\n",
      "44/42:\n",
      "points = hv.Points(embedding, label=\"Polemicists & Dedicated UMAP embedding plot (datashaded)\")\n",
      "\n",
      "%output size=300\n",
      "%opts RGB [bgcolor=\"black\" show_grid=False xaxis=None yaxis=None]\n",
      "from colorcet import fire\n",
      "datashade.cmap=fire[50:]\n",
      "\n",
      "dynspread(datashade(points))\n",
      "44/43:\n",
      "all_points = hv.Points(all_embedding, label=\"All Commenters UMAP embedding plot (datashaded)\")\n",
      "\n",
      "%output size=300\n",
      "%opts RGB [bgcolor=\"black\" show_grid=False xaxis=None yaxis=None]\n",
      "from colorcet import fire\n",
      "datashade.cmap=fire[50:]\n",
      "\n",
      "dynspread(datashade(all_points))\n",
      "44/44: datashade(points)\n",
      "44/45: datashade(all_points)\n",
      "44/46: st.group_cat.distinct()\n",
      "44/47: st.group_cat.unique()\n",
      "44/48: st[st.group_cat=='Basers'].get_loc()\n",
      "44/49: st[st.group_cat=='Basers'].reset_index().index\n",
      "44/50: np.where(st.group_cat=='Basers')\n",
      "44/51: all_points[np.where(st.group_cat=='Basers')]\n",
      "44/52: all_points[np.where(st.group_cat=='Basers'), :]\n",
      "44/53: all_points[np.where(st.group_cat=='Basers')]\n",
      "44/54: all_points.shape\n",
      "44/55: all_points[np.where(st.group_cat=='Basers'), :]\n",
      "44/56: all_points.shape\n",
      "44/57:\n",
      "res = pd.DataFrame(all_points,columns=['x', 'y']).set_index(mat.index)\n",
      "df.head()\n",
      "44/58: mat.index\n",
      "44/59:\n",
      "res = pd.DataFrame(all_points,columns=['x', 'y'])\n",
      "pd.concat(mat.index, res)\n",
      "44/60: res = pd.DataFrame(all_points,columns=['x', 'y'])\n",
      "44/61: all_points\n",
      "44/62:\n",
      "res = pd.DataFrame(all_embedding,columns=['x', 'y'])\n",
      "pd.concat(mat.index, res)\n",
      "44/63:\n",
      "res = pd.DataFrame(all_embedding,columns=['x', 'y'])\n",
      "pd.concat(mat.reset_index['from_id'], res)\n",
      "44/64:\n",
      "res = pd.DataFrame(all_embedding,columns=['x', 'y'])\n",
      "pd.concat(mat.reset_index()['from_id'], res)\n",
      "44/65: mat.reset_index()['from_id']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/66: mat.reset_index()[['from_id']]\n",
      "44/67: mat.head()\n",
      "44/68: mat.reset_index()\n",
      "44/69: mat.index = mat.index.astype(str)\n",
      "44/70:\n",
      "mat.index = mat.index.astype(str)\n",
      "mat.reset_index()['from_id']\n",
      "44/71: mat.dtypes()\n",
      "44/72: mat.dtypes\n",
      "44/73: mat.head()\n",
      "44/74: mat.describe()\n",
      "44/75: mat.index()\n",
      "44/76: mat.index\n",
      "44/77: mat.reset_index()\n",
      "44/78:\n",
      "res = pd.DataFrame(all_embedding,columns=['x', 'y'])\n",
      "pd.concat(mat.index.to_frame(index=False), res)\n",
      "44/79:\n",
      "res = pd.DataFrame(all_embedding,columns=['x', 'y'])\n",
      "pd.concat([mat.index.to_frame(index=False), res])\n",
      "44/80: res.head()\n",
      "44/81:\n",
      "res = pd.DataFrame(all_embedding,columns=['x', 'y'])\n",
      "pd.concat([mat.index.to_frame(index=False), res], axis=1)\n",
      "44/82:\n",
      "res = pd.DataFrame(all_embedding,columns=['x', 'y'])\n",
      "res = (pd.concat([mat.index.to_frame(index=False), res], axis=1)\n",
      "       .merge(st[['group_cat']], how='left'))\n",
      "44/83: st.head()\n",
      "44/84:\n",
      "res = pd.DataFrame(all_embedding,columns=['x', 'y'])\n",
      "res = (pd.concat([mat.index.to_frame(index=False), res], axis=1)\n",
      "       .merge(st[['group_cat']], how='left', on='from_id'))\n",
      "44/85:\n",
      "res = pd.DataFrame(all_embedding,columns=['x', 'y'])\n",
      "res = (pd.concat([mat.index.to_frame(index=False), res], axis=1)\n",
      "       .merge(st[['from_id', 'group_cat']], how='left', on='from_id'))\n",
      "44/86: res.dtypes()\n",
      "44/87: res.dtypes\n",
      "44/88: mat.index = mat.index.astype(int)\n",
      "44/89:\n",
      "#res = pd.DataFrame(all_embedding,columns=['x', 'y'])\n",
      "res = (pd.concat([mat.index.to_frame(index=False), res], axis=1)\n",
      "       .merge(st[['from_id', 'group_cat']], how='left', on='from_id'))\n",
      "44/90: apd = {c: res[res.group_cat==c] for c in res.group_cat.unique()}\n",
      "44/91: apd\n",
      "44/92: apd = {c: res.loc[res.group_cat==c, ['x', 'y']] for c in res.group_cat.unique()}\n",
      "44/93: apd\n",
      "44/94: apd = {c: hv.Points(res.loc[res.group_cat==c, ['x', 'y']]) for c in res.group_cat.unique()}\n",
      "44/95:\n",
      "kdims=['d1','d2']\n",
      "num_ks=8\n",
      "\n",
      "apd = {c: hv.Points(res.loc[res.group_cat==c, ['x', 'y']]) for c in res.group_cat.unique()}\n",
      "\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k')))\n",
      "44/96:\n",
      "kdims=['d1','d2']\n",
      "num_ks=8\n",
      "\n",
      "apd = {c: hv.Points(res.loc[res.group_cat==c, ['x', 'y']]) for c in res.group_cat.unique()}\n",
      "\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k')))\n",
      "\n",
      "apdspread\n",
      "44/97:\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k')))\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(enumerate(Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * gaussspread\n",
      "44/98:\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k')))\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(enumerate(Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/99:\n",
      "kdims=['d1','d2']\n",
      "num_ks=res.group_cat.nunique()\n",
      "\n",
      "apd = {c: hv.Points(res.loc[res.group_cat==c, ['x', 'y']]) for c in res.group_cat.unique()}\n",
      "\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k')))\n",
      "\n",
      "apdspread\n",
      "44/100:\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k')))\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(res.group_cat.unique(), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/101: res.group_cat.unique()\n",
      "44/102: list(zip(res.group_cat.unique(), Sets1to3[0:num_ks]))\n",
      "44/103: {k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key}\n",
      "44/104:\n",
      "%opts RGB [bgcolor=\"white\"]\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k')))\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(res.group_cat.unique(), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/105:\n",
      "%opts RGB [bgcolor=\"black\"]\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k')))\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(res.group_cat.unique(), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/106: apd\n",
      "44/107:\n",
      "%opts RGB [bgcolor=\"black\"]\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k')))\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.group_cat.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/108:\n",
      "%opts RGB [bgcolor=\"black\"]\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k'))\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.group_cat.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/109:\n",
      "%opts RGB [bgcolor=\"black\"]\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k')))\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.group_cat.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/110:\n",
      "%opts RGB [bgcolor=\"black\"]\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k'), label=\"All Commenters UMAP embedding plot (datashaded), colored by group\"))\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.group_cat.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/111:\n",
      "%opts RGB [bgcolor=\"black\"]\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k', label=\"All Commenters UMAP embedding plot (datashaded), colored by group\"), aggregator=ds.count_cat('k')))\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.group_cat.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/112:\n",
      "%opts RGB [bgcolor=\"black\"]\n",
      "%%opts Scatter [tools=['hover', 'box_select']]\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k')))\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.group_cat.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/113:\n",
      "%opts RGB [bgcolor=\"black\"]\n",
      "%opts Scatter [tools=['hover', 'box_select']] (line_color=\"black\" fill_color=\"red\" size=10)\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k')))\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.group_cat.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/114:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts Scatter [tools=['hover', 'box_select']]\n",
      "from holoviews.streams import RangeXY\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k')))\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.group_cat.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/115:\n",
      "%opts RGB [bgcolor=\"black\"]\n",
      "from holoviews.streams import RangeXY\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k')))\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.group_cat.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/116:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "from holoviews.streams import RangeXY\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k'))) * \\\n",
      "    hv.util.Dynamic(aggregate(points, width=10, height=10, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.group_cat.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/117:\n",
      "import numpy as np\n",
      "import holoviews as hv\n",
      "import datashader as ds\n",
      "from holoviews.operation.datashader import aggregate, datashade, shade, dynspread\n",
      "from holoviews.operation import decimate\n",
      "hv.extension('bokeh','matplotlib')\n",
      "decimate.max_samples=1000\n",
      "dynspread.max_px=20\n",
      "dynspread.threshold=0.5\n",
      "44/118:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "from holoviews.streams import RangeXY\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k'))) * \\\n",
      "    hv.util.Dynamic(aggregate(points, width=10, height=10, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.group_cat.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/119:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "from holoviews.streams import RangeXY\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k'))) * \\\n",
      "    hv.util.Dynamic(aggregate(points, width=5, height=5, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.group_cat.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/120:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "from holoviews.streams import RangeXY\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k'))) * \\\n",
      "    hv.util.Dynamic(aggregate(points, width=2, height=2, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.group_cat.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/121:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "from holoviews.streams import RangeXY\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k'))) * \\\n",
      "    hv.util.Dynamic(aggregate(points, width=20, height=20, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.group_cat.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/122:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "from holoviews.streams import RangeXY\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k'))) * \\\n",
      "    hv.util.Dynamic(aggregate(points, width=100, height=100, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.group_cat.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/123:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "from holoviews.streams import RangeXY\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k'), cmap=Sets1to3)) * \\\n",
      "    hv.util.Dynamic(aggregate(points, width=100, height=100, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.group_cat.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/124:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "from holoviews.streams import RangeXY\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k'), cmap=Sets1to3)) * \\\n",
      "    hv.util.Dynamic(aggregate(apd, width=100, height=100, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.group_cat.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/125:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "from holoviews.streams import RangeXY\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k'), cmap=Sets1to3)) * \\\n",
      "    hv.util.Dynamic(aggregate(all_points, width=100, height=100, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.group_cat.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/126:\n",
      "#res = pd.DataFrame(all_embedding,columns=['x', 'y'])\n",
      "res = (pd.concat([mat.index.to_frame(index=False), res], axis=1)\n",
      "       .merge(st[['from_id', 'group_cat']], how='left', on='from_id'))\n",
      "\n",
      "res.loc[res.group_cat.str.startswith('Ded'), 'group_cat'] = 'Dedicated'\n",
      "44/127:\n",
      "res = pd.DataFrame(all_embedding,columns=['x', 'y'])\n",
      "res = (pd.concat([mat.index.to_frame(index=False), res], axis=1)\n",
      "       .merge(st[['from_id', 'group_cat']], how='left', on='from_id'))\n",
      "\n",
      "res.loc[res.group_cat.str.startswith('Ded'), 'group_cat'] = 'Dedicated'\n",
      "44/128:\n",
      "kdims=['d1','d2']\n",
      "num_ks=res.group_cat.nunique()\n",
      "\n",
      "apd = {c: hv.Points(res.loc[res.group_cat==c, ['x', 'y']]) for c in res.group_cat.unique()}\n",
      "\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k')))\n",
      "\n",
      "apdspread\n",
      "44/129: {k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key}\n",
      "44/130:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "from holoviews.streams import RangeXY\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k'), cmap=Sets1to3)) * \\\n",
      "    hv.util.Dynamic(aggregate(all_points, width=100, height=100, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.group_cat.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/131:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "from holoviews.streams import RangeXY\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k'), cmap=Sets1to3)) * \\\n",
      "    hv.util.Dynamic(aggregate(all_points, width=100, height=100, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.group_cat.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/132:\n",
      "all_points = hv.Points(all_embedding, label=\"All Commenters UMAP embedding plot (datashaded)\")\n",
      "\n",
      "%output size=300\n",
      "%opts RGB [bgcolor=\"black\" show_grid=False xaxis=None yaxis=None]\n",
      "from colorcet import fire\n",
      "datashade.cmap=fire[50:]\n",
      "\n",
      "dynspread(datashade(all_points))\n",
      "44/133:\n",
      "topmk = mat.idxmax(axis=1).reset_index().drop('from_id', axis=1).rename(columns={0:'topmk'})\n",
      "topmk.head()\n",
      "44/134: top_commented = list(st.groupby('name').id.sum().sort_values(ascending=False).index)\n",
      "44/135:\n",
      "topmk = mat.idxmax(axis=1).reset_index().drop('from_id', axis=1).rename(columns={0:'topmk'})\n",
      "topmk.loc[topmk.notin(top_commented[:6]), 'topmk'] = 'Other'\n",
      "\n",
      "topmk.head()\n",
      "44/136:\n",
      "topmk = mat.idxmax(axis=1).reset_index().drop('from_id', axis=1).rename(columns={0:'topmk'})\n",
      "topmk.loc[~topmk.isin(top_commented[:6]), 'topmk'] = 'Other'\n",
      "\n",
      "topmk.head()\n",
      "44/137:\n",
      "topmk = mat.idxmax(axis=1).reset_index().drop('from_id', axis=1).rename(columns={0:'topmk'})\n",
      "topmk.loc[~topmk.isin(top_commented[:6]), 'topmk'] = 'Other'\n",
      "\n",
      "topmk.head()\n",
      "44/138:\n",
      "topmk = mat.idxmax(axis=1).reset_index().drop('from_id', axis=1).rename(columns={0:'topmk'})\n",
      "topmk.ix[~topmk.isin(top_commented[:6]), 'topmk'] = 'Other'\n",
      "\n",
      "topmk.head()\n",
      "44/139:\n",
      "topmk = mat.idxmax(axis=1).reset_index().drop('from_id', axis=1).rename(columns={0:'topmk'})\n",
      "topmk.loc[~topmk.isin(top_commented[:6]), 'topmk'] = 'Other'\n",
      "\n",
      "topmk.head()\n",
      "44/140:\n",
      "topmk = mat.idxmax(axis=1).reset_index().drop('from_id', axis=1).rename(columns={0:'topmk'})\n",
      "topmk.loc[:, 'topmk'] = 'Other'\n",
      "\n",
      "topmk.head()\n",
      "44/141:\n",
      "topmk = mat.idxmax(axis=1).reset_index().drop('from_id', axis=1).rename(columns={0:'topmk'})\n",
      "topmk.loc[(~topmk.isin(top_commented[:6])), 'topmk'] = 'Other'\n",
      "\n",
      "topmk.head()\n",
      "44/142:\n",
      "topmk = mat.idxmax(axis=1).reset_index().drop('from_id', axis=1).rename(columns={0:'topmk'})\n",
      "topmk.loc[!topmk.isin(top_commented[:6]), 'topmk'] = 'Other'\n",
      "\n",
      "topmk.head()\n",
      "44/143:\n",
      "topmk = mat.idxmax(axis=1).reset_index().drop('from_id', axis=1).rename(columns={0:'topmk'})\n",
      "topmk.loc[topmk.isin(top_commented[6:]), 'topmk'] = 'Other'\n",
      "\n",
      "topmk.head()\n",
      "44/144:\n",
      "topmk = mat.idxmax(axis=1).reset_index().drop('from_id', axis=1).rename(columns={0:'topmk'})\n",
      "#topmk.loc[topmk.isin(top_commented[6:]), 'topmk'] = 'Other'\n",
      "\n",
      "topmk.head()\n",
      "44/145:\n",
      "topmk = mat.idxmax(axis=1).reset_index().drop('from_id', axis=1).rename(columns={0:'topmk'})\n",
      "topmk.iloc[topmk.isin(top_commented[6:]), 0] = 'Other'\n",
      "\n",
      "topmk.head()\n",
      "44/146:\n",
      "topmk = mat.idxmax(axis=1).reset_index().drop('from_id', axis=1).rename(columns={0:'topmk'})\n",
      "topmk.loc[topmk.isin(top_commented[6:])] = 'Other'\n",
      "\n",
      "topmk.head()\n",
      "44/147: top_commented[6:]\n",
      "44/148: top_commented[8:]\n",
      "44/149:\n",
      "topmk = mat.idxmax(axis=1).reset_index().drop('from_id', axis=1).rename(columns={0:'topmk'})\n",
      "\n",
      "topmk.head()\n",
      "44/150: topmk.loc[topmk.isin(top_commented[8:]), 'topmk'] = 'Other'\n",
      "44/151: topmk.loc[topmk.isin(top_commented[8:])] = 'Other'\n",
      "44/152:\n",
      "topmk = mat.idxmax(axis=1).reset_index().drop('from_id', axis=1).rename(columns={0:'topmk'}).to_series()\n",
      "\n",
      "topmk.head()\n",
      "44/153:\n",
      "topmk = mat.idxmax(axis=1).reset_index().drop('from_id', axis=1).rename(columns={0:'topmk'})[['topmk']]\n",
      "\n",
      "topmk.head()\n",
      "44/154:\n",
      "topmk = mat.idxmax(axis=1).reset_index().drop('from_id', axis=1).rename(columns={0:'topmk'})['topmk']\n",
      "\n",
      "topmk.head()\n",
      "44/155: topmk.loc[topmk.isin(top_commented[8:])] = 'Other'\n",
      "44/156:\n",
      "topmk = mat.idxmax(axis=1).reset_index().drop('from_id', axis=1).rename(columns={0:'topmk'})['topmk']\n",
      "topmk.loc[topmk.isin(top_commented[8:])] = 'Other'\n",
      "\n",
      "topmk.head()\n",
      "44/157: topmk.head(20)\n",
      "44/158:\n",
      "res = pd.concat([res, topmk])\n",
      "res.head()\n",
      "44/159:\n",
      "res = pd.DataFrame(all_embedding,columns=['x', 'y'])\n",
      "res = (pd.concat([mat.index.to_frame(index=False), res], axis=1)\n",
      "       .merge(st[['from_id', 'group_cat']], how='left', on='from_id'))\n",
      "\n",
      "res.loc[res.group_cat.str.startswith('Ded'), 'group_cat'] = 'Dedicated'\n",
      "44/160:\n",
      "res = pd.concat([res, topmk], axis=1)\n",
      "res.head()\n",
      "44/161:\n",
      "kdims=['d1','d2']\n",
      "num_ks=res.group_cat.nunique()\n",
      "\n",
      "aptopmk = {c: hv.Points(res.loc[res.group_cat==c, ['x', 'y']]) for c in res.topmk.unique()}\n",
      "\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k')))\n",
      "\n",
      "apdspread\n",
      "44/162:\n",
      "kdims=['d1','d2']\n",
      "num_ks=res.topmk.nunique()\n",
      "\n",
      "aptopmk = {c: hv.Points(res.loc[res.group_cat==c, ['x', 'y']]) for c in res.topmk.unique()}\n",
      "\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k')))\n",
      "\n",
      "apdspread\n",
      "44/163:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "from holoviews.streams import RangeXY\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(aptopmk, kdims='k'), aggregator=ds.count_cat('k'), cmap=Sets1to3)) * \\\n",
      "    hv.util.Dynamic(aggregate(all_points, width=100, height=100, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.topmk.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/164:\n",
      "kdims=['d1','d2']\n",
      "num_ks=res.topmk.nunique()\n",
      "\n",
      "aptopmk = {c: hv.Points(res.loc[res.group_cat==c, ['x', 'y']]) for c in res.topmk.unique()}\n",
      "\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(aptopmk, kdims='k'), aggregator=ds.count_cat('k')))\n",
      "\n",
      "apdspread\n",
      "44/165:\n",
      "kdims=['d1','d2']\n",
      "num_ks=res.topmk.nunique()\n",
      "\n",
      "aptopmk = {c: hv.Points(res.loc[res.topmk==c, ['x', 'y']]) for c in res.topmk.unique()}\n",
      "\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(aptopmk, kdims='k'), aggregator=ds.count_cat('k')))\n",
      "\n",
      "apdspread\n",
      "44/166:\n",
      "kdims=['d1','d2']\n",
      "num_ks=res.topmk.nunique()\n",
      "\n",
      "aptopmk = {c: hv.Points(res.loc[res.topmk==c, ['x', 'y']]) for c in res.topmk.unique()}\n",
      "\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(aptopmk, kdims='k'), aggregator=ds.count_cat('k')))\n",
      "\n",
      "apdspread\n",
      "44/167:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "from holoviews.streams import RangeXY\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(aptopmk, kdims='k'), aggregator=ds.count_cat('k'), cmap=Sets1to3)) * \\\n",
      "    hv.util.Dynamic(aggregate(all_points, width=100, height=100, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.topmk.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/168: {c: hv.Points(res.loc[res.topmk==c, ['x', 'y']]) for c in res.topmk.unique()}\n",
      "44/169: res[res.topmk.isna()].shape\n",
      "44/170:\n",
      "res = pd.DataFrame(all_embedding,columns=['x', 'y'])\n",
      "res = (pd.concat([mat.index.to_frame(index=False), res], axis=1)\n",
      "       .merge(st[['from_id', 'group_cat']], how='left', on='from_id'))\n",
      "\n",
      "res.loc[res.group_cat.str.startswith('Ded'), 'group_cat'] = 'Dedicated'\n",
      "44/171: res.head()\n",
      "44/172: mat.head()\n",
      "44/173:\n",
      "res = pd.DataFrame(all_embedding,columns=['x', 'y'])\n",
      "res.head()\n",
      "44/174:\n",
      "res = (pd.concat([mat.index.to_frame(index=False), res], axis=1)\n",
      "       .merge(st[['from_id', 'group_cat']], how='left', on='from_id'))\n",
      "res.head()\n",
      "44/175:\n",
      "res = pd.DataFrame(all_embedding,columns=['x', 'y'])\n",
      "res.shape\n",
      "44/176:\n",
      "res = (pd.concat([mat.index.to_frame(index=False), res], axis=1)\n",
      "       .merge(st[['from_id', 'group_cat']], how='left', on='from_id'))\n",
      "res.shape\n",
      "44/177:\n",
      "res = pd.concat([mat.index.to_frame(index=False), res], axis=1)\n",
      "       \n",
      "res.shape\n",
      "44/178:\n",
      "res = pd.DataFrame(all_embedding,columns=['x', 'y'])\n",
      "res.shape\n",
      "44/179:\n",
      "res = pd.concat([mat.index.to_frame(index=False), res], axis=1)\n",
      "       \n",
      "res.shape\n",
      "44/180: res = res.merge(st[['from_id', 'group_cat']], how='left', on='from_id'))\n",
      "44/181:\n",
      "res = res.merge(st[['from_id', 'group_cat']], how='left', on='from_id')\n",
      "res.shape\n",
      "44/182:\n",
      "res = (pd.concat([mat.index.to_frame(index=False), res], axis=1)\n",
      "       .merge(st[['from_id', 'group_cat']], how='left', on='from_id')\n",
      "       .drop_duplicates())\n",
      "res.shape\n",
      "44/183:\n",
      "res = pd.DataFrame(all_embedding,columns=['x', 'y'])\n",
      "res.shape\n",
      "44/184:\n",
      "res = (pd.concat([mat.index.to_frame(index=False), res], axis=1)\n",
      "       .merge(st[['from_id', 'group_cat']], how='left', on='from_id')\n",
      "       .drop_duplicates())\n",
      "res.shape\n",
      "44/185:\n",
      "res = pd.DataFrame(all_embedding,columns=['x', 'y'])\n",
      "res = (pd.concat([mat.index.to_frame(index=False), res], axis=1)\n",
      "       .merge(st[['from_id', 'group_cat']], how='left', on='from_id')\n",
      "       .drop_duplicates())\n",
      "res.shape\n",
      "44/186: res.loc[res.group_cat.str.startswith('Ded'), 'group_cat'] = 'Dedicated'\n",
      "44/187: res.head()\n",
      "44/188:\n",
      "res = pd.concat([res, topmk], axis=1)\n",
      "res.head()\n",
      "44/189:\n",
      "res = pd.DataFrame(all_embedding,columns=['x', 'y'])\n",
      "res = (pd.concat([mat.index.to_frame(index=False), res], axis=1)\n",
      "       .merge(st[['from_id', 'group_cat']], how='left', on='from_id')\n",
      "       .drop_duplicates().reset_index())\n",
      "res.head()\n",
      "44/190:\n",
      "res = pd.DataFrame(all_embedding,columns=['x', 'y'])\n",
      "res = (pd.concat([mat.index.to_frame(index=False), res], axis=1)\n",
      "       .merge(st[['from_id', 'group_cat']], how='left', on='from_id')\n",
      "       .drop_duplicates().reset_index().drop('index', axis=1))\n",
      "res.head()\n",
      "44/191: res.loc[res.group_cat.str.startswith('Ded'), 'group_cat'] = 'Dedicated'\n",
      "44/192: res.head()\n",
      "44/193:\n",
      "topmk = mat.idxmax(axis=1).reset_index().drop('from_id', axis=1).rename(columns={0:'topmk'})['topmk']\n",
      "topmk.loc[topmk.isin(top_commented[8:])] = 'Other'\n",
      "44/194:\n",
      "res = pd.concat([res, topmk], axis=1)\n",
      "res.head()\n",
      "44/195: res[res.topmk.isna()].shape\n",
      "44/196:\n",
      "kdims=['d1','d2']\n",
      "num_ks=res.topmk.nunique()\n",
      "\n",
      "aptopmk = {c: hv.Points(res.loc[res.topmk==c, ['x', 'y']]) for c in res.topmk.unique()}\n",
      "\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(aptopmk, kdims='k'), aggregator=ds.count_cat('k')))\n",
      "\n",
      "apdspread\n",
      "44/197:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "from holoviews.streams import RangeXY\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(aptopmk, kdims='k'), aggregator=ds.count_cat('k'), cmap=Sets1to3)) * \\\n",
      "    hv.util.Dynamic(aggregate(all_points, width=100, height=100, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.topmk.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/198:\n",
      "kdims=['d1','d2']\n",
      "num_ks=res.topmk.nunique()\n",
      "\n",
      "aptopmk = {c: hv.Points(res.loc[res.topmk==c, ['x', 'y']]) for c in res.topmk.unique()}\n",
      "45/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "45/2:\n",
      "%%time\n",
      "df = pd.read_pickle('turf/data_df.pkl.gz', compression='gzip')\n",
      "45/3: df.dtypes\n",
      "45/4:\n",
      "sdf = pd.read_pickle('turf/shared_df.pkl.gz', compression='gzip')\n",
      "sdf.dtypes\n",
      "45/5: st = pd.read_pickle('turf/total_stats_no_multiindex.pkl.gz', compression='gzip')\n",
      "45/6:\n",
      "# OLD STUFF\n",
      "#save(st, 'turf/total_stats.pkl.gz')\n",
      "#st = pd.read_pickle( 'turf/total_stats.pkl.gz', compression='gzip')\n",
      "45/7: st.head()\n",
      "45/8: st.shape\n",
      "45/9: df[df.is_post==0].shape\n",
      "45/10: print ('Total number of comments: %d' % st.id.sum())\n",
      "45/11: print ('Total number of distinct users (commenters): %d' % st.from_id.nunique())\n",
      "45/12:\n",
      "counts = st.groupby('from_id').id.sum()\n",
      "one_timers = counts[counts==1]\n",
      "\n",
      "print ('Total number of one-timers: %d' % one_timers.shape[0])\n",
      "print ('Ratio of one timers from all users: %.2f' % (one_timers.shape[0] / st.from_id.nunique()))\n",
      "45/13: mat = st.pivot(index='from_id', columns='name', values='id')\n",
      "45/14: mat[(mat.count(axis=1)==1)].shape[0]\n",
      "45/15: mat.loc[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu==1), 'Benjamin_Netanyahu'].shape[0]\n",
      "45/16:\n",
      "print ('Ratio of Bibi from one-timers: %.2f' % (mat[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu==1)].shape[0] \n",
      " / counts[counts==1].shape[0]))\n",
      "45/17: print ('Ratio of users that commented 10 times and up: %.2f' % (counts[counts>=10].shape[0]/st.from_id.nunique()))\n",
      "45/18: (st.reset_index().groupby('from_id').name.nunique().value_counts().head(20)/st.reset_index().from_id.nunique()).plot(kind='bar')\n",
      "45/19: (st.reset_index().groupby('from_id').id.sum().value_counts().head(20)/st.reset_index().from_id.nunique()).plot(kind='bar')\n",
      "45/20:\n",
      "fans_users = (st.reset_index()\n",
      " .loc[st.groupby('from_id').name.transform('nunique')==1, ['from_id', 'name', 'party']])\n",
      "45/21: fans = ( fans_users.name.value_counts())\n",
      "45/22: fans.head(10)\n",
      "45/23:\n",
      "print ('Total number of fans: %d' % fans.sum())\n",
      "print ('Ratio of fans from all users: %.2f' % (fans.sum() / st.from_id.nunique()))\n",
      "print ('Ratio of one-timers from fans: %.2f' % (counts[counts==1].shape[0] / fans.sum()))\n",
      "45/24:\n",
      "(pd.concat([fans, \n",
      "            fans/st.from_id.nunique(), \n",
      "            fans/st.groupby('name').from_id.nunique()],\n",
      "            axis=1, keys=['num_uniques', 'ratio_from_all_users', 'ratio_from_own_users'])\n",
      "             .sort_values(by='ratio_from_own_users', ascending=False)).head(10)\n",
      "45/25:\n",
      "mkdet = pd.read_csv('turf/mk_details.csv', sep='\\t')\n",
      "mkdet.head()\n",
      "45/26: df['party'] = df['name'].map(mkdet.set_index('folder_name')['party'])\n",
      "45/27: df.groupby(['party', 'langdetect']).size().unstack()\n",
      "45/28: st['party'] = st['name'].map(mkdet.set_index('folder_name')['party'])\n",
      "45/29:\n",
      "two_mks_plus = (st.reset_index()\n",
      " .loc[st.groupby('from_id').name.transform('nunique')>1, ['from_id', 'name', 'party']])\n",
      "45/30: two_mks_plus.head()\n",
      "45/31: two_mks_plus.from_id.nunique()\n",
      "45/32: two_mks_plus.party.value_counts()\n",
      "45/33: two_mks_plus.groupby('party').from_id.nunique()\n",
      "45/34: base = two_mks_plus[two_mks_plus.groupby('from_id').party.transform('nunique')==1]\n",
      "45/35:\n",
      "print ('Total number of basers: %d' % base.from_id.nunique())\n",
      "print ('Ratio of basers from all users: %.2f' % (base.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of basers from two-mks-plus: %.2f' % (base.from_id.nunique() / two_mks_plus.from_id.nunique()))\n",
      "45/36: st.party.unique()\n",
      "45/37:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "45/38:\n",
      "%%time\n",
      "df = pd.read_pickle('turf/data_df.pkl.gz', compression='gzip')\n",
      "45/39: df.dtypes\n",
      "45/40:\n",
      "sdf = pd.read_pickle('turf/shared_df.pkl.gz', compression='gzip')\n",
      "sdf.dtypes\n",
      "45/41: st = pd.read_pickle('turf/total_stats_no_multiindex.pkl.gz', compression='gzip')\n",
      "45/42:\n",
      "# OLD STUFF\n",
      "#save(st, 'turf/total_stats.pkl.gz')\n",
      "#st = pd.read_pickle( 'turf/total_stats.pkl.gz', compression='gzip')\n",
      "45/43: st.head()\n",
      "45/44: st.shape\n",
      "45/45: df[df.is_post==0].shape\n",
      "45/46: print ('Total number of comments: %d' % st.id.sum())\n",
      "45/47: print ('Total number of distinct users (commenters): %d' % st.from_id.nunique())\n",
      "45/48:\n",
      "counts = st.groupby('from_id').id.sum()\n",
      "one_timers = counts[counts==1]\n",
      "\n",
      "print ('Total number of one-timers: %d' % one_timers.shape[0])\n",
      "print ('Ratio of one timers from all users: %.2f' % (one_timers.shape[0] / st.from_id.nunique()))\n",
      "45/49: mat = st.pivot(index='from_id', columns='name', values='id')\n",
      "45/50: mat[(mat.count(axis=1)==1)].shape[0]\n",
      "45/51: mat.loc[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu==1), 'Benjamin_Netanyahu'].shape[0]\n",
      "45/52:\n",
      "print ('Ratio of Bibi from one-timers: %.2f' % (mat[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu==1)].shape[0] \n",
      " / counts[counts==1].shape[0]))\n",
      "45/53: print ('Ratio of users that commented 10 times and up: %.2f' % (counts[counts>=10].shape[0]/st.from_id.nunique()))\n",
      "45/54: (st.reset_index().groupby('from_id').name.nunique().value_counts().head(20)/st.reset_index().from_id.nunique()).plot(kind='bar')\n",
      "45/55: (st.reset_index().groupby('from_id').id.sum().value_counts().head(20)/st.reset_index().from_id.nunique()).plot(kind='bar')\n",
      "45/56:\n",
      "fans_users = (st.reset_index()\n",
      " .loc[st.groupby('from_id').name.transform('nunique')==1, ['from_id', 'name', 'party']])\n",
      "45/57: fans = ( fans_users.name.value_counts())\n",
      "45/58: fans.head(10)\n",
      "45/59:\n",
      "print ('Total number of fans: %d' % fans.sum())\n",
      "print ('Ratio of fans from all users: %.2f' % (fans.sum() / st.from_id.nunique()))\n",
      "print ('Ratio of one-timers from fans: %.2f' % (counts[counts==1].shape[0] / fans.sum()))\n",
      "45/60:\n",
      "(pd.concat([fans, \n",
      "            fans/st.from_id.nunique(), \n",
      "            fans/st.groupby('name').from_id.nunique()],\n",
      "            axis=1, keys=['num_uniques', 'ratio_from_all_users', 'ratio_from_own_users'])\n",
      "             .sort_values(by='ratio_from_own_users', ascending=False)).head(10)\n",
      "45/61:\n",
      "mkdet = pd.read_csv('turf/mk_details.csv', sep='\\t')\n",
      "mkdet.head()\n",
      "45/62: df['party'] = df['name'].map(mkdet.set_index('folder_name')['party'])\n",
      "45/63: df.groupby(['party', 'langdetect']).size().unstack()\n",
      "45/64: st['party'] = st['name'].map(mkdet.set_index('folder_name')['party'])\n",
      "45/65:\n",
      "polem = (st.reset_index()\n",
      " .loc[st.groupby('from_id').name.transform('nunique')>1, ['from_id', 'name', 'party']])\n",
      "45/66: polem.head()\n",
      "45/67: polem.from_id.nunique()\n",
      "45/68: polem.party.value_counts()\n",
      "45/69: polem.groupby('party').from_id.nunique()\n",
      "45/70: base = polem[polem.groupby('from_id').party.transform('nunique')==1]\n",
      "45/71:\n",
      "print ('Total number of basers: %d' % base.from_id.nunique())\n",
      "print ('Ratio of basers from all users: %.2f' % (base.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of basers from Polemicists: %.2f' % (base.from_id.nunique() / polem.from_id.nunique()))\n",
      "45/72: st.party.unique()\n",
      "45/73:\n",
      "camps = { 'מרצ': 'opposition',\n",
      "          'יש עתיד': 'opposition',\n",
      "          'ליכוד': 'coalition',\n",
      "          'הבית היהודי בראשות נפתלי בנט': 'coalition',\n",
      "          'המחנה הציוני': 'opposition',\n",
      "          'הרשימה המשותפת (חד”ש, רע”מ, בל”ד, תע”ל)': 'opposition',\n",
      "          'כולנו בראשות משה כחלון': 'coalition',\n",
      "          'ישראל ביתנו': 'coalition' }\n",
      "45/74: st['camp'] = st['party'].map(camps)\n",
      "45/75: polem['camp'] = polem['party'].map(camps)\n",
      "45/76: polem.groupby('camp').from_id.nunique()\n",
      "45/77: campers = polem[polem.groupby('from_id').camp.transform('nunique')==1]\n",
      "45/78:\n",
      "print ('Total number of campers: %d' % campers.from_id.nunique())\n",
      "print ('Ratio of campers from all users: %.2f' % (campers.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of campers from polem: %.2f' % (campers.from_id.nunique() / polem.from_id.nunique()))\n",
      "print ('Coalition | Opposition ratio from campers: %.2f | %.2f' % \n",
      "       ((campers[campers.camp=='coalition'].from_id.nunique() / campers.from_id.nunique()),\n",
      "       (campers[campers.camp=='opposition'].from_id.nunique() / campers.from_id.nunique())))\n",
      "cg = st.groupby('camp')\n",
      "print ('Coalition | Opposition ratios from total users: %.2f | %.2f' % \n",
      "       ((cg.from_id.nunique()['coalition'] / st.from_id.nunique()),\n",
      "       ((cg.from_id.nunique()['opposition'] / st.from_id.nunique()))))\n",
      "\n",
      "print ('Coalition | Opposition total comments: %.2f | %.2f' % \n",
      "       ((cg.id.sum()['coalition'] / st.id.sum()),\n",
      "       ((cg.id.sum()['opposition'] / st.id.sum()))))\n",
      "45/79: top_commented = list(st.groupby('name').id.sum().sort_values(ascending=False).index)\n",
      "45/80:\n",
      "top_10_commented = top_commented[:10]\n",
      "top_10_commented\n",
      "45/81:\n",
      "#celebz =  polem[polem.groupby('from_id').name.transform(lambda x: x.isin(top_10_commented).all())]\n",
      "celebz = polem[polem.from_id.isin(mat[mat.loc[:, top_commented[10:]].isna().all(axis=1)].index)]\n",
      "45/82: num_celebiez_no_base_camp = len(set(celebz.from_id.drop_duplicates().values).difference(set(campers.from_id.drop_duplicates().values)).difference(set(base.from_id.drop_duplicates().values)))\n",
      "45/83:\n",
      "print ('Total number of celeb followers (celebies): %d' % celebz.from_id.nunique())\n",
      "print ('Ratio of celebies from all users: %.2f' % (celebz.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of celebies from polem: %.2f' % (celebz.from_id.nunique() / polem.from_id.nunique()))\n",
      "print ('Remove campers and base from celebies: %d' % num_celebiez_no_base_camp)\n",
      "45/84: top_commented[:10]\n",
      "45/85: fans.reindex(top_commented).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "45/86:\n",
      "only_bibi = mat.loc[:, top_commented[1:]].isna().all(axis=1)\n",
      "only_bibi[only_bibi].shape\n",
      "45/87:\n",
      "def get_accum_df(mat, order):\n",
      "    onlys = []\n",
      "    \n",
      "    for i, name in enumerate(order):\n",
      "        only = mat.loc[:, order[i+1:]].isna().all(axis=1)\n",
      "        onlys.append(only[only].shape[0])\n",
      "        \n",
      "    user_accum_df = pd.DataFrame({'mk': order, 'user_accum': onlys})\n",
      "    user_accum_df['party'] = user_accum_df['mk'].map(mkdet.set_index('folder_name')['party'])\n",
      "    user_accum_df['camp'] = user_accum_df['party'].map(camps)\n",
      "    user_accum_df['delta'] = user_accum_df.user_accum.diff()\n",
      "    unique_delta_ratio = (fans.reindex(top_commented) / user_accum_df.set_index('mk').delta).drop('Benjamin_Netanyahu')\n",
      "    user_accum_df['unique_delta_ratio'] = user_accum_df['mk'].map(unique_delta_ratio)\n",
      "    user_accum_df['delta_n'] = user_accum_df['delta'] / user_accum_df['delta'].max()\n",
      "    \n",
      "    return user_accum_df\n",
      "45/88:\n",
      "user_accum_df = get_accum_df(mat, top_commented)\n",
      "user_accum_df.head()\n",
      "45/89: user_accum_df.set_index('mk').user_accum.plot(kind='bar', figsize=(15,10), rot=75)\n",
      "45/90:\n",
      "import altair as alt\n",
      "alt.renderers.enable('notebook')\n",
      "45/91:\n",
      "def chart_user_accum(accum_df, order, order_name):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('user_accum',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'user_accum']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = f'User accumulation by {order_name} order - left to right, only users that comment inside the group until the current column')\n",
      "\n",
      "\n",
      "def chart_user_accum_delta(accum_df, order, order_name):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('delta',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'delta']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = f'User accumulation delta by {order_name} order - left to right, only users that comment inside the group until the current column')\n",
      "\n",
      "\n",
      "def chart_user_accum_unique_delta_ratio(accum_df, order, order_name):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('unique_delta_ratio',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'unique_delta_ratio']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = f'Page unique count divided by user_accum delta by {order_name} order (to measure how much a page is part of the group)')\n",
      "45/92:\n",
      "(chart_user_accum(user_accum_df, top_commented, 'Top Commented') \n",
      " & chart_user_accum_delta(user_accum_df, top_commented, 'Top Commented')\n",
      " & chart_user_accum_unique_delta_ratio(user_accum_df, top_commented, 'Top Commented')).properties(title='Top Commented')\n",
      "45/93: user_accum_df[['delta_n', 'unique_delta_ratio']].div(user_accum_df[['delta_n', 'unique_delta_ratio']].sum(axis=1), axis=0).plot(kind='bar', stacked=True, figsize=(15,10), rot=75, width=1.0)\n",
      "45/94:\n",
      "print ('First celebiez group: ', top_commented[:6])\n",
      "print ('Second celebiez group: ', top_commented[:15])\n",
      "45/95: top_mean_commented = list(df.loc[df.is_post==1].groupby('name').comment_count.mean().sort_values(ascending=False).index)\n",
      "45/96: df.loc[df.is_post==1].groupby('name').comment_count.mean().sort_values(ascending=False).head()\n",
      "45/97: fans.reindex(top_mean_commented).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "45/98: mean_com_accum_df = get_accum_df(mat, top_mean_commented)\n",
      "45/99:\n",
      "(chart_user_accum(mean_com_accum_df, top_mean_commented, 'Top Mean Commented') \n",
      " & chart_user_accum_delta(mean_com_accum_df, top_mean_commented, 'Top Mean Commented') \n",
      " & chart_user_accum_unique_delta_ratio(mean_com_accum_df, top_mean_commented, 'Top Mean Commented')).properties(title='Top Mean Commented')\n",
      "45/100: top_liked = list(df.loc[df.is_post==1].groupby('name').like_count.mean().sort_values(ascending=False).index)\n",
      "45/101: fans.reindex(top_liked).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "45/102: like_accum_df = get_accum_df(mat, top_liked)\n",
      "45/103:\n",
      "(chart_user_accum(like_accum_df, top_liked, 'Top Liked') \n",
      " & chart_user_accum_delta(like_accum_df, top_liked, 'Top Liked') \n",
      " & chart_user_accum_unique_delta_ratio(like_accum_df, top_liked, 'Top Liked')).properties(title='Top Liked')\n",
      "45/104: top_mean_commented = list(df.loc[df.is_post==1].groupby('name').comment_count.mean().sort_values(ascending=False).index)\n",
      "45/105: df.loc[df.is_post==1].groupby('name').comment_count.mean().sort_values(ascending=False).head()\n",
      "45/106: fans.reindex(top_mean_commented).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "45/107: mean_com_accum_df = get_accum_df(mat, top_mean_commented)\n",
      "45/108:\n",
      "(chart_user_accum(mean_com_accum_df, top_mean_commented, 'Top Mean Commented') \n",
      " & chart_user_accum_delta(mean_com_accum_df, top_mean_commented, 'Top Mean Commented') \n",
      " & chart_user_accum_unique_delta_ratio(mean_com_accum_df, top_mean_commented, 'Top Mean Commented')).properties(title='Top Mean Commented')\n",
      "45/109: top_liked = list(df.loc[df.is_post==1].groupby('name').like_count.mean().sort_values(ascending=False).index)\n",
      "45/110: fans.reindex(top_liked).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "45/111: like_accum_df = get_accum_df(mat, top_liked)\n",
      "45/112:\n",
      "(chart_user_accum(like_accum_df, top_liked, 'Top Liked') \n",
      " & chart_user_accum_delta(like_accum_df, top_liked, 'Top Liked') \n",
      " & chart_user_accum_unique_delta_ratio(like_accum_df, top_liked, 'Top Liked')).properties(title='Top Liked')\n",
      "45/113: top_shared = list(df.loc[df.is_post==1].groupby('name').share_count.mean().sort_values(ascending=False).index)\n",
      "45/114: fans.reindex(top_shared).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "45/115:\n",
      "share_accum_df = get_accum_df(mat, top_shared)\n",
      "share_accum_df.head()\n",
      "45/116:\n",
      "(chart_user_accum(share_accum_df, top_shared, 'Top Shared') \n",
      " & chart_user_accum_delta(share_accum_df, top_shared, 'Top Shared') \n",
      " & chart_user_accum_unique_delta_ratio(share_accum_df, top_shared, 'Top Shared')).properties(title='Top Shared')\n",
      "45/117:\n",
      "ts = (df.loc[df.is_post==1].groupby('name').share_count.mean().sort_values(ascending=False))\n",
      "tmc = (df.loc[df.is_post==1].groupby('name').comment_count.mean().sort_values(ascending=False))\n",
      "tl = (df.loc[df.is_post==1].groupby('name').like_count.mean().sort_values(ascending=False))\n",
      "all_scores = pd.concat([tmc, tl, ts], axis=1)\n",
      "all_scores_div = all_scores.div(all_scores.sum(axis=0), axis=1)\n",
      "all_scores['total_score'] = all_scores['like_count'] + 1.1*all_scores['comment_count'] + 1.2*all_scores['share_count']\n",
      "all_scores_div['total_score'] = all_scores_div['like_count'] + 1.1*all_scores_div['comment_count'] + 1.2*all_scores_div['share_count']\n",
      "all_scores.sort_values(by='total_score', ascending=False).head(10)\n",
      "45/118: all_scores_div.sort_values(by='total_score', ascending=False).head(20)\n",
      "45/119:\n",
      "all_accum_df = get_accum_df(mat, list(all_scores_div.sort_values(by='total_score', ascending=False).index))\n",
      "(chart_user_accum(all_accum_df, list(all_scores_div.sort_values(by='total_score', ascending=False).index), 'All Score') \n",
      " & chart_user_accum_delta(all_accum_df, list(all_scores_div.sort_values(by='total_score', ascending=False).index), 'All Score') \n",
      " & chart_user_accum_unique_delta_ratio(all_accum_df, list(all_scores_div.sort_values(by='total_score', ascending=False).index), 'All Score')).properties(title='All Score')\n",
      "45/120: new_celebies = polem[polem.from_id.isin(mat[mat.loc[:, list(all_scores_div.sort_values(by='total_score', ascending=False).index)[12:]].isna().all(axis=1)].index)]\n",
      "45/121: new_celebies.from_id.nunique(), polem.from_id.nunique()\n",
      "45/122: user_counts = st.groupby('from_id').id.sum()\n",
      "45/123:\n",
      "perc = pd.qcut(user_counts, 20, duplicates='drop')\n",
      "perc.value_counts(sort=False)\n",
      "45/124:\n",
      "perc_bah = pd.cut(user_counts, [0,1,2,3,4,5,7,11,24,9359])\n",
      "(perc_bah.value_counts(sort=False))\n",
      "45/125: (perc_bah.value_counts(sort=False)/ user_counts.shape[0])\n",
      "45/126: (perc_bah.value_counts(sort=False)/ user_counts.shape[0]).plot(kind='bar',figsize=(10,7))\n",
      "45/127:\n",
      "dedicated = st[st.from_id.isin(user_counts[user_counts>24].index)]\n",
      "dedicated.from_id.nunique()\n",
      "45/128: st[st.from_id.isin(user_counts[user_counts>24].index)].id.sum() / st.id.sum()\n",
      "45/129: st[st.from_id.isin(user_counts[user_counts>11].index)].from_id.nunique() / st.from_id.nunique()\n",
      "45/130: st[st.from_id.isin(user_counts[user_counts>11].index)].id.sum() / st.id.sum()\n",
      "45/131:\n",
      "campers = campers[(~campers.from_id.isin(base.from_id))& (~campers.from_id.isin(dedicated.from_id))]\n",
      "campers.from_id.nunique()\n",
      "45/132:\n",
      "base = base[(~base.from_id.isin(dedicated.from_id))]\n",
      "base.from_id.nunique()\n",
      "45/133:\n",
      "polem = polem[(~polem.from_id.isin(base.from_id)) \n",
      "                            & (~polem.from_id.isin(campers.from_id))\n",
      "                            & (~polem.from_id.isin(dedicated.from_id))]\n",
      "polem.from_id.nunique()\n",
      "45/134: campers.from_id.nunique() + base.from_id.nunique() + polem.from_id.nunique()\n",
      "45/135:\n",
      "fans_users = fans_users[~fans_users.from_id.isin(one_timers.index)\n",
      "                        & (~fans_users.from_id.isin(dedicated.from_id))]\n",
      "fans_users.from_id.nunique()\n",
      "45/136: ded_count = dedicated.from_id.nunique()\n",
      "45/137:\n",
      "ded_fans_users = (dedicated.loc[st.groupby('from_id').name.transform('nunique')==1, ['from_id', 'name', 'party']])\n",
      "ded_fans_users.from_id.nunique()/ded_count, dedicated.from_id.nunique()/ded_count\n",
      "45/138:\n",
      "ded_polem = (dedicated.loc[st.groupby('from_id').name.transform('nunique')>1, ['from_id', 'name', 'party', 'camp']])\n",
      "ded_base = ded_polem[ded_polem.groupby('from_id').party.transform('nunique')==1]\n",
      "ded_campers = ded_polem[ded_polem.groupby('from_id').camp.transform('nunique')==1]\n",
      "ded_celebies = ded_polem[ded_polem.from_id.isin(mat[mat.loc[:, list(all_scores_div.sort_values(by='total_score', ascending=False).index)[12:]].isna().all(axis=1)].index)]\n",
      "dedicated.from_id.nunique(), ded_polem.from_id.nunique()/ded_count, ded_base.from_id.nunique()/ded_count, ded_campers.from_id.nunique()/ded_count, ded_celebies.from_id.nunique()/ded_count\n",
      "45/139:\n",
      "ded_campers = ded_campers[(~ded_campers.from_id.isin(ded_base.from_id))]\n",
      "ded_campers.from_id.nunique()\n",
      "45/140:\n",
      "ded_polem = ded_polem[(~ded_polem.from_id.isin(ded_base.from_id)) \n",
      "                                    & (~ded_polem.from_id.isin(ded_campers.from_id))]\n",
      "ded_polem.from_id.nunique()\n",
      "45/141:\n",
      "ded_celebies = ded_celebies[(~ded_celebies.from_id.isin(ded_polem.from_id)) \n",
      "                                    & (~ded_celebies.from_id.isin(ded_campers.from_id))\n",
      "                                    & (~ded_celebies.from_id.isin(ded_base.from_id))]\n",
      "ded_celebies.from_id.nunique()\n",
      "45/142: ded_campers.from_id.nunique() + ded_base.from_id.nunique() + ded_polem.from_id.nunique() + ded_fans_users.from_id.nunique() + ded_celebies.from_id.nunique()\n",
      "45/143:\n",
      "labels = ('Dedicated Fans', 'Dedicated Basers', 'Dedicated Campers', 'Dedicated Celebies', 'Dedicated Polemicists')\n",
      "ratios = (ded_fans_users.from_id.nunique()/ded_count, ded_base.from_id.nunique()/ded_count, ded_campers.from_id.nunique()/ded_count, ded_celebies.from_id.nunique()/ded_count, ded_polem.from_id.nunique()/ded_count)\n",
      "dict(zip(labels,[round(x, 2) for x in ratios]))\n",
      "45/144:\n",
      "labels = ('All', 'One Timers', 'Fans', 'Polemicists', 'Basers', 'Campers')\n",
      "sets = [set(st.from_id.values), set(one_timers.index), set(fans_users.from_id.values), set(polem.from_id.values), set(base.from_id.values), set(campers.from_id.values)]\n",
      "\n",
      "set_dict = dict(zip(labels, sets))\n",
      "45/145:\n",
      "inter_set_dict = {}\n",
      "for i in set_dict:\n",
      "    inter_set_dict[i] = {}\n",
      "    for j in set_dict:\n",
      "        inter_set_dict[i][j] = len(set_dict[i].intersection(set_dict[j]))\n",
      "45/146: pd.DataFrame(inter_set_dict).reindex(labels)\n",
      "45/147: pd.DataFrame(inter_set_dict).reindex(labels).to_csv('turf/group_intersections.csv')\n",
      "45/148: from matplotlib_venn import venn3\n",
      "45/149: df[df.is_post==1].shape[0], df[df.is_post==1].comment_count.mean()\n",
      "45/150: df[df.is_post==1].groupby('name').comment_count.mean().sort_values().plot(kind='barh', figsize=(10,15))\n",
      "45/151: one_timers.shape\n",
      "45/152:\n",
      "one_timers_page = st[st.from_id.isin(one_timers.index)].groupby('name').size()\n",
      "one_timers_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "45/153:\n",
      "gfans = st[st.from_id.isin(fans_users.from_id)].groupby('from_id')\n",
      "gfans.id.sum().mean(), gfans.post_id.sum().mean(), gfans.id.sum().mean()/ gfans.post_id.sum().mean()\n",
      "45/154:\n",
      "fans_page = st[st.from_id.isin(fans_users.from_id)].groupby('name').size()\n",
      "fans_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "45/155:\n",
      "gbase = st[st.from_id.isin(base.from_id)].groupby('from_id')\n",
      "gbase.id.sum().mean(), gbase.post_id.sum().mean(), gbase.id.sum().mean()/ gbase.post_id.sum().mean()\n",
      "45/156:\n",
      "base_page = st[st.from_id.isin(base.from_id)].groupby('name').size()\n",
      "base_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "45/157:\n",
      "gcamp = st[st.from_id.isin(campers.from_id)].groupby('from_id')\n",
      "gcamp.id.sum().mean(), gcamp.post_id.sum().mean(), gcamp.id.sum().mean()/ gcamp.post_id.sum().mean()\n",
      "45/158: st[st.from_id.isin(campers.from_id)].id.sum()\n",
      "45/159:\n",
      "camp_page = st[st.from_id.isin(campers.from_id)].groupby('name').size()\n",
      "camp_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "45/160: st[(st.from_id.isin(polem.from_id))].id.sum()\n",
      "45/161:\n",
      "gpolem = st[(st.from_id.isin(polem.from_id))].groupby('from_id')\n",
      "gpolem.id.sum().mean(), gpolem.post_id.sum().mean(), gpolem.id.sum().mean()/gpolem.post_id.sum().mean()\n",
      "45/162:\n",
      "polem_page = st[st.from_id.isin(polem.from_id)].groupby('name').size()\n",
      "polem_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "45/163:\n",
      "gded = dedicated.groupby('from_id')\n",
      "gded.id.sum().mean(), gded.post_id.sum().mean(), gded.id.sum().mean()/ gded.post_id.sum().mean()\n",
      "45/164:\n",
      "ded_page = st[st.from_id.isin(dedicated.from_id)].groupby('name').size()\n",
      "ded_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "45/165:\n",
      "page_cats = pd.concat([one_timers_page, fans_page, base_page, camp_page, polem_page, ded_page], axis=1,\n",
      "          keys=['One-timers', 'Fans', 'Basers', 'Campers', 'Polemicists', 'Dedicated'])\n",
      "45/166: page_cats = page_cats.stack().reset_index().rename(columns={'level_1': 'Category', 0: 'user_count'})\n",
      "45/167:\n",
      "regular = alt.Chart(page_cats).mark_bar().encode(\n",
      "    alt.X('user_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('Category:N'),\n",
      "    tooltip = ['name', 'Category','user_count']\n",
      ").properties(height=1200, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('user_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "45/168:\n",
      "labels = ('One Timers', 'Fans', 'Polemicists', 'Basers', 'Campers', 'Dedicated')\n",
      "sets = [set(one_timers.index), \n",
      "        set(fans_users.from_id.values), \n",
      "        set(polem.from_id.values), \n",
      "        set(base.from_id.values), \n",
      "        set(campers.from_id.values), \n",
      "        set(dedicated.from_id.values)]\n",
      "#df = df.drop('group_cat', axis=1)\n",
      "for l, s in zip(labels, sets):\n",
      "    df.loc[df.from_id.isin(s), 'group_cat'] = l\n",
      "\n",
      "df['group_cat'] = pd.Categorical(df.group_cat)\n",
      "df.group_cat.value_counts()/df.shape[0]\n",
      "45/169:\n",
      "for l, s in zip(labels, sets):\n",
      "    st.loc[st.from_id.isin(s), 'group_cat'] = l\n",
      "st['group_cat'] = pd.Categorical(st.group_cat)\n",
      "st.groupby('group_cat').id.sum()/df.shape[0]\n",
      "45/170: st.to_pickle('turf/total_stats_no_multiindex_groups.pkl.gz', compression='gzip')\n",
      "45/171: df.group_cat.value_counts()/df.shape[0]\n",
      "45/172:\n",
      "labels = ('Dedicated Fans', 'Dedicated Basers', 'Dedicated Polemicists', 'Dedicated Campers', 'Dedicated Celebies')\n",
      "sets = [set(ded_fans_users.from_id.values), \n",
      "        set(ded_base.from_id.values), \n",
      "        set(ded_polem.from_id.values), \n",
      "        set(ded_campers.from_id.values),\n",
      "        set(ded_celebies.from_id.values),]\n",
      "\n",
      "for l, s in zip(labels, sets):\n",
      "    st.loc[st.from_id.isin(s), 'new_group_cat'] = l\n",
      "    \n",
      "st['new_group_cat'] = pd.Categorical(st.new_group_cat)\n",
      "st.groupby('new_group_cat').id.sum()/df.shape[0]\n",
      "45/173:\n",
      "st.new_group_cat = st.new_group_cat.combine_first(st.group_cat)\n",
      "st.groupby('group_cat').id.sum()/df.shape[0]\n",
      "45/174: st.to_pickle('turf/total_stats_no_multiindex_groups_ded_cuts.pkl.gz', compression='gzip')\n",
      "45/175:\n",
      "group_lang = df.groupby(['name', 'group_cat', 'langdetect']).size().reset_index().rename(columns = {0: 'comment_count'})\n",
      "group_lang['langdetect'] = group_lang.langdetect.cat.add_categories(['other'])\n",
      "group_lang.loc[group_lang.langdetect=='cy', 'langdetect'] = 'ru'\n",
      "top_lang = group_lang.groupby('langdetect').comment_count.sum().sort_values(ascending=False).head(9).index\n",
      "group_lang.loc[~group_lang.langdetect.isin(top_lang), 'langdetect'] = 'other'\n",
      "group_lang = group_lang.groupby(['name', 'group_cat', 'langdetect']).comment_count.sum().reset_index()\n",
      "\n",
      "group_lang.langdetect.value_counts()\n",
      "45/176: alt.data_transformers.enable('default', max_rows=None)\n",
      "45/177:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#a6cee3','#1f78b4','#ff7f00','#fdbf6f','#e31a1c','#cab2d6','#33a02c'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['name', 'group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(width=150)\n",
      "\n",
      "(normalized).facet(column='group_cat').properties(title='Comment Count by MK, User Group Category and Language Detected')\n",
      "45/178:\n",
      "regular = alt.Chart(group_lang.groupby(['group_cat', 'langdetect']).comment_count.sum().reset_index()).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#a6cee3','#1f78b4','#ff7f00','#fdbf6f','#e31a1c','#cab2d6','#33a02c'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ")\n",
      "\n",
      "(normalized).properties(title='Comment Count by User Group Category and Language Detected')\n",
      "45/179:\n",
      "regular = alt.Chart(group_lang[group_lang.name!='Benjamin_Netanyahu'].groupby(['group_cat', 'langdetect']).comment_count.sum().reset_index()).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#a6cee3','#1f78b4','#ff7f00','#fdbf6f','#e31a1c','#cab2d6','#33a02c'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ")\n",
      "\n",
      "(normalized).properties(title='Comment Count by User Group Category and Language Detected (No Netanyahu)')\n",
      "45/180:\n",
      "post_lang = df[df.is_post==0].groupby(['name', 'group_cat', 'langdetect', 'post_id']).size().reset_index().rename(columns = {0: 'comment_count'})\n",
      "post_lang['langdetect'] = post_lang.langdetect.cat.add_categories(['other'])\n",
      "post_lang.loc[post_lang.langdetect=='cy', 'langdetect'] = 'ru'\n",
      "top_post_lang = post_lang.groupby('langdetect').comment_count.sum().sort_values(ascending=False).head(9).index\n",
      "post_lang.loc[~post_lang.langdetect.isin(top_post_lang), 'langdetect'] = 'other'\n",
      "post_lang['langdetect'] = post_lang.langdetect.cat.remove_unused_categories()\n",
      "post_lang = post_lang.groupby(['name', 'group_cat', 'langdetect', 'post_id']).comment_count.sum().reset_index()\n",
      "post_lang[post_lang.name =='Benjamin_Netanyahu'].groupby(['group_cat', 'langdetect']).comment_count.median()\n",
      "45/181:\n",
      "high = post_lang[(post_lang.name =='Benjamin_Netanyahu') & (post_lang.comment_count>1000) & (post_lang.langdetect=='en')]\n",
      "high\n",
      "45/182: df[(df.is_post==1) & (df.post_id.isin(high.post_id))][['comment_count', 'langdetect']]\n",
      "45/183:\n",
      "regular = alt.Chart(post_lang[(post_lang.name =='Benjamin_Netanyahu')]).mark_bar().encode(\n",
      "    alt.X('sum(comment_count):Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('post_id:N', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#a6cee3','#1f78b4','#ff7f00','#fdbf6f','#e31a1c','#cab2d6','#33a02c'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['post_id', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('sum(comment_count):Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(width=150)\n",
      "\n",
      "(normalized).facet(column='group_cat').properties(title='Comment Count by Post ID, User Group Category and Language Detected')\n",
      "45/184: normalized | regular\n",
      "45/185: df[df.post_id=='268108602075_10153717170732076']\n",
      "45/186:\n",
      "fig, ax = plt.subplots(figsize=(20, 15))\n",
      "sns.boxplot(data=post_lang[(post_lang.name =='Benjamin_Netanyahu')], x='langdetect', y='comment_count', hue='group_cat', ax=ax)\n",
      "45/187: st.dtypes\n",
      "45/188: df.head(post_lang[post_lang.name =='Benjamin_Netanyahu']).mark_point.encode\n",
      "44/199:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "from holoviews.streams import RangeXY\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k'), cmap=Sets1to3)) * \\\n",
      "    hv.util.Dynamic(aggregate(all_points, width=100, height=100, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.group_cat.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/200:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "44/201:\n",
      "import altair as alt\n",
      "alt.renderers.enable('notebook')\n",
      "44/202:\n",
      "df = pd.read_pickle('turf/data_df.pkl.gz', compression='gzip')\n",
      "df.dtypes\n",
      "44/203:\n",
      "sdf = pd.read_pickle('turf/shared_df.pkl.gz', compression='gzip')\n",
      "sdf.dtypes\n",
      "44/204: st = pd.read_pickle('turf/total_stats_no_multiindex_groups_ded_cuts.pkl.gz', compression='gzip')\n",
      "44/205: st.dtypes\n",
      "44/206: tn = pd.read_pickle('turf/times_names.pkl.gz', compression='gzip')\n",
      "44/207: tn['period_length'] = tn.period_length/day_in_seconds\n",
      "44/208:\n",
      "import numpy as np\n",
      "import holoviews as hv\n",
      "import datashader as ds\n",
      "from holoviews.operation.datashader import aggregate, datashade, shade, dynspread\n",
      "from holoviews.operation import decimate\n",
      "hv.extension('bokeh','matplotlib')\n",
      "decimate.max_samples=1000\n",
      "dynspread.max_px=20\n",
      "dynspread.threshold=0.5\n",
      "44/209: ds.__version__\n",
      "44/210:\n",
      "if os.path.exists('turf/umap_all.h5'):\n",
      "    with h5py.File('turf/umap_all.h5', 'r') as hf:\n",
      "        all_embedding = hf['umap_all'][:]\n",
      "else:\n",
      "    all_embedding = umap.UMAP().fit_transform(mat.values)\n",
      "    with h5py.File('turf/umap_all.h5', 'w') as hf:\n",
      "        hf.create_dataset(\"umap_all\",  data=embedding)\n",
      "44/211:\n",
      "all_points = hv.Points(all_embedding, label=\"All Commenters UMAP embedding plot (datashaded)\")\n",
      "\n",
      "%output size=300\n",
      "%opts RGB [bgcolor=\"black\" show_grid=False xaxis=None yaxis=None]\n",
      "from colorcet import fire\n",
      "datashade.cmap=fire[50:]\n",
      "\n",
      "dynspread(datashade(all_points))\n",
      "44/212: datashade(all_points)\n",
      "44/213: mat.head()\n",
      "44/214:\n",
      "res = pd.DataFrame(all_embedding,columns=['x', 'y'])\n",
      "res = (pd.concat([mat.index.to_frame(index=False), res], axis=1)\n",
      "       .merge(st[['from_id', 'group_cat']], how='left', on='from_id')\n",
      "       .drop_duplicates().reset_index().drop('index', axis=1))\n",
      "res.head()\n",
      "44/215: res.loc[res.group_cat.str.startswith('Ded'), 'group_cat'] = 'Dedicated'\n",
      "44/216: res.head()\n",
      "44/217:\n",
      "kdims=['d1','d2']\n",
      "num_ks=res.group_cat.nunique()\n",
      "\n",
      "apd = {c: hv.Points(res.loc[res.group_cat==c, ['x', 'y']]) for c in res.group_cat.unique()}\n",
      "\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k')))\n",
      "\n",
      "apdspread\n",
      "44/218: {k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key}\n",
      "44/219:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "from holoviews.streams import RangeXY\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k'), cmap=Sets1to3)) * \\\n",
      "    hv.util.Dynamic(aggregate(all_points, width=100, height=100, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.group_cat.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/220:\n",
      "if os.path.exists('turf/umap_all.h5'):\n",
      "    with h5py.File('turf/umap_all.h5', 'r') as hf:\n",
      "        all_embedding = hf['umap_all'][:]\n",
      "else:\n",
      "    all_embedding = umap.UMAP().fit_transform(mat.values)\n",
      "    with h5py.File('turf/umap_all.h5', 'w') as hf:\n",
      "        hf.create_dataset(\"umap_all\",  data=embedding)\n",
      "44/221:\n",
      "all_points = hv.Points(all_embedding, label=\"All Commenters UMAP embedding plot (datashaded)\")\n",
      "\n",
      "%output size=300\n",
      "%opts RGB [bgcolor=\"black\" show_grid=False xaxis=None yaxis=None]\n",
      "from colorcet import fire\n",
      "datashade.cmap=fire[50:]\n",
      "\n",
      "dynspread(datashade(all_points))\n",
      "44/222: datashade(all_points)\n",
      "44/223: mat.head()\n",
      "44/224: mat.head()\n",
      "44/225:\n",
      "res = pd.DataFrame(all_embedding,columns=['x', 'y'])\n",
      "res = (pd.concat([mat.index.to_frame(index=False), res], axis=1)\n",
      "       .merge(st[['from_id', 'group_cat']], how='left', on='from_id')\n",
      "       .drop_duplicates().reset_index().drop('index', axis=1))\n",
      "res.head()\n",
      "44/226: res.loc[res.group_cat.str.startswith('Ded'), 'group_cat'] = 'Dedicated'\n",
      "44/227: res.head()\n",
      "44/228:\n",
      "kdims=['d1','d2']\n",
      "num_ks=res.group_cat.nunique()\n",
      "\n",
      "apd = {c: hv.Points(res.loc[res.group_cat==c, ['x', 'y']]) for c in res.group_cat.unique()}\n",
      "\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k')))\n",
      "\n",
      "apdspread\n",
      "44/229: {k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key}\n",
      "44/230:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "from holoviews.streams import RangeXY\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k'), cmap=Sets1to3)) * \\\n",
      "    hv.util.Dynamic(aggregate(all_points, width=100, height=100, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.group_cat.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/231:\n",
      "kdims=['d1','d2']\n",
      "num_ks=res.topmk.nunique()\n",
      "\n",
      "aptopmk = {c: hv.Points(res.loc[res.topmk==c, ['x', 'y']]) for c in res.topmk.unique()}\n",
      "44/232:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "from holoviews.streams import RangeXY\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(aptopmk, kdims='k'), aggregator=ds.count_cat('k'), cmap=Sets1to3)) * \\\n",
      "    hv.util.Dynamic(aggregate(all_points, width=100, height=100, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.topmk.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/233:\n",
      "topmk = mat.idxmax(axis=1).reset_index().drop('from_id', axis=1).rename(columns={0:'topmk'})['topmk']\n",
      "topmk.loc[topmk.isin(top_commented[8:])] = 'Other'\n",
      "44/234:\n",
      "res = pd.concat([res, topmk], axis=1)\n",
      "res.head()\n",
      "44/235:\n",
      "kdims=['d1','d2']\n",
      "num_ks=res.topmk.nunique()\n",
      "\n",
      "aptopmk = {c: hv.Points(res.loc[res.topmk==c, ['x', 'y']]) for c in res.topmk.unique()}\n",
      "44/236:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "from holoviews.streams import RangeXY\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(aptopmk, kdims='k'), aggregator=ds.count_cat('k'), cmap=Sets1to3)) * \\\n",
      "    hv.util.Dynamic(aggregate(all_points, width=100, height=100, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.topmk.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/237:\n",
      "nposts = df.post_id.nunique()\n",
      "npages = st.name.nunique()\n",
      "ncomments = st.id.sum()\n",
      "nusers = st.from_id.nunique()\n",
      "\n",
      "pd.DataFrame([{ 'No. Posts': nposts, 'No. Pages': npages, 'No. Comments': ncomments, 'No. Users': nusers}]).T\n",
      "44/238:\n",
      "groups = (st.groupby('group_cat')\n",
      "          .agg({'from_id': 'nunique', 'id': 'sum'})\\\n",
      "          .rename(columns={'from_id': 'users', 'id': 'comments'}))\n",
      "groups['users_ratio'] = groups.users / nusers\n",
      "groups['comments_ratio'] = groups.comments / ncomments\n",
      "groups['ratios_ratio'] = groups.comments_ratio / groups.users_ratio\n",
      "\n",
      "groups = groups[['users', 'users_ratio', 'comments', 'comments_ratio', 'ratios_ratio']]\n",
      "groups = groups.reindex(['One Timers', \n",
      "                         'Fans', 'Dedicated Fans', \n",
      "                         'Basers', 'Dedicated Basers', \n",
      "                         'Campers', 'Dedicated Campers',\n",
      "                         'Polemicists', 'Dedicated Polemicists',\n",
      "                         'Dedicated Celebies'])\n",
      "groups\n",
      "44/239: groups[['users_ratio', 'comments_ratio']].plot(kind='bar', figsize=(15, 11), rot=30)\n",
      "44/240:\n",
      "groups = (st.groupby('group_cat')\n",
      "          .agg({'from_id': 'nunique', 'id': 'sum'})\\\n",
      "          .rename(columns={'from_id': 'users', 'id': 'comments'}))\n",
      "groups['users_ratio'] = groups.users / nusers\n",
      "groups['comments_ratio'] = groups.comments / ncomments\n",
      "groups['ratios_ratio'] = groups.comments_ratio / groups.users_ratio\n",
      "\n",
      "groups = groups[['users', 'users_ratio', 'comments', 'comments_ratio', 'ratios_ratio']]\n",
      "groups = groups.reindex(['One Timers', \n",
      "                         'Fans', 'Dedicated Fans', \n",
      "                         'Basers', 'Dedicated Basers', \n",
      "                         'Campers', 'Dedicated Campers',\n",
      "                         'Polemicists', 'Dedicated Polemicists',])\n",
      "groups\n",
      "44/241:\n",
      "groups = (st.groupby('new_group_cat')\n",
      "          .agg({'from_id': 'nunique', 'id': 'sum'})\\\n",
      "          .rename(columns={'from_id': 'users', 'id': 'comments'}))\n",
      "groups['users_ratio'] = groups.users / nusers\n",
      "groups['comments_ratio'] = groups.comments / ncomments\n",
      "groups['ratios_ratio'] = groups.comments_ratio / groups.users_ratio\n",
      "\n",
      "groups = groups[['users', 'users_ratio', 'comments', 'comments_ratio', 'ratios_ratio']]\n",
      "groups = groups.reindex(['One Timers', \n",
      "                         'Fans', 'Dedicated Fans', \n",
      "                         'Basers', 'Dedicated Basers', \n",
      "                         'Campers', 'Dedicated Campers',\n",
      "                         'Polemicists', 'Dedicated Polemicists',])\n",
      "groups\n",
      "44/242: groups[['users_ratio', 'comments_ratio']].plot(kind='bar', figsize=(15, 11), rot=30)\n",
      "44/243: print(groups.to_html())\n",
      "44/244:\n",
      "page_groups = st.groupby(['name', 'new_group_cat']).id.sum().unstack().reindex(top_commented)\n",
      "page_groups = page_groups[['One Timers', \n",
      "                         'Fans', 'Dedicated Fans', \n",
      "                         'Basers', 'Dedicated Basers', \n",
      "                         'Campers', 'Dedicated Campers',\n",
      "                         'Polemicists', 'Dedicated Polemicists',]]\n",
      "page_groups = np.round(page_groups.div(page_groups.sum(axis=1), axis=0), 2)\n",
      "page_groups\n",
      "44/245: print(page_groups.fillna(0).to_html())\n",
      "44/246:\n",
      "page_cats_alt = page_groups.stack().reset_index().rename(columns={0: 'user_count'}).fillna(0)\n",
      "regular = alt.Chart(page_cats_alt).mark_bar().encode(\n",
      "    alt.X('user_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    tooltip = ['name', 'group_cat','user_count']\n",
      ").properties(height=1500, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('user_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "44/247:\n",
      "page_cats_alt = page_groups.stack().reset_index().rename(columns={0: 'user_count'}).fillna(0)\n",
      "regular = alt.Chart(page_cats_alt).mark_bar().encode(\n",
      "    alt.X('user_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('new_group_cat:N'),\n",
      "    tooltip = ['name', 'new_group_cat','user_count']\n",
      ").properties(height=1500, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('user_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "44/248:\n",
      "page_groups = st.groupby(['name', 'new_group_cat']).id.sum().unstack().reindex(top_commented)\n",
      "page_groups = page_groups[['One Timers', \n",
      "                         'Fans', 'Dedicated Fans', \n",
      "                         'Basers', 'Dedicated Basers', \n",
      "                         'Campers', 'Dedicated Campers',\n",
      "                         'Polemicists', 'Dedicated Polemicists',]]\n",
      "page_groups_ratio = np.round(page_groups.div(page_groups.sum(axis=1), axis=0), 2)\n",
      "page_groups_ratio\n",
      "44/249: print(page_groups_ratio.fillna(0).to_html())\n",
      "44/250:\n",
      "page_cats_alt = page_groups.stack().reset_index().rename(columns={0: 'user_count'}).fillna(0)\n",
      "regular = alt.Chart(page_cats_alt).mark_bar().encode(\n",
      "    alt.X('user_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('new_group_cat:N'),\n",
      "    tooltip = ['name', 'new_group_cat','user_count']\n",
      ").properties(height=1500, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('user_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "45/189:\n",
      "regular = alt.Chart(page_cats).mark_bar().encode(\n",
      "    alt.X('user_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('Category:N'),\n",
      "    tooltip = ['name', 'Category','user_count']\n",
      ").properties(height=1200, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('user_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "44/251:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "from holoviews.streams import RangeXY\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k'), cmap=Sets1to3)) * \\\n",
      "    hv.util.Dynamic(aggregate(all_points, width=100, height=100, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.group_cat.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/252:\n",
      "import numpy as np\n",
      "import holoviews as hv\n",
      "import datashader as ds\n",
      "from holoviews.operation.datashader import aggregate, datashade, shade, dynspread\n",
      "from holoviews.operation import decimate\n",
      "hv.extension('bokeh','matplotlib')\n",
      "decimate.max_samples=1000\n",
      "dynspread.max_px=20\n",
      "dynspread.threshold=0.5\n",
      "44/253: ds.__version__\n",
      "44/254:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "from holoviews.streams import RangeXY\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k'), cmap=Sets1to3)) * \\\n",
      "    hv.util.Dynamic(aggregate(all_points, width=100, height=100, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.group_cat.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/255:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "from holoviews.streams import RangeXY\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(aptopmk, kdims='k'), aggregator=ds.count_cat('k'), cmap=Sets1to3)) * \\\n",
      "    hv.util.Dynamic(aggregate(all_points, width=100, height=100, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.topmk.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/256:\n",
      "import numpy as np\n",
      "import holoviews as hv\n",
      "import datashader as ds\n",
      "from holoviews.operation.datashader import aggregate, datashade, shade, dynspread\n",
      "from holoviews.operation import decimate\n",
      "hv.extension('bokeh','matplotlib')\n",
      "decimate.max_samples=1000\n",
      "dynspread.max_px=20\n",
      "dynspread.threshold=0.5\n",
      "44/257:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "from holoviews.streams import RangeXY\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k'), cmap=Sets1to3)) * \\\n",
      "    hv.util.Dynamic(aggregate(all_points, width=100, height=100, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.group_cat.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/258:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "from holoviews.streams import RangeXY\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(aptopmk, kdims='k'), aggregator=ds.count_cat('k'), cmap=Sets1to3)) * \\\n",
      "    hv.util.Dynamic(aggregate(all_points, width=100, height=100, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.topmk.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/259:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "from holoviews.streams import RangeXY\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(aptopmk, kdims='k'), aggregator=ds.count_cat('k'), cmap=Sets1to3)) * \\\n",
      "    hv.util.Dynamic(aggregate(all_points, width=100, height=100, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.topmk.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/260:\n",
      "bibi_cont = pd.read_csv('turf/bibi_content_group_counts.csv')\n",
      "bibi_cont.head()\n",
      "44/261:\n",
      "bibi_cont = pd.read_csv('turf/bibi_content_group_counts.csv')\n",
      "bibi_cont.head().set_index('Category').stack()\n",
      "44/262:\n",
      "bibi_cont = pd.read_csv('turf/bibi_content_group_counts.csv').set_index('Category')\n",
      "bibi_cont.head()\n",
      "44/263:\n",
      "herz_cont = pd.read_csv('turf/herzog_content_group_counts.csv').set_index('Category')\n",
      "bibi_cont.head()\n",
      "44/264:\n",
      "herz_cont = pd.read_csv('turf/herzog_content_group_counts.csv').set_index('Category')\n",
      "herz_cont.head()\n",
      "44/265: bibi_cont.stack().to_frame()\n",
      "44/266: bibi_cont.stack().reset_index()\n",
      "44/267: bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "44/268:\n",
      "bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "\n",
      "regular = alt.Chart(page_cats_alt).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    tooltip = ['Category', 'group_cat','comment_count']\n",
      ").properties(height=1500, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N', sort=top_commented,  title=None),\n",
      ").properties(title='Comment Ratio')\n",
      "(normalized | regular)\n",
      "44/269:\n",
      "bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "\n",
      "regular = alt.Chart(bibi_stacked).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    tooltip = ['Category', 'group_cat','comment_count']\n",
      ").properties(height=1500, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N', sort=top_commented,  title=None),\n",
      ").properties(title='Comment Ratio')\n",
      "(normalized | regular)\n",
      "44/270:\n",
      "bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "\n",
      "regular = alt.Chart(bibi_stacked).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    tooltip = ['Category', 'group_cat','comment_count']\n",
      ").properties(height=1500, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N', sort=top_commented,  title=None),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "herz_stacked = herz_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "\n",
      "regular = alt.Chart(herz_stacked).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    tooltip = ['Category', 'group_cat','comment_count']\n",
      ").properties(height=1500, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N', sort=top_commented,  title=None),\n",
      ").properties(title='Comment Ratio')\n",
      "(normalized | regular)\n",
      "\n",
      "(normalized | regular)\n",
      "44/271:\n",
      "bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "\n",
      "regular = alt.Chart(bibi_stacked).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    tooltip = ['Category', 'group_cat','comment_count']\n",
      ").properties(height=1500, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N', sort=top_commented,  title=None),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "herz_stacked = herz_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "\n",
      "hregular = alt.Chart(herz_stacked).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    tooltip = ['Category', 'group_cat','comment_count']\n",
      ").properties(height=1500, width=300, title='User Count')\n",
      "\n",
      "hnormalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N', sort=top_commented,  title=None),\n",
      ").properties(title='Comment Ratio')\n",
      "(normalized | regular)\n",
      "\n",
      "(normalized | hnormalized | regular | hregular)\n",
      "44/272:\n",
      "bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "\n",
      "regular = alt.Chart(bibi_stacked).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    tooltip = ['Category', 'group_cat','comment_count']\n",
      ").properties(height=1500, width=300, title='Comment Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N', sort=top_commented,  title=None),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "herz_stacked = herz_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "\n",
      "hregular = alt.Chart(herz_stacked).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    tooltip = ['Category', 'group_cat','comment_count']\n",
      ").properties(height=1500, width=300, title='Comment Count')\n",
      "\n",
      "hnormalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N', sort=top_commented,  title=None),\n",
      ").properties(title='Comment Ratio')\n",
      "(normalized | regular)\n",
      "\n",
      "(normalized | hnormalized | regular | hregular)\n",
      "44/273:\n",
      "bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "\n",
      "regular = alt.Chart(bibi_stacked).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    tooltip = ['Category', 'group_cat','comment_count']\n",
      ").properties(height=1500, width=300, title='Comment Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N', sort=top_commented,  title=None),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "herz_stacked = herz_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "\n",
      "hregular = alt.Chart(herz_stacked).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    tooltip = ['Category', 'group_cat','comment_count']\n",
      ").properties(height=1500, width=300, title='Comment Count')\n",
      "\n",
      "hnormalized = hregular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N', sort=top_commented,  title=None),\n",
      ").properties(title='Comment Ratio')\n",
      "(normalized | regular)\n",
      "\n",
      "(normalized | hnormalized | regular | hregular)\n",
      "44/274:\n",
      "bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "\n",
      "regular = alt.Chart(bibi_stacked).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    tooltip = ['Category', 'group_cat','comment_count']\n",
      ").properties(height=1500, width=200, title='Comment Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N', sort=top_commented,  title=None),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "herz_stacked = herz_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "\n",
      "hregular = alt.Chart(herz_stacked).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    tooltip = ['Category', 'group_cat','comment_count']\n",
      ").properties(height=1500, width=300, title='Comment Count')\n",
      "\n",
      "hnormalized = hregular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N', sort=top_commented,  title=None),\n",
      ").properties(title='Comment Ratio')\n",
      "(normalized | regular)\n",
      "\n",
      "(normalized | hnormalized | regular | hregular)\n",
      "44/275:\n",
      "bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "\n",
      "regular = alt.Chart(bibi_stacked).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    tooltip = ['Category', 'group_cat','comment_count']\n",
      ").properties(height=1500, width=150, title='Comment Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N', sort=top_commented,  title=None),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "herz_stacked = herz_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "\n",
      "hregular = alt.Chart(herz_stacked).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    tooltip = ['Category', 'group_cat','comment_count']\n",
      ").properties(height=1500, width=300, title='Comment Count')\n",
      "\n",
      "hnormalized = hregular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N', sort=top_commented,  title=None),\n",
      ").properties(title='Comment Ratio')\n",
      "(normalized | regular)\n",
      "\n",
      "(normalized | hnormalized | regular | hregular)\n",
      "44/276:\n",
      "bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "\n",
      "regular = alt.Chart(bibi_stacked).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    tooltip = ['Category', 'group_cat','comment_count']\n",
      ").properties(height=1500, width=150, title='Comment Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N', sort=top_commented,  title=None),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "herz_stacked = herz_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "\n",
      "hregular = alt.Chart(herz_stacked).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    tooltip = ['Category', 'group_cat','comment_count']\n",
      ").properties(height=1500, width=150, title='Comment Count')\n",
      "\n",
      "hnormalized = hregular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N', sort=top_commented,  title=None),\n",
      ").properties(title='Comment Ratio')\n",
      "(normalized | regular)\n",
      "\n",
      "(normalized | hnormalized | regular | hregular)\n",
      "44/277:\n",
      "herz_cont['name'] = 'Herzog'\n",
      "bibi_cont['name'] = 'Netanyahu'\n",
      "\n",
      "herz_cont.merge('bibi_cont', how='outer')\n",
      "44/278:\n",
      "herz_cont['name'] = 'Herzog'\n",
      "bibi_cont['name'] = 'Netanyahu'\n",
      "\n",
      "herz_cont.merge('bibi_cont', how='outer', index=True)\n",
      "44/279:\n",
      "herz_cont['name'] = 'Herzog'\n",
      "bibi_cont['name'] = 'Netanyahu'\n",
      "\n",
      "herz_cont.merge('bibi_cont', how='outer', on_index=True)\n",
      "44/280:\n",
      "herz_cont['name'] = 'Herzog'\n",
      "bibi_cont['name'] = 'Netanyahu'\n",
      "\n",
      "herz_cont.merge('bibi_cont', how='outer', left_index=True, right_index=True)\n",
      "44/281:\n",
      "herz_cont['name'] = 'Herzog'\n",
      "bibi_cont['name'] = 'Netanyahu'\n",
      "\n",
      "herz_cont.merge(bibi_cont, how='outer', left_index=True, right_index=True)\n",
      "44/282:\n",
      "herz_cont['name'] = 'Herzog'\n",
      "bibi_cont['name'] = 'Netanyahu'\n",
      "\n",
      "herz_cont.reset_index().merge(bibi_cont.reset_index(), how='outer', on=['Category', 'name'])\n",
      "44/283: herz_cont.index+bibi_cont.index\n",
      "44/284: herz_cont.index.union(bibi_cont.index)\n",
      "44/285: herz_cont.index.union(bibi_cont.index).shape\n",
      "44/286:\n",
      "herz_cont['name'] = 'Herzog'\n",
      "bibi_cont['name'] = 'Netanyahu'\n",
      "\n",
      "herz_cont.reindex(herz_cont.index.union(bibi_cont.index)).reset_index().merge(bibi_cont.reset_index(), how='left', on=['Category', 'name'])\n",
      "44/287: bibi_cont.reset_index()\n",
      "44/288:\n",
      "herz_cont['name'] = 'Herzog'\n",
      "bibi_cont['name'] = 'Netanyahu'\n",
      "\n",
      "pd.concat([herz_cont.reindex(herz_cont.index.union(bibi_cont.index)).reset_index(), bibi_cont.reset_index()])\n",
      "44/289:\n",
      "herz_cont['name'] = 'Herzog'\n",
      "bibi_cont['name'] = 'Netanyahu'\n",
      "\n",
      "pd.concat([herz_cont.reindex(herz_cont.index.union(bibi_cont.index)).reset_index(), bibi_cont.reindex(herz_cont.index.union(bibi_cont.index)).reset_index()])\n",
      "44/290:\n",
      "herz_cont['name'] = 'Herzog'\n",
      "bibi_cont['name'] = 'Netanyahu'\n",
      "\n",
      "cont = pd.concat(\n",
      "    [herz_cont.reindex(herz_cont.index.union(bibi_cont.index)).reset_index(), \n",
      "     bibi_cont.reindex(herz_cont.index.union(bibi_cont.index)).reset_index()])\n",
      "44/291:\n",
      "herz_cont['name'] = 'Herzog'\n",
      "bibi_cont['name'] = 'Netanyahu'\n",
      "\n",
      "cont = pd.concat(\n",
      "    [herz_cont.reindex(herz_cont.index.union(bibi_cont.index)).reset_index(), \n",
      "     bibi_cont.reindex(herz_cont.index.union(bibi_cont.index)).reset_index()]).fillna(0)\n",
      "44/292:\n",
      "bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "\n",
      "regular = alt.Chart(cont).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    tooltip = ['Category', 'group_cat','comment_count']\n",
      "    column='name'\n",
      ").properties(height=1500, width=150, title='Comment Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N', sort=top_commented,  title=None),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "\n",
      "(normalized | hnormalized | regular | hregular)\n",
      "44/293:\n",
      "bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "\n",
      "regular = alt.Chart(cont).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    tooltip = ['Category', 'group_cat','comment_count']m\n",
      "    column='name'\n",
      ").properties(height=1500, width=150, title='Comment Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N', sort=top_commented,  title=None),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "\n",
      "(normalized | hnormalized | regular | hregular)\n",
      "44/294:\n",
      "bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "\n",
      "regular = alt.Chart(cont).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    tooltip = ['Category', 'group_cat','comment_count'],\n",
      "    column='name'\n",
      ").properties(height=1500, width=150, title='Comment Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N', sort=top_commented,  title=None),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "\n",
      "(normalized | hnormalized | regular | hregular)\n",
      "44/295: cont.head()\n",
      "44/296:\n",
      "herz_cont['name'] = 'Herzog'\n",
      "bibi_cont['name'] = 'Netanyahu'\n",
      "\n",
      "cont = pd.concat(\n",
      "    [herz_cont.reindex(herz_cont.index.union(bibi_cont.index)).reset_index(), \n",
      "     bibi_cont.reindex(herz_cont.index.union(bibi_cont.index)).reset_index()]).fillna(0).set_index(['name', 'Category'])\n",
      "44/297:\n",
      "herz_cont['name'] = 'Herzog'\n",
      "bibi_cont['name'] = 'Netanyahu'\n",
      "\n",
      "cont = pd.concat(\n",
      "    [herz_cont.reindex(herz_cont.index.union(bibi_cont.index)).reset_index(), \n",
      "     bibi_cont.reindex(herz_cont.index.union(bibi_cont.index)).reset_index()]).fillna(0).set_index(['name', 'Category'])\n",
      "44/298: cont.head()\n",
      "44/299: cont.stack().reset_index()\n",
      "44/300:\n",
      "bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "cont_stacked = cont.stack().reset_index().rename(columns = {'level_2': 'group_cat', 0: 'comment_count'})\n",
      "regular = alt.Chart(cont).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    tooltip = ['Category', 'group_cat','comment_count'],\n",
      "    column='name'\n",
      ").properties(height=1500, width=150, title='Comment Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N', sort=top_commented,  title=None),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "\n",
      "(normalized | hnormalized | regular | hregular)\n",
      "44/301:\n",
      "cont_stacked = cont.stack().reset_index().rename(columns = {'level_2': 'group_cat', 0: 'comment_count'})\n",
      "cont_stacked.head()\n",
      "44/302:\n",
      "bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "regular = alt.Chart(cont_stacked).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    tooltip = ['Category', 'group_cat','comment_count'],\n",
      "    column='name'\n",
      ").properties(height=1500, width=150, title='Comment Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N', sort=top_commented,  title=None),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "\n",
      "(normalized | hnormalized | regular | hregular)\n",
      "44/303:\n",
      "bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "regular = alt.Chart(cont_stacked).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    tooltip = ['Category', 'group_cat','comment_count'],\n",
      "    column='name'\n",
      ").properties(height=1500, width=150, title='Comment Count')\n",
      "\n",
      "#normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N', sort=top_commented,  title=None),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "\n",
      "#(normalized | hnormalized | regular | hregular)\n",
      "44/304:\n",
      "bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "regular = alt.Chart(cont_stacked).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    tooltip = ['Category', 'group_cat','comment_count'],\n",
      "    column='name'\n",
      ").properties(height=1500, width=150, title='Comment Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N', sort=top_commented,  title=None),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "\n",
      "regular\n",
      "44/305:\n",
      "bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "regular = alt.Chart(cont_stacked).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    tooltip = ['Category', 'group_cat','comment_count'],\n",
      "    column='page'\n",
      ").properties(height=1500, width=150, title='Comment Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N', sort=top_commented,  title=None),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "\n",
      "regular\n",
      "44/306:\n",
      "herz_cont['page'] = 'Herzog'\n",
      "bibi_cont['page'] = 'Netanyahu'\n",
      "\n",
      "cont = pd.concat(\n",
      "    [herz_cont.reindex(herz_cont.index.union(bibi_cont.index)).reset_index(), \n",
      "     bibi_cont.reindex(herz_cont.index.union(bibi_cont.index)).reset_index()]).fillna(0).set_index(['page', 'Category'])\n",
      "44/307:\n",
      "cont_stacked = cont.stack().reset_index().rename(columns = {'level_2': 'group_cat', 0: 'comment_count'})\n",
      "cont_stacked.head()\n",
      "44/308:\n",
      "bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "regular = alt.Chart(cont_stacked).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    tooltip = ['Category', 'group_cat','comment_count'],\n",
      "    column='page'\n",
      ").properties(height=1500, width=150, title='Comment Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N', sort=top_commented,  title=None),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "\n",
      "regular\n",
      "44/309:\n",
      "bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "regular = alt.Chart(cont_stacked).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    tooltip = ['Category', 'group_cat','comment_count'],\n",
      "    column='page'\n",
      ").properties(height=1500, width=150, title='Comment Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N', sort=top_commented,  title=None),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "\n",
      "regular\n",
      "44/310: cont_stacked.page.value_counts()\n",
      "44/311:\n",
      "herz_cont = herz_cont.reindex(herz_cont.index.union(bibi_cont.index)).reset_index()\n",
      "herz_cont['page'] = 'Herzog'\n",
      "bibi_cont = bibi_cont.reindex(herz_cont.index.union(bibi_cont.index)).reset_index()\n",
      "bibi_cont['page'] = 'Netanyahu'\n",
      "\n",
      "cont = pd.concat(\n",
      "    [herz_cont, bibi_cont]).fillna(0).set_index(['page', 'Category'])\n",
      "44/312:\n",
      "cont_stacked = cont.stack().reset_index().rename(columns = {'level_2': 'group_cat', 0: 'comment_count'})\n",
      "cont_stacked.head()\n",
      "44/313: cont_stacked.page.value_counts()\n",
      "44/314: cont_stacked.page.value_counts()\n",
      "44/315:\n",
      "bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "regular = alt.Chart(cont_stacked).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    tooltip = ['Category', 'group_cat','comment_count'],\n",
      "    column='page'\n",
      ").properties(height=1500, width=150, title='Comment Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N', sort=top_commented,  title=None),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "\n",
      "regular\n",
      "44/316:\n",
      "bibi_cont = pd.read_csv('turf/bibi_content_group_counts.csv').set_index('Category')\n",
      "bibi_cont.head()\n",
      "44/317:\n",
      "herz_cont = pd.read_csv('turf/herzog_content_group_counts.csv').set_index('Category')\n",
      "herz_cont.head()\n",
      "44/318:\n",
      "herz_cont = herz_cont.reindex(herz_cont.index.union(bibi_cont.index)).reset_index()\n",
      "herz_cont['page'] = 'Herzog'\n",
      "bibi_cont = bibi_cont.reindex(herz_cont.index.union(bibi_cont.index)).reset_index()\n",
      "bibi_cont['page'] = 'Netanyahu'\n",
      "\n",
      "cont = pd.concat(\n",
      "    [herz_cont, bibi_cont]).fillna(0).set_index(['page', 'Category'])\n",
      "44/319:\n",
      "cont_stacked = cont.stack().reset_index().rename(columns = {'level_2': 'group_cat', 0: 'comment_count'})\n",
      "cont_stacked.head()\n",
      "44/320: cont_stacked.page.value_counts()\n",
      "44/321:\n",
      "bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "regular = alt.Chart(cont_stacked).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    tooltip = ['Category', 'group_cat','comment_count'],\n",
      "    column='page'\n",
      ").properties(height=1500, width=150, title='Comment Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N', sort=top_commented,  title=None),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "\n",
      "regular\n",
      "44/322:\n",
      "bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "regular = alt.Chart(cont_stacked).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "    column='page'\n",
      ").properties(height=1500, width=150, title='Comment Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N', sort=top_commented,  title=None),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "\n",
      "regular\n",
      "44/323:\n",
      "bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "regular = alt.Chart(cont_stacked).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis()),\n",
      "    alt.Y('Category:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "    column='page'\n",
      ").properties(height=1500, width=150, title='Comment Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N', sort=top_commented,  title=None),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "\n",
      "regular\n",
      "44/324:\n",
      "bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "regular = alt.Chart(cont_stacked).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', sort=top_commented, title=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "    column='page'\n",
      ").properties(height=1500, width=150, title='Comment Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N', sort=top_commented,  title=None),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "\n",
      "regular\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/325:\n",
      "bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "regular = alt.Chart(cont_stacked).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', sort=top_commented, title=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "    column='page:N'\n",
      ").properties(height=1500, width=150, title='Comment Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N', sort=top_commented,  title=None),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "\n",
      "regular\n",
      "44/326: cont\n",
      "44/327: bibi_cont\n",
      "44/328:\n",
      "bibi_cont = pd.read_csv('turf/bibi_content_group_counts.csv').set_index('Category')\n",
      "bibi_cont.head()\n",
      "44/329: bibi_cont\n",
      "44/330:\n",
      "herz_cont = herz_cont.reindex(herz_cont.index.union(bibi_cont.index)).reset_index()\n",
      "herz_cont['page'] = 'Herzog'\n",
      "bibi_cont = bibi_cont.reindex(herz_cont.index.union(bibi_cont.index)).reset_index()\n",
      "bibi_cont['page'] = 'Netanyahu'\n",
      "\n",
      "cont = pd.concat(\n",
      "    [herz_cont, bibi_cont]).fillna(0).set_index(['page', 'Category'])\n",
      "44/331: bibi_cont\n",
      "44/332:\n",
      "herz_cont = herz_cont.reindex(herz_cont.index.union(bibi_cont.index)).reset_index()\n",
      "herz_cont['page'] = 'Herzog'\n",
      "bibi_cont.index = bibi_cont.index.union(herz_cont.index)\n",
      "bibi_cont['page'] = 'Netanyahu'\n",
      "\n",
      "cont = pd.concat(\n",
      "    [herz_cont, bibi_cont]).fillna(0).set_index(['page', 'Category'])\n",
      "44/333:\n",
      "herz_cont = bibi_cont.index.union(herz_cont.index)\n",
      "herz_cont['page'] = 'Herzog'\n",
      "bibi_cont.index = bibi_cont.index.union(herz_cont.index)\n",
      "bibi_cont['page'] = 'Netanyahu'\n",
      "\n",
      "cont = pd.concat(\n",
      "    [herz_cont, bibi_cont]).fillna(0).set_index(['page', 'Category'])\n",
      "44/334:\n",
      "herz_cont = herz_cont.index.union(bibi_cont.index)\n",
      "herz_cont['page'] = 'Herzog'\n",
      "bibi_cont.index = bibi_cont.index.union(herz_cont.index)\n",
      "bibi_cont['page'] = 'Netanyahu'\n",
      "\n",
      "cont = pd.concat(\n",
      "    [herz_cont, bibi_cont]).fillna(0).set_index(['page', 'Category'])\n",
      "44/335:\n",
      "bibi_cont = pd.read_csv('turf/bibi_content_group_counts.csv').set_index('Category')\n",
      "bibi_cont.head()\n",
      "44/336:\n",
      "herz_cont = pd.read_csv('turf/herzog_content_group_counts.csv').set_index('Category')\n",
      "herz_cont.head()\n",
      "44/337:\n",
      "herz_cont = herz_cont.index.union(bibi_cont.index)\n",
      "herz_cont['page'] = 'Herzog'\n",
      "bibi_cont.index = bibi_cont.index.union(herz_cont.index)\n",
      "bibi_cont['page'] = 'Netanyahu'\n",
      "\n",
      "cont = pd.concat(\n",
      "    [herz_cont, bibi_cont]).fillna(0).set_index(['page', 'Category'])\n",
      "44/338:\n",
      "herz_cont.index = herz_cont.index.union(bibi_cont.index)\n",
      "herz_cont['page'] = 'Herzog'\n",
      "bibi_cont.index = bibi_cont.index.union(herz_cont.index)\n",
      "bibi_cont['page'] = 'Netanyahu'\n",
      "\n",
      "cont = pd.concat(\n",
      "    [herz_cont, bibi_cont]).fillna(0).set_index(['page', 'Category'])\n",
      "44/339:\n",
      "bibi_cont = pd.read_csv('turf/bibi_content_group_counts.csv').set_index('Category')\n",
      "bibi_cont.head()\n",
      "44/340:\n",
      "herz_cont = pd.read_csv('turf/herzog_content_group_counts.csv').set_index('Category')\n",
      "herz_cont.head()\n",
      "44/341:\n",
      "herz_cont.index = herz_cont.index.union(bibi_cont.index)\n",
      "herz_cont['page'] = 'Herzog'\n",
      "bibi_cont.index = bibi_cont.index.union(herz_cont.index)\n",
      "bibi_cont['page'] = 'Netanyahu'\n",
      "\n",
      "cont = pd.concat(\n",
      "    [herz_cont, bibi_cont]).fillna(0).set_index(['page', 'Category'])\n",
      "44/342:\n",
      "herz_cont = herz_cont.reindex(herz_cont.index.union(bibi_cont.index)).reset_index()\n",
      "herz_cont['page'] = 'Herzog'\n",
      "bibi_cont = bibi_cont.reindex(bibi_cont.index.union(herz_cont.index)).reset_index()\n",
      "bibi_cont['page'] = 'Netanyahu'\n",
      "\n",
      "cont = pd.concat(\n",
      "    [herz_cont, bibi_cont]).fillna(0).set_index(['page', 'Category'])\n",
      "44/343:\n",
      "bibi_cont = pd.read_csv('turf/bibi_content_group_counts.csv').set_index('Category')\n",
      "bibi_cont.head()\n",
      "44/344:\n",
      "herz_cont = pd.read_csv('turf/herzog_content_group_counts.csv').set_index('Category')\n",
      "herz_cont.head()\n",
      "44/345:\n",
      "herz_cont = herz_cont.reindex(herz_cont.index.union(bibi_cont.index)).reset_index()\n",
      "herz_cont['page'] = 'Herzog'\n",
      "bibi_cont = bibi_cont.reindex(bibi_cont.index.union(herz_cont.index)).reset_index()\n",
      "bibi_cont['page'] = 'Netanyahu'\n",
      "\n",
      "cont = pd.concat(\n",
      "    [herz_cont, bibi_cont]).fillna(0).set_index(['page', 'Category'])\n",
      "44/346: bibi_cont\n",
      "44/347:\n",
      "herz_cont = herz_cont.reindex(herz_cont.index.union(bibi_cont.index)).reset_index()\n",
      "herz_cont['page'] = 'Herzog'\n",
      "#bibi_cont = bibi_cont.reindex(bibi_cont.index.union(herz_cont.index)).reset_index()\n",
      "bibi_cont['page'] = 'Netanyahu'\n",
      "\n",
      "cont = pd.concat(\n",
      "    [herz_cont, bibi_cont]).fillna(0).set_index(['page', 'Category'])\n",
      "44/348:\n",
      "bibi_cont = pd.read_csv('turf/bibi_content_group_counts.csv').set_index('Category')\n",
      "bibi_cont.head()\n",
      "44/349:\n",
      "herz_cont = pd.read_csv('turf/herzog_content_group_counts.csv').set_index('Category')\n",
      "herz_cont.head()\n",
      "44/350:\n",
      "herz_cont = herz_cont.reindex(herz_cont.index.union(bibi_cont.index)).reset_index()\n",
      "herz_cont['page'] = 'Herzog'\n",
      "#bibi_cont = bibi_cont.reindex(bibi_cont.index.union(herz_cont.index)).reset_index()\n",
      "bibi_cont['page'] = 'Netanyahu'\n",
      "\n",
      "cont = pd.concat(\n",
      "    [herz_cont, bibi_cont]).fillna(0).set_index(['page', 'Category'])\n",
      "44/351:\n",
      "bibi_cont = pd.read_csv('turf/bibi_content_group_counts.csv').set_index('Category')\n",
      "bibi_cont\n",
      "44/352:\n",
      "bibi_cont = pd.read_csv('turf/bibi_content_group_counts.csv').set_index('Category')\n",
      "bibi_cont.head()\n",
      "44/353:\n",
      "herz_cont = herz_cont.reindex(herz_cont.index.union(bibi_cont.index)).reset_index()\n",
      "herz_cont['page'] = 'Herzog'\n",
      "#bibi_cont = bibi_cont.reindex(bibi_cont.index.union(herz_cont.index)).reset_index()\n",
      "bibi_cont['page'] = 'Netanyahu'\n",
      "\n",
      "cont = pd.concat(\n",
      "    [herz_cont, bibi_cont]).fillna(0).set_index(['page', 'Category'])\n",
      "44/354:\n",
      "bibi_cont = (pd.read_csv('turf/bibi_content_group_counts.csv').set_index('Category')\n",
      "             .assign(page = 'Netanyahu'))\n",
      "bibi_cont.head()\n",
      "44/355:\n",
      "bibi_cont = (pd.read_csv('turf/bibi_content_group_counts.csv').set_index('Category')\n",
      "             .assign(page = 'Netanyahu'))\n",
      "bibi_cont.head()\n",
      "44/356:\n",
      "herz_cont = (pd.read_csv('turf/herzog_content_group_counts.csv').set_index('Category')\n",
      "             .assign(page = 'Herzog'))\n",
      "herz_cont.head()\n",
      "44/357:\n",
      "herz_cont = herz_cont.reindex(herz_cont.index.union(bibi_cont.index)).reset_index()\n",
      "#bibi_cont = bibi_cont.reindex(bibi_cont.index.union(herz_cont.index)).reset_index()\n",
      "\n",
      "cont = pd.concat(\n",
      "    [herz_cont, bibi_cont]).fillna(0).set_index(['page', 'Category'])\n",
      "44/358: bibi_cont\n",
      "44/359:\n",
      "cont_stacked = cont.stack().reset_index().rename(columns = {'level_2': 'group_cat', 0: 'comment_count'})\n",
      "cont_stacked.head()\n",
      "44/360: cont_stacked.page.value_counts()\n",
      "44/361:\n",
      "bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "regular = alt.Chart(cont_stacked).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', sort=top_commented, title=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "    column='page:N'\n",
      ").properties(height=1500, width=150, title='Comment Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N', sort=top_commented,  title=None),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "\n",
      "regular\n",
      "44/362:\n",
      "herz_cont = herz_cont.reindex(herz_cont.index.union(bibi_cont.index)).reset_index()\n",
      "bibi_cont = bibi_cont.reindex(bibi_cont.index.union(herz_cont.index)).reset_index()\n",
      "\n",
      "cont = pd.concat(\n",
      "    [herz_cont, bibi_cont]).fillna(0).set_index(['page', 'Category'])\n",
      "44/363:\n",
      "bibi_cont = (pd.read_csv('turf/bibi_content_group_counts.csv').set_index('Category')\n",
      "             .assign(page = 'Netanyahu'))\n",
      "bibi_cont.head()\n",
      "44/364:\n",
      "herz_cont = (pd.read_csv('turf/herzog_content_group_counts.csv').set_index('Category')\n",
      "             .assign(page = 'Herzog'))\n",
      "herz_cont.head()\n",
      "44/365:\n",
      "herz_cont = herz_cont.reindex(herz_cont.index.union(bibi_cont.index)).reset_index()\n",
      "bibi_cont = bibi_cont.reindex(bibi_cont.index.union(herz_cont.index)).reset_index()\n",
      "\n",
      "cont = pd.concat(\n",
      "    [herz_cont, bibi_cont]).fillna(0).set_index(['page', 'Category'])\n",
      "44/366: bibi_cont\n",
      "44/367:\n",
      "bibi_cont = (pd.read_csv('turf/bibi_content_group_counts.csv').set_index('Category')\n",
      "             .assign(page = 'Netanyahu'))\n",
      "bibi_cont.head()\n",
      "44/368:\n",
      "herz_cont = (pd.read_csv('turf/herzog_content_group_counts.csv').set_index('Category')\n",
      "             .assign(page = 'Herzog'))\n",
      "herz_cont.head()\n",
      "44/369:\n",
      "herz_cont = herz_cont.reindex(herz_cont.index.union(bibi_cont.index))\n",
      "bibi_cont = bibi_cont.reindex(bibi_cont.index.union(herz_cont.index))\n",
      "\n",
      "cont = pd.concat(\n",
      "    [herz_cont.reset_index(), bibi_cont.reset_index()]).fillna(0).set_index(['page', 'Category'])\n",
      "44/370: bibi_cont\n",
      "44/371:\n",
      "cont_stacked = cont.stack().reset_index().rename(columns = {'level_2': 'group_cat', 0: 'comment_count'})\n",
      "cont_stacked.head()\n",
      "44/372:\n",
      "cont_stacked = cont.stack().reset_index().rename(columns = {'level_2': 'group_cat', 0: 'comment_count'})\n",
      "cont_stacked.head()\n",
      "44/373: cont_stacked.page.value_counts()\n",
      "44/374:\n",
      "bibi_cont = (pd.read_csv('turf/bibi_content_group_counts.csv').set_index('Category')\n",
      "             .assign(page = 'Netanyahu'))\n",
      "bibi_cont.head()\n",
      "44/375:\n",
      "herz_cont = (pd.read_csv('turf/herzog_content_group_counts.csv').set_index('Category')\n",
      "             .assign(page = 'Herzog'))\n",
      "herz_cont.head()\n",
      "44/376:\n",
      "herz_cont = herz_cont.reindex(herz_cont.index.union(bibi_cont.index))\n",
      "bibi_cont = bibi_cont.reindex(bibi_cont.index.union(herz_cont.index))\n",
      "\n",
      "cont = pd.concat(\n",
      "    [herz_cont.reset_index(), bibi_cont.reset_index()]).fillna(0).set_index(['page', 'Category'])\n",
      "44/377:\n",
      "cont_stacked = cont.stack().reset_index().rename(columns = {'level_2': 'group_cat', 0: 'comment_count'})\n",
      "cont_stacked.head()\n",
      "44/378: cont_stacked.page.value_counts()\n",
      "44/379:\n",
      "bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "regular = alt.Chart(cont_stacked).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', sort=top_commented, title=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "    column='page:N'\n",
      ").properties(height=1500, width=150, title='Comment Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N', sort=top_commented,  title=None),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "\n",
      "regular\n",
      "44/380: bibi_cont\n",
      "44/381:\n",
      "herz_cont = herz_cont.reindex(herz_cont.index.union(bibi_cont.index))\n",
      "bibi_cont = bibi_cont.reindex(bibi_cont.index.union(herz_cont.index))\n",
      "bibi_cont['page'] = 'Netanyahu'\n",
      "cont = pd.concat(\n",
      "    [herz_cont.reset_index(), bibi_cont.reset_index()]).fillna(0).set_index(['page', 'Category'])\n",
      "44/382: bibi_cont\n",
      "44/383:\n",
      "cont_stacked = cont.stack().reset_index().rename(columns = {'level_2': 'group_cat', 0: 'comment_count'})\n",
      "cont_stacked.head()\n",
      "44/384: cont_stacked.page.value_counts()\n",
      "44/385:\n",
      "bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "regular = alt.Chart(cont_stacked).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', sort=top_commented, title=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "    column='page:N'\n",
      ").properties(height=1500, width=150, title='Comment Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N', sort=top_commented,  title=None),\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "\n",
      "regular\n",
      "44/386:\n",
      "bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "regular = alt.Chart(cont_stacked).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', sort=top_commented, title=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "    column='page:N'\n",
      ").properties(height=1500, width=150, title='Comment Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N', sort=top_commented,  title=None),\n",
      "    column='page:N'\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "\n",
      "regular | normalized\n",
      "44/387:\n",
      "bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "regular = alt.Chart(cont_stacked).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', sort=top_commented, title=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "    column='page:N'\n",
      ").properties(height=1500, width=150, title='Comment Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N', sort=top_commented,  title=None),\n",
      "    column='page:N'\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "\n",
      "(regular | normalized)\n",
      "44/388:\n",
      "bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "regular = alt.Chart(cont_stacked).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', title=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "    column='page:N'\n",
      ").properties(height=1500, width=150, title='Comment Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N',  title=None),\n",
      "    column='page:N'\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "\n",
      "(regular | normalized)\n",
      "44/389:\n",
      "bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "regular = alt.Chart(cont_stacked).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', title=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "    #column='page:N'\n",
      ").properties(height=1500, width=150, title='Comment Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N',  title=None),\n",
      "    column='page:N'\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "\n",
      "(regular | normalized)\n",
      "44/390:\n",
      "#bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "regular = alt.Chart(cont_stacked).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', title=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "    column='page:N'\n",
      ").properties(height=1500, width=150, title='Comment Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N',  title=None),\n",
      "    #column='page:N'\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "\n",
      "(regular | normalized)\n",
      "44/391:\n",
      "#bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "regular = alt.Chart(cont_stacked).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', title=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "    #column='page:N'\n",
      ").properties(height=1500, width=150, title='Comment Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N',  title=None),\n",
      "    #column='page:N'\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "\n",
      "(regular | normalized)\n",
      "44/392:\n",
      "#bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "regular = alt.Chart(cont_stacked).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', axis=None, title=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "    #column='page:N'\n",
      ").properties(height=1500, width=150, title='Comment Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N',  title=None),\n",
      "    #column='page:N'\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "\n",
      "(regular | normalized)\n",
      "44/393:\n",
      "#bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "regular = alt.Chart(cont_stacked).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', title=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "    #column='page:N'\n",
      ").properties(height=1500, width=150, title='Comment Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N', axis=None, title=None),\n",
      "    #column='page:N'\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "\n",
      "(regular | normalized)\n",
      "44/394:\n",
      "#bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "regular = alt.Chart(cont_stacked).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', title=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "    column='page:N'\n",
      ").properties(height=1500, width=150, title='Comment Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N',  title=None),\n",
      "    #column='page:N'\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "\n",
      "(normalized | regular)\n",
      "44/395:\n",
      "#bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "regular = alt.Chart(cont_stacked).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('Category:N', axis=None, title=None),\n",
      "    alt.Color('group_cat:N'),\n",
      "    #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "    #column='page:N'\n",
      ").properties(height=1500, width=150, title='Comment Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('Category:N',  title=None),\n",
      "    #column='page:N'\n",
      ").properties(title='Comment Ratio')\n",
      "\n",
      "\n",
      "(normalized | regular)\n",
      "44/396:\n",
      "#bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "\n",
      "\n",
      "row = alt.hconcat()\n",
      "for page in cont_stacked.page.unique():\n",
      "    regular = alt.Chart(cont_stacked).mark_bar().encode(\n",
      "        alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "        alt.Y('Category:N', axis=None, title=None),\n",
      "        alt.Color('group_cat:N'),\n",
      "        #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "        #column='page:N'\n",
      "    ).properties(height=1500, width=150, title='Comment Count')\n",
      "\n",
      "    normalized = regular.encode(    \n",
      "        alt.X('comment_count:Q',\n",
      "            axis=alt.Axis(title=None),\n",
      "            stack='normalize'\n",
      "            ),\n",
      "        alt.Y('Category:N',  title=None),\n",
      "        #column='page:N'\n",
      "    ).properties(title='Comment Ratio')\n",
      "    row |= (normalized | regular)\n",
      "    \n",
      "(normalized | regular)\n",
      "44/397:\n",
      "#bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "\n",
      "\n",
      "row = alt.hconcat()\n",
      "for page in cont_stacked.page.unique():\n",
      "    regular = alt.Chart(cont_stacked).mark_bar().encode(\n",
      "        alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "        alt.Y('Category:N', axis=None, title=None),\n",
      "        alt.Color('group_cat:N'),\n",
      "        #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "        #column='page:N'\n",
      "    ).properties(height=1500, width=150, title='Comment Count')\n",
      "\n",
      "    normalized = regular.encode(    \n",
      "        alt.X('comment_count:Q',\n",
      "            axis=alt.Axis(title=None),\n",
      "            stack='normalize'\n",
      "            ),\n",
      "        alt.Y('Category:N',  title=None),\n",
      "        #column='page:N'\n",
      "    ).properties(title='Comment Ratio')\n",
      "    row |= (normalized | regular)\n",
      "    \n",
      "row\n",
      "44/398:\n",
      "#bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "\n",
      "\n",
      "row = alt.hconcat()\n",
      "for page in cont_stacked.page.unique():\n",
      "    regular = alt.Chart(cont_stacked[cont_stacked.page==page]).mark_bar().encode(\n",
      "        alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "        alt.Y('Category:N', axis=None, title=None),\n",
      "        alt.Color('group_cat:N'),\n",
      "        #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "        #column='page:N'\n",
      "    ).properties(height=1500, width=150, title='Comment Count')\n",
      "\n",
      "    normalized = regular.encode(    \n",
      "        alt.X('comment_count:Q',\n",
      "            axis=alt.Axis(title=None),\n",
      "            stack='normalize'\n",
      "            ),\n",
      "        alt.Y('Category:N',  title=None),\n",
      "        #column='page:N'\n",
      "    ).properties(title='Comment Ratio')\n",
      "    row |= (normalized | regular)\n",
      "    \n",
      "row\n",
      "44/399:\n",
      "#bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "\n",
      "\n",
      "row = alt.hconcat()\n",
      "for page in cont_stacked.page.unique():\n",
      "    regular = alt.Chart(cont_stacked[cont_stacked.page==page]).mark_bar().encode(\n",
      "        alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "        alt.Y('Category:N', axis=None, title=None),\n",
      "        alt.Color('group_cat:N'),\n",
      "        #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "        #column='page:N'\n",
      "    ).properties(height=1500, width=150, title=page + 'Comment Count')\n",
      "\n",
      "    normalized = regular.encode(    \n",
      "        alt.X('comment_count:Q',\n",
      "            axis=alt.Axis(title=None),\n",
      "            stack='normalize'\n",
      "            ),\n",
      "        alt.Y('Category:N',  title=None),\n",
      "        #column='page:N'\n",
      "    ).properties(title=page + ' Comment Ratio')\n",
      "    row |= (normalized | regular)\n",
      "    \n",
      "row\n",
      "44/400:\n",
      "#bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "\n",
      "\n",
      "row = alt.hconcat()\n",
      "for page in cont_stacked.page.unique():\n",
      "    regular = alt.Chart(cont_stacked[cont_stacked.page==page]).mark_bar().encode(\n",
      "        alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "        alt.Y('Category:N', axis=None, title=None),\n",
      "        alt.Color('group_cat:N'),\n",
      "        #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "        #column='page:N'\n",
      "    ).properties(height=1500, width=150, title=page + ' Comment Count')\n",
      "\n",
      "    normalized = regular.encode(    \n",
      "        alt.X('comment_count:Q',\n",
      "            axis=alt.Axis(title=None),\n",
      "            stack='normalize'\n",
      "            ),\n",
      "        alt.Y('Category:N',  title=None),\n",
      "        #column='page:N'\n",
      "    ).properties(title=page + ' Comment Ratio')\n",
      "    row |= (normalized | regular)\n",
      "    \n",
      "row\n",
      "44/401:\n",
      "#bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "\n",
      "\n",
      "row = alt.hconcat()\n",
      "for page in cont_stacked.page.unique():\n",
      "    regular = alt.Chart(cont_stacked[cont_stacked.page==page]).mark_bar().encode(\n",
      "        alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "        alt.Y('Category:N', axis=None, title=None),\n",
      "        alt.Color('group_cat:N'),\n",
      "        #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "        #column='page:N'\n",
      "    ).properties(height=1500, width=150, title=page + ' Comment Count')        \n",
      "\n",
      "    normalized = regular.encode(    \n",
      "        alt.X('comment_count:Q',\n",
      "            axis=alt.Axis(title=None),\n",
      "            stack='normalize'\n",
      "            ),\n",
      "        alt.Y('Category:N',  title=None),\n",
      "        #column='page:N'\n",
      "    ).properties(title=page + ' Comment Ratio')\n",
      "    if(page!='Herzog'):\n",
      "        normalized = normalized.encode(alt.Y('Category:N', axis=None, title=None),)\n",
      "    row |= (normalized | regular)\n",
      "    \n",
      "row\n",
      "46/1:\n",
      "times = pd.read_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "st = pd.read_pickle('turf/total_stats_no_multiindex_groups_ded_cuts.pkl.gz', compression='gzip')\n",
      "46/2:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "46/3: sns.__version__\n",
      "46/4:\n",
      "times = pd.read_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "st = pd.read_pickle('turf/total_stats_no_multiindex_groups_ded_cuts.pkl.gz', compression='gzip')\n",
      "46/5: st.dtypes\n",
      "46/6: from itertools import product\n",
      "46/7: times.dtypes\n",
      "46/8:\n",
      "columns = ['elapsed_time',  'created_time', 'created_month', 'created_day']\n",
      "stats = ['nunique', 'min', 'max']\n",
      "new_cols = ['from_id'] + ['_'.join(x) for x in product(columns, stats)] + ['period_length']\n",
      "new_cols\n",
      "46/9: times.columns = new_cols\n",
      "46/10: st[['from_id', 'group_cat']].head()\n",
      "46/11: times = times.merge(st[['from_id', 'group_cat']].drop_duplicates(), how='left', on='from_id')\n",
      "46/12: day_in_seconds = 60*60*24\n",
      "46/13: times.groupby('group_cat').period_length.median()/day_in_seconds\n",
      "46/14: times = times.merge(st[['from_id', 'group_cat']].drop_duplicates(), how='left', on='from_id')\n",
      "46/15: day_in_seconds = 60*60*24\n",
      "46/16: times.groupby('group_cat').period_length.median()/day_in_seconds\n",
      "46/17:\n",
      "times = pd.read_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "st = pd.read_pickle('turf/total_stats_no_multiindex_groups_ded_cuts.pkl.gz', compression='gzip')\n",
      "46/18: st.dtypes\n",
      "46/19: from itertools import product\n",
      "46/20: times.dtypes\n",
      "46/21:\n",
      "columns = ['elapsed_time',  'created_time', 'created_month', 'created_day']\n",
      "stats = ['nunique', 'min', 'max']\n",
      "new_cols = ['from_id'] + ['_'.join(x) for x in product(columns, stats)] + ['period_length']\n",
      "new_cols\n",
      "46/22: times.columns = new_cols\n",
      "46/23: st[['from_id', 'group_cat']].head()\n",
      "46/24: times = times.merge(st[['from_id', 'group_cat']].drop_duplicates(), how='left', on='from_id')\n",
      "46/25: day_in_seconds = 60*60*24\n",
      "46/26: times.groupby('group_cat').period_length.median()/day_in_seconds\n",
      "46/27: times['period_length'] = times.period_length/day_in_seconds\n",
      "46/28: times[times.period_length<850].hist(bins=30, column='period_length', by='group_cat', layout=(11,1), sharex=True, figsize=(10,15), grid=True)\n",
      "46/29: tn = pd.read_pickle('turf/times_names.pkl.gz', compression='gzip')\n",
      "46/30: tn['period_length'] = tn.period_length/day_in_seconds\n",
      "46/31: tn.groupby(['group_cat']).period_length.median().to_frame().T\n",
      "46/32: tn.groupby(['name']).period_length.agg('mean')\n",
      "46/33:\n",
      "def get_heatmap(kind, field='period_length'):\n",
      "    mean_mk = tn.groupby(['name'])[field].agg(kind)\n",
      "    mean_group_cat = tn.groupby(['group_cat'])[field].agg(kind).to_frame().T\n",
      "    mean_group_cat.index=[kind]\n",
      "    mean_group_cat[kind] = tn[field].agg(kind)\n",
      "    mean_heatmap = (tn.groupby(['name', 'group_cat'])[field].agg(kind)\n",
      "                    .unstack())\n",
      "    mean_heatmap[kind] = mean_mk\n",
      "    mean_heatmap = mean_heatmap.sort_values(kind, ascending=False)\n",
      "    mean_heatmap.index = mean_heatmap.index.add_categories(kind)\n",
      "    mean_heatmap = mean_heatmap.append(mean_group_cat)\n",
      "    mean_heatmap = mean_heatmap.rename(columns = {'Dedicated Polemicists': 'Dedicated Polemicists', 'Polemicists': 'Polemicists'})\n",
      "    mean_heatmap = mean_heatmap[['One Timers', 'Fans', 'Basers', 'Campers', 'Polemicists', 'Dedicated Fans', 'Dedicated Basers',\n",
      "           'Dedicated Campers', 'Dedicated Polemicists', 'Dedicated Celebies', kind]]\n",
      "    return mean_heatmap\n",
      "mean_heatmap = get_heatmap('mean')\n",
      "median_heatmap = get_heatmap('median')\n",
      "mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "46/34:\n",
      "def get_heatmap(kind, field='period_length'):\n",
      "    mean_mk = tn.groupby(['name'])[field].agg(kind)\n",
      "    mean_group_cat = tn.groupby(['group_cat'])[field].agg(kind).to_frame().T\n",
      "    mean_group_cat.index=[kind]\n",
      "    mean_group_cat[kind] = tn[field].agg(kind)\n",
      "    mean_heatmap = (tn.groupby(['name', 'group_cat'])[field].agg(kind)\n",
      "                    .unstack())\n",
      "    mean_heatmap[kind] = mean_mk\n",
      "    mean_heatmap = mean_heatmap.sort_values(kind, ascending=False)\n",
      "    mean_heatmap.index = mean_heatmap.index.add_categories(kind)\n",
      "    mean_heatmap = mean_heatmap.append(mean_group_cat)\n",
      "    mean_heatmap = mean_heatmap.rename(columns = {'Dedicated Polemicists': 'Dedicated Polemicists', 'Polemicists': 'Polemicists'})\n",
      "    mean_heatmap = mean_heatmap[['One Timers', 'Fans', 'Basers', 'Campers', 'Polemicists', 'Dedicated Fans', 'Dedicated Basers',\n",
      "           'Dedicated Campers', 'Dedicated Polemicists', kind]]\n",
      "    return mean_heatmap\n",
      "mean_heatmap = get_heatmap('mean')\n",
      "median_heatmap = get_heatmap('median')\n",
      "mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "46/35:\n",
      "def get_heatmap(kind, field='period_length'):\n",
      "    mean_mk = tn.groupby(['name'])[field].agg(kind)\n",
      "    mean_group_cat = tn.groupby(['group_cat'])[field].agg(kind).to_frame().T\n",
      "    mean_group_cat.index=[kind]\n",
      "    mean_group_cat[kind] = tn[field].agg(kind)\n",
      "    mean_heatmap = (tn.groupby(['name', 'new_group_cat'])[field].agg(kind)\n",
      "                    .unstack())\n",
      "    mean_heatmap[kind] = mean_mk\n",
      "    mean_heatmap = mean_heatmap.sort_values(kind, ascending=False)\n",
      "    mean_heatmap.index = mean_heatmap.index.add_categories(kind)\n",
      "    mean_heatmap = mean_heatmap.append(mean_group_cat)\n",
      "    mean_heatmap = mean_heatmap.rename(columns = {'Dedicated Polemicists': 'Dedicated Polemicists', 'Polemicists': 'Polemicists'})\n",
      "    mean_heatmap = mean_heatmap[['One Timers', 'Fans', 'Basers', 'Campers', 'Polemicists', 'Dedicated Fans', 'Dedicated Basers',\n",
      "           'Dedicated Campers', 'Dedicated Polemicists', kind]]\n",
      "    return mean_heatmap\n",
      "mean_heatmap = get_heatmap('mean')\n",
      "median_heatmap = get_heatmap('median')\n",
      "mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "46/36:\n",
      "stats = ['nunique', 'min', 'max']\n",
      "\n",
      "tn = (df.loc[df.is_post==0, ['from_id', 'name', 'elapsed_time',  'created_time', 'created_month', 'created_day', 'created_day_of_week', 'created_hour',]]\n",
      "      #.dropna(subset=['post_id'])\n",
      "      .groupby(by=['from_id', 'name'])\n",
      "      .agg(stats)\n",
      "      .assign(period_length = lambda x: (x['created_time']['max'] - x['created_time']['min']).dt.total_seconds())\n",
      "      .reset_index()\n",
      "     )\n",
      "\n",
      "columns = ['elapsed_time',  'created_time', 'created_month', 'created_day']\n",
      "stats = ['nunique', 'min', 'max']\n",
      "new_cols = ['from_id', 'name'] + ['_'.join(x) for x in product(columns, stats)] + ['period_length']\n",
      "tn.columns = new_cols\n",
      "tn.head()\n",
      "46/37:\n",
      "tn = tn.merge(st[['from_id', 'group_cat']].drop_duplicates(), how='left', on='from_id')\n",
      "tn.head()\n",
      "46/38:\n",
      "%%time\n",
      "df = pd.read_pickle('turf/data_df.pkl.gz', compression='gzip')\n",
      "46/39: df.dtypes\n",
      "46/40:\n",
      "sdf = pd.read_pickle('turf/shared_df.pkl.gz', compression='gzip')\n",
      "sdf.dtypes\n",
      "46/41:\n",
      "stats = ['nunique', 'min', 'max']\n",
      "\n",
      "tn = (df.loc[df.is_post==0, ['from_id', 'name', 'elapsed_time',  'created_time', 'created_month', 'created_day', 'created_day_of_week', 'created_hour',]]\n",
      "      #.dropna(subset=['post_id'])\n",
      "      .groupby(by=['from_id', 'name'])\n",
      "      .agg(stats)\n",
      "      .assign(period_length = lambda x: (x['created_time']['max'] - x['created_time']['min']).dt.total_seconds())\n",
      "      .reset_index()\n",
      "     )\n",
      "\n",
      "columns = ['elapsed_time',  'created_time', 'created_month', 'created_day']\n",
      "stats = ['nunique', 'min', 'max']\n",
      "new_cols = ['from_id', 'name'] + ['_'.join(x) for x in product(columns, stats)] + ['period_length']\n",
      "tn.columns = new_cols\n",
      "tn.head()\n",
      "46/42:\n",
      "tn = tn.merge(st[['from_id', 'group_cat']].drop_duplicates(), how='left', on='from_id')\n",
      "tn.head()\n",
      "46/43: tn.to_pickle('turf/times_names.pkl.gz', compression='gzip')\n",
      "46/44: tn = pd.read_pickle('turf/times_names.pkl.gz', compression='gzip')\n",
      "46/45: tn['period_length'] = tn.period_length/day_in_seconds\n",
      "46/46: tn.groupby(['group_cat']).period_length.median().to_frame().T\n",
      "46/47: tn.groupby(['name']).period_length.agg('mean')\n",
      "46/48:\n",
      "def get_heatmap(kind, field='period_length'):\n",
      "    mean_mk = tn.groupby(['name'])[field].agg(kind)\n",
      "    mean_group_cat = tn.groupby(['group_cat'])[field].agg(kind).to_frame().T\n",
      "    mean_group_cat.index=[kind]\n",
      "    mean_group_cat[kind] = tn[field].agg(kind)\n",
      "    mean_heatmap = (tn.groupby(['name', 'new_group_cat'])[field].agg(kind)\n",
      "                    .unstack())\n",
      "    mean_heatmap[kind] = mean_mk\n",
      "    mean_heatmap = mean_heatmap.sort_values(kind, ascending=False)\n",
      "    mean_heatmap.index = mean_heatmap.index.add_categories(kind)\n",
      "    mean_heatmap = mean_heatmap.append(mean_group_cat)\n",
      "    mean_heatmap = mean_heatmap.rename(columns = {'Dedicated Polemicists': 'Dedicated Polemicists', 'Polemicists': 'Polemicists'})\n",
      "    mean_heatmap = mean_heatmap[['One Timers', 'Fans', 'Basers', 'Campers', 'Polemicists', 'Dedicated Fans', 'Dedicated Basers',\n",
      "           'Dedicated Campers', 'Dedicated Polemicists', kind]]\n",
      "    return mean_heatmap\n",
      "mean_heatmap = get_heatmap('mean')\n",
      "median_heatmap = get_heatmap('median')\n",
      "mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "46/49:\n",
      "def get_heatmap(kind, field='period_length'):\n",
      "    mean_mk = tn.groupby(['name'])[field].agg(kind)\n",
      "    mean_group_cat = tn.groupby(['group_cat'])[field].agg(kind).to_frame().T\n",
      "    mean_group_cat.index=[kind]\n",
      "    mean_group_cat[kind] = tn[field].agg(kind)\n",
      "    mean_heatmap = (tn.groupby(['name', 'group_cat'])[field].agg(kind)\n",
      "                    .unstack())\n",
      "    mean_heatmap[kind] = mean_mk\n",
      "    mean_heatmap = mean_heatmap.sort_values(kind, ascending=False)\n",
      "    mean_heatmap.index = mean_heatmap.index.add_categories(kind)\n",
      "    mean_heatmap = mean_heatmap.append(mean_group_cat)\n",
      "#    mean_heatmap = mean_heatmap.rename(columns = {'Dedicated Polemicists': 'Dedicated Polemicists', 'Polemicists': 'Polemicists'})\n",
      "    mean_heatmap = mean_heatmap[['One Timers', 'Fans', 'Basers', 'Campers', 'Polemicists', 'Dedicated Fans', 'Dedicated Basers',\n",
      "           'Dedicated Campers', 'Dedicated Polemicists', kind]]\n",
      "    return mean_heatmap\n",
      "mean_heatmap = get_heatmap('mean')\n",
      "median_heatmap = get_heatmap('median')\n",
      "mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "46/50:\n",
      "def get_heatmap(kind, field='period_length'):\n",
      "    mean_mk = tn.groupby(['name'])[field].agg(kind)\n",
      "    mean_group_cat = tn.groupby(['group_cat'])[field].agg(kind).to_frame().T\n",
      "    mean_group_cat.index=[kind]\n",
      "    mean_group_cat[kind] = tn[field].agg(kind)\n",
      "    mean_heatmap = (tn.groupby(['name', 'group_cat'])[field].agg(kind)\n",
      "                    .unstack())\n",
      "    mean_heatmap[kind] = mean_mk\n",
      "    mean_heatmap = mean_heatmap.sort_values(kind, ascending=False)\n",
      "    mean_heatmap.index = mean_heatmap.index.add_categories(kind)\n",
      "    mean_heatmap = mean_heatmap.append(mean_group_cat)\n",
      "#    mean_heatmap = mean_heatmap.rename(columns = {'Dedicated Polemicists': 'Dedicated Polemicists', 'Polemicists': 'Polemicists'})\n",
      "#    mean_heatmap = mean_heatmap[['One Timers', 'Fans', 'Basers', 'Campers', 'Polemicists', 'Dedicated Fans', 'Dedicated Basers',\n",
      "#           'Dedicated Campers', 'Dedicated Polemicists', kind]]\n",
      "    return mean_heatmap\n",
      "mean_heatmap = get_heatmap('mean')\n",
      "median_heatmap = get_heatmap('median')\n",
      "mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "46/51: mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "46/52: tn.head()\n",
      "46/53: tn.head()\n",
      "46/54:\n",
      "def get_heatmap(kind, field='period_length'):\n",
      "    mean_mk = tn.groupby(['name'])[field].agg(kind)\n",
      "    mean_group_cat = tn.groupby(['group_cat'])[field].agg(kind).to_frame().T\n",
      "    mean_group_cat.index=[kind]\n",
      "    mean_group_cat[kind] = tn[field].agg(kind)\n",
      "    mean_heatmap = (tn.groupby(['name', 'group_cat'])[field].agg(kind)\n",
      "                    .unstack())\n",
      "    mean_heatmap[kind] = mean_mk\n",
      "    mean_heatmap = mean_heatmap.sort_values(kind, ascending=False)\n",
      "    mean_heatmap.index = mean_heatmap.index.add_categories(kind)\n",
      "    mean_heatmap = mean_heatmap.append(mean_group_cat)\n",
      "#    mean_heatmap = mean_heatmap.rename(columns = {'Dedicated Polemicists': 'Dedicated Polemicists', 'Polemicists': 'Polemicists'})\n",
      "#    mean_heatmap = mean_heatmap[['One Timers', 'Fans', 'Basers', 'Campers', 'Polemicists', 'Dedicated Fans', 'Dedicated Basers',\n",
      "#           'Dedicated Campers', 'Dedicated Polemicists', kind]]\n",
      "    return mean_heatmap\n",
      "mean_heatmap = get_heatmap('mean')\n",
      "median_heatmap = get_heatmap('median')\n",
      "mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "46/55:\n",
      "def get_heatmap(kind, field='period_length'):\n",
      "    mean_mk = tn.groupby(['name'])[field].agg(kind)\n",
      "    mean_group_cat = tn.groupby(['group_cat'])[field].agg(kind).to_frame().T\n",
      "    mean_group_cat.index=[kind]\n",
      "    mean_group_cat[kind] = tn[field].agg(kind)\n",
      "    mean_heatmap = (tn.groupby(['name', 'group_cat'])[field].agg(kind)\n",
      "                    .unstack())\n",
      "    mean_heatmap[kind] = mean_mk\n",
      "    mean_heatmap = mean_heatmap.sort_values(kind, ascending=False)\n",
      "    mean_heatmap.index = mean_heatmap.index.add_categories(kind)\n",
      "    mean_heatmap = mean_heatmap.append(mean_group_cat)\n",
      "#    mean_heatmap = mean_heatmap.rename(columns = {'Dedicated Polemicists': 'Dedicated Polemicists', 'Polemicists': 'Polemicists'})\n",
      "#    mean_heatmap = mean_heatmap[['One Timers', 'Fans', 'Basers', 'Campers', 'Polemicists', 'Dedicated Fans', 'Dedicated Basers',\n",
      "#           'Dedicated Campers', 'Dedicated Polemicists', kind]]\n",
      "    return mean_heatmap\n",
      "mean_heatmap = get_heatmap('mean')\n",
      "median_heatmap = get_heatmap('median')\n",
      "mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "46/56:\n",
      "mean_mk = tn.groupby(['name'])[field].agg(kind)\n",
      "mean_group_cat = tn.groupby(['group_cat'])[field].agg(kind).to_frame().T\n",
      "mean_group_cat.index=[kind]\n",
      "mean_group_cat[kind] = tn[field].agg(kind)\n",
      "mean_heatmap = (tn.groupby(['name', 'group_cat'])[field].agg(kind)\n",
      "                .unstack())\n",
      "mean_heatmap.head()\n",
      "46/57:\n",
      "mean_mk = tn.groupby(['name'])['period_length'].agg(kind)\n",
      "mean_group_cat = tn.groupby(['group_cat'])[field].agg(kind).to_frame().T\n",
      "mean_group_cat.index=[kind]\n",
      "mean_group_cat[kind] = tn[field].agg(kind)\n",
      "mean_heatmap = (tn.groupby(['name', 'group_cat'])[field].agg(kind)\n",
      "                .unstack())\n",
      "mean_heatmap.head()\n",
      "46/58:\n",
      "mean_mk = tn.groupby(['name'])['period_length'].agg('mean')\n",
      "mean_group_cat = tn.groupby(['group_cat'])[field].agg(kind).to_frame().T\n",
      "mean_group_cat.index=['mean']\n",
      "mean_group_cat[kind] = tn[field].agg('mean')\n",
      "mean_heatmap = (tn.groupby(['name', 'group_cat'])['period_length'].agg('mean')\n",
      "                .unstack())\n",
      "mean_heatmap.head()\n",
      "46/59:\n",
      "mean_mk = tn.groupby(['name'])['period_length'].agg('mean')\n",
      "mean_group_cat = tn.groupby(['group_cat'])['period_length'].agg('mean').to_frame().T\n",
      "mean_group_cat.index=['mean']\n",
      "mean_group_cat[kind] = tn[field].agg('mean')\n",
      "mean_heatmap = (tn.groupby(['name', 'group_cat'])['period_length'].agg('mean')\n",
      "                .unstack())\n",
      "mean_heatmap.head()\n",
      "46/60:\n",
      "mean_mk = tn.groupby(['name'])['period_length'].agg('mean')\n",
      "mean_group_cat = tn.groupby(['group_cat'])['period_length'].agg('mean').to_frame().T\n",
      "mean_group_cat.index=['mean']\n",
      "mean_group_cat[kind] = tn['period_length'].agg('mean')\n",
      "mean_heatmap = (tn.groupby(['name', 'group_cat'])['period_length'].agg('mean')\n",
      "                .unstack())\n",
      "mean_heatmap.head()\n",
      "46/61:\n",
      "mean_mk = tn.groupby(['name'])['period_length'].agg('mean')\n",
      "mean_group_cat = tn.groupby(['group_cat'])['period_length'].agg('mean').to_frame().T\n",
      "mean_group_cat.index=['mean']\n",
      "mean_group_cat['mean'] = tn['period_length'].agg('mean')\n",
      "mean_heatmap = (tn.groupby(['name', 'group_cat'])['period_length'].agg('mean')\n",
      "                .unstack())\n",
      "mean_heatmap.head()\n",
      "46/62:\n",
      "mean_mk = tn.groupby(['name'])['period_length'].agg('mean')\n",
      "mean_group_cat = tn.groupby(['group_cat'])['period_length'].agg('mean').to_frame().T\n",
      "mean_group_cat.index=['mean']\n",
      "\n",
      "mean_heatmap.head()\n",
      "46/63:\n",
      "mean_mk = tn.groupby(['name'])['period_length'].agg('mean')\n",
      "mean_group_cat = tn.groupby(['group_cat'])['period_length'].agg('mean').to_frame().T\n",
      "mean_group_cat.index=['mean']\n",
      "\n",
      "mean_group_cat.head()\n",
      "46/64:\n",
      "mean_mk = tn.groupby(['name'])['period_length'].agg('mean')\n",
      "mean_group_cat = tn.groupby(['group_cat'])['period_length'].agg('mean').to_frame().T\n",
      "mean_group_cat.index=['mean']\n",
      "mean_group_cat['mean'] = tn['period_length'].agg('mean')\n",
      "\n",
      "mean_group_cat.head()\n",
      "46/65:\n",
      "def get_heatmap(kind, field='period_length'):\n",
      "    mean_mk = tn.groupby(['name'])[field].agg(kind)\n",
      "    mean_group_cat = tn.groupby(['group_cat'])[field].agg(kind).to_frame().T\n",
      "    mean_group_cat.index=[kind]\n",
      "    mean_group_cat[kind] = tn[field].agg(kind).astype(str)\n",
      "    mean_heatmap = (tn.groupby(['name', 'group_cat'])[field].agg(kind)\n",
      "                    .unstack())\n",
      "    mean_heatmap[kind] = mean_mk\n",
      "    mean_heatmap = mean_heatmap.sort_values(kind, ascending=False)\n",
      "    mean_heatmap.index = mean_heatmap.index.add_categories(kind)\n",
      "    mean_heatmap = mean_heatmap.append(mean_group_cat)\n",
      "#    mean_heatmap = mean_heatmap.rename(columns = {'Dedicated Polemicists': 'Dedicated Polemicists', 'Polemicists': 'Polemicists'})\n",
      "#    mean_heatmap = mean_heatmap[['One Timers', 'Fans', 'Basers', 'Campers', 'Polemicists', 'Dedicated Fans', 'Dedicated Basers',\n",
      "#           'Dedicated Campers', 'Dedicated Polemicists', kind]]\n",
      "    return mean_heatmap\n",
      "mean_heatmap = get_heatmap('mean')\n",
      "median_heatmap = get_heatmap('median')\n",
      "mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "46/66:\n",
      "def get_heatmap(kind, field='period_length'):\n",
      "    mean_mk = tn.groupby(['name'])[field].agg(kind)\n",
      "    mean_group_cat = tn.groupby(['group_cat'])[field].agg(kind).to_frame().T\n",
      "    mean_group_cat.index = mean_group_cat.index.astype(str)\n",
      "    mean_group_cat.index=[kind]\n",
      "    mean_group_cat[kind] = tn[field].agg(kind)\n",
      "    mean_heatmap = (tn.groupby(['name', 'group_cat'])[field].agg(kind)\n",
      "                    .unstack())\n",
      "    mean_heatmap[kind] = mean_mk\n",
      "    mean_heatmap = mean_heatmap.sort_values(kind, ascending=False)\n",
      "    mean_heatmap.index = mean_heatmap.index.add_categories(kind)\n",
      "    mean_heatmap = mean_heatmap.append(mean_group_cat)\n",
      "#    mean_heatmap = mean_heatmap.rename(columns = {'Dedicated Polemicists': 'Dedicated Polemicists', 'Polemicists': 'Polemicists'})\n",
      "#    mean_heatmap = mean_heatmap[['One Timers', 'Fans', 'Basers', 'Campers', 'Polemicists', 'Dedicated Fans', 'Dedicated Basers',\n",
      "#           'Dedicated Campers', 'Dedicated Polemicists', kind]]\n",
      "    return mean_heatmap\n",
      "mean_heatmap = get_heatmap('mean')\n",
      "median_heatmap = get_heatmap('median')\n",
      "mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "46/67:\n",
      "mean_mk = tn.groupby(['name'])['period_length'].agg('mean')\n",
      "mean_group_cat = tn.groupby(['group_cat'])['period_length'].agg('mean').to_frame().T\n",
      "mean_group_cat.index=['mean']\n",
      "mean_group_cat['mean'] = tn['period_length'].agg('mean')\n",
      "mean_group_cat.head()\n",
      "46/68:\n",
      "mean_mk = tn.groupby(['name'])['period_length'].agg('mean')\n",
      "mean_group_cat = tn.groupby(['group_cat'])['period_length'].agg('mean').to_frame().T\n",
      "mean_group_cat.index=['mean']\n",
      "\n",
      "mean_group_cat.head()\n",
      "46/69:\n",
      "mean_mk = tn.groupby(['name'])['period_length'].agg('mean')\n",
      "mean_group_cat = tn.groupby(['group_cat'])['period_length'].agg('mean').to_frame().T\n",
      "mean_group_cat.index=['mean']\n",
      "\n",
      "mean_group_cat.columns\n",
      "46/70:\n",
      "def get_heatmap(kind, field='period_length'):\n",
      "    mean_mk = tn.groupby(['name'])[field].agg(kind)\n",
      "    mean_group_cat = tn.groupby(['group_cat'])[field].agg(kind).to_frame().T\n",
      "    mean_group_cat.columns = mean_group_cat.columns.astype(str)\n",
      "    mean_group_cat.index=[kind]\n",
      "    mean_group_cat[kind] = tn[field].agg(kind)\n",
      "    mean_heatmap = (tn.groupby(['name', 'group_cat'])[field].agg(kind)\n",
      "                    .unstack())\n",
      "    mean_heatmap[kind] = mean_mk\n",
      "    mean_heatmap = mean_heatmap.sort_values(kind, ascending=False)\n",
      "    mean_heatmap.index = mean_heatmap.index.add_categories(kind)\n",
      "    mean_heatmap = mean_heatmap.append(mean_group_cat)\n",
      "#    mean_heatmap = mean_heatmap.rename(columns = {'Dedicated Polemicists': 'Dedicated Polemicists', 'Polemicists': 'Polemicists'})\n",
      "#    mean_heatmap = mean_heatmap[['One Timers', 'Fans', 'Basers', 'Campers', 'Polemicists', 'Dedicated Fans', 'Dedicated Basers',\n",
      "#           'Dedicated Campers', 'Dedicated Polemicists', kind]]\n",
      "    return mean_heatmap\n",
      "mean_heatmap = get_heatmap('mean')\n",
      "median_heatmap = get_heatmap('median')\n",
      "mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "46/71:\n",
      "def get_heatmap(kind, field='period_length'):\n",
      "    mean_mk = tn.groupby(['name'])[field].agg(kind)\n",
      "    mean_group_cat = tn.groupby(['group_cat'])[field].agg(kind).to_frame().T\n",
      "    mean_group_cat.columns = mean_group_cat.columns.astype(str)\n",
      "    mean_group_cat.index=[kind]\n",
      "    mean_group_cat[kind] = tn[field].agg(kind)\n",
      "    mean_heatmap = (tn.groupby(['name', 'group_cat'])[field].agg(kind)\n",
      "                    .unstack())\n",
      "    mean_heatmap[kind] = mean_mk\n",
      "    mean_heatmap = mean_heatmap.sort_values(kind, ascending=False)\n",
      "    mean_heatmap.index = mean_heatmap.index.add_categories(kind)\n",
      "    mean_heatmap = mean_heatmap.append(mean_group_cat)\n",
      "#    mean_heatmap = mean_heatmap.rename(columns = {'Dedicated Polemicists': 'Dedicated Polemicists', 'Polemicists': 'Polemicists'})\n",
      "#    mean_heatmap = mean_heatmap[['One Timers', 'Fans', 'Basers', 'Campers', 'Polemicists', 'Dedicated Fans', 'Dedicated Basers',\n",
      "#           'Dedicated Campers', 'Dedicated Polemicists', kind]]\n",
      "    return mean_heatmap\n",
      "mean_heatmap = get_heatmap('mean')\n",
      "median_heatmap = get_heatmap('median')\n",
      "mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "46/72:\n",
      "mean_mk = tn.groupby(['name'])['period_length'].agg('mean')\n",
      "mean_group_cat = tn.groupby(['group_cat'])['period_length'].agg('mean').to_frame().T\n",
      "mean_group_cat.index=['mean']\n",
      "\n",
      "mean_group_cat.columns\n",
      "46/73:\n",
      "mean_mk = tn.groupby(['name'])['period_length'].agg('mean')\n",
      "mean_group_cat = tn.groupby(['group_cat'])['period_length'].agg('mean').to_frame().T\n",
      "mean_group_cat.index=['mean']\n",
      "\n",
      "mean_group_cat.columns.astype(str)\n",
      "46/74:\n",
      "def get_heatmap(kind, field='period_length'):\n",
      "    mean_mk = tn.groupby(['name'])[field].agg(kind)\n",
      "    mean_group_cat = tn.groupby(['group_cat'])[field].agg(kind).to_frame().T\n",
      "    mean_group_cat.columns = mean_group_cat.columns.astype(str)\n",
      "    #mean_group_cat.index=[kind]\n",
      "    mean_group_cat[kind] = tn[field].agg(kind)\n",
      "    mean_heatmap = (tn.groupby(['name', 'group_cat'])[field].agg(kind)\n",
      "                    .unstack())\n",
      "    mean_heatmap[kind] = mean_mk\n",
      "    mean_heatmap = mean_heatmap.sort_values(kind, ascending=False)\n",
      "    mean_heatmap.index = mean_heatmap.index.add_categories(kind)\n",
      "    mean_heatmap = mean_heatmap.append(mean_group_cat)\n",
      "#    mean_heatmap = mean_heatmap.rename(columns = {'Dedicated Polemicists': 'Dedicated Polemicists', 'Polemicists': 'Polemicists'})\n",
      "#    mean_heatmap = mean_heatmap[['One Timers', 'Fans', 'Basers', 'Campers', 'Polemicists', 'Dedicated Fans', 'Dedicated Basers',\n",
      "#           'Dedicated Campers', 'Dedicated Polemicists', kind]]\n",
      "    return mean_heatmap\n",
      "mean_heatmap = get_heatmap('mean')\n",
      "median_heatmap = get_heatmap('median')\n",
      "mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "46/75:\n",
      "mean_mk = tn.groupby(['name'])['period_length'].agg('mean')\n",
      "mean_group_cat = tn.groupby(['group_cat'])['period_length'].agg('mean').to_frame().T\n",
      "mean_group_cat.index=['mean']\n",
      "mean_group_cat['mean'] = tn['period_length'].agg('mean')\n",
      "mean_heatmap = (tn.groupby(['name', 'group_cat'])['period_length'].agg('mean')\n",
      "                .unstack())\n",
      "mean_heatmap.head()\n",
      "46/76:\n",
      "mean_mk = tn.groupby(['name'])['period_length'].agg('mean')\n",
      "mean_group_cat = tn.groupby(['group_cat'])['period_length'].agg('mean').to_frame().T\n",
      "mean_group_cat.columns = mean_group_cat.columns.astype(str)\n",
      "mean_group_cat.index=['mean']\n",
      "mean_group_cat['mean'] = tn['period_length'].agg('mean')\n",
      "mean_heatmap = (tn.groupby(['name', 'group_cat'])['period_length'].agg('mean')\n",
      "                .unstack())\n",
      "mean_heatmap.head()\n",
      "46/77:\n",
      "mean_mk = tn.groupby(['name'])['period_length'].agg('mean')\n",
      "mean_group_cat = tn.groupby(['group_cat'])['period_length'].agg('mean').to_frame().T\n",
      "mean_group_cat.columns = mean_group_cat.columns.astype(str)\n",
      "mean_group_cat.index=['mean']\n",
      "mean_group_cat['mean'] = tn['period_length'].agg('mean')\n",
      "mean_heatmap = (tn.groupby(['name', 'group_cat'])['period_length'].agg('mean')\n",
      "                .unstack())\n",
      "mean_heatmap.head()\n",
      "mean_heatmap[kind] = mean_mk\n",
      "46/78:\n",
      "mean_mk = tn.groupby(['name'])['period_length'].agg('mean')\n",
      "mean_group_cat = tn.groupby(['group_cat'])['period_length'].agg('mean').to_frame().T\n",
      "mean_group_cat.columns = mean_group_cat.columns.astype(str)\n",
      "mean_group_cat.index=['mean']\n",
      "mean_group_cat['mean'] = tn['period_length'].agg('mean')\n",
      "mean_heatmap = (tn.groupby(['name', 'group_cat'])['period_length'].agg('mean')\n",
      "                .unstack())\n",
      "mean_heatmap.head()\n",
      "mean_heatmap['mean'] = mean_mk\n",
      "46/79:\n",
      "mean_mk = tn.groupby(['name'])['period_length'].agg('mean')\n",
      "mean_group_cat = tn.groupby(['group_cat'])['period_length'].agg('mean').to_frame().T\n",
      "mean_group_cat.columns = mean_group_cat.columns.astype(str)\n",
      "mean_group_cat.index=['mean']\n",
      "mean_group_cat['mean'] = tn['period_length'].agg('mean')\n",
      "mean_heatmap = (tn.groupby(['name', 'group_cat'])['period_length'].agg('mean')\n",
      "                .unstack())\n",
      "mean_group_cat.columns = mean_group_cat.columns.astype(str)\n",
      "mean_heatmap.head()\n",
      "mean_heatmap['mean'] = mean_mk\n",
      "46/80:\n",
      "mean_mk = tn.groupby(['name'])['period_length'].agg('mean')\n",
      "mean_group_cat = tn.groupby(['group_cat'])['period_length'].agg('mean').to_frame().T\n",
      "mean_group_cat.columns = mean_group_cat.columns.astype(str)\n",
      "mean_group_cat.index=['mean']\n",
      "mean_group_cat['mean'] = tn['period_length'].agg('mean')\n",
      "mean_heatmap = (tn.groupby(['name', 'group_cat'])['period_length'].agg('mean')\n",
      "                .unstack())\n",
      "mean_heatmap.columns = mean_heatmap.columns.astype(str)\n",
      "mean_heatmap.head()\n",
      "mean_heatmap['mean'] = mean_mk\n",
      "46/81:\n",
      "def get_heatmap(kind, field='period_length'):\n",
      "    mean_mk = tn.groupby(['name'])[field].agg(kind)\n",
      "    mean_group_cat = tn.groupby(['group_cat'])[field].agg(kind).to_frame().T\n",
      "    mean_group_cat.columns = mean_group_cat.columns.astype(str)\n",
      "    mean_group_cat.index=[kind]\n",
      "    mean_group_cat[kind] = tn[field].agg(kind)\n",
      "    mean_heatmap = (tn.groupby(['name', 'group_cat'])[field].agg(kind)\n",
      "                    .unstack())\n",
      "    mean_heatmap.columns = mean_heatmap.columns.astype(str)\n",
      "    mean_heatmap[kind] = mean_mk\n",
      "    mean_heatmap = mean_heatmap.sort_values(kind, ascending=False)\n",
      "    mean_heatmap.index = mean_heatmap.index.add_categories(kind)\n",
      "    mean_heatmap = mean_heatmap.append(mean_group_cat)\n",
      "#    mean_heatmap = mean_heatmap.rename(columns = {'Dedicated Polemicists': 'Dedicated Polemicists', 'Polemicists': 'Polemicists'})\n",
      "#    mean_heatmap = mean_heatmap[['One Timers', 'Fans', 'Basers', 'Campers', 'Polemicists', 'Dedicated Fans', 'Dedicated Basers',\n",
      "#           'Dedicated Campers', 'Dedicated Polemicists', kind]]\n",
      "    return mean_heatmap\n",
      "mean_heatmap = get_heatmap('mean')\n",
      "median_heatmap = get_heatmap('median')\n",
      "mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "46/82: mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "46/83:\n",
      ">>> g = sns.catplot(x=\"group_cat\", y=\"period_length\",\n",
      "...                 #hue=\"group_cat\", \n",
      "                    order = order,\n",
      "                    row=\"name\", #row_order= list(mean_mk.sort_values(ascending=False).index),\n",
      "...                 data=tn[tn.period_length<850], kind=\"box\",\n",
      "...                 height=4, aspect=5);\n",
      "46/84: order = ['One Timers', 'Fans', 'Basers', 'Campers', 'Polemicists', 'Dedicated']\n",
      "46/85: median_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "46/86:\n",
      "html_heatmap = median_heatmap.style.background_gradient(cmap='viridis').highlight_null('red').render()\n",
      "open('outputs/median_period_length.html', 'w').write(html_heatmap)\n",
      "46/87:\n",
      ">>> g = sns.catplot(x=\"group_cat\", y=\"period_length\",\n",
      "...                 #hue=\"group_cat\", \n",
      "                    order = order,\n",
      "                    row=\"name\", #row_order= list(mean_mk.sort_values(ascending=False).index),\n",
      "...                 data=tn[tn.period_length<850], kind=\"box\",\n",
      "...                 height=4, aspect=5);\n",
      "46/88: times[times.period_length<850].hist(bins=30, column='created_day_nunique', by='group_cat', layout=(11,1), sharex=True, figsize=(10,15), grid=True)\n",
      "46/89: times.hist(bins=30, column='created_day_nunique', by='group_cat', layout=(11,1), sharex=True, figsize=(10,15), grid=True)\n",
      "46/90: times.hist(bins=30, column='created_day_nunique', by='group_cat', layout=(11,1), figsize=(10,15), grid=True)\n",
      "46/91: times.hist(bins=30, column='created_day_nunique', by='group_cat', layout=(11,1), figsize=(10,15))\n",
      "46/92: times.hist(bins=30, column='created_day_nunique', by='group_cat', layout=(6,1), figsize=(10,15))\n",
      "46/93: times[times.period_length<850].hist(bins=30, column='period_length', by='group_cat', layout=(6,1), sharex=True, figsize=(10,15), grid=True)\n",
      "46/94: sns.boxplot(times, y='created_day_nunique', x='group_cat')\n",
      "46/95: sns.boxplot(data=times, y='created_day_nunique', x='group_cat')\n",
      "46/96: sns.boxplot(data=times, y='created_day_nunique', x='group_cat', height=7, aspect=2)\n",
      "46/97:\n",
      "fig, ax = plt.subplots(figsize=(20, 15))\n",
      "\n",
      "sns.boxplot(data=times, y='created_day_nunique', x='group_cat', ax=ax)\n",
      "46/98: sns.boxplot(data=times[time.group_cat!='Dedicated'], y='created_day_nunique', x='group_cat', ax=ax)\n",
      "46/99: sns.boxplot(data=times[times.group_cat!='Dedicated'], y='created_day_nunique', x='group_cat', ax=ax)\n",
      "46/100:\n",
      "fig, ax = plt.subplots(figsize=(20, 15))\n",
      "sns.boxplot(data=times[times.group_cat!='Dedicated'], y='created_day_nunique', x='group_cat', ax=ax)\n",
      "46/101:\n",
      "fig, ax = plt.subplots(figsize=(20, 15))\n",
      "\n",
      "sns.boxplot(data=times, y='created_day_nunique', x='group_cat', ax=ax)\n",
      "46/102:\n",
      "fig, ax = plt.subplots(figsize=(20, 15))\n",
      "sns.violinplot(data=times[times.group_cat!='Dedicated'], y='created_day_nunique', x='group_cat', ax=ax)\n",
      "46/103:\n",
      "fig, ax = plt.subplots(figsize=(20, 15))\n",
      "sns.swarmplot(data=times[times.group_cat!='Dedicated'], y='created_day_nunique', x='group_cat', ax=ax)\n",
      "46/104:\n",
      "fig, ax = plt.subplots(figsize=(20, 15))\n",
      "sns.violinplot(data=times[times.group_cat!='Dedicated'], y='created_day_nunique', x='group_cat', ax=ax)\n",
      "46/105:\n",
      "fig, ax = plt.subplots(figsize=(20, 15))\n",
      "sns.boxplot(data=times[times.group_cat!='Dedicated'], y='created_day_nunique', x='group_cat', ax=ax)\n",
      "46/106: times.hist(bins=30, column='created_day_nunique', by='group_cat', layout=(6,1), figsize=(10,15), title='Mean days of activity by group')\n",
      "46/107: times.hist(bins=30, column='created_day_nunique', by='group_cat', layout=(6,1), figsize=(10,15))\n",
      "46/108: times.hist(bins=30, column='created_month_nunique', by='group_cat', layout=(6,1), figsize=(10,15))\n",
      "46/109: times.hist(bins=30, column='created_month_nunique', by='group_cat', sharex=True, layout=(6,1), figsize=(10,15))\n",
      "46/110:\n",
      "fig, ax = plt.subplots(figsize=(20, 15))\n",
      "\n",
      "sns.boxplot(data=times, y='created_month_nunique', x='group_cat', ax=ax)\n",
      "46/111:\n",
      "fig, ax = plt.subplots(figsize=(20, 15))\n",
      "sns.boxplot(data=times[times.group_cat!='Dedicated'], y='created_month_nunique', x='group_cat', ax=ax)\n",
      "46/112: times.groupby('group_cat').period_length.median()/day_in_seconds\n",
      "46/113: day_in_seconds = 60*60*24\n",
      "46/114: times.groupby('group_cat').period_length.median()/day_in_seconds\n",
      "46/115: times.groupby('group_cat').period_length.median()\n",
      "46/116: times.groupby('group_cat').created_month_nunique.median()\n",
      "46/117: times.groupby('group_cat').created_day_nunique.median()\n",
      "44/402:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "%%output size=400\n",
      "\n",
      "from holoviews.streams import RangeXY\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k'), cmap=Sets1to3)) * \\\n",
      "    hv.util.Dynamic(aggregate(all_points, width=100, height=100, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.group_cat.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/403:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "%%output size=600\n",
      "\n",
      "from holoviews.streams import RangeXY\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k'), cmap=Sets1to3)) * \\\n",
      "    hv.util.Dynamic(aggregate(all_points, width=100, height=100, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.group_cat.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/404:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "%%output size=400\n",
      "\n",
      "from holoviews.streams import RangeXY\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k'), cmap=Sets1to3)) * \\\n",
      "    hv.util.Dynamic(aggregate(all_points, width=100, height=100, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.group_cat.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/405: mean_comments = st.groupby(['new_group_cat', 'from_id']).id.sum().reset_index()groupby('new_group_cat').mean()\n",
      "44/406: mean_comments = st.groupby(['new_group_cat', 'from_id']).id.sum().reset_index().groupby('new_group_cat').mean()\n",
      "44/407:\n",
      "mean_comments = st.groupby(['new_group_cat', 'from_id']).id.sum().reset_index().groupby('new_group_cat').mean()\n",
      "mean_comments\n",
      "44/408:\n",
      "mean_comments = st.groupby(['new_group_cat', 'from_id']).id.sum().reset_index().drop('from_id', axis=1).groupby('new_group_cat').mean()\n",
      "mean_comments\n",
      "44/409:\n",
      "mean_comments = st.groupby(['new_group_cat', 'from_id']).id.sum().reset_index().drop('from_id', axis=1).groupby('new_group_cat').mean()\n",
      "mean_comments = mean_comments.reindex(['One Timers', \n",
      "                         'Fans', 'Dedicated Fans', \n",
      "                         'Basers', 'Dedicated Basers', \n",
      "                         'Campers', 'Dedicated Campers',\n",
      "                         'Polemicists', 'Dedicated Polemicists',])\n",
      "44/410:\n",
      "mean_comments = st.groupby(['new_group_cat', 'from_id']).id.sum().reset_index().drop('from_id', axis=1).groupby('new_group_cat').mean()\n",
      "mean_comments = mean_comments.reindex(['One Timers', \n",
      "                         'Fans', 'Dedicated Fans', \n",
      "                         'Basers', 'Dedicated Basers', \n",
      "                         'Campers', 'Dedicated Campers',\n",
      "                         'Polemicists', 'Dedicated Polemicists',])\n",
      "mean_comments\n",
      "44/411:\n",
      "mean_comments = st.groupby(['new_group_cat', 'from_id']).id.sum().reset_index().drop('from_id', axis=1).groupby('new_group_cat').mean()\n",
      "mean_comments = mean_comments.reindex(['One Timers', \n",
      "                         'Fans', 'Dedicated Fans', \n",
      "                         'Basers', 'Dedicated Basers', \n",
      "                         'Campers', 'Dedicated Campers',\n",
      "                         'Polemicists', 'Dedicated Polemicists',])\n",
      "mean_comments['id']\n",
      "44/412:\n",
      "mean_comments = st.groupby(['new_group_cat', 'from_id']).id.sum().reset_index().drop('from_id', axis=1).groupby('new_group_cat').mean()\n",
      "mean_comments = mean_comments.reindex(['One Timers', \n",
      "                         'Fans', 'Dedicated Fans', \n",
      "                         'Basers', 'Dedicated Basers', \n",
      "                         'Campers', 'Dedicated Campers',\n",
      "                         'Polemicists', 'Dedicated Polemicists',])\n",
      "median_comments['id']\n",
      "44/413:\n",
      "med_comments = (st.groupby(['new_group_cat', 'from_id'])\n",
      "                 .id.sum().reset_index()\n",
      "                 .drop('from_id', axis=1).groupby('new_group_cat').median()\n",
      "                )\n",
      "med_comments = med_comments.reindex(['One Timers', \n",
      "                         'Fans', 'Dedicated Fans', \n",
      "                         'Basers', 'Dedicated Basers', \n",
      "                         'Campers', 'Dedicated Campers',\n",
      "                         'Polemicists', 'Dedicated Polemicists',])\n",
      "med_comments['id']\n",
      "46/118:\n",
      "gord = ['One Timers', \n",
      "        'Fans', \n",
      "        'Basers', \n",
      "        'Campers', \n",
      "        'Polemicists',]\n",
      "times.hist(bins=30, column='created_month_nunique', by='group_cat', oder=gord, sharex=True, layout=(6,1), figsize=(10,15))\n",
      "46/119:\n",
      "gord = ['One Timers', \n",
      "        'Fans', \n",
      "        'Basers', \n",
      "        'Campers', \n",
      "        'Polemicists',]\n",
      "times.hist(bins=30, column='created_month_nunique', by='group_cat', order=gord, sharex=True, layout=(6,1), figsize=(10,15))\n",
      "46/120:\n",
      "gord = ['One Timers', \n",
      "        'Fans', \n",
      "        'Basers', \n",
      "        'Campers', \n",
      "        'Polemicists',]\n",
      "times.hist(bins=30, column='created_month_nunique', by='group_cat', sharex=True, layout=(6,1), figsize=(10,15))\n",
      "46/121:\n",
      ">>> ax = sns.scatterplot(x=\"period_length\", y=\"created_month_nunique\", hue=\"group_cat\",\n",
      "...                      data=tips)\n",
      "46/122:\n",
      ">>> ax = sns.scatterplot(x=\"period_length\", y=\"created_month_nunique\", hue=\"group_cat\",\n",
      "...                      data=times)\n",
      "46/123:\n",
      "fig, ax = plt.subplots(figsize=(20, 15))\n",
      "sns.scatterplot(x=\"period_length\", y=\"created_month_nunique\", hue=\"group_cat\",\n",
      "                      data=times, ax=ax)\n",
      "46/124:\n",
      "all_points = hv.Points(times[['period_length', 'created_month_nunique']], label=\"\")\n",
      "\n",
      "%output size=300\n",
      "%opts RGB [bgcolor=\"black\" show_grid=False xaxis=None yaxis=None]\n",
      "from colorcet import fire\n",
      "datashade.cmap=fire[50:]\n",
      "\n",
      "dynspread(datashade(all_points))\n",
      "46/125:\n",
      "import numpy as np\n",
      "import holoviews as hv\n",
      "import datashader as ds\n",
      "from holoviews.operation.datashader import aggregate, datashade, shade, dynspread\n",
      "from holoviews.operation import decimate\n",
      "hv.extension('bokeh','matplotlib')\n",
      "decimate.max_samples=1000\n",
      "dynspread.max_px=20\n",
      "dynspread.threshold=0.5\n",
      "46/126:\n",
      "all_points = hv.Points(times[['period_length', 'created_month_nunique']], label=\"\")\n",
      "\n",
      "%output size=300\n",
      "%opts RGB [bgcolor=\"black\" show_grid=False xaxis=None yaxis=None]\n",
      "from colorcet import fire\n",
      "datashade.cmap=fire[50:]\n",
      "\n",
      "dynspread(datashade(all_points))\n",
      "46/127:\n",
      "all_points = hv.Points(times[['period_length', 'created_month_nunique']], label=\"\")\n",
      "\n",
      "%output size=300\n",
      "%opts RGB [bgcolor=\"black\"]\n",
      "from colorcet import fire\n",
      "datashade.cmap=fire[50:]\n",
      "\n",
      "dynspread(datashade(all_points))\n",
      "46/128:\n",
      "all_points = hv.Points(times[['period_length', 'created_month_nunique']], label=\"\")\n",
      "\n",
      "%output size=300\n",
      "%opts RGB [bgcolor=\"black\" show_grid=True\n",
      "from colorcet import fire\n",
      "datashade.cmap=fire[50:]\n",
      "\n",
      "dynspread(datashade(all_points))\n",
      "46/129:\n",
      "all_points = hv.Points(times[['period_length', 'created_month_nunique']], label=\"\")\n",
      "\n",
      "%output size=300\n",
      "%opts RGB [bgcolor=\"black\" show_grid=True]\n",
      "from colorcet import fire\n",
      "datashade.cmap=fire[50:]\n",
      "\n",
      "dynspread(datashade(all_points))\n",
      "46/130:\n",
      "all_points = hv.Points(times[['period_length', 'created_month_nunique']], label=\"\")\n",
      "\n",
      "%output size=300\n",
      "%opts RGB [bgcolor=\"black\" show_grid=True]\n",
      "from colorcet import fire\n",
      "datashade.cmap=fire\n",
      "\n",
      "dynspread(datashade(all_points))\n",
      "46/131:\n",
      "all_points = hv.Points(times[['period_length', 'created_month_nunique']], label=\"\")\n",
      "\n",
      "%output size=300\n",
      "%opts RGB [bgcolor=\"black\" show_grid=True]\n",
      "from colorcet import fire\n",
      "datashade.cmap=fire[50:]\n",
      "\n",
      "dynspread(datashade(all_points))\n",
      "46/132:\n",
      "import numpy as np\n",
      "import holoviews as hv\n",
      "import datashader as ds\n",
      "from holoviews.operation.datashader import aggregate, datashade, shade, dynspread\n",
      "from holoviews.operation import decimate\n",
      "hv.extension('bokeh','matplotlib')\n",
      "decimate.max_samples=1000\n",
      "dynspread.max_px=20\n",
      "dynspread.threshold=0.5\n",
      "46/133:\n",
      "all_points = hv.Points(times[['period_length', 'created_month_nunique']], label=\"\")\n",
      "\n",
      "%output size=300\n",
      "%opts RGB [bgcolor=\"black\" show_grid=True]\n",
      "from colorcet import fire\n",
      "datashade.cmap=fire[50:]\n",
      "\n",
      "dynspread(datashade(all_points))\n",
      "46/134:\n",
      "all_points = hv.Points(times[['created_day_nunique', 'created_month_nunique']], label=\"\")\n",
      "\n",
      "%output size=300\n",
      "%opts RGB [bgcolor=\"black\" show_grid=True]\n",
      "from colorcet import fire\n",
      "datashade.cmap=fire[50:]\n",
      "\n",
      "dynspread(datashade(all_points))\n",
      "46/135:\n",
      "fig, ax = plt.subplots(figsize=(20, 15))\n",
      "sns.boxplot(x='created_month_nunique', y='created_day_nunique', ax=ax)\n",
      "46/136:\n",
      "fig, ax = plt.subplots(figsize=(20, 15))\n",
      "sns.boxplot(x='created_month_nunique', y='created_day_nunique', data=times,ax=ax)\n",
      "46/137: times.groupby('created_month_nunique').created_day.nunique.median()\n",
      "46/138: times.groupby('created_month_nunique').created_day_nunique.median()\n",
      "46/139: times.groupby('created_month_nunique').created_day_nunique.median().delta()\n",
      "46/140: times.groupby('created_month_nunique').created_day_nunique.median().diff()\n",
      "46/141: times.groupby('created_month_nunique').created_day_nunique.median().diff().plot()\n",
      "46/142: times.groupby('created_month_nunique').created_day_nunique.median().diff()\n",
      "46/143: times.groupby('created_month_nunique').created_day_nunique.median()\n",
      "46/144: times.groupby('created_month_nunique').created_day_nunique.median().diff()\n",
      "46/145:\n",
      "fig, ax = plt.subplots(figsize=(20, 15))\n",
      "sns.boxplot(x='created_month_nunique', y='created_day_nunique', row='group_cat', data=times,ax=ax)\n",
      "46/146:\n",
      "fig, ax = plt.subplots(figsize=(20, 15))\n",
      "sns.boxplot(x='created_month_nunique', y='created_day_nunique', data=times,ax=ax)\n",
      "46/147:\n",
      ">>> g = sns.catplot(x=\"created_month_nunique\", y=\"created_day_nunique\",\n",
      "...                 #hue=\"group_cat\", \n",
      "                    #order = order,\n",
      "                    row=\"group_cat\", #row_order= list(mean_mk.sort_values(ascending=False).index),\n",
      "...                 data=times,\n",
      "                    kind=\"box\",\n",
      "...                 height=4, aspect=5);\n",
      "46/148:\n",
      ">>> g = sns.catplot(x=\"created_month_nunique\", y=\"created_day_nunique\",\n",
      "...                 #hue=\"group_cat\", \n",
      "                    #order = order,\n",
      "                    row=\"group_cat\", #row_order= list(mean_mk.sort_values(ascending=False).index),\n",
      "...                 data=times,\n",
      "                    kind=\"box\",\n",
      "                    sharey=False,\n",
      "...                 height=4, aspect=5);\n",
      "46/149: times.head()\n",
      "46/150: tn.head()\n",
      "44/414:\n",
      "#bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "\n",
      "order = cont_stacked.groupby('Category').comment_count.sum()\n",
      "\n",
      "row = alt.hconcat()\n",
      "for page in cont_stacked.page.unique():\n",
      "    regular = alt.Chart(cont_stacked[cont_stacked.page==page]).mark_bar().encode(\n",
      "        alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "        alt.Y('Category:N', axis=None, title=None),\n",
      "        alt.Color('group_cat:N'),\n",
      "        #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "        #column='page:N'\n",
      "    ).properties(height=1500, width=150, title=page + ' Comment Count')        \n",
      "\n",
      "    normalized = regular.encode(    \n",
      "        alt.X('comment_count:Q',\n",
      "            axis=alt.Axis(title=None),\n",
      "            stack='normalize'\n",
      "            ),\n",
      "        alt.Y('Category:N',  title=None),\n",
      "        #column='page:N'\n",
      "    ).properties(title=page + ' Comment Ratio')\n",
      "    if(page!='Herzog'):\n",
      "        normalized = normalized.encode(alt.Y('Category:N', axis=None, title=None),)\n",
      "    row |= (normalized | regular)\n",
      "    \n",
      "row\n",
      "44/415:\n",
      "#bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "\n",
      "order = cont_stacked.groupby('Category').comment_count.sum()\n",
      "\n",
      "row = alt.hconcat()\n",
      "for page in cont_stacked.page.unique():\n",
      "    regular = alt.Chart(cont_stacked[cont_stacked.page==page]).mark_bar().encode(\n",
      "        alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "        alt.Y('Category:N', axis=None, sort=order, title=None),\n",
      "        alt.Color('group_cat:N'),\n",
      "        #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "        #column='page:N'\n",
      "    ).properties(height=1500, width=150, title=page + ' Comment Count')        \n",
      "\n",
      "    normalized = regular.encode(    \n",
      "        alt.X('comment_count:Q',\n",
      "            axis=alt.Axis(title=None),\n",
      "            stack='normalize'\n",
      "            ),\n",
      "        alt.Y('Category:N',  title=None),\n",
      "        #column='page:N'\n",
      "    ).properties(title=page + ' Comment Ratio')\n",
      "    if(page!='Herzog'):\n",
      "        normalized = normalized.encode(alt.Y('Category:N', axis=None, title=None),)\n",
      "    row |= (normalized | regular)\n",
      "    \n",
      "row\n",
      "44/416:\n",
      "order = cont_stacked.groupby('Category').comment_count.sum()\n",
      "order.head()\n",
      "44/417:\n",
      "order = list(cont_stacked.groupby('Category').comment_count.sum().index)\n",
      "order\n",
      "44/418:\n",
      "#bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "\n",
      "row = alt.hconcat()\n",
      "for page in cont_stacked.page.unique():\n",
      "    regular = alt.Chart(cont_stacked[cont_stacked.page==page]).mark_bar().encode(\n",
      "        alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "        alt.Y('Category:N', axis=None, sort=order, title=None),\n",
      "        alt.Color('group_cat:N'),\n",
      "        #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "        #column='page:N'\n",
      "    ).properties(height=1500, width=150, title=page + ' Comment Count')        \n",
      "\n",
      "    normalized = regular.encode(    \n",
      "        alt.X('comment_count:Q',\n",
      "            axis=alt.Axis(title=None),\n",
      "            stack='normalize'\n",
      "            ),\n",
      "        alt.Y('Category:N',  title=None),\n",
      "        #column='page:N'\n",
      "    ).properties(title=page + ' Comment Ratio')\n",
      "    if(page!='Herzog'):\n",
      "        normalized = normalized.encode(alt.Y('Category:N', axis=None, title=None),)\n",
      "    row |= (normalized | regular)\n",
      "    \n",
      "row\n",
      "44/419:\n",
      "#bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "\n",
      "row = alt.hconcat()\n",
      "for page in cont_stacked.page.unique():\n",
      "    regular = alt.Chart(cont_stacked[cont_stacked.page==page]).mark_bar().encode(\n",
      "        alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "        alt.Y('Category:N', axis=None, sort=order, title=None),\n",
      "        alt.Color('group_cat:N'),\n",
      "        #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "        #column='page:N'\n",
      "    ).properties(height=1500, width=150, title=page + ' Comment Count')        \n",
      "\n",
      "    normalized = regular.encode(    \n",
      "        alt.X('comment_count:Q',\n",
      "            axis=alt.Axis(title=None),\n",
      "            stack='normalize'\n",
      "            ),\n",
      "        alt.Y('Category:N',  sort=order, title=None),\n",
      "        #column='page:N'\n",
      "    ).properties(title=page + ' Comment Ratio')\n",
      "    if(page!='Herzog'):\n",
      "        normalized = normalized.encode(alt.Y('Category:N', axis=None, title=None),)\n",
      "    row |= (normalized | regular)\n",
      "    \n",
      "row\n",
      "44/420:\n",
      "#bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "\n",
      "row = alt.hconcat()\n",
      "for page in cont_stacked.page.unique():\n",
      "    regular = alt.Chart(cont_stacked[cont_stacked.page==page]).mark_bar().encode(\n",
      "        alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "        alt.Y('Category:N', axis=None, sort=order, title=None),\n",
      "        alt.Color('group_cat:N'),\n",
      "        #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "        #column='page:N'\n",
      "    ).properties(height=1500, width=150, title=page + ' Comment Count')        \n",
      "\n",
      "    normalized = regular.encode(    \n",
      "        alt.X('comment_count:Q',\n",
      "            axis=alt.Axis(title=None),\n",
      "            stack='normalize'\n",
      "            ),\n",
      "        alt.Y('Category:N',  sort=order, title=None),\n",
      "        #column='page:N'\n",
      "    ).properties(title=page + ' Comment Ratio')\n",
      "    if(page!='Herzog'):\n",
      "        normalized = normalized.encode(alt.Y('Category:N', sort=order, axis=None, title=None),)\n",
      "    row |= (normalized | regular)\n",
      "    \n",
      "row\n",
      "44/421:\n",
      "order = list(cont_stacked.groupby('Category').comment_count.sum().sort_values(ascending=False).index)\n",
      "order\n",
      "44/422:\n",
      "#bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "\n",
      "row = alt.hconcat()\n",
      "for page in cont_stacked.page.unique():\n",
      "    regular = alt.Chart(cont_stacked[cont_stacked.page==page]).mark_bar().encode(\n",
      "        alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "        alt.Y('Category:N', axis=None, sort=order, title=None),\n",
      "        alt.Color('group_cat:N'),\n",
      "        #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "        #column='page:N'\n",
      "    ).properties(height=1500, width=150, title=page + ' Comment Count')        \n",
      "\n",
      "    normalized = regular.encode(    \n",
      "        alt.X('comment_count:Q',\n",
      "            axis=alt.Axis(title=None),\n",
      "            stack='normalize'\n",
      "            ),\n",
      "        alt.Y('Category:N',  sort=order, title=None),\n",
      "        #column='page:N'\n",
      "    ).properties(title=page + ' Comment Ratio')\n",
      "    if(page!='Herzog'):\n",
      "        normalized = normalized.encode(alt.Y('Category:N', sort=order, axis=None, title=None),)\n",
      "    row |= (normalized | regular)\n",
      "    \n",
      "row\n",
      "44/423:\n",
      "order = list(cont_stacked[cont_stacked.page=='Netanyahu'].groupby('Category').comment_count.sum().sort_values(ascending=False).index)\n",
      "order\n",
      "44/424:\n",
      "#bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "\n",
      "row = alt.hconcat()\n",
      "for page in cont_stacked.page.unique():\n",
      "    regular = alt.Chart(cont_stacked[cont_stacked.page==page]).mark_bar().encode(\n",
      "        alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "        alt.Y('Category:N', axis=None, sort=order, title=None),\n",
      "        alt.Color('group_cat:N'),\n",
      "        #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "        #column='page:N'\n",
      "    ).properties(height=1500, width=150, title=page + ' Comment Count')        \n",
      "\n",
      "    normalized = regular.encode(    \n",
      "        alt.X('comment_count:Q',\n",
      "            axis=alt.Axis(title=None),\n",
      "            stack='normalize'\n",
      "            ),\n",
      "        alt.Y('Category:N',  sort=order, title=None),\n",
      "        #column='page:N'\n",
      "    ).properties(title=page + ' Comment Ratio')\n",
      "    if(page!='Herzog'):\n",
      "        normalized = normalized.encode(alt.Y('Category:N', sort=order, axis=None, title=None),)\n",
      "    row |= (normalized | regular)\n",
      "    \n",
      "row\n",
      "44/425:\n",
      "content_types = pd.read_csv('turf/content_groups_types.csv')\n",
      "content_types.head()\n",
      "44/426:\n",
      "content_types = (pd.read_csv('turf/content_groups_types.csv')[['Category', 'Type']]\n",
      "                 .drop_duplicates())\n",
      "content_types.head()\n",
      "44/427:\n",
      "content_types = (pd.read_csv('turf/content_groups_types.csv')[['Category', 'Type']]\n",
      "                 .drop_duplicates().set_index('Category')['type'])\n",
      "content_types.head()\n",
      "44/428:\n",
      "content_types = (pd.read_csv('turf/content_groups_types.csv')[['Category', 'Type']]\n",
      "                 .drop_duplicates().set_index('Category')['Type'])\n",
      "content_types.head()\n",
      "44/429:\n",
      "cont_stacked = cont.stack().reset_index().rename(columns = {'level_2': 'group_cat', 0: 'comment_count'})\n",
      "cont_stacked.type = cont_stacked.Category.map(content_types)\n",
      "cont_stacked.head()\n",
      "44/430:\n",
      "cont_stacked = cont.stack().reset_index().rename(columns = {'level_2': 'group_cat', 0: 'comment_count'})\n",
      "cont_stacked['type'] = cont_stacked.Category.map(content_types)\n",
      "cont_stacked.head()\n",
      "44/431: cont_stacked.Category\n",
      "44/432:\n",
      "content_types = (pd.read_csv('turf/content_groups_types.csv')[['Category', 'Type']]\n",
      "                 .drop_duplicates().set_index('Category')['Type'])\n",
      "content_types.head()\n",
      "44/433: cont_stacked.Category.map(content_types)\n",
      "44/434:\n",
      "content_types = (pd.read_csv('turf/content_groups_types.csv')[['Category', 'Type']]\n",
      "                 .drop_duplicates().set_index('Category')['Type'].str.lower())\n",
      "content_types.head()\n",
      "44/435: content_types\n",
      "44/436: cont_stacked.Category.map(content_types)\n",
      "44/437:\n",
      "cont_stacked = cont.stack().reset_index().rename(columns = {'level_2': 'group_cat', 0: 'comment_count'})\n",
      "cont_stacked['type'] = cont_stacked.Category.map(content_types)\n",
      "cont_stacked.head()\n",
      "44/438:\n",
      "content_types = (pd.read_csv('turf/content_groups_types.csv')[['Category', 'Type']]\n",
      "                 .drop_duplicates().set_index('Category')['Type'].str.lower())\n",
      "content_types.head()\n",
      "44/439: content_types\n",
      "44/440: cont_stacked.Category.map(content_types)\n",
      "44/441:\n",
      "cont_stacked = cont.stack().reset_index().rename(columns = {'level_2': 'group_cat', 0: 'comment_count'})\n",
      "cont_stacked['type'] = cont_stacked.Category.map(content_types)\n",
      "cont_stacked.head(20)\n",
      "44/442: cont_stacked.isna(subset='type')\n",
      "44/443: cont_stacked.isna()\n",
      "44/444: cont_stacked[cont_stacked.type.isna()]\n",
      "44/445:\n",
      "cont_stacked = cont.stack().reset_index().rename(columns = {'level_2': 'group_cat', 0: 'comment_count'})\n",
      "cont_stacked['type'] = cont_stacked.Category.map(content_types)\n",
      "cont_stacked.head()\n",
      "44/446:\n",
      "content_types = (pd.read_csv('turf/content_groups_types.csv')[['Category', 'Type']]\n",
      "                 .drop_duplicates().set_index('Category')['Type'].str.lower())\n",
      "content_types.head()\n",
      "44/447:\n",
      "cont_stacked = cont.stack().reset_index().rename(columns = {'level_2': 'group_cat', 0: 'comment_count'})\n",
      "cont_stacked['type'] = cont_stacked.Category.map(content_types)\n",
      "cont_stacked.head()\n",
      "44/448: cont_stacked[cont_stacked.type.isna()]\n",
      "44/449: cont_stacked.page.value_counts()\n",
      "44/450:\n",
      "order = list(cont_stacked[cont_stacked.page=='Netanyahu'].groupby('Category').comment_count.sum().sort_values(ascending=False).index)\n",
      "order\n",
      "44/451:\n",
      "#bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "\n",
      "row = alt.hconcat()\n",
      "for page in cont_stacked.page.unique():\n",
      "    regular = alt.Chart(cont_stacked[cont_stacked.page==page]).mark_bar().encode(\n",
      "        alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "        alt.Y('Category:N', axis=None, sort=order, title=None),\n",
      "        alt.Color('group_cat:N'),\n",
      "        #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "        #column='page:N'\n",
      "    ).properties(height=1500, width=150, title=page + ' Comment Count')        \n",
      "\n",
      "    normalized = regular.encode(    \n",
      "        alt.X('comment_count:Q',\n",
      "            axis=alt.Axis(title=None),\n",
      "            stack='normalize'\n",
      "            ),\n",
      "        alt.Y('Category:N',  sort=order, title=None),\n",
      "        #column='page:N'\n",
      "    ).properties(title=page + ' Comment Ratio')\n",
      "    if(page!='Herzog'):\n",
      "        normalized = normalized.encode(alt.Y('Category:N', sort=order, axis=None, title=None),)\n",
      "    row |= (normalized | regular)\n",
      "    \n",
      "row\n",
      "44/452:\n",
      "row = alt.hconcat()\n",
      "cont_type='discursive'\n",
      "for page in cont_stacked.page.unique():\n",
      "    regular = alt.Chart(cont_stacked[(cont_stacked.page==page) & (cont_stacked.type==cont_type)]).mark_bar().encode(\n",
      "        alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "        alt.Y('Category:N', axis=None, sort=order, title=None),\n",
      "        alt.Color('group_cat:N'),\n",
      "        #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "        #column='page:N'\n",
      "    ).properties(height=1500, width=150, title=page + ' Comment Count')        \n",
      "\n",
      "    normalized = regular.encode(    \n",
      "        alt.X('comment_count:Q',\n",
      "            axis=alt.Axis(title=None),\n",
      "            stack='normalize'\n",
      "            ),\n",
      "        alt.Y('Category:N',  sort=order, title=None),\n",
      "        #column='page:N'\n",
      "    ).properties(title=page + ' Comment Ratio')\n",
      "    if(page!='Herzog'):\n",
      "        normalized = normalized.encode(alt.Y('Category:N', sort=order, axis=None, title=None),)\n",
      "    row |= (normalized | regular)\n",
      "    \n",
      "row\n",
      "44/453:\n",
      "row = alt.hconcat()\n",
      "cont_type='phatic'\n",
      "for page in cont_stacked.page.unique():\n",
      "    regular = alt.Chart(cont_stacked[(cont_stacked.page==page) & (cont_stacked.type==cont_type)]).mark_bar().encode(\n",
      "        alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "        alt.Y('Category:N', axis=None, sort=order, title=None),\n",
      "        alt.Color('group_cat:N'),\n",
      "        #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "        #column='page:N'\n",
      "    ).properties(height=1500, width=150, title=page + ' Comment Count')        \n",
      "\n",
      "    normalized = regular.encode(    \n",
      "        alt.X('comment_count:Q',\n",
      "            axis=alt.Axis(title=None),\n",
      "            stack='normalize'\n",
      "            ),\n",
      "        alt.Y('Category:N',  sort=order, title=None),\n",
      "        #column='page:N'\n",
      "    ).properties(title=page + ' Comment Ratio')\n",
      "    if(page!='Herzog'):\n",
      "        normalized = normalized.encode(alt.Y('Category:N', sort=order, axis=None, title=None),)\n",
      "    row |= (normalized | regular)\n",
      "    \n",
      "row\n",
      "44/454: content_types.type.value_counts()\n",
      "44/455: content_types\n",
      "44/456: content_types.unique()\n",
      "44/457:\n",
      "row = alt.hconcat()\n",
      "cont_type='phatic'\n",
      "for page in cont_stacked.page.unique():\n",
      "    regular = alt.Chart(cont_stacked[(cont_stacked.page==page) & (cont_stacked.type==cont_type)]).mark_bar().encode(\n",
      "        alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "        alt.Y('Category:N', axis=None, sort=order, title=None),\n",
      "        alt.Color('group_cat:N'),\n",
      "        #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "        #column='page:N'\n",
      "    ).properties(width=150, title=page + ' Comment Count')        \n",
      "\n",
      "    normalized = regular.encode(    \n",
      "        alt.X('comment_count:Q',\n",
      "            axis=alt.Axis(title=None),\n",
      "            stack='normalize'\n",
      "            ),\n",
      "        alt.Y('Category:N',  sort=order, title=None),\n",
      "        #column='page:N'\n",
      "    ).properties(title=page + ' Comment Ratio')\n",
      "    if(page!='Herzog'):\n",
      "        normalized = normalized.encode(alt.Y('Category:N', sort=order, axis=None, title=None),)\n",
      "    row |= (normalized | regular)\n",
      "    \n",
      "row\n",
      "44/458:\n",
      "row = alt.hconcat()\n",
      "cont_type='discursive'\n",
      "for page in cont_stacked.page.unique():\n",
      "    regular = alt.Chart(cont_stacked[(cont_stacked.page==page) & (cont_stacked.type==cont_type)]).mark_bar().encode(\n",
      "        alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "        alt.Y('Category:N', axis=None, sort=order, title=None),\n",
      "        alt.Color('group_cat:N'),\n",
      "        #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "        #column='page:N'\n",
      "    ).properties(width=150, title=page + ' Comment Count')        \n",
      "\n",
      "    normalized = regular.encode(    \n",
      "        alt.X('comment_count:Q',\n",
      "            axis=alt.Axis(title=None),\n",
      "            stack='normalize'\n",
      "            ),\n",
      "        alt.Y('Category:N',  sort=order, title=None),\n",
      "        #column='page:N'\n",
      "    ).properties(title=page + ' Comment Ratio')\n",
      "    if(page!='Herzog'):\n",
      "        normalized = normalized.encode(alt.Y('Category:N', sort=order, axis=None, title=None),)\n",
      "    row |= (normalized | regular)\n",
      "    \n",
      "row\n",
      "44/459:\n",
      "row = alt.hconcat()\n",
      "cont_type='praise'\n",
      "for page in cont_stacked.page.unique():\n",
      "    regular = alt.Chart(cont_stacked[(cont_stacked.page==page) & (cont_stacked.type==cont_type)]).mark_bar().encode(\n",
      "        alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "        alt.Y('Category:N', axis=None, sort=order, title=None),\n",
      "        alt.Color('group_cat:N'),\n",
      "        #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "        #column='page:N'\n",
      "    ).properties(height=1500, width=150, title=page + ' Comment Count')        \n",
      "\n",
      "    normalized = regular.encode(    \n",
      "        alt.X('comment_count:Q',\n",
      "            axis=alt.Axis(title=None),\n",
      "            stack='normalize'\n",
      "            ),\n",
      "        alt.Y('Category:N',  sort=order, title=None),\n",
      "        #column='page:N'\n",
      "    ).properties(title=page + ' Comment Ratio')\n",
      "    if(page!='Herzog'):\n",
      "        normalized = normalized.encode(alt.Y('Category:N', sort=order, axis=None, title=None),)\n",
      "    row |= (normalized | regular)\n",
      "    \n",
      "row\n",
      "44/460:\n",
      "row = alt.hconcat()\n",
      "cont_type='praise'\n",
      "for page in cont_stacked.page.unique():\n",
      "    regular = alt.Chart(cont_stacked[(cont_stacked.page==page) & (cont_stacked.type==cont_type)]).mark_bar().encode(\n",
      "        alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "        alt.Y('Category:N', axis=None, sort=order, title=None),\n",
      "        alt.Color('group_cat:N'),\n",
      "        #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "        #column='page:N'\n",
      "    ).properties(width=150, title=page + ' Comment Count')        \n",
      "\n",
      "    normalized = regular.encode(    \n",
      "        alt.X('comment_count:Q',\n",
      "            axis=alt.Axis(title=None),\n",
      "            stack='normalize'\n",
      "            ),\n",
      "        alt.Y('Category:N',  sort=order, title=None),\n",
      "        #column='page:N'\n",
      "    ).properties(title=page + ' Comment Ratio')\n",
      "    if(page!='Herzog'):\n",
      "        normalized = normalized.encode(alt.Y('Category:N', sort=order, axis=None, title=None),)\n",
      "    row |= (normalized | regular)\n",
      "    \n",
      "row\n",
      "44/461:\n",
      "row = alt.hconcat()\n",
      "cont_type='discursive'\n",
      "for page in cont_stacked.page.unique():\n",
      "    regular = alt.Chart(cont_stacked[(cont_stacked.page==page) & (cont_stacked.type==cont_type)]).mark_bar().encode(\n",
      "        alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "        alt.Y('Category:N', axis=None, sort=order, title=None),\n",
      "        alt.Color('group_cat:N'),\n",
      "        #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "        #column='page:N'\n",
      "    ).properties(width=150, title=page + ' Comment Count')        \n",
      "\n",
      "    normalized = regular.encode(    \n",
      "        alt.X('comment_count:Q',\n",
      "            axis=alt.Axis(title=None),\n",
      "            stack='normalize'\n",
      "            ),\n",
      "        alt.Y('Category:N',  sort=order, title=None),\n",
      "        #column='page:N'\n",
      "    ).properties(title=page + ' Comment Ratio')\n",
      "    if(page!='Herzog'):\n",
      "        normalized = normalized.encode(alt.Y('Category:N', sort=order, axis=None, title=None),)\n",
      "    row |= (normalized | regular)\n",
      "    \n",
      "row.properties(title=cont_type)\n",
      "44/462:\n",
      "rows = alt.vconcat()\n",
      "for cont_type in types:\n",
      "    row = alt.hconcat()\n",
      "    for page in cont_stacked.page.unique():\n",
      "        regular = alt.Chart(cont_stacked[(cont_stacked.page==page) & (cont_stacked.type==cont_type)]).mark_bar().encode(\n",
      "            alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "            alt.Y('Category:N', axis=None, sort=order, title=None),\n",
      "            alt.Color('group_cat:N'),\n",
      "            #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "            #column='page:N'\n",
      "        ).properties(width=150, title=page + ' Comment Count')        \n",
      "\n",
      "        normalized = regular.encode(    \n",
      "            alt.X('comment_count:Q',\n",
      "                axis=alt.Axis(title=None),\n",
      "                stack='normalize'\n",
      "                ),\n",
      "            alt.Y('Category:N',  sort=order, title=None),\n",
      "            #column='page:N'\n",
      "        ).properties(title=page + ' Comment Ratio')\n",
      "        if(page!='Herzog'):\n",
      "            normalized = normalized.encode(alt.Y('Category:N', sort=order, axis=None, title=None),)\n",
      "        row |= (normalized | regular).properties(title=cont_type)\n",
      "    rows &= row\n",
      "    \n",
      "rows\n",
      "44/463: types = content_types.unique()\n",
      "44/464: types = content_types.unique()\n",
      "44/465:\n",
      "rows = alt.vconcat()\n",
      "for cont_type in types:\n",
      "    row = alt.hconcat()\n",
      "    for page in cont_stacked.page.unique():\n",
      "        regular = alt.Chart(cont_stacked[(cont_stacked.page==page) & (cont_stacked.type==cont_type)]).mark_bar().encode(\n",
      "            alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "            alt.Y('Category:N', axis=None, sort=order, title=None),\n",
      "            alt.Color('group_cat:N'),\n",
      "            #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "            #column='page:N'\n",
      "        ).properties(width=150, title=page + ' Comment Count')        \n",
      "\n",
      "        normalized = regular.encode(    \n",
      "            alt.X('comment_count:Q',\n",
      "                axis=alt.Axis(title=None),\n",
      "                stack='normalize'\n",
      "                ),\n",
      "            alt.Y('Category:N',  sort=order, title=None),\n",
      "            #column='page:N'\n",
      "        ).properties(title=page + ' Comment Ratio')\n",
      "        if(page!='Herzog'):\n",
      "            normalized = normalized.encode(alt.Y('Category:N', sort=order, axis=None, title=None),)\n",
      "        row |= (normalized | regular).properties(title=cont_type)\n",
      "    rows &= row\n",
      "    \n",
      "rows\n",
      "44/466:\n",
      "row = alt.hconcat()\n",
      "cont_type='phatic'\n",
      "for page in cont_stacked.page.unique():\n",
      "    regular = alt.Chart(cont_stacked[(cont_stacked.page==page) & (cont_stacked.type==cont_type)]).mark_bar().encode(\n",
      "        alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "        alt.Y('Category:N', axis=None, sort=order, title=None),\n",
      "        alt.Color('group_cat:N'),\n",
      "        #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "        #column='page:N'\n",
      "    ).properties(width=150, title=page + ' Comment Count')        \n",
      "\n",
      "    normalized = regular.encode(    \n",
      "        alt.X('comment_count:Q',\n",
      "            axis=alt.Axis(title=None),\n",
      "            stack='normalize'\n",
      "            ),\n",
      "        alt.Y('Category:N',  sort=order, title=None),\n",
      "        #column='page:N'\n",
      "    ).properties(title=page + ' Comment Ratio')\n",
      "    if(page!='Herzog'):\n",
      "        normalized = normalized.encode(alt.Y('Category:N', sort=order, axis=None, title=None),)\n",
      "    row |= (normalized | regular)\n",
      "    \n",
      "row\n",
      "44/467:\n",
      "row = alt.hconcat()\n",
      "cont_type='praise'\n",
      "for page in cont_stacked.page.unique():\n",
      "    regular = alt.Chart(cont_stacked[(cont_stacked.page==page) & (cont_stacked.type==cont_type)]).mark_bar().encode(\n",
      "        alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "        alt.Y('Category:N', axis=None, sort=order, title=None),\n",
      "        alt.Color('group_cat:N'),\n",
      "        #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "        #column='page:N'\n",
      "    ).properties(width=150, title=page + ' Comment Count')        \n",
      "\n",
      "    normalized = regular.encode(    \n",
      "        alt.X('comment_count:Q',\n",
      "            axis=alt.Axis(title=None),\n",
      "            stack='normalize'\n",
      "            ),\n",
      "        alt.Y('Category:N',  sort=order, title=None),\n",
      "        #column='page:N'\n",
      "    ).properties(title=page + ' Comment Ratio')\n",
      "    if(page!='Herzog'):\n",
      "        normalized = normalized.encode(alt.Y('Category:N', sort=order, axis=None, title=None),)\n",
      "    row |= (normalized | regular)\n",
      "    \n",
      "row\n",
      "44/468:\n",
      "rows = alt.vconcat()\n",
      "for cont_type in types:\n",
      "    row = alt.hconcat()\n",
      "    for page in cont_stacked.page.unique():\n",
      "        regular = alt.Chart(cont_stacked[(cont_stacked.page==page) & (cont_stacked.type==cont_type)]).mark_bar().encode(\n",
      "            alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "            alt.Y('Category:N', axis=None, sort=order, title=cont_type),\n",
      "            alt.Color('group_cat:N'),\n",
      "            #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "            #column='page:N'\n",
      "        ).properties(width=150, title=page + ' Comment Count')        \n",
      "\n",
      "        normalized = regular.encode(    \n",
      "            alt.X('comment_count:Q',\n",
      "                axis=alt.Axis(title=None),\n",
      "                stack='normalize'\n",
      "                ),\n",
      "            alt.Y('Category:N',  sort=order, title=None),\n",
      "            #column='page:N'\n",
      "        ).properties(title=page + ' Comment Ratio')\n",
      "        if(page!='Herzog'):\n",
      "            normalized = normalized.encode(alt.Y('Category:N', sort=order, axis=None, title=None),)\n",
      "        row |= (normalized | regular)\n",
      "    rows &= row\n",
      "    \n",
      "rows\n",
      "44/469: types = content_types.dropna().unique()\n",
      "44/470:\n",
      "rows = alt.vconcat()\n",
      "for cont_type in types:\n",
      "    row = alt.hconcat()\n",
      "    for page in cont_stacked.page.unique():\n",
      "        regular = alt.Chart(cont_stacked[(cont_stacked.page==page) & (cont_stacked.type==cont_type)]).mark_bar().encode(\n",
      "            alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "            alt.Y('Category:N', axis=None, sort=order, title=cont_type),\n",
      "            alt.Color('group_cat:N'),\n",
      "            #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "            #column='page:N'\n",
      "        ).properties(width=150, title=page + ' Comment Count')        \n",
      "\n",
      "        normalized = regular.encode(    \n",
      "            alt.X('comment_count:Q',\n",
      "                axis=alt.Axis(title=None),\n",
      "                stack='normalize'\n",
      "                ),\n",
      "            alt.Y('Category:N',  sort=order, title=None),\n",
      "            #column='page:N'\n",
      "        ).properties(title=page + ' Comment Ratio')\n",
      "        if(page!='Herzog'):\n",
      "            normalized = normalized.encode(alt.Y('Category:N', sort=order, axis=None, title=None),)\n",
      "        row |= (normalized | regular)\n",
      "    rows &= row\n",
      "    \n",
      "rows\n",
      "44/471:\n",
      "rows = alt.vconcat()\n",
      "for cont_type in types:\n",
      "    row = alt.hconcat()\n",
      "    for page in cont_stacked.page.unique():\n",
      "        regular = alt.Chart(cont_stacked[(cont_stacked.page==page) & (cont_stacked.type==cont_type)]).mark_bar().encode(\n",
      "            alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "            alt.Y('Category:N', axis=None, sort=order),\n",
      "            alt.Color('group_cat:N'),\n",
      "            #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "            #column='page:N'\n",
      "        ).properties(width=150, title=page + ' Comment Count')        \n",
      "\n",
      "        normalized = regular.encode(    \n",
      "            alt.X('comment_count:Q',\n",
      "                axis=alt.Axis(title=None),\n",
      "                stack='normalize'\n",
      "                ),\n",
      "            alt.Y('Category:N',  sort=order, title=cont_type),\n",
      "            #column='page:N'\n",
      "        ).properties(title=page + ' Comment Ratio')\n",
      "        if(page!='Herzog'):\n",
      "            normalized = normalized.encode(alt.Y('Category:N', sort=order, axis=None, title=None),)\n",
      "        row |= (normalized | regular)\n",
      "    rows &= row\n",
      "    \n",
      "rows\n",
      "44/472: types = ['praise', 'discursive', 'phatic']#content_types.dropna().unique()\n",
      "44/473:\n",
      "rows = alt.vconcat()\n",
      "for cont_type in types:\n",
      "    row = alt.hconcat()\n",
      "    for page in cont_stacked.page.unique():\n",
      "        regular = alt.Chart(cont_stacked[(cont_stacked.page==page) & (cont_stacked.type==cont_type)]).mark_bar().encode(\n",
      "            alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "            alt.Y('Category:N', axis=None, sort=order),\n",
      "            alt.Color('group_cat:N'),\n",
      "            #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "            #column='page:N'\n",
      "        ).properties(width=150, title=page + ' Comment Count')        \n",
      "\n",
      "        normalized = regular.encode(    \n",
      "            alt.X('comment_count:Q',\n",
      "                axis=alt.Axis(title=None),\n",
      "                stack='normalize'\n",
      "                ),\n",
      "            alt.Y('Category:N',  sort=order, title=cont_type),\n",
      "            #column='page:N'\n",
      "        ).properties(title=page + ' Comment Ratio')\n",
      "        if(page!='Herzog'):\n",
      "            normalized = normalized.encode(alt.Y('Category:N', sort=order, axis=None, title=None),)\n",
      "        row |= (normalized | regular)\n",
      "    rows &= row\n",
      "    \n",
      "rows\n",
      "44/474:\n",
      "rows = alt.vconcat()\n",
      "for cont_type in types:\n",
      "    row = alt.hconcat()\n",
      "    for page in cont_stacked.page.unique():\n",
      "        regular = alt.Chart(cont_stacked[(cont_stacked.page==page) & (cont_stacked.type==cont_type) & (~cont_stacked.Category.isin(cont_filter))]).mark_bar().encode(\n",
      "            alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "            alt.Y('Category:N', axis=None, sort=order),\n",
      "            alt.Color('group_cat:N'),\n",
      "            #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "            #column='page:N'\n",
      "        ).properties(width=150, title=page + ' Comment Count')        \n",
      "\n",
      "        normalized = regular.encode(    \n",
      "            alt.X('comment_count:Q',\n",
      "                axis=alt.Axis(title=None),\n",
      "                stack='normalize'\n",
      "                ),\n",
      "            alt.Y('Category:N',  sort=order, title=cont_type),\n",
      "            #column='page:N'\n",
      "        ).properties(title=page + ' Comment Ratio')\n",
      "        if(page!='Herzog'):\n",
      "            normalized = normalized.encode(alt.Y('Category:N', sort=order, axis=None, title=None),)\n",
      "        row |= (normalized | regular)\n",
      "    rows &= row\n",
      "    \n",
      "rows\n",
      "44/475:\n",
      "types = ['praise', 'discursive', 'phatic']#content_types.dropna().unique()\n",
      "cont_filter = ['Anti-BDS', 'BDS', 'Troll']\n",
      "44/476:\n",
      "rows = alt.vconcat()\n",
      "for cont_type in types:\n",
      "    row = alt.hconcat()\n",
      "    for page in cont_stacked.page.unique():\n",
      "        regular = alt.Chart(cont_stacked[(cont_stacked.page==page) & (cont_stacked.type==cont_type) & (~cont_stacked.Category.isin(cont_filter))]).mark_bar().encode(\n",
      "            alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "            alt.Y('Category:N', axis=None, sort=order),\n",
      "            alt.Color('group_cat:N'),\n",
      "            #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "            #column='page:N'\n",
      "        ).properties(width=150, title=page + ' Comment Count')        \n",
      "\n",
      "        normalized = regular.encode(    \n",
      "            alt.X('comment_count:Q',\n",
      "                axis=alt.Axis(title=None),\n",
      "                stack='normalize'\n",
      "                ),\n",
      "            alt.Y('Category:N',  sort=order, title=cont_type),\n",
      "            #column='page:N'\n",
      "        ).properties(title=page + ' Comment Ratio')\n",
      "        if(page!='Herzog'):\n",
      "            normalized = normalized.encode(alt.Y('Category:N', sort=order, axis=None, title=None),)\n",
      "        row |= (normalized | regular)\n",
      "    rows &= row\n",
      "    \n",
      "rows\n",
      "44/477:\n",
      "#bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "\n",
      "row = alt.hconcat()\n",
      "for page in cont_stacked.page.unique():\n",
      "    regular = alt.Chart(cont_stacked[cont_stacked.page==page]).mark_bar().encode(\n",
      "        alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "        alt.Y('Category:N', axis=None, sort=order, title=None),\n",
      "        alt.Color('group_cat:N'),\n",
      "        #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "        #column='page:N'\n",
      "    ).properties(height=1500, width=150, title=page + ' Comment Count')        \n",
      "\n",
      "    normalized = regular.encode(    \n",
      "        alt.X('comment_count:Q',\n",
      "            axis=alt.Axis(title=None),\n",
      "            stack='normalize'\n",
      "            ),\n",
      "        alt.Y('Category:N',  sort=order, title=None),\n",
      "        #column='page:N'\n",
      "    ).properties(title=page + ' Comment Ratio')\n",
      "    if(page!='Herzog'):\n",
      "        normalized = normalized.encode(alt.Y('Category:N', sort=order, axis=None, title=None),)\n",
      "    row |= (normalized | regular)\n",
      "    \n",
      "row\n",
      "44/478:\n",
      "content_types = (pd.read_csv('turf/content_groups_types.csv')[['Category', 'Type']]\n",
      "                 .drop_duplicates().set_index('Category')['Type'].str.lower())\n",
      "content_types.head()\n",
      "44/479:\n",
      "cont_stacked = cont.stack().reset_index().rename(columns = {'level_2': 'group_cat', 0: 'comment_count'})\n",
      "cont_stacked['type'] = cont_stacked.Category.map(content_types)\n",
      "cont_stacked.head()\n",
      "44/480: cont_stacked.page.value_counts()\n",
      "44/481:\n",
      "order = list(cont_stacked[cont_stacked.page=='Netanyahu'].groupby('Category').comment_count.sum().sort_values(ascending=False).index)\n",
      "order\n",
      "44/482:\n",
      "#bibi_stacked = bibi_cont.stack().reset_index().rename(columns = {'level_1': 'group_cat', 0: 'comment_count'})\n",
      "\n",
      "row = alt.hconcat()\n",
      "for page in cont_stacked.page.unique():\n",
      "    regular = alt.Chart(cont_stacked[cont_stacked.page==page]).mark_bar().encode(\n",
      "        alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "        alt.Y('Category:N', axis=None, sort=order, title=None),\n",
      "        alt.Color('group_cat:N'),\n",
      "        #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "        #column='page:N'\n",
      "    ).properties(height=1500, width=150, title=page + ' Comment Count')        \n",
      "\n",
      "    normalized = regular.encode(    \n",
      "        alt.X('comment_count:Q',\n",
      "            axis=alt.Axis(title=None),\n",
      "            stack='normalize'\n",
      "            ),\n",
      "        alt.Y('Category:N',  sort=order, title=None),\n",
      "        #column='page:N'\n",
      "    ).properties(title=page + ' Comment Ratio')\n",
      "    if(page!='Herzog'):\n",
      "        normalized = normalized.encode(alt.Y('Category:N', sort=order, axis=None, title=None),)\n",
      "    row |= (normalized | regular)\n",
      "    \n",
      "row\n",
      "44/483:\n",
      "types = ['praise', 'discursive', 'phatic']#content_types.dropna().unique()\n",
      "cont_filter = ['Anti-BDS', 'BDS', 'Troll']\n",
      "44/484:\n",
      "rows = alt.vconcat()\n",
      "for cont_type in types:\n",
      "    row = alt.hconcat()\n",
      "    for page in cont_stacked.page.unique():\n",
      "        regular = alt.Chart(cont_stacked[(cont_stacked.page==page) & (cont_stacked.type==cont_type) & (~cont_stacked.Category.isin(cont_filter))]).mark_bar().encode(\n",
      "            alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "            alt.Y('Category:N', axis=None, sort=order),\n",
      "            alt.Color('group_cat:N'),\n",
      "            #tooltip = ['Category', 'group_cat','comment_count'],\n",
      "            #column='page:N'\n",
      "        ).properties(width=150, title=page + ' Comment Count')        \n",
      "\n",
      "        normalized = regular.encode(    \n",
      "            alt.X('comment_count:Q',\n",
      "                axis=alt.Axis(title=None),\n",
      "                stack='normalize'\n",
      "                ),\n",
      "            alt.Y('Category:N',  sort=order, title=cont_type),\n",
      "            #column='page:N'\n",
      "        ).properties(title=page + ' Comment Ratio')\n",
      "        if(page!='Herzog'):\n",
      "            normalized = normalized.encode(alt.Y('Category:N', sort=order, axis=None, title=None),)\n",
      "        row |= (normalized | regular)\n",
      "    rows &= row\n",
      "    \n",
      "rows\n",
      "44/485:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "%%output size=400\n",
      "\n",
      "from holoviews.streams import RangeXY\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k'), cmap=Sets1to3)) * \\\n",
      "    hv.util.Dynamic(aggregate(all_points, width=100, height=100, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.group_cat.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/486:\n",
      "import numpy as np\n",
      "import holoviews as hv\n",
      "import datashader as ds\n",
      "from holoviews.operation.datashader import aggregate, datashade, shade, dynspread\n",
      "from holoviews.operation import decimate\n",
      "hv.extension('bokeh','matplotlib')\n",
      "decimate.max_samples=1000\n",
      "dynspread.max_px=20\n",
      "dynspread.threshold=0.5\n",
      "44/487:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "%%output size=400\n",
      "\n",
      "from holoviews.streams import RangeXY\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k'), cmap=Sets1to3)) * \\\n",
      "    hv.util.Dynamic(aggregate(all_points, width=100, height=100, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.group_cat.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/488:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "%%output size=300\n",
      "\n",
      "from holoviews.streams import RangeXY\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k'), cmap=Sets1to3)) * \\\n",
      "    hv.util.Dynamic(aggregate(all_points, width=100, height=100, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.group_cat.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "44/489:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "from holoviews.streams import RangeXY\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(aptopmk, kdims='k'), aggregator=ds.count_cat('k'), cmap=Sets1to3)) * \\\n",
      "    hv.util.Dynamic(aggregate(all_points, width=100, height=100, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.topmk.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "49/1:\n",
      "import wikipediaapi as wa\n",
      "import pandas as pd\n",
      "import networkx as nx\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "import seaborn as sns\n",
      "sns.set(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
      "sns.set_context(\"talk\")\n",
      "sns.set_palette('Set2', 10)\n",
      "49/2: wiki = wa.Wikipedia('he')\n",
      "49/3: zuk_he = wiki.page('מבצע_צוק_איתן')\n",
      "49/4:\n",
      "def print_langlinks(page):\n",
      "    langlinks = page.langlinks\n",
      "    for k in sorted(langlinks.keys()):\n",
      "        v = langlinks[k]\n",
      "        print(\"%s: %s - %s: %s\" % (k, v.language, v.title, v.fullurl))\n",
      "\n",
      "print_langlinks(zuk_he)\n",
      "49/5:\n",
      "def print_sections(sections, level=0):\n",
      "        for s in sections:\n",
      "                print(\"%s: %s - %s\" % (\"*\" * (level + 1), s.title, s.text[0:40]))\n",
      "                print_sections(s.sections, level + 1)\n",
      "#print_sections(zuk_he.sections)\n",
      "49/6: zuk_he.links\n",
      "49/7:\n",
      "ll = zuk_he.langlinks\n",
      "ll['he'] = ll['ar'].langlinks['he']\n",
      "49/8: import requests\n",
      "50/1: frl = from_pickle('data/rev_link_df.pkl.gz', compression='gzip')\n",
      "50/2: frl = pd.from_pickle('data/rev_link_df.pkl.gz', compression='gzip')\n",
      "50/3:\n",
      "import wikipediaapi as wa\n",
      "import pandas as pd\n",
      "import networkx as nx\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "import seaborn as sns\n",
      "sns.set(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
      "sns.set_context(\"talk\")\n",
      "sns.set_palette('Set2', 10)\n",
      "50/4: wiki = wa.Wikipedia('he')\n",
      "50/5: zuk_he = wiki.page('מבצע_צוק_איתן')\n",
      "50/6: frl = pd.from_pickle('data/rev_link_df.pkl.gz', compression='gzip')\n",
      "50/7: frl = pd.read_pickle('data/rev_link_df.pkl.gz', compression='gzip')\n",
      "50/8:\n",
      "import altair as alt\n",
      "alt.renderers.enable('notebook')\n",
      "\n",
      "link_appear = frl.groupby('link').timestamp.min().to_frame().resample('M', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "\n",
      "alt.Chart(link_appear).mark_bar().encode(\n",
      "    x='timestamp:T',\n",
      "    y='links:Q'\n",
      ").properties\n",
      "50/9:\n",
      "import altair as alt\n",
      "alt.renderers.enable('notebook')\n",
      "\n",
      "link_appear = frl.groupby('link').timestamp.min().to_frame().resample('M', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "\n",
      "alt.Chart(link_appear).mark_bar().encode(\n",
      "    x='timestamp:T',\n",
      "    y='links:Q'\n",
      ").properties\n",
      "50/10:\n",
      "import altair as alt\n",
      "alt.renderers.enable('notebook')\n",
      "50/11:\n",
      "link_appear = frl.groupby('link').timestamp.min().to_frame().resample('M', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "\n",
      "alt.Chart(link_appear).mark_bar().encode(\n",
      "    x='timestamp:T',\n",
      "    y='links:Q'\n",
      ").properties\n",
      "50/12:\n",
      "link_appear = frl.groupby('link').timestamp.min().to_frame().resample('M', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "\n",
      "alt.Chart(link_appear).mark_bar().encode(\n",
      "    x='timestamp:T',\n",
      "    y='links:Q'\n",
      ")\n",
      "50/13:\n",
      "first_link_appear = frl.groupby('link').timestamp.min().to_frame().resample('D', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "first_link_appear['link_accum'] = first_link_appear.links.cumsum()\n",
      "first_link_appear['link_pct_change'] = first_link_appear.link_accum.pct_change()\n",
      "\n",
      "bar = alt.Chart(first_link_appear[(first_link_appear.timestamp>='2014-07-05') & (first_link_appear.timestamp<'2014-11-01')]).mark_bar().encode(\n",
      "    x='timestamp:T',\n",
      "    y='link_accum:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "line = alt.Chart(creation_dates_day[(creation_dates_day.timestamp>='2014-07-05') & (creation_dates_day.timestamp<'2014-11-01')]).mark_line(color='red').encode(\n",
      "    x='timestamp:T',\n",
      "    y='pages:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "alt.layer(\n",
      "    bar,\n",
      "    line\n",
      ").resolve_scale(\n",
      "    y='independent'\n",
      ").properties(title='Protective Edge Wikipedia pages count and external link apperances')\n",
      "50/14:\n",
      "creation_dates_day = creation_dates.resample('D', on='timestamp').size().reset_index().rename(columns={0:'pages_created'})\n",
      "creation_dates_day['pages'] = creation_dates_day.pages_created.cumsum()\n",
      "creation_dates_day['pages_pct'] = creation_dates_day.pages / creation_dates_day.pages.max()\n",
      "\n",
      "creation_dates_day\n",
      "50/15:\n",
      "first_link_appear = frl.groupby('link').timestamp.min().to_frame().resample('D', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "first_link_appear['link_accum'] = first_link_appear.links.cumsum()\n",
      "first_link_appear['link_pct'] = first_link_appear.link_accum / first_link_appear.link_accum.max()\n",
      "\n",
      "\n",
      "bar = alt.Chart(first_link_appear[(first_link_appear.timestamp>='2014-07-05') & (first_link_appear.timestamp<'2018-11-01')]).mark_line(color='blue').encode(\n",
      "    x='timestamp:T',\n",
      "    y='link_pct:Q',\n",
      "    tooltip = 'timestamp'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "line = alt.Chart(creation_dates_day[(creation_dates_day.timestamp>='2014-07-05') & (creation_dates_day.timestamp<'2018-11-01')]).mark_line(color='red').encode(\n",
      "    x='timestamp:T',\n",
      "    y='pages_pct:Q',\n",
      "    tooltip = 'timestamp'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "alt.layer(\n",
      "    bar,\n",
      "    line\n",
      ").interactive().properties(title='Protective Edge Wikipedia pages count and external link apperances (pct from max number for each of them)')\n",
      "50/16:\n",
      "pd.set_option('display.max_colwidth', -1)\n",
      "frl[frl.link.str.contains('archive.org')].drop_duplicates(subset=['lang', 'link'])\n",
      "50/17: frl.dtypes\n",
      "50/18:\n",
      "import requests\n",
      "\n",
      "params = { 'link': 'http://web.archive.org/web/20090626014827/http://www.idf.il/1133-20871-he/Dover.aspx',\n",
      "           'timestamp': '20140708',\n",
      "         }\n",
      "requests.get('https://archive.org/wayback/available', params=params).json()\n",
      "50/19:\n",
      "import requests\n",
      "\n",
      "params = { 'link': 'http://web.archive.org/web/20090626014827/http://www.idf.il/1133-20871-he/Dover.aspx',\n",
      "           'timestamp': '20140708',\n",
      "         }\n",
      "requests.get('https://archive.org/wayback/available', params=params).text\n",
      "50/20:\n",
      "import requests\n",
      "\n",
      "params = { 'link': 'http://web.archive.org/web/20090626014827/http://www.idf.il/1133-20871-he/Dover.aspx',\n",
      "           'timestamp': '20140708',\n",
      "         }\n",
      "requests.get('https://archive.org/wayback/available', params=params).text()\n",
      "50/21:\n",
      "import requests\n",
      "\n",
      "params = { 'url': 'http://web.archive.org/web/20090626014827/http://www.idf.il/1133-20871-he/Dover.aspx',\n",
      "           'timestamp': '20140708',\n",
      "         }\n",
      "requests.get('https://archive.org/wayback/available', params=params).json()\n",
      "50/22: frl.head()\n",
      "50/23:\n",
      "import requests\n",
      "\n",
      "params = { 'url': 'http://www.jewishpress.com/news/breaking-news/idfs-operation-protective-edge-begins-against-gaza/2014/07/08/',\n",
      "           'timestamp': '20140708',\n",
      "         }\n",
      "requests.get('https://archive.org/wayback/available', params=params).json()\n",
      "50/24: frl.timestamp.head()\n",
      "50/25: frl.timestamp.head().dt.strftime('%Y%m%d%H%M%S')\n",
      "50/26: frl.head().apply(lambda x: print(x['link'], link['timestamp']))\n",
      "50/27: frl.head().apply(lambda x: print(x[['link']], link['timestamp']))\n",
      "50/28: frl.head().apply(lambda x: print(x))\n",
      "50/29: frl.head().apply(lambda x: print(x), axis=1)\n",
      "50/30: frl.head().apply(lambda x: (x['link'], link['timestamp']), axis=1)\n",
      "50/31: frl.head().apply(lambda x: print(x['link'], link['timestamp']), axis=1)\n",
      "50/32: frl.head().apply(lambda x: print(x), axis=1)\n",
      "50/33: frl.head().apply(lambda x: print(x['link'], link['timestamp']), axis=1)\n",
      "50/34: frl.head()\n",
      "50/35: frl.head().apply(lambda x: print(x.loc['link'], link['timestamp']), axis=1)\n",
      "50/36: frl.head().apply(lambda x: print(x.loc['link'], link.loc['timestamp']), axis=1)\n",
      "50/37: frl.head().apply(lambda x: print(x.iloc[0], link.iloc[1]), axis=1)\n",
      "50/38: frl.head().apply(lambda x: print(x.loc['link'], x.loc['timestamp']), axis=1)\n",
      "50/39:\n",
      "(\n",
      "    frl\n",
      "    .head()\n",
      "    .assign(ts = lambda x: x.timestamp.dt.strftime('%Y%m%d%H%M%S'))\n",
      "    .apply(lambda x: print(x.loc['link'], x.loc['ts']), axis=1)\n",
      ")\n",
      "50/40:\n",
      "(\n",
      "    frl\n",
      "    .head()\n",
      "    .assign(ts = lambda x: x.timestamp.dt.strftime('%Y%m%d%H%M%S'))\n",
      "    .apply(lambda x: requests.get('https://archive.org/wayback/available', params=('url': x.link, 'timestamp': x.ts)).json(), axis=1)\n",
      ")\n",
      "50/41:\n",
      "(\n",
      "    frl\n",
      "    .head()\n",
      "    .assign(ts = lambda x: x.timestamp.dt.strftime('%Y%m%d%H%M%S'))\n",
      "    .apply(lambda x: requests.get('https://archive.org/wayback/available', params={'url': x.link, 'timestamp': x.ts}).json(), axis=1)\n",
      ")\n",
      "50/42: frl.groupby.timetamp.min().head()\n",
      "50/43: frl.groupby.timestamp.min().head()\n",
      "50/44: frl.groupby('link').timestamp.min().head()\n",
      "50/45: frl.groupby('link').timestamp.min().to_frame()\n",
      "50/46: frl.groupby('link').timestamp.min().to_frame().reset_index()\n",
      "50/47: links_earliest = frl.groupby('link').timestamp.min().to_frame().reset_index()\n",
      "50/48:\n",
      "links_earliest['wb_avail'] = (\n",
      "    links_earliest\n",
      "    .assign(ts = lambda x: x.timestamp.dt.strftime('%Y%m%d%H%M%S'))\n",
      "    .apply(lambda x: requests.get('https://archive.org/wayback/available', params={'url': x.link, 'timestamp': x.ts}).json(), axis=1)\n",
      ")\n",
      "50/49: links_earliest.head()\n",
      "50/50: links_earliest.head()\n",
      "50/51: links_earliest.head()\n",
      "50/52: links_earliest.to_pickle('data/links_wayback.pkl.gz', compression='gzip')\n",
      "50/53: wb = pd.read_pickle('data/links_wayback.pkl.gz', compression='gzip')\n",
      "50/54:\n",
      "wb = pd.read_pickle('data/links_wayback.pkl.gz', compression='gzip')\n",
      "wb.head()\n",
      "50/55: wb_exp = pd.concat([wb.drop(['wb_avail'], axis=1), wb['wb_avail'].apply(pd.Series)], axis=1)\n",
      "50/56: wb.head()\n",
      "50/57: wb_exp.head()\n",
      "50/58: wb_exp = wb['wb_avail'].apply(pd.Series)\n",
      "50/59: wb_exp.head()\n",
      "50/60:\n",
      "def flatten(d, parent_key='', sep='_'):\n",
      "    items = []\n",
      "    for k, v in d.items():\n",
      "        new_key = parent_key + sep + k if parent_key else k\n",
      "        if isinstance(v, collections.MutableMapping):\n",
      "            items.extend(flatten(v, new_key, sep=sep).items())\n",
      "        else:\n",
      "            items.append((new_key, v))\n",
      "    return dict(items)\n",
      "50/61: wb_exp = wb['wb_avail'].apply(lambda x: flatten(x).apply(pd.Series)\n",
      "50/62: wb_exp = wb['wb_avail'].apply(lambda x: flatten(x)).apply(pd.Series)\n",
      "50/63:\n",
      "import collections\n",
      "def flatten(d, parent_key='', sep='_'):\n",
      "    items = []\n",
      "    for k, v in d.items():\n",
      "        new_key = parent_key + sep + k if parent_key else k\n",
      "        if isinstance(v, collections.MutableMapping):\n",
      "            items.extend(flatten(v, new_key, sep=sep).items())\n",
      "        else:\n",
      "            items.append((new_key, v))\n",
      "    return dict(items)\n",
      "50/64: wb_exp = wb['wb_avail'].apply(lambda x: flatten(x)).apply(pd.Series)\n",
      "50/65: wb_exp.head()\n",
      "50/66: wb_exp.dropna().head()\n",
      "50/67: wb_exp.dropna().shape\n",
      "50/68: wb_exp.shape\n",
      "50/69: wb_exp.dropna().shape\n",
      "50/70: wb_exp.dropna().shape\n",
      "50/71: wb_exp.shape\n",
      "50/72: wb_exp.to_pickle('data/links_wayback_expanded.pkl.gz', compression='gzip')\n",
      "50/73: wb_exp = pd.read_pickle('data/links_wayback_expanded.pkl.gz', compression='gzip')\n",
      "50/74: wb_exp.head()\n",
      "50/75: wb_exp[wb_exp.url.str.contains('wayback')]\n",
      "50/76: wb_exp[wb_exp.url.str.contains('archive.org')]\n",
      "50/77: wb_exp.dropna().head()\n",
      "51/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "51/2: sns.__version__\n",
      "51/3:\n",
      "%%time\n",
      "df = pd.read_pickle('turf/data_df.pkl.gz', compression='gzip')\n",
      "51/4: df.dtypes\n",
      "51/5:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "51/6: sns.__version__\n",
      "51/7:\n",
      "times = pd.read_pickle('turf/time_stats.pkl.gz', compression='gzip')\n",
      "st = pd.read_pickle('turf/total_stats_no_multiindex_groups_ded_cuts.pkl.gz', compression='gzip')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/8: st.dtypes\n",
      "51/9: from itertools import product\n",
      "51/10: times.dtypes\n",
      "51/11:\n",
      "columns = ['elapsed_time',  'created_time', 'created_month', 'created_day']\n",
      "stats = ['nunique', 'min', 'max']\n",
      "new_cols = ['from_id'] + ['_'.join(x) for x in product(columns, stats)] + ['period_length']\n",
      "new_cols\n",
      "51/12: times.columns = new_cols\n",
      "51/13: st[['from_id', 'group_cat']].head()\n",
      "51/14: times = times.merge(st[['from_id', 'group_cat']].drop_duplicates(), how='left', on='from_id')\n",
      "51/15: day_in_seconds = 60*60*24\n",
      "51/16: times.groupby('group_cat').period_length.median()\n",
      "51/17: times.groupby('group_cat').created_day_nunique.median()\n",
      "51/18: times.groupby('group_cat').created_month_nunique.median()\n",
      "51/19: times['period_length'] = times.period_length/day_in_seconds\n",
      "51/20: times[times.period_length<850].hist(bins=30, column='period_length', by='group_cat', layout=(6,1), sharex=True, figsize=(10,15), grid=True)\n",
      "51/21:\n",
      "stats = ['nunique', 'min', 'max']\n",
      "\n",
      "tn = (df.loc[df.is_post==0, ['from_id', 'name', 'elapsed_time',  'created_time', 'created_month', 'created_day', 'created_day_of_week', 'created_hour',]]\n",
      "      #.dropna(subset=['post_id'])\n",
      "      .groupby(by=['from_id', 'name'])\n",
      "      .agg(stats)\n",
      "      .assign(period_length = lambda x: (x['created_time']['max'] - x['created_time']['min']).dt.total_seconds())\n",
      "      .reset_index()\n",
      "     )\n",
      "\n",
      "columns = ['elapsed_time',  'created_time', 'created_month', 'created_day']\n",
      "stats = ['nunique', 'min', 'max']\n",
      "new_cols = ['from_id', 'name'] + ['_'.join(x) for x in product(columns, stats)] + ['period_length']\n",
      "tn.columns = new_cols\n",
      "tn.head()\n",
      "51/22:\n",
      "tn = tn.merge(st[['from_id', 'group_cat']].drop_duplicates(), how='left', on='from_id')\n",
      "tn.head()\n",
      "51/23: df.dtypes\n",
      "51/24:\n",
      "%%time\n",
      "df = pd.read_pickle('turf/data_df.pkl.gz', compression='gzip')\n",
      "51/25: df.dtypes\n",
      "51/26:\n",
      "stats = ['nunique', 'min', 'max']\n",
      "\n",
      "tn = (df.loc[df.is_post==0, ['from_id', 'name', 'elapsed_time',  'created_time', 'created_month', 'created_day', 'created_day_of_week', 'created_hour',]]\n",
      "      #.dropna(subset=['post_id'])\n",
      "      .groupby(by=['from_id', 'name'])\n",
      "      .agg(stats)\n",
      "      .assign(period_length = lambda x: (x['created_time']['max'] - x['created_time']['min']).dt.total_seconds())\n",
      "      .reset_index()\n",
      "     )\n",
      "\n",
      "columns = ['elapsed_time',  'created_time', 'created_month', 'created_day']\n",
      "stats = ['nunique', 'min', 'max']\n",
      "new_cols = ['from_id', 'name'] + ['_'.join(x) for x in product(columns, stats)] + ['period_length']\n",
      "tn.columns = new_cols\n",
      "tn.head()\n",
      "51/27: tn = pd.read_pickle('turf/times_names.pkl.gz', compression='gzip')\n",
      "51/28: tn['period_length'] = tn.period_length/day_in_seconds\n",
      "51/29: tn.groupby(['group_cat']).period_length.median().to_frame().T\n",
      "51/30: tn.groupby(['name']).period_length.agg('mean')\n",
      "51/31:\n",
      "def get_heatmap(kind, field='period_length'):\n",
      "    mean_mk = tn.groupby(['name'])[field].agg(kind)\n",
      "    mean_group_cat = tn.groupby(['group_cat'])[field].agg(kind).to_frame().T\n",
      "    mean_group_cat.columns = mean_group_cat.columns.astype(str)\n",
      "    mean_group_cat.index=[kind]\n",
      "    mean_group_cat[kind] = tn[field].agg(kind)\n",
      "    mean_heatmap = (tn.groupby(['name', 'group_cat'])[field].agg(kind)\n",
      "                    .unstack())\n",
      "    mean_heatmap.columns = mean_heatmap.columns.astype(str)\n",
      "    mean_heatmap[kind] = mean_mk\n",
      "    mean_heatmap = mean_heatmap.sort_values(kind, ascending=False)\n",
      "    mean_heatmap.index = mean_heatmap.index.add_categories(kind)\n",
      "    mean_heatmap = mean_heatmap.append(mean_group_cat)\n",
      "#    mean_heatmap = mean_heatmap.rename(columns = {'Dedicated Polemicists': 'Dedicated Polemicists', 'Polemicists': 'Polemicists'})\n",
      "#    mean_heatmap = mean_heatmap[['One Timers', 'Fans', 'Basers', 'Campers', 'Polemicists', 'Dedicated Fans', 'Dedicated Basers',\n",
      "#           'Dedicated Campers', 'Dedicated Polemicists', kind]]\n",
      "    return mean_heatmap\n",
      "mean_heatmap = get_heatmap('mean')\n",
      "median_heatmap = get_heatmap('median')\n",
      "mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red')\n",
      "51/32: html_heatmap = mean_heatmap.style.background_gradient(cmap='viridis').highlight_null('red').render()\n",
      "51/33: times.hist(bins=30, column='created_day_nunique', by='group_cat', layout=(6,1), figsize=(10,15))\n",
      "51/34:\n",
      "fig, ax = plt.subplots(figsize=(20, 15))\n",
      "\n",
      "sns.boxplot(data=times, y='created_day_nunique', x='group_cat', ax=ax)\n",
      "51/35:\n",
      "fig, ax = plt.subplots(figsize=(20, 15))\n",
      "\n",
      "sns.boxplot(data=times, y='created_month_nunique', x='group_cat', ax=ax)\n",
      "ax.set(ylabel='Number of unique active months', xlabel='User Type')\n",
      "51/36:\n",
      "fig, ax = plt.subplots(figsize=(20, 15))\n",
      "\n",
      "sns.boxplot(data=times, y='created_month_nunique', x='group_cat', ax=ax)\n",
      "ax.set(ylabel='Number of Active Months', xlabel='User Type')\n",
      "51/37:\n",
      "fig, ax = plt.subplots(figsize=(30, 20))\n",
      "\n",
      "sns.boxplot(data=times, y='created_month_nunique', x='group_cat', ax=ax)\n",
      "ax.set(ylabel='Number of Active Months', xlabel='User Type')\n",
      "51/38:\n",
      "fig, ax = plt.subplots(figsize=(30, 20))\n",
      "\n",
      "sns.boxplot(data=times, y='created_month_nunique', x='group_cat', ax=ax)\n",
      "ax.set(ylabel='Number of active months', xlabel='User type')\n",
      "51/39:\n",
      "fig, ax = plt.subplots(figsize=(20, 15))\n",
      "\n",
      "sns.boxplot(data=times, y='created_month_nunique', x='group_cat', ax=ax)\n",
      "ax.set(ylabel='Number of active months', xlabel='User type')\n",
      "51/40:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('paper')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "51/41:\n",
      "fig, ax = plt.subplots(figsize=(20, 15))\n",
      "\n",
      "sns.boxplot(data=times, y='created_month_nunique', x='group_cat', ax=ax)\n",
      "ax.set(ylabel='Number of active months', xlabel='User type')\n",
      "51/42:\n",
      "fig, ax = plt.subplots(figsize=(10, 7))\n",
      "\n",
      "sns.boxplot(data=times, y='created_month_nunique', x='group_cat', ax=ax)\n",
      "ax.set(ylabel='Number of active months', xlabel='User type')\n",
      "51/43:\n",
      "fig, ax = plt.subplots(figsize=(20, 15))\n",
      "\n",
      "sns.boxplot(data=times, y='created_month_nunique', x='group_cat', ax=ax)\n",
      "ax.set(ylabel='Number of active months', xlabel='User type')\n",
      "51/44: sns.set_context('poster')\n",
      "51/45:\n",
      "fig, ax = plt.subplots(figsize=(20, 15))\n",
      "\n",
      "sns.boxplot(data=times, y='created_month_nunique', x='group_cat', ax=ax)\n",
      "ax.set(ylabel='Number of active months', xlabel='User type')\n",
      "51/46:\n",
      "fig, ax = plt.subplots(figsize=(30, 20))\n",
      "\n",
      "sns.boxplot(data=times, y='created_month_nunique', x='group_cat', ax=ax)\n",
      "ax.set(ylabel='Number of active months', xlabel='User type')\n",
      "51/47:\n",
      "fig, ax = plt.subplots(figsize=(28, 21))\n",
      "\n",
      "sns.boxplot(data=times, y='created_month_nunique', x='group_cat', ax=ax)\n",
      "ax.set(ylabel='Number of active months', xlabel='User type')\n",
      "51/48:\n",
      "fig, ax = plt.subplots(figsize=(20, 15))\n",
      "\n",
      "sns.boxplot(data=times, y='created_month_nunique', x='group_cat', ax=ax)\n",
      "ax.set(ylabel='Number of active months', xlabel='User type')\n",
      "51/49: fig.save_figure('active_months_boxplot.jpg', dpi=1200)\n",
      "51/50: fig.savefig('active_months_boxplot.jpg', dpi=1200)\n",
      "51/51:\n",
      "fig, ax = plt.subplots(figsize=(15, 12))\n",
      "\n",
      "sns.boxplot(data=times, y='created_month_nunique', x='group_cat', ax=ax)\n",
      "ax.set(ylabel='Number of active months', xlabel='User type')\n",
      "51/52: sns.set_context('paper')\n",
      "51/53:\n",
      "fig, ax = plt.subplots(figsize=(15, 12))\n",
      "\n",
      "sns.boxplot(data=times, y='created_month_nunique', x='group_cat', ax=ax)\n",
      "ax.set(ylabel='Number of active months', xlabel='User type')\n",
      "51/54: sns.set_context('poster')\n",
      "51/55: sns.set_context('poster')\n",
      "51/56:\n",
      "fig, ax = plt.subplots(figsize=(15, 12))\n",
      "\n",
      "sns.boxplot(data=times, y='created_month_nunique', x='group_cat', ax=ax)\n",
      "ax.set(ylabel='Number of active months', xlabel='User type')\n",
      "51/57: sns.set_context('talk')\n",
      "51/58:\n",
      "fig, ax = plt.subplots(figsize=(15, 12))\n",
      "\n",
      "sns.boxplot(data=times, y='created_month_nunique', x='group_cat', ax=ax)\n",
      "ax.set(ylabel='Number of active months', xlabel='User type')\n",
      "51/59:\n",
      "fig, ax = plt.subplots(figsize=(15, 12))\n",
      "\n",
      "sns.boxplot(data=times, y='created_month_nunique', x='group_cat', ax=ax, color='grey')\n",
      "ax.set(ylabel='Number of active months', xlabel='User type')\n",
      "51/60:\n",
      "fig, ax = plt.subplots(figsize=(15, 12))\n",
      "\n",
      "sns.boxplot(data=times, y='created_month_nunique', x='group_cat', ax=ax, color='grey', saturation=0.5)\n",
      "ax.set(ylabel='Number of active months', xlabel='User type')\n",
      "51/61:\n",
      "fig, ax = plt.subplots(figsize=(15, 12))\n",
      "\n",
      "sns.boxplot(data=times, y='created_month_nunique', x='group_cat', ax=ax, color='grey', saturation=0.1)\n",
      "ax.set(ylabel='Number of active months', xlabel='User type')\n",
      "51/62:\n",
      "fig, ax = plt.subplots(figsize=(15, 12))\n",
      "\n",
      "sns.boxplot(data=times, y='created_month_nunique', x='group_cat', ax=ax, color='lightgrey', saturation=0.1)\n",
      "ax.set(ylabel='Number of active months', xlabel='User type')\n",
      "51/63: fig.savefig('active_months_boxplot.jpg', dpi=1200)\n",
      "51/64:\n",
      "fig, ax = plt.subplots(figsize=(15, 12))\n",
      "\n",
      "sns.boxplot(data=times, y='created_month_nunique', x='group_cat', ax=ax, color='lightgrey')\n",
      "ax.set(ylabel='Number of active months', xlabel='User type')\n",
      "52/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "52/2:\n",
      "import altair as alt\n",
      "alt.renderers.enable('notebook')\n",
      "52/3:\n",
      "df = pd.read_pickle('turf/data_df.pkl.gz', compression='gzip')\n",
      "df.dtypes\n",
      "52/4:\n",
      "sdf = pd.read_pickle('turf/shared_df.pkl.gz', compression='gzip')\n",
      "sdf.dtypes\n",
      "52/5: st = pd.read_pickle('turf/total_stats_no_multiindex_groups_ded_cuts.pkl.gz', compression='gzip')\n",
      "52/6: st.dtypes\n",
      "52/7: tn = pd.read_pickle('turf/times_names.pkl.gz', compression='gzip')\n",
      "52/8: tn['period_length'] = tn.period_length/day_in_seconds\n",
      "52/9: tn = pd.read_pickle('turf/times_names.pkl.gz', compression='gzip')\n",
      "52/10: tn['period_length'] = tn.period_length/day_in_seconds\n",
      "52/11:\n",
      "import numpy as np\n",
      "import holoviews as hv\n",
      "import datashader as ds\n",
      "from holoviews.operation.datashader import aggregate, datashade, shade, dynspread\n",
      "from holoviews.operation import decimate\n",
      "hv.extension('bokeh','matplotlib')\n",
      "decimate.max_samples=1000\n",
      "dynspread.max_px=20\n",
      "dynspread.threshold=0.5\n",
      "52/12: ds.__version__\n",
      "52/13:\n",
      "points = hv.Points(embedding, label=\"Polemicists & Dedicated UMAP embedding plot (datashaded)\")\n",
      "\n",
      "%output size=300\n",
      "%opts RGB [bgcolor=\"black\" show_grid=False xaxis=None yaxis=None]\n",
      "from colorcet import fire\n",
      "datashade.cmap=fire[50:]\n",
      "\n",
      "dynspread(datashade(points))\n",
      "53/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "53/2:\n",
      "%%time\n",
      "df = pd.read_pickle('turf/data_df.pkl.gz', compression='gzip')\n",
      "53/3: df.dtypes\n",
      "53/4: st = pd.read_pickle('turf/total_stats_no_multiindex.pkl.gz', compression='gzip')\n",
      "53/5:\n",
      "# OLD STUFF\n",
      "#save(st, 'turf/total_stats.pkl.gz')\n",
      "#st = pd.read_pickle( 'turf/total_stats.pkl.gz', compression='gzip')\n",
      "53/6: st.head()\n",
      "53/7: st.shape\n",
      "53/8: df[df.is_post==0].shape\n",
      "53/9: print ('Total number of comments: %d' % st.id.sum())\n",
      "53/10: print ('Total number of distinct users (commenters): %d' % st.from_id.nunique())\n",
      "53/11:\n",
      "group_lang = df.groupby(['name', 'group_cat', 'langdetect']).size().reset_index().rename(columns = {0: 'comment_count'})\n",
      "group_lang['langdetect'] = group_lang.langdetect.cat.add_categories(['other'])\n",
      "group_lang.loc[group_lang.langdetect=='cy', 'langdetect'] = 'ru'\n",
      "top_lang = group_lang.groupby('langdetect').comment_count.sum().sort_values(ascending=False).head(9).index\n",
      "group_lang.loc[~group_lang.langdetect.isin(top_lang), 'langdetect'] = 'other'\n",
      "group_lang = group_lang.groupby(['name', 'group_cat', 'langdetect']).comment_count.sum().reset_index()\n",
      "\n",
      "group_lang.langdetect.value_counts()\n",
      "53/12: df.head()\n",
      "53/13: df.dtypes\n",
      "53/14: st = pd.read_pickle('turf/total_stats_no_multiindex_groups.pkl.gz', compression='gzip')\n",
      "53/15:\n",
      "labels = ('One Timers', 'Fans', 'Polemicists', 'Basers', 'Campers', 'Dedicated')\n",
      "sets = [set(one_timers.index), \n",
      "        set(fans_users.from_id.values), \n",
      "        set(polem.from_id.values), \n",
      "        set(base.from_id.values), \n",
      "        set(campers.from_id.values), \n",
      "        set(dedicated.from_id.values)]\n",
      "#df = df.drop('group_cat', axis=1)\n",
      "for l, s in zip(labels, sets):\n",
      "    df.loc[df.from_id.isin(s), 'group_cat'] = l\n",
      "\n",
      "df['group_cat'] = pd.Categorical(df.group_cat)\n",
      "df.group_cat.value_counts()/df.shape[0]\n",
      "53/16:\n",
      "for l, s in zip(labels, sets):\n",
      "    st.loc[st.from_id.isin(s), 'group_cat'] = l\n",
      "st['group_cat'] = pd.Categorical(st.group_cat)\n",
      "st.groupby('group_cat').id.sum()/df.shape[0]\n",
      "53/17: st = pd.read_pickle('turf/total_stats_no_multiindex_groups.pkl.gz', compression='gzip')\n",
      "53/18: st = pd.read_pickle('turf/total_stats_no_multiindex.pkl.gz', compression='gzip')\n",
      "53/19:\n",
      "# OLD STUFF\n",
      "#save(st, 'turf/total_stats.pkl.gz')\n",
      "#st = pd.read_pickle( 'turf/total_stats.pkl.gz', compression='gzip')\n",
      "53/20: st.head()\n",
      "53/21: st.shape\n",
      "53/22: df[df.is_post==0].shape\n",
      "53/23: print ('Total number of comments: %d' % st.id.sum())\n",
      "53/24: print ('Total number of distinct users (commenters): %d' % st.from_id.nunique())\n",
      "53/25:\n",
      "counts = st.groupby('from_id').id.sum()\n",
      "one_timers = counts[counts==1]\n",
      "\n",
      "print ('Total number of one-timers: %d' % one_timers.shape[0])\n",
      "print ('Ratio of one timers from all users: %.2f' % (one_timers.shape[0] / st.from_id.nunique()))\n",
      "53/26: mat = st.pivot(index='from_id', columns='name', values='id')\n",
      "53/27: mat[(mat.count(axis=1)==1)].shape[0]\n",
      "53/28: mat.loc[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu==1), 'Benjamin_Netanyahu'].shape[0]\n",
      "53/29:\n",
      "print ('Ratio of Bibi from one-timers: %.2f' % (mat[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu==1)].shape[0] \n",
      " / counts[counts==1].shape[0]))\n",
      "53/30: print ('Ratio of users that commented 10 times and up: %.2f' % (counts[counts>=10].shape[0]/st.from_id.nunique()))\n",
      "53/31: (st.reset_index().groupby('from_id').name.nunique().value_counts().head(20)/st.reset_index().from_id.nunique()).plot(kind='bar')\n",
      "53/32: (st.reset_index().groupby('from_id').id.sum().value_counts().head(20)/st.reset_index().from_id.nunique()).plot(kind='bar')\n",
      "53/33:\n",
      "fans_users = (st.reset_index()\n",
      " .loc[st.groupby('from_id').name.transform('nunique')==1, ['from_id', 'name', 'party']])\n",
      "53/34: fans = ( fans_users.name.value_counts())\n",
      "53/35: fans.head(10)\n",
      "53/36:\n",
      "print ('Total number of fans: %d' % fans.sum())\n",
      "print ('Ratio of fans from all users: %.2f' % (fans.sum() / st.from_id.nunique()))\n",
      "print ('Ratio of one-timers from fans: %.2f' % (counts[counts==1].shape[0] / fans.sum()))\n",
      "53/37:\n",
      "(pd.concat([fans, \n",
      "            fans/st.from_id.nunique(), \n",
      "            fans/st.groupby('name').from_id.nunique()],\n",
      "            axis=1, keys=['num_uniques', 'ratio_from_all_users', 'ratio_from_own_users'])\n",
      "             .sort_values(by='ratio_from_own_users', ascending=False)).head(10)\n",
      "53/38:\n",
      "mkdet = pd.read_csv('turf/mk_details.csv', sep='\\t')\n",
      "mkdet.head()\n",
      "53/39: df['party'] = df['name'].map(mkdet.set_index('folder_name')['party'])\n",
      "53/40: df.groupby(['party', 'langdetect']).size().unstack()\n",
      "53/41: st['party'] = st['name'].map(mkdet.set_index('folder_name')['party'])\n",
      "53/42:\n",
      "polem = (st.reset_index()\n",
      " .loc[st.groupby('from_id').name.transform('nunique')>1, ['from_id', 'name', 'party']])\n",
      "53/43: polem.head()\n",
      "53/44: polem.from_id.nunique()\n",
      "53/45: polem.party.value_counts()\n",
      "53/46: polem.groupby('party').from_id.nunique()\n",
      "53/47: base = polem[polem.groupby('from_id').party.transform('nunique')==1]\n",
      "53/48:\n",
      "print ('Total number of basers: %d' % base.from_id.nunique())\n",
      "print ('Ratio of basers from all users: %.2f' % (base.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of basers from Polemicists: %.2f' % (base.from_id.nunique() / polem.from_id.nunique()))\n",
      "53/49: st.party.unique()\n",
      "53/50:\n",
      "camps = { 'מרצ': 'opposition',\n",
      "          'יש עתיד': 'opposition',\n",
      "          'ליכוד': 'coalition',\n",
      "          'הבית היהודי בראשות נפתלי בנט': 'coalition',\n",
      "          'המחנה הציוני': 'opposition',\n",
      "          'הרשימה המשותפת (חד”ש, רע”מ, בל”ד, תע”ל)': 'opposition',\n",
      "          'כולנו בראשות משה כחלון': 'coalition',\n",
      "          'ישראל ביתנו': 'coalition' }\n",
      "53/51: st['camp'] = st['party'].map(camps)\n",
      "53/52: polem['camp'] = polem['party'].map(camps)\n",
      "53/53: polem.groupby('camp').from_id.nunique()\n",
      "53/54: campers = polem[polem.groupby('from_id').camp.transform('nunique')==1]\n",
      "53/55:\n",
      "print ('Total number of campers: %d' % campers.from_id.nunique())\n",
      "print ('Ratio of campers from all users: %.2f' % (campers.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of campers from polem: %.2f' % (campers.from_id.nunique() / polem.from_id.nunique()))\n",
      "print ('Coalition | Opposition ratio from campers: %.2f | %.2f' % \n",
      "       ((campers[campers.camp=='coalition'].from_id.nunique() / campers.from_id.nunique()),\n",
      "       (campers[campers.camp=='opposition'].from_id.nunique() / campers.from_id.nunique())))\n",
      "cg = st.groupby('camp')\n",
      "print ('Coalition | Opposition ratios from total users: %.2f | %.2f' % \n",
      "       ((cg.from_id.nunique()['coalition'] / st.from_id.nunique()),\n",
      "       ((cg.from_id.nunique()['opposition'] / st.from_id.nunique()))))\n",
      "\n",
      "print ('Coalition | Opposition total comments: %.2f | %.2f' % \n",
      "       ((cg.id.sum()['coalition'] / st.id.sum()),\n",
      "       ((cg.id.sum()['opposition'] / st.id.sum()))))\n",
      "53/56: top_commented = list(st.groupby('name').id.sum().sort_values(ascending=False).index)\n",
      "53/57:\n",
      "top_10_commented = top_commented[:10]\n",
      "top_10_commented\n",
      "53/58:\n",
      "#celebz =  polem[polem.groupby('from_id').name.transform(lambda x: x.isin(top_10_commented).all())]\n",
      "celebz = polem[polem.from_id.isin(mat[mat.loc[:, top_commented[10:]].isna().all(axis=1)].index)]\n",
      "53/59: num_celebiez_no_base_camp = len(set(celebz.from_id.drop_duplicates().values).difference(set(campers.from_id.drop_duplicates().values)).difference(set(base.from_id.drop_duplicates().values)))\n",
      "53/60:\n",
      "print ('Total number of celeb followers (celebies): %d' % celebz.from_id.nunique())\n",
      "print ('Ratio of celebies from all users: %.2f' % (celebz.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of celebies from polem: %.2f' % (celebz.from_id.nunique() / polem.from_id.nunique()))\n",
      "print ('Remove campers and base from celebies: %d' % num_celebiez_no_base_camp)\n",
      "53/61: top_commented[:10]\n",
      "53/62: fans.reindex(top_commented).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "53/63:\n",
      "only_bibi = mat.loc[:, top_commented[1:]].isna().all(axis=1)\n",
      "only_bibi[only_bibi].shape\n",
      "53/64:\n",
      "def get_accum_df(mat, order):\n",
      "    onlys = []\n",
      "    \n",
      "    for i, name in enumerate(order):\n",
      "        only = mat.loc[:, order[i+1:]].isna().all(axis=1)\n",
      "        onlys.append(only[only].shape[0])\n",
      "        \n",
      "    user_accum_df = pd.DataFrame({'mk': order, 'user_accum': onlys})\n",
      "    user_accum_df['party'] = user_accum_df['mk'].map(mkdet.set_index('folder_name')['party'])\n",
      "    user_accum_df['camp'] = user_accum_df['party'].map(camps)\n",
      "    user_accum_df['delta'] = user_accum_df.user_accum.diff()\n",
      "    unique_delta_ratio = (fans.reindex(top_commented) / user_accum_df.set_index('mk').delta).drop('Benjamin_Netanyahu')\n",
      "    user_accum_df['unique_delta_ratio'] = user_accum_df['mk'].map(unique_delta_ratio)\n",
      "    user_accum_df['delta_n'] = user_accum_df['delta'] / user_accum_df['delta'].max()\n",
      "    \n",
      "    return user_accum_df\n",
      "53/65:\n",
      "user_accum_df = get_accum_df(mat, top_commented)\n",
      "user_accum_df.head()\n",
      "53/66: user_accum_df.set_index('mk').user_accum.plot(kind='bar', figsize=(15,10), rot=75)\n",
      "53/67:\n",
      "import altair as alt\n",
      "alt.renderers.enable('notebook')\n",
      "53/68:\n",
      "def chart_user_accum(accum_df, order, order_name):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('user_accum',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'user_accum']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = f'User accumulation by {order_name} order - left to right, only users that comment inside the group until the current column')\n",
      "\n",
      "\n",
      "def chart_user_accum_delta(accum_df, order, order_name):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('delta',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'delta']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = f'User accumulation delta by {order_name} order - left to right, only users that comment inside the group until the current column')\n",
      "\n",
      "\n",
      "def chart_user_accum_unique_delta_ratio(accum_df, order, order_name):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('unique_delta_ratio',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'unique_delta_ratio']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = f'Page unique count divided by user_accum delta by {order_name} order (to measure how much a page is part of the group)')\n",
      "53/69:\n",
      "(chart_user_accum(user_accum_df, top_commented, 'Top Commented') \n",
      " & chart_user_accum_delta(user_accum_df, top_commented, 'Top Commented')\n",
      " & chart_user_accum_unique_delta_ratio(user_accum_df, top_commented, 'Top Commented')).properties(title='Top Commented')\n",
      "53/70: user_accum_df[['delta_n', 'unique_delta_ratio']].div(user_accum_df[['delta_n', 'unique_delta_ratio']].sum(axis=1), axis=0).plot(kind='bar', stacked=True, figsize=(15,10), rot=75, width=1.0)\n",
      "53/71:\n",
      "print ('First celebiez group: ', top_commented[:6])\n",
      "print ('Second celebiez group: ', top_commented[:15])\n",
      "53/72: top_mean_commented = list(df.loc[df.is_post==1].groupby('name').comment_count.mean().sort_values(ascending=False).index)\n",
      "53/73: df.loc[df.is_post==1].groupby('name').comment_count.mean().sort_values(ascending=False).head()\n",
      "53/74: fans.reindex(top_mean_commented).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "53/75: mean_com_accum_df = get_accum_df(mat, top_mean_commented)\n",
      "53/76:\n",
      "(chart_user_accum(mean_com_accum_df, top_mean_commented, 'Top Mean Commented') \n",
      " & chart_user_accum_delta(mean_com_accum_df, top_mean_commented, 'Top Mean Commented') \n",
      " & chart_user_accum_unique_delta_ratio(mean_com_accum_df, top_mean_commented, 'Top Mean Commented')).properties(title='Top Mean Commented')\n",
      "53/77: top_liked = list(df.loc[df.is_post==1].groupby('name').like_count.mean().sort_values(ascending=False).index)\n",
      "53/78: fans.reindex(top_liked).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "53/79: like_accum_df = get_accum_df(mat, top_liked)\n",
      "53/80:\n",
      "(chart_user_accum(like_accum_df, top_liked, 'Top Liked') \n",
      " & chart_user_accum_delta(like_accum_df, top_liked, 'Top Liked') \n",
      " & chart_user_accum_unique_delta_ratio(like_accum_df, top_liked, 'Top Liked')).properties(title='Top Liked')\n",
      "53/81: top_shared = list(df.loc[df.is_post==1].groupby('name').share_count.mean().sort_values(ascending=False).index)\n",
      "53/82: fans.reindex(top_shared).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "53/83:\n",
      "share_accum_df = get_accum_df(mat, top_shared)\n",
      "share_accum_df.head()\n",
      "53/84:\n",
      "(chart_user_accum(share_accum_df, top_shared, 'Top Shared') \n",
      " & chart_user_accum_delta(share_accum_df, top_shared, 'Top Shared') \n",
      " & chart_user_accum_unique_delta_ratio(share_accum_df, top_shared, 'Top Shared')).properties(title='Top Shared')\n",
      "53/85:\n",
      "ts = (df.loc[df.is_post==1].groupby('name').share_count.mean().sort_values(ascending=False))\n",
      "tmc = (df.loc[df.is_post==1].groupby('name').comment_count.mean().sort_values(ascending=False))\n",
      "tl = (df.loc[df.is_post==1].groupby('name').like_count.mean().sort_values(ascending=False))\n",
      "all_scores = pd.concat([tmc, tl, ts], axis=1)\n",
      "all_scores_div = all_scores.div(all_scores.sum(axis=0), axis=1)\n",
      "all_scores['total_score'] = all_scores['like_count'] + 1.1*all_scores['comment_count'] + 1.2*all_scores['share_count']\n",
      "all_scores_div['total_score'] = all_scores_div['like_count'] + 1.1*all_scores_div['comment_count'] + 1.2*all_scores_div['share_count']\n",
      "all_scores.sort_values(by='total_score', ascending=False).head(10)\n",
      "53/86: all_scores_div.sort_values(by='total_score', ascending=False).head(20)\n",
      "53/87:\n",
      "all_accum_df = get_accum_df(mat, list(all_scores_div.sort_values(by='total_score', ascending=False).index))\n",
      "(chart_user_accum(all_accum_df, list(all_scores_div.sort_values(by='total_score', ascending=False).index), 'All Score') \n",
      " & chart_user_accum_delta(all_accum_df, list(all_scores_div.sort_values(by='total_score', ascending=False).index), 'All Score') \n",
      " & chart_user_accum_unique_delta_ratio(all_accum_df, list(all_scores_div.sort_values(by='total_score', ascending=False).index), 'All Score')).properties(title='All Score')\n",
      "53/88: new_celebies = polem[polem.from_id.isin(mat[mat.loc[:, list(all_scores_div.sort_values(by='total_score', ascending=False).index)[12:]].isna().all(axis=1)].index)]\n",
      "53/89: new_celebies.from_id.nunique(), polem.from_id.nunique()\n",
      "53/90: user_counts = st.groupby('from_id').id.sum()\n",
      "53/91:\n",
      "perc = pd.qcut(user_counts, 20, duplicates='drop')\n",
      "perc.value_counts(sort=False)\n",
      "53/92:\n",
      "perc_bah = pd.cut(user_counts, [0,1,2,3,4,5,7,11,24,9359])\n",
      "(perc_bah.value_counts(sort=False))\n",
      "53/93: (perc_bah.value_counts(sort=False)/ user_counts.shape[0])\n",
      "53/94: (perc_bah.value_counts(sort=False)/ user_counts.shape[0]).plot(kind='bar',figsize=(10,7))\n",
      "53/95:\n",
      "dedicated = st[st.from_id.isin(user_counts[user_counts>24].index)]\n",
      "dedicated.from_id.nunique()\n",
      "53/96: st[st.from_id.isin(user_counts[user_counts>24].index)].id.sum() / st.id.sum()\n",
      "53/97: st[st.from_id.isin(user_counts[user_counts>11].index)].from_id.nunique() / st.from_id.nunique()\n",
      "53/98: st[st.from_id.isin(user_counts[user_counts>11].index)].id.sum() / st.id.sum()\n",
      "53/99:\n",
      "campers = campers[(~campers.from_id.isin(base.from_id))& (~campers.from_id.isin(dedicated.from_id))]\n",
      "campers.from_id.nunique()\n",
      "53/100:\n",
      "base = base[(~base.from_id.isin(dedicated.from_id))]\n",
      "base.from_id.nunique()\n",
      "53/101:\n",
      "polem = polem[(~polem.from_id.isin(base.from_id)) \n",
      "                            & (~polem.from_id.isin(campers.from_id))\n",
      "                            & (~polem.from_id.isin(dedicated.from_id))]\n",
      "polem.from_id.nunique()\n",
      "53/102: campers.from_id.nunique() + base.from_id.nunique() + polem.from_id.nunique()\n",
      "53/103:\n",
      "fans_users = fans_users[~fans_users.from_id.isin(one_timers.index)\n",
      "                        & (~fans_users.from_id.isin(dedicated.from_id))]\n",
      "fans_users.from_id.nunique()\n",
      "53/104: ded_count = dedicated.from_id.nunique()\n",
      "53/105:\n",
      "ded_fans_users = (dedicated.loc[st.groupby('from_id').name.transform('nunique')==1, ['from_id', 'name', 'party']])\n",
      "ded_fans_users.from_id.nunique()/ded_count, dedicated.from_id.nunique()/ded_count\n",
      "53/106:\n",
      "ded_polem = (dedicated.loc[st.groupby('from_id').name.transform('nunique')>1, ['from_id', 'name', 'party', 'camp']])\n",
      "ded_base = ded_polem[ded_polem.groupby('from_id').party.transform('nunique')==1]\n",
      "ded_campers = ded_polem[ded_polem.groupby('from_id').camp.transform('nunique')==1]\n",
      "ded_celebies = ded_polem[ded_polem.from_id.isin(mat[mat.loc[:, list(all_scores_div.sort_values(by='total_score', ascending=False).index)[12:]].isna().all(axis=1)].index)]\n",
      "dedicated.from_id.nunique(), ded_polem.from_id.nunique()/ded_count, ded_base.from_id.nunique()/ded_count, ded_campers.from_id.nunique()/ded_count, ded_celebies.from_id.nunique()/ded_count\n",
      "53/107:\n",
      "ded_campers = ded_campers[(~ded_campers.from_id.isin(ded_base.from_id))]\n",
      "ded_campers.from_id.nunique()\n",
      "53/108:\n",
      "ded_polem = ded_polem[(~ded_polem.from_id.isin(ded_base.from_id)) \n",
      "                                    & (~ded_polem.from_id.isin(ded_campers.from_id))]\n",
      "ded_polem.from_id.nunique()\n",
      "53/109:\n",
      "ded_celebies = ded_celebies[(~ded_celebies.from_id.isin(ded_polem.from_id)) \n",
      "                                    & (~ded_celebies.from_id.isin(ded_campers.from_id))\n",
      "                                    & (~ded_celebies.from_id.isin(ded_base.from_id))]\n",
      "ded_celebies.from_id.nunique()\n",
      "53/110: ded_campers.from_id.nunique() + ded_base.from_id.nunique() + ded_polem.from_id.nunique() + ded_fans_users.from_id.nunique() + ded_celebies.from_id.nunique()\n",
      "53/111:\n",
      "labels = ('Dedicated Fans', 'Dedicated Basers', 'Dedicated Campers', 'Dedicated Celebies', 'Dedicated Polemicists')\n",
      "ratios = (ded_fans_users.from_id.nunique()/ded_count, ded_base.from_id.nunique()/ded_count, ded_campers.from_id.nunique()/ded_count, ded_celebies.from_id.nunique()/ded_count, ded_polem.from_id.nunique()/ded_count)\n",
      "dict(zip(labels,[round(x, 2) for x in ratios]))\n",
      "53/112:\n",
      "labels = ('All', 'One Timers', 'Fans', 'Polemicists', 'Basers', 'Campers')\n",
      "sets = [set(st.from_id.values), set(one_timers.index), set(fans_users.from_id.values), set(polem.from_id.values), set(base.from_id.values), set(campers.from_id.values)]\n",
      "\n",
      "set_dict = dict(zip(labels, sets))\n",
      "53/113:\n",
      "inter_set_dict = {}\n",
      "for i in set_dict:\n",
      "    inter_set_dict[i] = {}\n",
      "    for j in set_dict:\n",
      "        inter_set_dict[i][j] = len(set_dict[i].intersection(set_dict[j]))\n",
      "53/114: pd.DataFrame(inter_set_dict).reindex(labels)\n",
      "53/115: pd.DataFrame(inter_set_dict).reindex(labels).to_csv('turf/group_intersections.csv')\n",
      "53/116: from matplotlib_venn import venn3\n",
      "53/117:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(one_timers.index), set(fans_users.from_id.values)], set_labels = ('All', 'One Timers', 'Fans', ), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.25)\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.20)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.20)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.09)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]-0.19)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.19)\n",
      "53/118: df[df.is_post==1].shape[0], df[df.is_post==1].comment_count.mean()\n",
      "53/119: df[df.is_post==1].groupby('name').comment_count.mean().sort_values().plot(kind='barh', figsize=(10,15))\n",
      "53/120: one_timers.shape\n",
      "53/121:\n",
      "one_timers_page = st[st.from_id.isin(one_timers.index)].groupby('name').size()\n",
      "one_timers_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "53/122:\n",
      "gfans = st[st.from_id.isin(fans_users.from_id)].groupby('from_id')\n",
      "gfans.id.sum().mean(), gfans.post_id.sum().mean(), gfans.id.sum().mean()/ gfans.post_id.sum().mean()\n",
      "53/123:\n",
      "fans_page = st[st.from_id.isin(fans_users.from_id)].groupby('name').size()\n",
      "fans_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "53/124:\n",
      "gbase = st[st.from_id.isin(base.from_id)].groupby('from_id')\n",
      "gbase.id.sum().mean(), gbase.post_id.sum().mean(), gbase.id.sum().mean()/ gbase.post_id.sum().mean()\n",
      "53/125:\n",
      "base_page = st[st.from_id.isin(base.from_id)].groupby('name').size()\n",
      "base_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "53/126:\n",
      "gcamp = st[st.from_id.isin(campers.from_id)].groupby('from_id')\n",
      "gcamp.id.sum().mean(), gcamp.post_id.sum().mean(), gcamp.id.sum().mean()/ gcamp.post_id.sum().mean()\n",
      "53/127: st[st.from_id.isin(campers.from_id)].id.sum()\n",
      "53/128:\n",
      "camp_page = st[st.from_id.isin(campers.from_id)].groupby('name').size()\n",
      "camp_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "53/129: st[(st.from_id.isin(polem.from_id))].id.sum()\n",
      "53/130:\n",
      "gpolem = st[(st.from_id.isin(polem.from_id))].groupby('from_id')\n",
      "gpolem.id.sum().mean(), gpolem.post_id.sum().mean(), gpolem.id.sum().mean()/gpolem.post_id.sum().mean()\n",
      "53/131:\n",
      "polem_page = st[st.from_id.isin(polem.from_id)].groupby('name').size()\n",
      "polem_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "53/132:\n",
      "gded = dedicated.groupby('from_id')\n",
      "gded.id.sum().mean(), gded.post_id.sum().mean(), gded.id.sum().mean()/ gded.post_id.sum().mean()\n",
      "53/133:\n",
      "ded_page = st[st.from_id.isin(dedicated.from_id)].groupby('name').size()\n",
      "ded_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "53/134:\n",
      "page_cats = pd.concat([one_timers_page, fans_page, base_page, camp_page, polem_page, ded_page], axis=1,\n",
      "          keys=['One-timers', 'Fans', 'Basers', 'Campers', 'Polemicists', 'Dedicated'])\n",
      "53/135: page_cats = page_cats.stack().reset_index().rename(columns={'level_1': 'Category', 0: 'user_count'})\n",
      "53/136:\n",
      "regular = alt.Chart(page_cats).mark_bar().encode(\n",
      "    alt.X('user_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('Category:N'),\n",
      "    tooltip = ['name', 'Category','user_count']\n",
      ").properties(height=1200, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('user_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "53/137:\n",
      "labels = ('One Timers', 'Fans', 'Polemicists', 'Basers', 'Campers', 'Dedicated')\n",
      "sets = [set(one_timers.index), \n",
      "        set(fans_users.from_id.values), \n",
      "        set(polem.from_id.values), \n",
      "        set(base.from_id.values), \n",
      "        set(campers.from_id.values), \n",
      "        set(dedicated.from_id.values)]\n",
      "#df = df.drop('group_cat', axis=1)\n",
      "for l, s in zip(labels, sets):\n",
      "    df.loc[df.from_id.isin(s), 'group_cat'] = l\n",
      "\n",
      "df['group_cat'] = pd.Categorical(df.group_cat)\n",
      "df.group_cat.value_counts()/df.shape[0]\n",
      "53/138:\n",
      "for l, s in zip(labels, sets):\n",
      "    st.loc[st.from_id.isin(s), 'group_cat'] = l\n",
      "st['group_cat'] = pd.Categorical(st.group_cat)\n",
      "st.groupby('group_cat').id.sum()/df.shape[0]\n",
      "53/139: st.to_pickle('turf/total_stats_no_multiindex_groups.pkl.gz', compression='gzip')\n",
      "53/140: df.group_cat.value_counts()/df.shape[0]\n",
      "53/141:\n",
      "labels = ('Dedicated Fans', 'Dedicated Basers', 'Dedicated Polemicists', 'Dedicated Campers', 'Dedicated Celebies')\n",
      "sets = [set(ded_fans_users.from_id.values), \n",
      "        set(ded_base.from_id.values), \n",
      "        set(ded_polem.from_id.values), \n",
      "        set(ded_campers.from_id.values),\n",
      "        set(ded_celebies.from_id.values),]\n",
      "\n",
      "for l, s in zip(labels, sets):\n",
      "    st.loc[st.from_id.isin(s), 'new_group_cat'] = l\n",
      "    \n",
      "st['new_group_cat'] = pd.Categorical(st.new_group_cat)\n",
      "st.groupby('new_group_cat').id.sum()/df.shape[0]\n",
      "53/142:\n",
      "st.new_group_cat = st.new_group_cat.combine_first(st.group_cat)\n",
      "st.groupby('group_cat').id.sum()/df.shape[0]\n",
      "53/143: st.to_pickle('turf/total_stats_no_multiindex_groups_ded_cuts.pkl.gz', compression='gzip')\n",
      "53/144: df.dtypes\n",
      "53/145:\n",
      "group_lang = df.groupby(['name', 'group_cat', 'langdetect']).size().reset_index().rename(columns = {0: 'comment_count'})\n",
      "group_lang['langdetect'] = group_lang.langdetect.cat.add_categories(['other'])\n",
      "group_lang.loc[group_lang.langdetect=='cy', 'langdetect'] = 'ru'\n",
      "top_lang = group_lang.groupby('langdetect').comment_count.sum().sort_values(ascending=False).head(9).index\n",
      "group_lang.loc[~group_lang.langdetect.isin(top_lang), 'langdetect'] = 'other'\n",
      "group_lang = group_lang.groupby(['name', 'group_cat', 'langdetect']).comment_count.sum().reset_index()\n",
      "\n",
      "group_lang.langdetect.value_counts()\n",
      "53/146: group_lang.head()\n",
      "53/147: group_lang.to_csv('turf/page_group_lang.csv')\n",
      "53/148: group_lang.to_csv('turf/page_group_lang.csv', index=False)\n",
      "53/149: group_lang.groupby('name', 'langdetect').comment_count.sum()\n",
      "53/150: group_lang.groupby(['name', 'langdetect']).comment_count.sum()\n",
      "53/151: group_lang.groupby(['name', 'langdetect']).comment_count.sum().unstack()\n",
      "53/152: group_lang.groupby(['name', 'langdetect']).comment_count.sum().unstack().fillna(0)\n",
      "53/153:\n",
      "name_lang = group_lang.groupby(['name', 'langdetect']).comment_count.sum().unstack().fillna(0)\n",
      "name_lang_ratio = name_lang.div(name_lang.sum())\n",
      "53/154: name_lang_ratio.head()\n",
      "53/155:\n",
      "name_lang = group_lang.groupby(['name', 'langdetect']).comment_count.sum().unstack().fillna(0)\n",
      "name_lang_ratio = name_lang.div(name_lang.sum(axis=1))\n",
      "53/156: name_lang_ratio.head()\n",
      "53/157: name_lang.sum(axis=1)\n",
      "53/158:\n",
      "name_lang = group_lang.groupby(['name', 'langdetect']).comment_count.sum().unstack().fillna(0)\n",
      "name_lang_ratio = name_lang.div(name_lang.sum(axis=1), axis=1)\n",
      "53/159: name_lang_ratio.head()\n",
      "53/160:\n",
      "name_lang = group_lang.groupby(['name', 'langdetect']).comment_count.sum().unstack().fillna(0)\n",
      "name_lang_ratio = name_lang.div(name_lang.sum(axis=1), axis=0)\n",
      "53/161: name_lang_ratio.head()\n",
      "53/162: name_lang_ratio.mean()\n",
      "53/163: name_lang_ratio[~'Benjamin_Netanyahu'].mean()\n",
      "53/164:\n",
      "name_lang = group_lang.groupby(['name', 'langdetect']).comment_count.sum().unstack().fillna(0)\n",
      "name_lang_ratio = name_lang.div(name_lang.sum(axis=1), axis=0).reset_index()\n",
      "53/165:\n",
      "name_lang = group_lang.groupby(['name', 'langdetect']).comment_count.sum().unstack().fillna(0)\n",
      "name_lang_ratio = name_lang.div(name_lang.sum(axis=1), axis=0)\n",
      "53/166: name_lang_ratio.head()\n",
      "53/167: name_lang_ratio.reset_index()\n",
      "53/168:\n",
      "name_lang_ratio.index = name_lang_ratio.index.astype(str)\n",
      "name_lang_ratio.reset_index()\n",
      "53/169: name_lang_ratio.index\n",
      "53/170:\n",
      "name_lang_ratio.index = name_lang_ratio.index.astype(str)\n",
      "name_lang_ratio.reset_index()\n",
      "53/171: name_lang_ratio.columns\n",
      "53/172: name_lang_ratio.columns = name_lang_ratio.columns.astype(str)\n",
      "53/173: name_lang_ratio[name_lang_ratio.name!='Benjamin_Netanyahu'].mean()\n",
      "53/174: name_lang_ratio.head()\n",
      "53/175:\n",
      "name_lang = group_lang.groupby(['name', 'langdetect']).comment_count.sum().unstack().fillna(0)\n",
      "name_lang_ratio = name_lang.div(name_lang.sum(axis=1), axis=0)\n",
      "name_lang_ratio.columns = name_lang_ratio.columns.astype(str)\n",
      "name_lang_ratio.index = name_lang_ratio.index.astype(str)\n",
      "name_lang_ratio = name_lang_ratio.reset_index()\n",
      "53/176: name_lang_ratio.head()\n",
      "53/177: name_lang_ratio[name_lang_ratio.name!='Benjamin_Netanyahu'].mean()\n",
      "53/178: name_lang_ratio[name_lang_ratio.name=='Benjamin_Netanyahu'].mean()\n",
      "53/179: name_lang_ratio[name_lang_ratio.name=='Benjamin_Netanyahu']\n",
      "53/180: name_lang_ratio[name_lang_ratio.name=='Benjamin_Netanyahu'].mean()\n",
      "53/181: name_lang_ratio[name_lang_ratio.name=='Benjamin_Netanyahu'].median()\n",
      "53/182: name_lang_ratio[name_lang_ratio.name!='Benjamin_Netanyahu'].median()\n",
      "53/183: name_lang_ratio[name_lang_ratio.name!='Benjamin_Netanyahu'].mean()\n",
      "53/184: name_lang_onet = group_lang[group_lang.group_cat=='One Timers'].groupby(['name', 'langdetect']).comment_count.sum().unstack().fillna(0)\n",
      "53/185:\n",
      "name_lang_onet = group_lang[group_lang.group_cat=='One Timers'].groupby(['name', 'langdetect']).comment_count.sum().unstack().fillna(0)\n",
      "name_lang_or = name_lang_onet.div(name_lang_onet.sum(axis=1), axis=0)\n",
      "name_lang_or.columns = name_lang_or.columns.astype(str)\n",
      "name_lang_or.index = name_lang_or.index.astype(str)\n",
      "name_lang_or = name_lang_or.reset_index()\n",
      "53/186: name_lang_ratio[name_lang_ratio.name=='Benjamin_Netanyahu'].mean()\n",
      "53/187: name_lang_or[name_lang_ratio.name=='Benjamin_Netanyahu'].mean()\n",
      "53/188: name_lang_or[name_lang_or.name=='Benjamin_Netanyahu'].mean()\n",
      "53/189: name_lang_fr[name_lang_fr.name=='Benjamin_Netanyahu'].mean()\n",
      "53/190:\n",
      "name_lang_f = group_lang[group_lang.group_cat=='Fans'].groupby(['name', 'langdetect']).comment_count.sum().unstack().fillna(0)\n",
      "name_lang_fr = name_lang_f.div(name_lang_f.sum(axis=1), axis=0)\n",
      "name_lang_fr.columns = name_lang_fr.columns.astype(str)\n",
      "name_lang_fr.index = name_lang_fr.index.astype(str)\n",
      "name_lang_fr = name_lang_fr.reset_index()\n",
      "53/191: name_lang_fr[name_lang_fr.name=='Benjamin_Netanyahu'].mean()\n",
      "52/14:\n",
      "nposts = df.post_id.nunique()\n",
      "npages = st.name.nunique()\n",
      "ncomments = st.id.sum()\n",
      "nusers = st.from_id.nunique()\n",
      "\n",
      "pd.DataFrame([{ 'No. Posts': nposts, 'No. Pages': npages, 'No. Comments': ncomments, 'No. Users': nusers}]).T\n",
      "52/15:\n",
      "groups = (st.groupby('new_group_cat')\n",
      "          .agg({'from_id': 'nunique', 'id': 'sum'})\\\n",
      "          .rename(columns={'from_id': 'users', 'id': 'comments'}))\n",
      "groups['users_ratio'] = groups.users / nusers\n",
      "groups['comments_ratio'] = groups.comments / ncomments\n",
      "groups['ratios_ratio'] = groups.comments_ratio / groups.users_ratio\n",
      "\n",
      "groups = groups[['users', 'users_ratio', 'comments', 'comments_ratio', 'ratios_ratio']]\n",
      "groups = groups.reindex(['One Timers', \n",
      "                         'Fans', 'Dedicated Fans', \n",
      "                         'Basers', 'Dedicated Basers', \n",
      "                         'Campers', 'Dedicated Campers',\n",
      "                         'Polemicists', 'Dedicated Polemicists',])\n",
      "groups\n",
      "52/16:\n",
      "med_comments = (st.groupby(['new_group_cat', 'from_id'])\n",
      "                 .id.sum().reset_index()\n",
      "                 .drop('from_id', axis=1).groupby('new_group_cat').median()\n",
      "                )\n",
      "med_comments = med_comments.reindex(['One Timers', \n",
      "                         'Fans', 'Dedicated Fans', \n",
      "                         'Basers', 'Dedicated Basers', \n",
      "                         'Campers', 'Dedicated Campers',\n",
      "                         'Polemicists', 'Dedicated Polemicists',])\n",
      "med_comments['id']\n",
      "52/17:\n",
      "page_groups = st.groupby(['name', 'new_group_cat']).id.sum().unstack().reindex(top_commented)\n",
      "page_groups = page_groups[['One Timers', \n",
      "                         'Fans', 'Dedicated Fans', \n",
      "                         'Basers', 'Dedicated Basers', \n",
      "                         'Campers', 'Dedicated Campers',\n",
      "                         'Polemicists', 'Dedicated Polemicists',]]\n",
      "page_groups_ratio = np.round(page_groups.div(page_groups.sum(axis=1), axis=0), 2)\n",
      "page_groups_ratio\n",
      "52/18: top_commented = list(st.groupby('name').id.sum().sort_values(ascending=False).index)\n",
      "52/19:\n",
      "page_groups = st.groupby(['name', 'new_group_cat']).id.sum().unstack().reindex(top_commented)\n",
      "page_groups = page_groups[['One Timers', \n",
      "                         'Fans', 'Dedicated Fans', \n",
      "                         'Basers', 'Dedicated Basers', \n",
      "                         'Campers', 'Dedicated Campers',\n",
      "                         'Polemicists', 'Dedicated Polemicists',]]\n",
      "page_groups_ratio = np.round(page_groups.div(page_groups.sum(axis=1), axis=0), 2)\n",
      "page_groups_ratio\n",
      "52/20: page_groups\n",
      "52/21: page_groups.astype(int)\n",
      "52/22: page_groups.fillna(0).astype(int)\n",
      "52/23: page_groups.fillna(0).astype(int).to_csv('turf/page_user_type_count.csv')\n",
      "53/192: df[df.langdetect=='so'].head(50).text\n",
      "53/193: df[df.langdetect=='id'].head(50).text\n",
      "53/194: df[df.langdetect=='so'].head(50).text\n",
      "53/195: df.iloc[24707]\n",
      "53/196: df.iloc[24707].text\n",
      "53/197: df[df.langdetect=='de'].head(50).text\n",
      "53/198: df[df.langdetect=='es'].head(50).text\n",
      "53/199: df[df.langdetect=='de'].head(50).text\n",
      "53/200:\n",
      "def get_group_lang(min_length=0):\n",
      "    group_lang = df[df.length>=min_len].groupby(['name', 'group_cat', 'langdetect']).size().reset_index().rename(columns = {0: 'comment_count'})\n",
      "    group_lang['langdetect'] = group_lang.langdetect.cat.add_categories(['other'])\n",
      "    group_lang.loc[group_lang.langdetect=='cy', 'langdetect'] = 'ru'\n",
      "    top_lang = group_lang.groupby('langdetect').comment_count.sum().sort_values(ascending=False).head(9).index\n",
      "    group_lang.loc[~group_lang.langdetect.isin(top_lang), 'langdetect'] = 'other'\n",
      "    group_lang = group_lang.groupby(['name', 'group_cat', 'langdetect']).comment_count.sum().reset_index()\n",
      "    return group_lang\n",
      "\n",
      "group_lang_50 = get_group_lang(50)\n",
      "53/201:\n",
      "def get_group_lang(min_len=0):\n",
      "    group_lang = df[df.length>=min_len].groupby(['name', 'group_cat', 'langdetect']).size().reset_index().rename(columns = {0: 'comment_count'})\n",
      "    group_lang['langdetect'] = group_lang.langdetect.cat.add_categories(['other'])\n",
      "    group_lang.loc[group_lang.langdetect=='cy', 'langdetect'] = 'ru'\n",
      "    top_lang = group_lang.groupby('langdetect').comment_count.sum().sort_values(ascending=False).head(9).index\n",
      "    group_lang.loc[~group_lang.langdetect.isin(top_lang), 'langdetect'] = 'other'\n",
      "    group_lang = group_lang.groupby(['name', 'group_cat', 'langdetect']).comment_count.sum().reset_index()\n",
      "    return group_lang\n",
      "\n",
      "group_lang_50 = get_group_lang(50)\n",
      "53/202: name_lang_ratio[name_lang_ratio.name!='Benjamin_Netanyahu'].mean()\n",
      "53/203: name_lang_50_r[name_lang_ratio.name!='Benjamin_Netanyahu'].mean()\n",
      "53/204:\n",
      "name_lang_50 = group_lang_50.groupby(['name', 'langdetect']).comment_count.sum().unstack().fillna(0)\n",
      "name_lang_50_r = name_lang_50.div(name_lang.sum(axis=1), axis=0)\n",
      "name_lang_50_r.columns = name_lang_50_r.columns.astype(str)\n",
      "name_lang_50_r.index = name_lang_50_r.index.astype(str)\n",
      "name_lang_50_r = name_lang_50_r.reset_index()\n",
      "53/205: name_lang_50_r[name_lang_ratio.name!='Benjamin_Netanyahu'].mean()\n",
      "53/206:\n",
      "name_lang_50 = group_lang_50.groupby(['name', 'langdetect']).comment_count.sum().unstack().fillna(0)\n",
      "name_lang_50_r = name_lang_50.div(name_lang_50.sum(axis=1), axis=0)\n",
      "name_lang_50_r.columns = name_lang_50_r.columns.astype(str)\n",
      "name_lang_50_r.index = name_lang_50_r.index.astype(str)\n",
      "name_lang_50_r = name_lang_50_r.reset_index()\n",
      "53/207: name_lang_50_r[name_lang_ratio.name!='Benjamin_Netanyahu'].mean()\n",
      "53/208: name_lang_50_r[name_lang_ratio.name=='Benjamin_Netanyahu'].mean()\n",
      "53/209: name_lang_50_r[name_lang_ratio.name!='Benjamin_Netanyahu'].median()\n",
      "53/210:\n",
      "def get_name_lang_ratios(group_lang, group_cat):\n",
      "    name_lang_onet = group_lang[group_lang.group_cat==group_cat].groupby(['name', 'langdetect']).comment_count.sum().unstack().fillna(0)\n",
      "    name_lang_or = name_lang_onet.div(name_lang_onet.sum(axis=1), axis=0)\n",
      "    name_lang_or.columns = name_lang_or.columns.astype(str)\n",
      "    name_lang_or.index = name_lang_or.index.astype(str)\n",
      "    name_lang_or = name_lang_or.reset_index()\n",
      "    return name_lang_or\n",
      "\n",
      "nlr_50_ot = get_name_lang_ratios(group_lang_50, 'One Timers')\n",
      "nlr_50_ot[nlr_50_ot.name=='Benjamin_Netanyahu']\n",
      "53/211:\n",
      "def get_name_lang_ratios(group_lang, group_cat):\n",
      "    name_lang_onet = group_lang[group_lang.group_cat==group_cat].groupby(['name', 'langdetect']).comment_count.sum().unstack().fillna(0)\n",
      "    name_lang_or = name_lang_onet.div(name_lang_onet.sum(axis=1), axis=0)\n",
      "    name_lang_or.columns = name_lang_or.columns.astype(str)\n",
      "    name_lang_or.index = name_lang_or.index.astype(str)\n",
      "    name_lang_or = name_lang_or.reset_index()\n",
      "    return name_lang_or\n",
      "\n",
      "nlr_50_ot = get_name_lang_ratios(group_lang_50, 'One Timers')\n",
      "nlr_50_ot[nlr_50_ot.name=='Benjamin_Netanyahu'].mean()\n",
      "53/212:\n",
      "nlr_50_f = get_name_lang_ratios(group_lang_50, 'One Timers')\n",
      "nlr_50_f[nlr_50_f.name=='Benjamin_Netanyahu'].mean()\n",
      "53/213:\n",
      "nlr_50_f = get_name_lang_ratios(group_lang_50, 'Fans')\n",
      "nlr_50_f[nlr_50_f.name=='Benjamin_Netanyahu'].mean()\n",
      "53/214: df[df.length>=50].shape[0] / df.shape[0]\n",
      "52/24: import os\n",
      "52/25: import h5py\n",
      "52/26:\n",
      "import umap\n",
      "if os.path.exists('turf/umap_mk.h5'):\n",
      "    with h5py.File('turf/umap_mk.h5', 'r') as hf:\n",
      "        embedding = hf['umap_mk'][:]\n",
      "else:\n",
      "    embedding = umap.UMAP().fit_transform(mat.loc[st[st.group_cat.str.contains('MK')].from_id.drop_duplicates().values].values)\n",
      "    with h5py.File('turf/umap_mk.h5', 'w') as hf:\n",
      "        hf.create_dataset(\"umap_mk\",  data=embedding)\n",
      "52/27: sns.set(style='white', rc={'figure.figsize':(12,8)})\n",
      "52/28: plt.scatter(embedding[:,0], embedding[:,1])\n",
      "52/29:\n",
      "import numpy as np\n",
      "import holoviews as hv\n",
      "import datashader as ds\n",
      "from holoviews.operation.datashader import aggregate, datashade, shade, dynspread\n",
      "from holoviews.operation import decimate\n",
      "hv.extension('bokeh','matplotlib')\n",
      "decimate.max_samples=1000\n",
      "dynspread.max_px=20\n",
      "dynspread.threshold=0.5\n",
      "52/30: ds.__version__\n",
      "52/31:\n",
      "points = hv.Points(embedding, label=\"Polemicists & Dedicated UMAP embedding plot (datashaded)\")\n",
      "\n",
      "%output size=300\n",
      "%opts RGB [bgcolor=\"black\" show_grid=False xaxis=None yaxis=None]\n",
      "from colorcet import fire\n",
      "datashade.cmap=fire[50:]\n",
      "\n",
      "dynspread(datashade(points))\n",
      "52/32: datashade(points)\n",
      "52/33:\n",
      "if os.path.exists('turf/umap_all.h5'):\n",
      "    with h5py.File('turf/umap_all.h5', 'r') as hf:\n",
      "        all_embedding = hf['umap_all'][:]\n",
      "else:\n",
      "    all_embedding = umap.UMAP().fit_transform(mat.values)\n",
      "    with h5py.File('turf/umap_all.h5', 'w') as hf:\n",
      "        hf.create_dataset(\"umap_all\",  data=embedding)\n",
      "52/34:\n",
      "all_points = hv.Points(all_embedding, label=\"All Commenters UMAP embedding plot (datashaded)\")\n",
      "\n",
      "%output size=300\n",
      "%opts RGB [bgcolor=\"black\" show_grid=False xaxis=None yaxis=None]\n",
      "from colorcet import fire\n",
      "datashade.cmap=fire[50:]\n",
      "\n",
      "dynspread(datashade(all_points))\n",
      "52/35: datashade(all_points)\n",
      "52/36: mat.head()\n",
      "52/37:\n",
      "points = hv.Points(embedding, label=\"Polemicists & Dedicated UMAP embedding plot (datashaded)\")\n",
      "\n",
      "%output size=1200\n",
      "%opts RGB [bgcolor=\"black\" show_grid=False xaxis=None yaxis=None]\n",
      "from colorcet import fire\n",
      "datashade.cmap=fire[50:]\n",
      "\n",
      "dynspread(datashade(points))\n",
      "52/38: datashade(points)\n",
      "52/39:\n",
      "if os.path.exists('turf/umap_all.h5'):\n",
      "    with h5py.File('turf/umap_all.h5', 'r') as hf:\n",
      "        all_embedding = hf['umap_all'][:]\n",
      "else:\n",
      "    all_embedding = umap.UMAP().fit_transform(mat.values)\n",
      "    with h5py.File('turf/umap_all.h5', 'w') as hf:\n",
      "        hf.create_dataset(\"umap_all\",  data=embedding)\n",
      "52/40: mat = st.pivot(index='from_id', columns='name', values='id').fillna(0)\n",
      "52/41: mat.head()\n",
      "52/42:\n",
      "if os.path.exists('turf/umap_all.h5'):\n",
      "    with h5py.File('turf/umap_all.h5', 'r') as hf:\n",
      "        all_embedding = hf['umap_all'][:]\n",
      "else:\n",
      "    all_embedding = umap.UMAP().fit_transform(mat.values)\n",
      "    with h5py.File('turf/umap_all.h5', 'w') as hf:\n",
      "        hf.create_dataset(\"umap_all\",  data=embedding)\n",
      "54/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "54/2:\n",
      "import altair as alt\n",
      "alt.renderers.enable('notebook')\n",
      "54/3:\n",
      "df = pd.read_pickle('turf/data_df.pkl.gz', compression='gzip')\n",
      "df.dtypes\n",
      "54/4:\n",
      "sdf = pd.read_pickle('turf/shared_df.pkl.gz', compression='gzip')\n",
      "sdf.dtypes\n",
      "54/5: st = pd.read_pickle('turf/total_stats_no_multiindex_groups_ded_cuts.pkl.gz', compression='gzip')\n",
      "54/6: st.dtypes\n",
      "54/7: tn = pd.read_pickle('turf/times_names.pkl.gz', compression='gzip')\n",
      "54/8: tn['period_length'] = tn.period_length/day_in_seconds\n",
      "54/9:\n",
      "if os.path.exists('turf/umap_all.h5'):\n",
      "    with h5py.File('turf/umap_all.h5', 'r') as hf:\n",
      "        all_embedding = hf['umap_all'][:]\n",
      "else:\n",
      "    all_embedding = umap.UMAP().fit_transform(mat.values)\n",
      "    with h5py.File('turf/umap_all.h5', 'w') as hf:\n",
      "        hf.create_dataset(\"umap_all\",  data=all_embedding)\n",
      "54/10: st.groupby('group_cat').from_id.nunique()\n",
      "54/11: mat.loc[st[st.group_cat.str.contains('MK')].from_id.drop_duplicates().values].shape\n",
      "54/12: mat = st.pivot(index='from_id', columns='name', values='id').fillna(0)\n",
      "54/13: mat.head()\n",
      "54/14: st.head()\n",
      "54/15: mat.loc[st[st.group_cat.str.contains('MK')].from_id.drop_duplicates().values].shape\n",
      "54/16: st.groupby('group_cat').from_id.nunique()\n",
      "54/17:\n",
      "if os.path.exists('turf/umap_all.h5'):\n",
      "    with h5py.File('turf/umap_all.h5', 'r') as hf:\n",
      "        all_embedding = hf['umap_all'][:]\n",
      "else:\n",
      "    all_embedding = umap.UMAP().fit_transform(mat.values)\n",
      "    with h5py.File('turf/umap_all.h5', 'w') as hf:\n",
      "        hf.create_dataset(\"umap_all\",  data=all_embedding)\n",
      "54/18: import os\n",
      "54/19: import h5py\n",
      "54/20:\n",
      "import numpy as np\n",
      "import holoviews as hv\n",
      "import datashader as ds\n",
      "from holoviews.operation.datashader import aggregate, datashade, shade, dynspread\n",
      "from holoviews.operation import decimate\n",
      "hv.extension('bokeh','matplotlib')\n",
      "decimate.max_samples=1000\n",
      "dynspread.max_px=20\n",
      "dynspread.threshold=0.5\n",
      "54/21: ds.__version__\n",
      "54/22:\n",
      "if os.path.exists('turf/umap_all.h5'):\n",
      "    with h5py.File('turf/umap_all.h5', 'r') as hf:\n",
      "        all_embedding = hf['umap_all'][:]\n",
      "else:\n",
      "    all_embedding = umap.UMAP().fit_transform(mat.values)\n",
      "    with h5py.File('turf/umap_all.h5', 'w') as hf:\n",
      "        hf.create_dataset(\"umap_all\",  data=all_embedding)\n",
      "54/23:\n",
      "all_points = hv.Points(all_embedding, label=\"All Commenters UMAP embedding plot (datashaded)\")\n",
      "\n",
      "%output size=300\n",
      "%opts RGB [bgcolor=\"black\" show_grid=False xaxis=None yaxis=None]\n",
      "from colorcet import fire\n",
      "datashade.cmap=fire[50:]\n",
      "\n",
      "dynspread(datashade(all_points))\n",
      "54/24: datashade(all_points)\n",
      "54/25:\n",
      "import umap\n",
      "if os.path.exists('turf/umap_mk.h5'):\n",
      "    with h5py.File('turf/umap_mk.h5', 'r') as hf:\n",
      "        embedding = hf['umap_mk'][:]\n",
      "else:\n",
      "    embedding = umap.UMAP().fit_transform(mat.loc[st[st.group_cat.str.contains('MK')].from_id.drop_duplicates().values].values)\n",
      "    with h5py.File('turf/umap_mk.h5', 'w') as hf:\n",
      "        hf.create_dataset(\"umap_mk\",  data=embedding)\n",
      "54/26: sns.set(style='white', rc={'figure.figsize':(12,8)})\n",
      "54/27: plt.scatter(embedding[:,0], embedding[:,1])\n",
      "54/28:\n",
      "import numpy as np\n",
      "import holoviews as hv\n",
      "import datashader as ds\n",
      "from holoviews.operation.datashader import aggregate, datashade, shade, dynspread\n",
      "from holoviews.operation import decimate\n",
      "hv.extension('bokeh','matplotlib')\n",
      "decimate.max_samples=1000\n",
      "dynspread.max_px=20\n",
      "dynspread.threshold=0.5\n",
      "54/29: ds.__version__\n",
      "54/30:\n",
      "points = hv.Points(embedding, label=\"Polemicists & Dedicated UMAP embedding plot (datashaded)\")\n",
      "\n",
      "%output size=300\n",
      "%opts RGB [bgcolor=\"black\" show_grid=False xaxis=None yaxis=None]\n",
      "from colorcet import fire\n",
      "datashade.cmap=fire[50:]\n",
      "\n",
      "dynspread(datashade(points))\n",
      "54/31: datashade(points)\n",
      "54/32:\n",
      "if os.path.exists('turf/umap_all.h5'):\n",
      "    with h5py.File('turf/umap_all.h5', 'r') as hf:\n",
      "        all_embedding = hf['umap_all'][:]\n",
      "else:\n",
      "    all_embedding = umap.UMAP().fit_transform(mat.values)\n",
      "    with h5py.File('turf/umap_all.h5', 'w') as hf:\n",
      "        hf.create_dataset(\"umap_all\",  data=all_embedding)\n",
      "54/33:\n",
      "all_points = hv.Points(all_embedding, label=\"All Commenters UMAP embedding plot (datashaded)\")\n",
      "\n",
      "%output size=300\n",
      "%opts RGB [bgcolor=\"black\" show_grid=False xaxis=None yaxis=None]\n",
      "from colorcet import fire\n",
      "datashade.cmap=fire[50:]\n",
      "\n",
      "dynspread(datashade(all_points))\n",
      "54/34: datashade(all_points)\n",
      "54/35: 1+2\n",
      "54/36:\n",
      "all_points = hv.Points(all_embedding, label=\"All Commenters UMAP embedding plot (datashaded)\")\n",
      "\n",
      "%output size=1500\n",
      "%opts RGB [bgcolor=\"black\" show_grid=False xaxis=None yaxis=None]\n",
      "from colorcet import fire\n",
      "datashade.cmap=fire[50:]\n",
      "\n",
      "dynspread(datashade(all_points))\n",
      "54/37:\n",
      "res = pd.DataFrame(all_embedding,columns=['x', 'y'])\n",
      "res = (pd.concat([mat.index.to_frame(index=False), res], axis=1)\n",
      "       .merge(st[['from_id', 'group_cat']], how='left', on='from_id')\n",
      "       .drop_duplicates().reset_index().drop('index', axis=1))\n",
      "res.head()\n",
      "54/38: res.loc[res.group_cat.str.startswith('Ded'), 'group_cat'] = 'Dedicated'\n",
      "54/39: res.head()\n",
      "54/40:\n",
      "kdims=['d1','d2']\n",
      "num_ks=res.group_cat.nunique()\n",
      "\n",
      "apd = {c: hv.Points(res.loc[res.group_cat==c, ['x', 'y']]) for c in res.group_cat.unique()}\n",
      "\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k')))\n",
      "\n",
      "apdspread\n",
      "54/41: {k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key}\n",
      "54/42:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "%%output size=1500\n",
      "\n",
      "from holoviews.streams import RangeXY\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k'), cmap=Sets1to3)) * \\\n",
      "    hv.util.Dynamic(aggregate(all_points, width=100, height=100, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.group_cat.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "54/43:\n",
      "topmk = mat.idxmax(axis=1).reset_index().drop('from_id', axis=1).rename(columns={0:'topmk'})['topmk']\n",
      "topmk.loc[topmk.isin(top_commented[8:])] = 'Other'\n",
      "54/44:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "from holoviews.streams import RangeXY\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(aptopmk, kdims='k'), aggregator=ds.count_cat('k'), cmap=Sets1to3)) * \\\n",
      "    hv.util.Dynamic(aggregate(all_points, width=100, height=100, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.topmk.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "54/45: datashade(all_points)\n",
      "54/46:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "%%output size=1500\n",
      "\n",
      "from holoviews.streams import RangeXY\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k'), cmap=Sets1to3)) * \\\n",
      "    hv.util.Dynamic(aggregate(all_points, width=100, height=100, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.group_cat.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "54/47: {k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key}\n",
      "54/48:\n",
      "topmk = mat.idxmax(axis=1).reset_index().drop('from_id', axis=1).rename(columns={0:'topmk'})['topmk']\n",
      "topmk.loc[topmk.isin(top_commented[8:])] = 'Other'\n",
      "54/49:\n",
      "all_points = hv.Points(all_embedding, label=\"All Commenters UMAP embedding plot (datashaded)\")\n",
      "\n",
      "%output size=300\n",
      "%opts RGB [bgcolor=\"black\" show_grid=False xaxis=None yaxis=None]\n",
      "from colorcet import fire\n",
      "datashade.cmap=fire[50:]\n",
      "\n",
      "dynspread(datashade(all_points))\n",
      "54/50: datashade(all_points)\n",
      "54/51:\n",
      "kdims=['d1','d2']\n",
      "num_ks=res.group_cat.nunique()\n",
      "\n",
      "apd = {c: hv.Points(res.loc[res.group_cat==c, ['x', 'y']]) for c in res.group_cat.unique()}\n",
      "\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k')))\n",
      "\n",
      "apdspread\n",
      "54/52:\n",
      "all_points = hv.Points(all_embedding, label=\"All Commenters UMAP embedding plot (datashaded)\")\n",
      "\n",
      "#%output size=300\n",
      "%opts RGB [bgcolor=\"black\" show_grid=False xaxis=None yaxis=None]\n",
      "from colorcet import fire\n",
      "datashade.cmap=fire[50:]\n",
      "\n",
      "dynspread(datashade(all_points)).opts(plot=dict(fig_size=1000))\n",
      "54/53: datashade(all_points).opts(plot=dict(fig_size=200))\n",
      "54/54:\n",
      "res = pd.DataFrame(all_embedding,columns=['x', 'y'])\n",
      "res = (pd.concat([mat.index.to_frame(index=False), res], axis=1)\n",
      "       .merge(st[['from_id', 'group_cat']], how='left', on='from_id')\n",
      "       .drop_duplicates().reset_index().drop('index', axis=1))\n",
      "res.head()\n",
      "54/55: 1+2\n",
      "54/56:\n",
      "res = pd.DataFrame(all_embedding,columns=['x', 'y'])\n",
      "res = (pd.concat([mat.index.to_frame(index=False), res], axis=1)\n",
      "       .merge(st[['from_id', 'group_cat']], how='left', on='from_id')\n",
      "       .drop_duplicates().reset_index().drop('index', axis=1))\n",
      "res.head()\n",
      "54/57: datashade(all_points).opts(plot=dict(fig_size=200))\n",
      "54/58:\n",
      "all_points = hv.Points(all_embedding, label=\"All Commenters UMAP embedding plot (datashaded)\")\n",
      "\n",
      "#%output size=300\n",
      "%opts RGB [bgcolor=\"black\" show_grid=False xaxis=None yaxis=None]\n",
      "from colorcet import fire\n",
      "datashade.cmap=fire[50:]\n",
      "\n",
      "dynspread(datashade(all_points)).opts(plot=dict(fig_size=1000))\n",
      "54/59:\n",
      "%output size=1500\n",
      "datashade(all_points)\n",
      "54/60:\n",
      "all_points = hv.Points(all_embedding, label=\"All Commenters UMAP embedding plot (datashaded)\")\n",
      "\n",
      "%output size=1500\n",
      "%opts RGB [bgcolor=\"black\" show_grid=False xaxis=None yaxis=None]\n",
      "from colorcet import fire\n",
      "datashade.cmap=fire[50:]\n",
      "\n",
      "dynspread(datashade(all_points))\n",
      "54/61: res.head()\n",
      "54/62:\n",
      "import numpy as np\n",
      "import holoviews as hv\n",
      "import datashader as ds\n",
      "from holoviews.operation.datashader import aggregate, datashade, shade, dynspread\n",
      "from holoviews.operation import decimate\n",
      "hv.extension('matplotlib')\n",
      "decimate.max_samples=1000\n",
      "dynspread.max_px=20\n",
      "dynspread.threshold=0.5\n",
      "54/63:\n",
      "import numpy as np\n",
      "import holoviews as hv\n",
      "import datashader as ds\n",
      "from holoviews.operation.datashader import aggregate, datashade, shade, dynspread\n",
      "from holoviews.operation import decimate\n",
      "#hv.extension('bokeh','matplotlib')\n",
      "hv.extension('matplotlib')\n",
      "\n",
      "decimate.max_samples=1000\n",
      "dynspread.max_px=20\n",
      "dynspread.threshold=0.5\n",
      "54/64: ds.__version__\n",
      "54/65:\n",
      "points = hv.Points(embedding, label=\"Polemicists & Dedicated UMAP embedding plot (datashaded)\")\n",
      "\n",
      "%output size=300\n",
      "%opts RGB [bgcolor=\"black\" show_grid=False xaxis=None yaxis=None]\n",
      "from colorcet import fire\n",
      "datashade.cmap=fire[50:]\n",
      "\n",
      "dynspread(datashade(points))\n",
      "54/66: datashade(points)\n",
      "54/67:\n",
      "%output size=1500\n",
      "%opts RGB [bgcolor=\"black\" show_grid=False xaxis=None yaxis=None]\n",
      "datashade(all_points)\n",
      "54/68:\n",
      "%output size=1500\n",
      "%opts RGB [bgcolor=\"black\" show_grid=False xaxis=None yaxis=None]\n",
      "datashade(all_points).opts(plot=dict(fig_size=1500))\n",
      "54/69:\n",
      "#%output size=1500\n",
      "%opts RGB [bgcolor=\"black\" show_grid=False xaxis=None yaxis=None]\n",
      "datashade(all_points).opts(plot=dict(fig_size=1500))\n",
      "54/70:\n",
      "res = pd.DataFrame(all_embedding,columns=['x', 'y'])\n",
      "res = (pd.concat([mat.index.to_frame(index=False), res], axis=1)\n",
      "       .merge(st[['from_id', 'group_cat']], how='left', on='from_id')\n",
      "       .drop_duplicates().reset_index().drop('index', axis=1))\n",
      "res.head()\n",
      "54/71: res.loc[res.group_cat.str.startswith('Ded'), 'group_cat'] = 'Dedicated'\n",
      "54/72: res.head()\n",
      "54/73:\n",
      "kdims=['d1','d2']\n",
      "num_ks=res.group_cat.nunique()\n",
      "\n",
      "apd = {c: hv.Points(res.loc[res.group_cat==c, ['x', 'y']]) for c in res.group_cat.unique()}\n",
      "\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k')))\n",
      "\n",
      "apdspread\n",
      "54/74:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "%%output size=1500\n",
      "\n",
      "from holoviews.streams import RangeXY\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k'), cmap=Sets1to3)) * \\\n",
      "    hv.util.Dynamic(aggregate(all_points, width=100, height=100, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "color_key = list(zip(sorted(res.group_cat.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "54/75:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "from holoviews.streams import RangeXY\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(aptopmk, kdims='k'), aggregator=ds.count_cat('k'), cmap=Sets1to3)) * \\\n",
      "    hv.util.Dynamic(aggregate(all_points, width=100, height=100, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "color_key = list(zip(sorted(res.topmk.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "54/76:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "%%output size=1500\n",
      "\n",
      "from holoviews.streams import RangeXY\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k'), cmap=Sets1to3)) * \\\n",
      "    hv.util.Dynamic(aggregate(all_points, width=100, height=100, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "color_key = list(zip(sorted(res.group_cat.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "54/77:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "%%output size=300\n",
      "\n",
      "from holoviews.streams import RangeXY\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k'), cmap=Sets1to3)) * \\\n",
      "    hv.util.Dynamic(aggregate(all_points, width=100, height=100, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "color_key = list(zip(sorted(res.group_cat.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "54/78:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "%%output size=300\n",
      "\n",
      "from holoviews.streams import RangeXY\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k'), cmap=Sets1to3)) * \\\n",
      "    hv.util.Dynamic(aggregate(all_points, width=100, height=100, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "color_key = list(zip(sorted(res.group_cat.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "(color_points * apdspread).opts(plot=dict(fig_size=1500))\n",
      "54/79:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "%%output size=300\n",
      "\n",
      "from holoviews.streams import RangeXY\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k'), cmap=Sets1to3)) * \\\n",
      "    hv.util.Dynamic(aggregate(all_points, width=100, height=100, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "color_key = list(zip(sorted(res.group_cat.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "(color_points * apdspread)\n",
      "54/80:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "%%output size=600\n",
      "\n",
      "from holoviews.streams import RangeXY\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k'), cmap=Sets1to3)) * \\\n",
      "    hv.util.Dynamic(aggregate(all_points, width=100, height=100, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "color_key = list(zip(sorted(res.group_cat.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "(color_points * apdspread)\n",
      "54/81: {k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key}\n",
      "54/82:\n",
      "topmk = mat.idxmax(axis=1).reset_index().drop('from_id', axis=1).rename(columns={0:'topmk'})['topmk']\n",
      "topmk.loc[topmk.isin(top_commented[8:])] = 'Other'\n",
      "54/83: top_commented = list(st.groupby('name').id.sum().sort_values(ascending=False).index)\n",
      "54/84:\n",
      "topmk = mat.idxmax(axis=1).reset_index().drop('from_id', axis=1).rename(columns={0:'topmk'})['topmk']\n",
      "topmk.loc[topmk.isin(top_commented[8:])] = 'Other'\n",
      "54/85:\n",
      "res = pd.concat([res, topmk], axis=1)\n",
      "res.head()\n",
      "54/86:\n",
      "kdims=['d1','d2']\n",
      "num_ks=res.topmk.nunique()\n",
      "\n",
      "aptopmk = {c: hv.Points(res.loc[res.topmk==c, ['x', 'y']]) for c in res.topmk.unique()}\n",
      "54/87:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "from holoviews.streams import RangeXY\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(aptopmk, kdims='k'), aggregator=ds.count_cat('k'), cmap=Sets1to3)) * \\\n",
      "    hv.util.Dynamic(aggregate(all_points, width=100, height=100, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "color_key = list(zip(sorted(res.topmk.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "54/88:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "%%output size=600\n",
      "\n",
      "from holoviews.streams import RangeXY\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(aptopmk, kdims='k'), aggregator=ds.count_cat('k'), cmap=Sets1to3)) * \\\n",
      "    hv.util.Dynamic(aggregate(all_points, width=100, height=100, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "color_key = list(zip(sorted(res.topmk.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "54/89:\n",
      "import numpy as np\n",
      "import holoviews as hv\n",
      "import datashader as ds\n",
      "from holoviews.operation.datashader import aggregate, datashade, shade, dynspread\n",
      "from holoviews.operation import decimate\n",
      "hv.extension('bokeh','matplotlib')\n",
      "#hv.extension('matplotlib')\n",
      "\n",
      "decimate.max_samples=1000\n",
      "dynspread.max_px=20\n",
      "dynspread.threshold=0.5\n",
      "54/90: ds.__version__\n",
      "54/91:\n",
      "kdims=['d1','d2']\n",
      "num_ks=res.group_cat.nunique()\n",
      "\n",
      "apd = {c: hv.Points(res.loc[res.group_cat==c, ['x', 'y']]) for c in res.group_cat.unique()}\n",
      "\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k')))\n",
      "\n",
      "apdspread\n",
      "54/92: all_points = hv.Points(all_embedding)#, label=\"All Commenters UMAP embedding plot (datashaded)\")\n",
      "54/93:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "%%output size=300\n",
      "\n",
      "from holoviews.streams import RangeXY\n",
      "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(apd, kdims='k'), aggregator=ds.count_cat('k'), cmap=Sets1to3)) * \\\n",
      "    hv.util.Dynamic(aggregate(all_points, width=100, height=100, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "color_key = list(zip(sorted(res.group_cat.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "(color_points * apdspread)\n",
      "54/94:\n",
      "%output size=300\n",
      "%opts RGB [bgcolor=\"black\" show_grid=False xaxis=None yaxis=None]\n",
      "from colorcet import fire\n",
      "datashade.cmap=fire[50:]\n",
      "\n",
      "dynspread(datashade(all_points))\n",
      "54/95:\n",
      "%%opts RGB [bgcolor=\"black\"]\n",
      "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
      "%%output size=300\n",
      "\n",
      "from holoviews.streams import RangeXY\n",
      "\n",
      "# definition copied here to ensure independent pan/zoom state for each dynamic plot\n",
      "apdspread = dynspread(datashade(hv.NdOverlay(aptopmk, kdims='k'), aggregator=ds.count_cat('k'), cmap=Sets1to3)) * \\\n",
      "    hv.util.Dynamic(aggregate(all_points, width=100, height=100, streams=[RangeXY]), operation=hv.QuadMesh)\n",
      "\n",
      "\n",
      "color_key = list(zip(sorted(res.topmk.unique()), Sets1to3[0:num_ks]))\n",
      "color_points = hv.NdOverlay({k: hv.Points([0,0], label=str(k)).options(color=v) for k, v in color_key})\n",
      "\n",
      "color_points * apdspread\n",
      "55/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "55/2:\n",
      "%%time\n",
      "df = pd.read_pickle('turf/data_df.pkl.gz', compression='gzip')\n",
      "55/3: st = pd.read_pickle('turf/total_stats_no_multiindex_groups.pkl.gz', compression='gzip')\n",
      "55/4:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "55/5:\n",
      "%%time\n",
      "df = pd.read_pickle('turf/data_df.pkl.gz', compression='gzip')\n",
      "55/6: df.dtypes\n",
      "55/7:\n",
      "sdf = pd.read_pickle('turf/shared_df.pkl.gz', compression='gzip')\n",
      "sdf.dtypes\n",
      "55/8: st = pd.read_pickle('turf/total_stats_no_multiindex.pkl.gz', compression='gzip')\n",
      "55/9:\n",
      "# OLD STUFF\n",
      "#save(st, 'turf/total_stats.pkl.gz')\n",
      "#st = pd.read_pickle( 'turf/total_stats.pkl.gz', compression='gzip')\n",
      "55/10: st.head()\n",
      "55/11: st.shape\n",
      "55/12: df[df.is_post==0].shape\n",
      "55/13: print ('Total number of comments: %d' % st.id.sum())\n",
      "55/14: print ('Total number of distinct users (commenters): %d' % st.from_id.nunique())\n",
      "55/15:\n",
      "counts = st.groupby('from_id').id.sum()\n",
      "one_timers = counts[counts==1]\n",
      "\n",
      "print ('Total number of one-timers: %d' % one_timers.shape[0])\n",
      "print ('Ratio of one timers from all users: %.2f' % (one_timers.shape[0] / st.from_id.nunique()))\n",
      "55/16: mat = st.pivot(index='from_id', columns='name', values='id')\n",
      "55/17: mat[(mat.count(axis=1)==1)].shape[0]\n",
      "55/18: mat.loc[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu==1), 'Benjamin_Netanyahu'].shape[0]\n",
      "55/19:\n",
      "print ('Ratio of Bibi from one-timers: %.2f' % (mat[(mat.count(axis=1)==1) & (mat.Benjamin_Netanyahu==1)].shape[0] \n",
      " / counts[counts==1].shape[0]))\n",
      "55/20: print ('Ratio of users that commented 10 times and up: %.2f' % (counts[counts>=10].shape[0]/st.from_id.nunique()))\n",
      "55/21: (st.reset_index().groupby('from_id').name.nunique().value_counts().head(20)/st.reset_index().from_id.nunique()).plot(kind='bar')\n",
      "55/22: (st.reset_index().groupby('from_id').id.sum().value_counts().head(20)/st.reset_index().from_id.nunique()).plot(kind='bar')\n",
      "55/23:\n",
      "fans_users = (st.reset_index()\n",
      " .loc[st.groupby('from_id').name.transform('nunique')==1, ['from_id', 'name', 'party']])\n",
      "55/24: fans = ( fans_users.name.value_counts())\n",
      "55/25: fans.head(10)\n",
      "55/26:\n",
      "print ('Total number of fans: %d' % fans.sum())\n",
      "print ('Ratio of fans from all users: %.2f' % (fans.sum() / st.from_id.nunique()))\n",
      "print ('Ratio of one-timers from fans: %.2f' % (counts[counts==1].shape[0] / fans.sum()))\n",
      "55/27:\n",
      "(pd.concat([fans, \n",
      "            fans/st.from_id.nunique(), \n",
      "            fans/st.groupby('name').from_id.nunique()],\n",
      "            axis=1, keys=['num_uniques', 'ratio_from_all_users', 'ratio_from_own_users'])\n",
      "             .sort_values(by='ratio_from_own_users', ascending=False)).head(10)\n",
      "55/28:\n",
      "mkdet = pd.read_csv('turf/mk_details.csv', sep='\\t')\n",
      "mkdet.head()\n",
      "55/29: df['party'] = df['name'].map(mkdet.set_index('folder_name')['party'])\n",
      "55/30: df.groupby(['party', 'langdetect']).size().unstack()\n",
      "55/31: st['party'] = st['name'].map(mkdet.set_index('folder_name')['party'])\n",
      "55/32:\n",
      "polem = (st.reset_index()\n",
      " .loc[st.groupby('from_id').name.transform('nunique')>1, ['from_id', 'name', 'party']])\n",
      "55/33: polem.head()\n",
      "55/34: polem.from_id.nunique()\n",
      "55/35: polem.party.value_counts()\n",
      "55/36: polem.groupby('party').from_id.nunique()\n",
      "55/37: base = polem[polem.groupby('from_id').party.transform('nunique')==1]\n",
      "55/38:\n",
      "print ('Total number of basers: %d' % base.from_id.nunique())\n",
      "print ('Ratio of basers from all users: %.2f' % (base.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of basers from Polemicists: %.2f' % (base.from_id.nunique() / polem.from_id.nunique()))\n",
      "55/39: st.party.unique()\n",
      "55/40:\n",
      "camps = { 'מרצ': 'opposition',\n",
      "          'יש עתיד': 'opposition',\n",
      "          'ליכוד': 'coalition',\n",
      "          'הבית היהודי בראשות נפתלי בנט': 'coalition',\n",
      "          'המחנה הציוני': 'opposition',\n",
      "          'הרשימה המשותפת (חד”ש, רע”מ, בל”ד, תע”ל)': 'opposition',\n",
      "          'כולנו בראשות משה כחלון': 'coalition',\n",
      "          'ישראל ביתנו': 'coalition' }\n",
      "55/41: st['camp'] = st['party'].map(camps)\n",
      "55/42: polem['camp'] = polem['party'].map(camps)\n",
      "55/43: polem.groupby('camp').from_id.nunique()\n",
      "55/44: campers = polem[polem.groupby('from_id').camp.transform('nunique')==1]\n",
      "55/45:\n",
      "print ('Total number of campers: %d' % campers.from_id.nunique())\n",
      "print ('Ratio of campers from all users: %.2f' % (campers.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of campers from polem: %.2f' % (campers.from_id.nunique() / polem.from_id.nunique()))\n",
      "print ('Coalition | Opposition ratio from campers: %.2f | %.2f' % \n",
      "       ((campers[campers.camp=='coalition'].from_id.nunique() / campers.from_id.nunique()),\n",
      "       (campers[campers.camp=='opposition'].from_id.nunique() / campers.from_id.nunique())))\n",
      "cg = st.groupby('camp')\n",
      "print ('Coalition | Opposition ratios from total users: %.2f | %.2f' % \n",
      "       ((cg.from_id.nunique()['coalition'] / st.from_id.nunique()),\n",
      "       ((cg.from_id.nunique()['opposition'] / st.from_id.nunique()))))\n",
      "\n",
      "print ('Coalition | Opposition total comments: %.2f | %.2f' % \n",
      "       ((cg.id.sum()['coalition'] / st.id.sum()),\n",
      "       ((cg.id.sum()['opposition'] / st.id.sum()))))\n",
      "55/46: top_commented = list(st.groupby('name').id.sum().sort_values(ascending=False).index)\n",
      "55/47:\n",
      "top_10_commented = top_commented[:10]\n",
      "top_10_commented\n",
      "55/48:\n",
      "#celebz =  polem[polem.groupby('from_id').name.transform(lambda x: x.isin(top_10_commented).all())]\n",
      "celebz = polem[polem.from_id.isin(mat[mat.loc[:, top_commented[10:]].isna().all(axis=1)].index)]\n",
      "55/49: num_celebiez_no_base_camp = len(set(celebz.from_id.drop_duplicates().values).difference(set(campers.from_id.drop_duplicates().values)).difference(set(base.from_id.drop_duplicates().values)))\n",
      "55/50:\n",
      "print ('Total number of celeb followers (celebies): %d' % celebz.from_id.nunique())\n",
      "print ('Ratio of celebies from all users: %.2f' % (celebz.from_id.nunique() / st.from_id.nunique()))\n",
      "print ('Ratio of celebies from polem: %.2f' % (celebz.from_id.nunique() / polem.from_id.nunique()))\n",
      "print ('Remove campers and base from celebies: %d' % num_celebiez_no_base_camp)\n",
      "55/51: top_commented[:10]\n",
      "55/52: fans.reindex(top_commented).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "55/53:\n",
      "only_bibi = mat.loc[:, top_commented[1:]].isna().all(axis=1)\n",
      "only_bibi[only_bibi].shape\n",
      "55/54:\n",
      "def get_accum_df(mat, order):\n",
      "    onlys = []\n",
      "    \n",
      "    for i, name in enumerate(order):\n",
      "        only = mat.loc[:, order[i+1:]].isna().all(axis=1)\n",
      "        onlys.append(only[only].shape[0])\n",
      "        \n",
      "    user_accum_df = pd.DataFrame({'mk': order, 'user_accum': onlys})\n",
      "    user_accum_df['party'] = user_accum_df['mk'].map(mkdet.set_index('folder_name')['party'])\n",
      "    user_accum_df['camp'] = user_accum_df['party'].map(camps)\n",
      "    user_accum_df['delta'] = user_accum_df.user_accum.diff()\n",
      "    unique_delta_ratio = (fans.reindex(top_commented) / user_accum_df.set_index('mk').delta).drop('Benjamin_Netanyahu')\n",
      "    user_accum_df['unique_delta_ratio'] = user_accum_df['mk'].map(unique_delta_ratio)\n",
      "    user_accum_df['delta_n'] = user_accum_df['delta'] / user_accum_df['delta'].max()\n",
      "    \n",
      "    return user_accum_df\n",
      "55/55:\n",
      "user_accum_df = get_accum_df(mat, top_commented)\n",
      "user_accum_df.head()\n",
      "55/56: user_accum_df.set_index('mk').user_accum.plot(kind='bar', figsize=(15,10), rot=75)\n",
      "55/57:\n",
      "import altair as alt\n",
      "alt.renderers.enable('notebook')\n",
      "55/58:\n",
      "def chart_user_accum(accum_df, order, order_name):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('user_accum',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'user_accum']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = f'User accumulation by {order_name} order - left to right, only users that comment inside the group until the current column')\n",
      "\n",
      "\n",
      "def chart_user_accum_delta(accum_df, order, order_name):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('delta',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'delta']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = f'User accumulation delta by {order_name} order - left to right, only users that comment inside the group until the current column')\n",
      "\n",
      "\n",
      "def chart_user_accum_unique_delta_ratio(accum_df, order, order_name):\n",
      "    return alt.Chart(accum_df.reset_index()).mark_bar().encode(\n",
      "        x = alt.X('mk', sort=order),\n",
      "        y = alt.Y('unique_delta_ratio',),\n",
      "        color = 'camp',\n",
      "        tooltip = ['index', 'mk', 'party', 'unique_delta_ratio']\n",
      "    ).properties(height=600,\n",
      "                 width=800,\n",
      "                 title = f'Page unique count divided by user_accum delta by {order_name} order (to measure how much a page is part of the group)')\n",
      "55/59:\n",
      "(chart_user_accum(user_accum_df, top_commented, 'Top Commented') \n",
      " & chart_user_accum_delta(user_accum_df, top_commented, 'Top Commented')\n",
      " & chart_user_accum_unique_delta_ratio(user_accum_df, top_commented, 'Top Commented')).properties(title='Top Commented')\n",
      "55/60: user_accum_df[['delta_n', 'unique_delta_ratio']].div(user_accum_df[['delta_n', 'unique_delta_ratio']].sum(axis=1), axis=0).plot(kind='bar', stacked=True, figsize=(15,10), rot=75, width=1.0)\n",
      "55/61:\n",
      "print ('First celebiez group: ', top_commented[:6])\n",
      "print ('Second celebiez group: ', top_commented[:15])\n",
      "55/62: top_mean_commented = list(df.loc[df.is_post==1].groupby('name').comment_count.mean().sort_values(ascending=False).index)\n",
      "55/63: df.loc[df.is_post==1].groupby('name').comment_count.mean().sort_values(ascending=False).head()\n",
      "55/64: fans.reindex(top_mean_commented).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "55/65: mean_com_accum_df = get_accum_df(mat, top_mean_commented)\n",
      "55/66:\n",
      "(chart_user_accum(mean_com_accum_df, top_mean_commented, 'Top Mean Commented') \n",
      " & chart_user_accum_delta(mean_com_accum_df, top_mean_commented, 'Top Mean Commented') \n",
      " & chart_user_accum_unique_delta_ratio(mean_com_accum_df, top_mean_commented, 'Top Mean Commented')).properties(title='Top Mean Commented')\n",
      "55/67: top_liked = list(df.loc[df.is_post==1].groupby('name').like_count.mean().sort_values(ascending=False).index)\n",
      "55/68: fans.reindex(top_liked).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "55/69: like_accum_df = get_accum_df(mat, top_liked)\n",
      "55/70:\n",
      "(chart_user_accum(like_accum_df, top_liked, 'Top Liked') \n",
      " & chart_user_accum_delta(like_accum_df, top_liked, 'Top Liked') \n",
      " & chart_user_accum_unique_delta_ratio(like_accum_df, top_liked, 'Top Liked')).properties(title='Top Liked')\n",
      "55/71: top_shared = list(df.loc[df.is_post==1].groupby('name').share_count.mean().sort_values(ascending=False).index)\n",
      "55/72: fans.reindex(top_shared).drop('Benjamin_Netanyahu').plot(kind='bar', figsize=(15,10), rot=75)\n",
      "55/73:\n",
      "share_accum_df = get_accum_df(mat, top_shared)\n",
      "share_accum_df.head()\n",
      "55/74:\n",
      "(chart_user_accum(share_accum_df, top_shared, 'Top Shared') \n",
      " & chart_user_accum_delta(share_accum_df, top_shared, 'Top Shared') \n",
      " & chart_user_accum_unique_delta_ratio(share_accum_df, top_shared, 'Top Shared')).properties(title='Top Shared')\n",
      "55/75:\n",
      "ts = (df.loc[df.is_post==1].groupby('name').share_count.mean().sort_values(ascending=False))\n",
      "tmc = (df.loc[df.is_post==1].groupby('name').comment_count.mean().sort_values(ascending=False))\n",
      "tl = (df.loc[df.is_post==1].groupby('name').like_count.mean().sort_values(ascending=False))\n",
      "all_scores = pd.concat([tmc, tl, ts], axis=1)\n",
      "all_scores_div = all_scores.div(all_scores.sum(axis=0), axis=1)\n",
      "all_scores['total_score'] = all_scores['like_count'] + 1.1*all_scores['comment_count'] + 1.2*all_scores['share_count']\n",
      "all_scores_div['total_score'] = all_scores_div['like_count'] + 1.1*all_scores_div['comment_count'] + 1.2*all_scores_div['share_count']\n",
      "all_scores.sort_values(by='total_score', ascending=False).head(10)\n",
      "55/76: all_scores_div.sort_values(by='total_score', ascending=False).head(20)\n",
      "55/77:\n",
      "all_accum_df = get_accum_df(mat, list(all_scores_div.sort_values(by='total_score', ascending=False).index))\n",
      "(chart_user_accum(all_accum_df, list(all_scores_div.sort_values(by='total_score', ascending=False).index), 'All Score') \n",
      " & chart_user_accum_delta(all_accum_df, list(all_scores_div.sort_values(by='total_score', ascending=False).index), 'All Score') \n",
      " & chart_user_accum_unique_delta_ratio(all_accum_df, list(all_scores_div.sort_values(by='total_score', ascending=False).index), 'All Score')).properties(title='All Score')\n",
      "55/78: new_celebies = polem[polem.from_id.isin(mat[mat.loc[:, list(all_scores_div.sort_values(by='total_score', ascending=False).index)[12:]].isna().all(axis=1)].index)]\n",
      "55/79: new_celebies.from_id.nunique(), polem.from_id.nunique()\n",
      "55/80: user_counts = st.groupby('from_id').id.sum()\n",
      "55/81:\n",
      "perc = pd.qcut(user_counts, 20, duplicates='drop')\n",
      "perc.value_counts(sort=False)\n",
      "55/82:\n",
      "perc_bah = pd.cut(user_counts, [0,1,2,3,4,5,7,11,24,9359])\n",
      "(perc_bah.value_counts(sort=False))\n",
      "55/83: (perc_bah.value_counts(sort=False)/ user_counts.shape[0])\n",
      "55/84: (perc_bah.value_counts(sort=False)/ user_counts.shape[0]).plot(kind='bar',figsize=(10,7))\n",
      "55/85:\n",
      "dedicated = st[st.from_id.isin(user_counts[user_counts>24].index)]\n",
      "dedicated.from_id.nunique()\n",
      "55/86: st[st.from_id.isin(user_counts[user_counts>24].index)].id.sum() / st.id.sum()\n",
      "55/87: st[st.from_id.isin(user_counts[user_counts>11].index)].from_id.nunique() / st.from_id.nunique()\n",
      "55/88: st[st.from_id.isin(user_counts[user_counts>11].index)].id.sum() / st.id.sum()\n",
      "55/89:\n",
      "campers = campers[(~campers.from_id.isin(base.from_id))& (~campers.from_id.isin(dedicated.from_id))]\n",
      "campers.from_id.nunique()\n",
      "55/90:\n",
      "base = base[(~base.from_id.isin(dedicated.from_id))]\n",
      "base.from_id.nunique()\n",
      "55/91:\n",
      "polem = polem[(~polem.from_id.isin(base.from_id)) \n",
      "                            & (~polem.from_id.isin(campers.from_id))\n",
      "                            & (~polem.from_id.isin(dedicated.from_id))]\n",
      "polem.from_id.nunique()\n",
      "55/92: campers.from_id.nunique() + base.from_id.nunique() + polem.from_id.nunique()\n",
      "55/93:\n",
      "fans_users = fans_users[~fans_users.from_id.isin(one_timers.index)\n",
      "                        & (~fans_users.from_id.isin(dedicated.from_id))]\n",
      "fans_users.from_id.nunique()\n",
      "55/94: ded_count = dedicated.from_id.nunique()\n",
      "55/95:\n",
      "ded_fans_users = (dedicated.loc[st.groupby('from_id').name.transform('nunique')==1, ['from_id', 'name', 'party']])\n",
      "ded_fans_users.from_id.nunique()/ded_count, dedicated.from_id.nunique()/ded_count\n",
      "55/96:\n",
      "ded_polem = (dedicated.loc[st.groupby('from_id').name.transform('nunique')>1, ['from_id', 'name', 'party', 'camp']])\n",
      "ded_base = ded_polem[ded_polem.groupby('from_id').party.transform('nunique')==1]\n",
      "ded_campers = ded_polem[ded_polem.groupby('from_id').camp.transform('nunique')==1]\n",
      "ded_celebies = ded_polem[ded_polem.from_id.isin(mat[mat.loc[:, list(all_scores_div.sort_values(by='total_score', ascending=False).index)[12:]].isna().all(axis=1)].index)]\n",
      "dedicated.from_id.nunique(), ded_polem.from_id.nunique()/ded_count, ded_base.from_id.nunique()/ded_count, ded_campers.from_id.nunique()/ded_count, ded_celebies.from_id.nunique()/ded_count\n",
      "55/97:\n",
      "regular = alt.Chart(page_cats[page_cats.name.isin(top_commented[:10])]).mark_bar().encode(\n",
      "    alt.X('user_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('Category:N'),\n",
      "    tooltip = ['name', 'Category','user_count']\n",
      ").properties(height=1200, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('user_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "55/98:\n",
      "gded = dedicated.groupby('from_id')\n",
      "gded.id.sum().mean(), gded.post_id.sum().mean(), gded.id.sum().mean()/ gded.post_id.sum().mean()\n",
      "55/99:\n",
      "ded_page = st[st.from_id.isin(dedicated.from_id)].groupby('name').size()\n",
      "ded_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "55/100:\n",
      "page_cats = pd.concat([one_timers_page, fans_page, base_page, camp_page, polem_page, ded_page], axis=1,\n",
      "          keys=['One-timers', 'Fans', 'Basers', 'Campers', 'Polemicists', 'Dedicated'])\n",
      "55/101:\n",
      "regular = alt.Chart(page_cats[page_cats.name.isin(top_commented[:10])]).mark_bar().encode(\n",
      "    alt.X('user_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('Category:N'),\n",
      "    tooltip = ['name', 'Category','user_count']\n",
      ").properties(height=1200, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('user_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "55/102:\n",
      "regular = alt.Chart(page_cats).mark_bar().encode(\n",
      "    alt.X('user_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('Category:N'),\n",
      "    tooltip = ['name', 'Category','user_count']\n",
      ").properties(height=1200, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('user_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "55/103: ax.get_figure().savefig('turf/venn_one_timers_fans.png')\n",
      "55/104:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(st.from_id.values), set(base.from_id.values), set(campers.from_id.values)], set_labels = ('All', 'Basers', 'Campers'), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.15)\n",
      "v.get_label_by_id('111').set_y(v.get_label_by_id('111').get_position()[1]-0.05)\n",
      "v.get_patch_by_id('101').set_color('blue')\n",
      "v.get_patch_by_id('111').set_color('green')\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.20)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.35)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.15)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]+0.25)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.1)\n",
      "55/105: ax.get_figure().savefig('turf/venn_basers_campers.png')\n",
      "55/106:\n",
      "fig, ax = plt.subplots(figsize=[10,10])\n",
      "\n",
      "v = venn3([set(polem.from_id.values), set(base.from_id.values), set(campers.from_id.values)], set_labels = ('polem', 'Basers', 'Campers'), ax=ax)\n",
      "v.get_label_by_id('111').set_x(v.get_label_by_id('111').get_position()[0]+0.15)\n",
      "v.get_label_by_id('111').set_y(v.get_label_by_id('111').get_position()[1]-0.05)\n",
      "v.get_patch_by_id('101').set_color('blue')\n",
      "v.get_patch_by_id('111').set_color('green')\n",
      "v.get_label_by_id('A').set_y(v.get_label_by_id('A').get_position()[1]-0.35)\n",
      "v.get_label_by_id('C').set_y(v.get_label_by_id('C').get_position()[1]+0.25)\n",
      "v.get_label_by_id('C').set_x(v.get_label_by_id('C').get_position()[1]+0.15)\n",
      "v.get_label_by_id('B').set_x(v.get_label_by_id('B').get_position()[1]+0.05)\n",
      "v.get_label_by_id('B').set_y(v.get_label_by_id('B').get_position()[1]-0.1)\n",
      "55/107: df[df.is_post==1].shape[0], df[df.is_post==1].comment_count.mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/108: df[df.is_post==1].groupby('name').comment_count.mean().sort_values().plot(kind='barh', figsize=(10,15))\n",
      "55/109: one_timers.shape\n",
      "55/110:\n",
      "one_timers_page = st[st.from_id.isin(one_timers.index)].groupby('name').size()\n",
      "one_timers_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "55/111:\n",
      "gfans = st[st.from_id.isin(fans_users.from_id)].groupby('from_id')\n",
      "gfans.id.sum().mean(), gfans.post_id.sum().mean(), gfans.id.sum().mean()/ gfans.post_id.sum().mean()\n",
      "55/112:\n",
      "fans_page = st[st.from_id.isin(fans_users.from_id)].groupby('name').size()\n",
      "fans_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "55/113:\n",
      "gbase = st[st.from_id.isin(base.from_id)].groupby('from_id')\n",
      "gbase.id.sum().mean(), gbase.post_id.sum().mean(), gbase.id.sum().mean()/ gbase.post_id.sum().mean()\n",
      "55/114:\n",
      "base_page = st[st.from_id.isin(base.from_id)].groupby('name').size()\n",
      "base_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "55/115:\n",
      "gcamp = st[st.from_id.isin(campers.from_id)].groupby('from_id')\n",
      "gcamp.id.sum().mean(), gcamp.post_id.sum().mean(), gcamp.id.sum().mean()/ gcamp.post_id.sum().mean()\n",
      "55/116: st[st.from_id.isin(campers.from_id)].id.sum()\n",
      "55/117:\n",
      "camp_page = st[st.from_id.isin(campers.from_id)].groupby('name').size()\n",
      "camp_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "55/118: st[(st.from_id.isin(polem.from_id))].id.sum()\n",
      "55/119:\n",
      "gpolem = st[(st.from_id.isin(polem.from_id))].groupby('from_id')\n",
      "gpolem.id.sum().mean(), gpolem.post_id.sum().mean(), gpolem.id.sum().mean()/gpolem.post_id.sum().mean()\n",
      "55/120:\n",
      "polem_page = st[st.from_id.isin(polem.from_id)].groupby('name').size()\n",
      "polem_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "55/121:\n",
      "gded = dedicated.groupby('from_id')\n",
      "gded.id.sum().mean(), gded.post_id.sum().mean(), gded.id.sum().mean()/ gded.post_id.sum().mean()\n",
      "55/122:\n",
      "ded_page = st[st.from_id.isin(dedicated.from_id)].groupby('name').size()\n",
      "ded_page.sort_values().plot(kind='barh', figsize=(10,15))\n",
      "55/123:\n",
      "page_cats = pd.concat([one_timers_page, fans_page, base_page, camp_page, polem_page, ded_page], axis=1,\n",
      "          keys=['One-timers', 'Fans', 'Basers', 'Campers', 'Polemicists', 'Dedicated'])\n",
      "55/124: page_cats = page_cats.stack().reset_index().rename(columns={'level_1': 'Category', 0: 'user_count'})\n",
      "55/125:\n",
      "regular = alt.Chart(page_cats).mark_bar().encode(\n",
      "    alt.X('user_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('Category:N'),\n",
      "    tooltip = ['name', 'Category','user_count']\n",
      ").properties(height=1200, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('user_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "55/126:\n",
      "regular = alt.Chart(page_cats[page_cats.name.isin(top_commented[:10])]).mark_bar().encode(\n",
      "    alt.X('user_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('Category:N'),\n",
      "    tooltip = ['name', 'Category','user_count']\n",
      ").properties(height=1200, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('user_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "55/127:\n",
      "regular = alt.Chart(page_cats[page_cats.name.isin(top_commented[:10])]).mark_bar().encode(\n",
      "    alt.X('user_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('Category:N'),\n",
      "    tooltip = ['name', 'Category','user_count']\n",
      ").properties(height=1200, width=600, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('user_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "55/128:\n",
      "regular = alt.Chart(page_cats[page_cats.name.isin(top_commented[:10])]).mark_bar().encode(\n",
      "    alt.X('user_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('Category:N'),\n",
      "    tooltip = ['name', 'Category','user_count']\n",
      ").properties(height=1200, width=1200, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('user_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "55/129:\n",
      "regular = alt.Chart(page_cats[page_cats.name.isin(top_commented[:10])]).mark_bar().encode(\n",
      "    alt.X('user_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('Category:N'),\n",
      "    tooltip = ['name', 'Category','user_count']\n",
      ").properties(height=, width=600, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('user_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "55/130:\n",
      "regular = alt.Chart(page_cats[page_cats.name.isin(top_commented[:10])]).mark_bar().encode(\n",
      "    alt.X('user_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('Category:N'),\n",
      "    tooltip = ['name', 'Category','user_count']\n",
      ").properties(height=300, width=600, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('user_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "55/131: (normalized | regular).save(scale_factor=200)\n",
      "55/132: (normalized | regular).save('turf/user_types_top10.jpg', scale_factor=200)\n",
      "55/133: (normalized | regular).save('turf/user_types_top10.png', scale_factor=200)\n",
      "55/134: (normalized | regular).save('turf/user_types_top10.png', scale_factor=200)\n",
      "55/135: (normalized | regular).save('turf/user_types_top10.png', scale_factor=200)\n",
      "55/136:\n",
      "regular = alt.Chart(page_cats[page_cats.name.isin(top_commented[:10])]).mark_bar().encode(\n",
      "    alt.X('user_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('Category:N'),\n",
      "    tooltip = ['name', 'Category','user_count']\n",
      ").properties(height=6000, width=12000, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('user_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "55/137:\n",
      "regular = alt.Chart(page_cats[page_cats.name.isin(top_commented[:10])]).mark_bar().encode(\n",
      "    alt.X('user_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('Category:N'),\n",
      "    tooltip = ['name', 'Category','user_count']\n",
      ").properties(height=3000, width=6000, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('user_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "55/138:\n",
      "regular = alt.Chart(page_cats[page_cats.name.isin(top_commented[:10])]).mark_bar().encode(\n",
      "    alt.X('user_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('Category:N'),\n",
      "    tooltip = ['name', 'Category','user_count']\n",
      ").properties(height=300, width=1200, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('user_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "55/139:\n",
      "regular = alt.Chart(page_cats[page_cats.name.isin(top_commented[:10])]).mark_bar().encode(\n",
      "    alt.X('user_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('Category:N'),\n",
      "    tooltip = ['name', 'Category','user_count']\n",
      ").properties(height=200, width=800, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('user_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "55/140:\n",
      "regular = alt.Chart(page_cats[page_cats.name.isin(top_commented[:10])]).mark_bar().encode(\n",
      "    alt.X('user_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('Category:N'),\n",
      "    tooltip = ['name', 'Category','user_count']\n",
      ").properties(height=300, width=1200, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('user_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "55/141:\n",
      "regular = alt.Chart(page_cats[page_cats.name.isin(top_commented[:10])]).mark_bar().encode(\n",
      "    alt.X('user_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('Category:N'),\n",
      "    tooltip = ['name', 'Category','user_count']\n",
      ").properties(height=200, width=800, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('user_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "55/142:\n",
      "regular = alt.Chart(page_cats[page_cats.name.isin(top_commented[:10])]).mark_bar().encode(\n",
      "    alt.X('user_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('Category:N'),\n",
      "    tooltip = ['name', 'Category','user_count']\n",
      ").properties(height=200, width=600, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('user_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "55/143:\n",
      "regular = alt.Chart(page_cats[page_cats.name.isin(top_commented[:10])]).mark_bar().encode(\n",
      "    alt.X('user_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('Category:N'),\n",
      "    tooltip = ['name', 'Category','user_count']\n",
      ").properties(height=200, width=300, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('user_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "55/144:\n",
      "regular = alt.Chart(page_cats[page_cats.name.isin(top_commented[:10])]).mark_bar().encode(\n",
      "    alt.X('user_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None, axis=None),\n",
      "    alt.Color('Category:N'),\n",
      "    tooltip = ['name', 'Category','user_count']\n",
      ").properties(height=200, width=400, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('user_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      "    alt.Y('name:N', sort=top_commented,  title=None),\n",
      ").properties(title='User Ratio')\n",
      "(normalized | regular)\n",
      "55/145:\n",
      "labels = ('One Timers', 'Fans', 'Polemicists', 'Basers', 'Campers', 'Dedicated')\n",
      "sets = [set(one_timers.index), \n",
      "        set(fans_users.from_id.values), \n",
      "        set(polem.from_id.values), \n",
      "        set(base.from_id.values), \n",
      "        set(campers.from_id.values), \n",
      "        set(dedicated.from_id.values)]\n",
      "#df = df.drop('group_cat', axis=1)\n",
      "for l, s in zip(labels, sets):\n",
      "    df.loc[df.from_id.isin(s), 'group_cat'] = l\n",
      "\n",
      "df['group_cat'] = pd.Categorical(df.group_cat)\n",
      "df.group_cat.value_counts()/df.shape[0]\n",
      "55/146:\n",
      "for l, s in zip(labels, sets):\n",
      "    st.loc[st.from_id.isin(s), 'group_cat'] = l\n",
      "st['group_cat'] = pd.Categorical(st.group_cat)\n",
      "st.groupby('group_cat').id.sum()/df.shape[0]\n",
      "55/147: st.to_pickle('turf/total_stats_no_multiindex_groups.pkl.gz', compression='gzip')\n",
      "55/148: df.group_cat.value_counts()/df.shape[0]\n",
      "55/149:\n",
      "labels = ('Dedicated Fans', 'Dedicated Basers', 'Dedicated Polemicists', 'Dedicated Campers', 'Dedicated Celebies')\n",
      "sets = [set(ded_fans_users.from_id.values), \n",
      "        set(ded_base.from_id.values), \n",
      "        set(ded_polem.from_id.values), \n",
      "        set(ded_campers.from_id.values),\n",
      "        set(ded_celebies.from_id.values),]\n",
      "\n",
      "for l, s in zip(labels, sets):\n",
      "    st.loc[st.from_id.isin(s), 'new_group_cat'] = l\n",
      "    \n",
      "st['new_group_cat'] = pd.Categorical(st.new_group_cat)\n",
      "st.groupby('new_group_cat').id.sum()/df.shape[0]\n",
      "55/150:\n",
      "st.new_group_cat = st.new_group_cat.combine_first(st.group_cat)\n",
      "st.groupby('group_cat').id.sum()/df.shape[0]\n",
      "55/151: st.to_pickle('turf/total_stats_no_multiindex_groups_ded_cuts.pkl.gz', compression='gzip')\n",
      "55/152:\n",
      "group_lang = df.groupby(['name', 'group_cat', 'langdetect']).size().reset_index().rename(columns = {0: 'comment_count'})\n",
      "group_lang['langdetect'] = group_lang.langdetect.cat.add_categories(['other'])\n",
      "group_lang.loc[group_lang.langdetect=='cy', 'langdetect'] = 'ru'\n",
      "top_lang = group_lang.groupby('langdetect').comment_count.sum().sort_values(ascending=False).head(9).index\n",
      "group_lang.loc[~group_lang.langdetect.isin(top_lang), 'langdetect'] = 'other'\n",
      "group_lang = group_lang.groupby(['name', 'group_cat', 'langdetect']).comment_count.sum().reset_index()\n",
      "\n",
      "group_lang.langdetect.value_counts()\n",
      "55/153: group_lang.to_csv('turf/page_group_lang.csv', index=False)\n",
      "55/154: alt.data_transformers.enable('default', max_rows=None)\n",
      "55/155:\n",
      "regular = alt.Chart(group_lang).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#a6cee3','#1f78b4','#ff7f00','#fdbf6f','#e31a1c','#cab2d6','#33a02c'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['name', 'group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(width=150)\n",
      "\n",
      "(normalized).facet(column='group_cat').properties(title='Comment Count by MK, User Group Category and Language Detected')\n",
      "55/156:\n",
      "regular = alt.Chart(group_lang.groupby(['group_cat', 'langdetect']).comment_count.sum().reset_index()).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#a6cee3','#1f78b4','#ff7f00','#fdbf6f','#e31a1c','#cab2d6','#33a02c'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ")\n",
      "\n",
      "(normalized).properties(title='Comment Count by User Group Category and Language Detected')\n",
      "55/157:\n",
      "regular = alt.Chart(group_lang[group_lang.name!='Benjamin_Netanyahu'].groupby(['group_cat', 'langdetect']).comment_count.sum().reset_index()).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#a6cee3','#1f78b4','#ff7f00','#fdbf6f','#e31a1c','#cab2d6','#33a02c'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ")\n",
      "\n",
      "(normalized).properties(title='Comment Count by User Group Category and Language Detected (No Netanyahu)')\n",
      "55/158: df[df.length>=50].shape[0] / df.shape[0]\n",
      "55/159:\n",
      "def get_group_lang(min_len=0):\n",
      "    group_lang = df[df.length>=min_len].groupby(['name', 'group_cat', 'langdetect']).size().reset_index().rename(columns = {0: 'comment_count'})\n",
      "    group_lang['langdetect'] = group_lang.langdetect.cat.add_categories(['other'])\n",
      "    group_lang.loc[group_lang.langdetect=='cy', 'langdetect'] = 'ru'\n",
      "    top_lang = group_lang.groupby('langdetect').comment_count.sum().sort_values(ascending=False).head(9).index\n",
      "    group_lang.loc[~group_lang.langdetect.isin(top_lang), 'langdetect'] = 'other'\n",
      "    group_lang = group_lang.groupby(['name', 'group_cat', 'langdetect']).comment_count.sum().reset_index()\n",
      "    return group_lang\n",
      "\n",
      "group_lang_50 = get_group_lang(50)\n",
      "55/160:\n",
      "name_lang_50 = group_lang_50.groupby(['name', 'langdetect']).comment_count.sum().unstack().fillna(0)\n",
      "name_lang_50_r = name_lang_50.div(name_lang_50.sum(axis=1), axis=0)\n",
      "name_lang_50_r.columns = name_lang_50_r.columns.astype(str)\n",
      "name_lang_50_r.index = name_lang_50_r.index.astype(str)\n",
      "name_lang_50_r = name_lang_50_r.reset_index()\n",
      "55/161: name_lang_50_r[name_lang_ratio.name=='Benjamin_Netanyahu'].mean()\n",
      "55/162:\n",
      "name_lang = group_lang.groupby(['name', 'langdetect']).comment_count.sum().unstack().fillna(0)\n",
      "name_lang_ratio = name_lang.div(name_lang.sum(axis=1), axis=0)\n",
      "name_lang_ratio.columns = name_lang_ratio.columns.astype(str)\n",
      "name_lang_ratio.index = name_lang_ratio.index.astype(str)\n",
      "name_lang_ratio = name_lang_ratio.reset_index()\n",
      "55/163: name_lang_ratio.head()\n",
      "55/164: name_lang_ratio[name_lang_ratio.name!='Benjamin_Netanyahu'].mean()\n",
      "55/165:\n",
      "name_lang_onet = group_lang[group_lang.group_cat=='One Timers'].groupby(['name', 'langdetect']).comment_count.sum().unstack().fillna(0)\n",
      "name_lang_or = name_lang_onet.div(name_lang_onet.sum(axis=1), axis=0)\n",
      "name_lang_or.columns = name_lang_or.columns.astype(str)\n",
      "name_lang_or.index = name_lang_or.index.astype(str)\n",
      "name_lang_or = name_lang_or.reset_index()\n",
      "55/166: name_lang_or[name_lang_or.name=='Benjamin_Netanyahu'].mean()\n",
      "55/167:\n",
      "name_lang_f = group_lang[group_lang.group_cat=='Fans'].groupby(['name', 'langdetect']).comment_count.sum().unstack().fillna(0)\n",
      "name_lang_fr = name_lang_f.div(name_lang_f.sum(axis=1), axis=0)\n",
      "name_lang_fr.columns = name_lang_fr.columns.astype(str)\n",
      "name_lang_fr.index = name_lang_fr.index.astype(str)\n",
      "name_lang_fr = name_lang_fr.reset_index()\n",
      "55/168: name_lang_fr[name_lang_fr.name=='Benjamin_Netanyahu'].mean()\n",
      "55/169: df[df.langdetect=='de'].head(50).text\n",
      "55/170: df.iloc[24707].text\n",
      "55/171: df[df.length>=50].shape[0] / df.shape[0]\n",
      "55/172:\n",
      "def get_group_lang(min_len=0):\n",
      "    group_lang = df[df.length>=min_len].groupby(['name', 'group_cat', 'langdetect']).size().reset_index().rename(columns = {0: 'comment_count'})\n",
      "    group_lang['langdetect'] = group_lang.langdetect.cat.add_categories(['other'])\n",
      "    group_lang.loc[group_lang.langdetect=='cy', 'langdetect'] = 'ru'\n",
      "    top_lang = group_lang.groupby('langdetect').comment_count.sum().sort_values(ascending=False).head(9).index\n",
      "    group_lang.loc[~group_lang.langdetect.isin(top_lang), 'langdetect'] = 'other'\n",
      "    group_lang = group_lang.groupby(['name', 'group_cat', 'langdetect']).comment_count.sum().reset_index()\n",
      "    return group_lang\n",
      "\n",
      "group_lang_50 = get_group_lang(50)\n",
      "55/173:\n",
      "name_lang_50 = group_lang_50.groupby(['name', 'langdetect']).comment_count.sum().unstack().fillna(0)\n",
      "name_lang_50_r = name_lang_50.div(name_lang_50.sum(axis=1), axis=0)\n",
      "name_lang_50_r.columns = name_lang_50_r.columns.astype(str)\n",
      "name_lang_50_r.index = name_lang_50_r.index.astype(str)\n",
      "name_lang_50_r = name_lang_50_r.reset_index()\n",
      "55/174: name_lang_50_r[name_lang_ratio.name=='Benjamin_Netanyahu'].mean()\n",
      "55/175: name_lang_50_r[name_lang_ratio.name!='Benjamin_Netanyahu'].median()\n",
      "55/176:\n",
      "def get_name_lang_ratios(group_lang, group_cat):\n",
      "    name_lang_onet = group_lang[group_lang.group_cat==group_cat].groupby(['name', 'langdetect']).comment_count.sum().unstack().fillna(0)\n",
      "    name_lang_or = name_lang_onet.div(name_lang_onet.sum(axis=1), axis=0)\n",
      "    name_lang_or.columns = name_lang_or.columns.astype(str)\n",
      "    name_lang_or.index = name_lang_or.index.astype(str)\n",
      "    name_lang_or = name_lang_or.reset_index()\n",
      "    return name_lang_or\n",
      "\n",
      "nlr_50_ot = get_name_lang_ratios(group_lang_50, 'One Timers')\n",
      "nlr_50_ot[nlr_50_ot.name=='Benjamin_Netanyahu'].mean()\n",
      "55/177:\n",
      "nlr_50_f = get_name_lang_ratios(group_lang_50, 'Fans')\n",
      "nlr_50_f[nlr_50_f.name=='Benjamin_Netanyahu'].mean()\n",
      "55/178:\n",
      "regular = alt.Chart(group_lang_50).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#a6cee3','#1f78b4','#ff7f00','#fdbf6f','#e31a1c','#cab2d6','#33a02c'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['name', 'group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(width=150)\n",
      "\n",
      "(normalized).facet(column='group_cat').properties(title='Comment Count by MK, User Group Category and Language Detected')\n",
      "55/179:\n",
      "regular = alt.Chart(group_lang_50).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('name:N', sort=top_commented, title=None),\n",
      "    alt.Color('langdetect:N',  \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#a6cee3','#1f78b4','#ff7f00','#fdbf6f','#e31a1c','#cab2d6','#33a02c'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['name', 'group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ").properties(width=150)\n",
      "\n",
      "(normalized).facet(column='group_cat').properties(title='Comment Count by MK, User Group Category and Language Detected')\n",
      "55/180:\n",
      "regular = alt.Chart(group_lang.groupby(['group_cat', 'langdetect']).comment_count.sum().reset_index()).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#a6cee3','#1f78b4','#ff7f00','#fdbf6f','#e31a1c','#cab2d6','#33a02c'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ")\n",
      "\n",
      "(normalized).properties(title='Comment Count by User Group Category and Language Detected')\n",
      "55/181:\n",
      "regular = alt.Chart(group_lang_50.groupby(['group_cat', 'langdetect']).comment_count.sum().reset_index()).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#a6cee3','#1f78b4','#ff7f00','#fdbf6f','#e31a1c','#cab2d6','#33a02c'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ")\n",
      "\n",
      "(normalized).properties(title='Comment Count by User Group Category and Language Detected')\n",
      "55/182:\n",
      "regular = alt.Chart(group_lang[group_lang_50.name!='Benjamin_Netanyahu'].groupby(['group_cat', 'langdetect']).comment_count.sum().reset_index()).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#a6cee3','#1f78b4','#ff7f00','#fdbf6f','#e31a1c','#cab2d6','#33a02c'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ")\n",
      "\n",
      "(normalized).properties(title='Comment Count by User Group Category and Language Detected (No Netanyahu)')\n",
      "55/183:\n",
      "regular = alt.Chart(group_lang_50.groupby(['group_cat', 'langdetect']).comment_count.sum().reset_index()).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#a6cee3','#1f78b4','#ff7f00','#fdbf6f','#e31a1c','#cab2d6','#33a02c'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ")\n",
      "\n",
      "(normalized).properties(title='Comment Count by User Group Category and Language Detected')\n",
      "55/184:\n",
      "regular = alt.Chart(group_lang[group_lang_50.name!='Benjamin_Netanyahu'].groupby(['group_cat', 'langdetect']).comment_count.sum().reset_index()).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#a6cee3','#1f78b4','#ff7f00','#fdbf6f','#e31a1c','#cab2d6','#33a02c'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ")\n",
      "\n",
      "(normalized).properties(title='Comment Count by User Group Category and Language Detected (No Netanyahu)')\n",
      "55/185:\n",
      "regular = alt.Chart(group_lang_50[group_lang_50.name!='Benjamin_Netanyahu'].groupby(['group_cat', 'langdetect']).comment_count.sum().reset_index()).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#a6cee3','#1f78b4','#ff7f00','#fdbf6f','#e31a1c','#cab2d6','#33a02c'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ")\n",
      "\n",
      "(normalized).properties(title='Comment Count by User Group Category and Language Detected (No Netanyahu)')\n",
      "55/186:\n",
      "regular = alt.Chart(group_lang_50.groupby(['group_cat', 'langdetect']).comment_count.sum().reset_index()).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#a6cee3','#1f78b4','#ff7f00','#fdbf6f','#e31a1c','#cab2d6','#33a02c'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['group_cat', 'langdetect', 'comment_count']\n",
      ")\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ")\n",
      "\n",
      "(normalized)#.properties(title='Comment Count by User Group Category and Language Detected')\n",
      "55/187:\n",
      "regular = alt.Chart(group_lang_50.groupby(['group_cat', 'langdetect']).comment_count.sum().reset_index()).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#a6cee3','#1f78b4','#ff7f00','#fdbf6f','#e31a1c','#cab2d6','#33a02c'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['group_cat', 'langdetect', 'comment_count']\n",
      ").properties(height=200, width=400, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ")\n",
      "\n",
      "(normalized)#.properties(title='Comment Count by User Group Category and Language Detected')\n",
      "55/188:\n",
      "regular = alt.Chart(group_lang_50.groupby(['group_cat', 'langdetect']).comment_count.sum().reset_index()).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#a6cee3','#1f78b4','#ff7f00','#fdbf6f','#e31a1c','#cab2d6','#33a02c'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['group_cat', 'langdetect', 'comment_count']\n",
      ").properties(height=150, width=400, title='User Count')\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ")\n",
      "\n",
      "(normalized)#.properties(title='Comment Count by User Group Category and Language Detected')\n",
      "55/189:\n",
      "regular = alt.Chart(group_lang_50.groupby(['group_cat', 'langdetect']).comment_count.sum().reset_index()).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#a6cee3','#1f78b4','#ff7f00','#fdbf6f','#e31a1c','#cab2d6','#33a02c'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['group_cat', 'langdetect', 'comment_count']\n",
      ").properties(height=150, width=400, title=None)\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ")\n",
      "\n",
      "(normalized)#.properties(title='Comment Count by User Group Category and Language Detected')\n",
      "55/190:\n",
      "regular = alt.Chart(group_lang_50.groupby(['group_cat', 'langdetect']).comment_count.sum().reset_index()).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#a6cee3','#1f78b4','#ff7f00','#fdbf6f','#e31a1c','#cab2d6','#33a02c'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['group_cat', 'langdetect', 'comment_count']\n",
      ").properties(height=150, width=400)\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ")\n",
      "\n",
      "(normalized)#.properties(title='Comment Count by User Group Category and Language Detected')\n",
      "55/191:\n",
      "regular = alt.Chart(group_lang_50[group_lang_50.name!='Benjamin_Netanyahu'].groupby(['group_cat', 'langdetect']).comment_count.sum().reset_index()).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#a6cee3','#1f78b4','#ff7f00','#fdbf6f','#e31a1c','#cab2d6','#33a02c'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['group_cat', 'langdetect', 'comment_count']\n",
      ").properties(height=150, width=400)\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ")\n",
      "\n",
      "(normalized).properties(title='Comment Count by User Group Category and Language Detected (No Netanyahu)')\n",
      "55/192:\n",
      "regular = alt.Chart(group_lang_50[group_lang_50.name!='Benjamin_Netanyahu'].groupby(['group_cat', 'langdetect']).comment_count.sum().reset_index()).mark_bar().encode(\n",
      "    alt.X('comment_count:Q', axis=alt.Axis(title=None)),\n",
      "    alt.Y('group_cat', title=None),\n",
      "    alt.Color('langdetect:N', \n",
      "               scale=alt.Scale(range=['#6a3d9a','#fb9a99','#b2df8a','#a6cee3','#1f78b4','#ff7f00','#fdbf6f','#e31a1c','#cab2d6','#33a02c'][::-1]),\n",
      "    ),\n",
      "    tooltip = ['group_cat', 'langdetect', 'comment_count']\n",
      ").properties(height=150, width=400)\n",
      "\n",
      "normalized = regular.encode(    \n",
      "    alt.X('comment_count:Q',\n",
      "        axis=alt.Axis(title=None),\n",
      "        stack='normalize'\n",
      "        ),\n",
      ")\n",
      "\n",
      "(normalized)#.properties(title='Comment Count by User Group Category and Language Detected (No Netanyahu)')\n",
      "57/1:\n",
      "import wikipediaapi as wa\n",
      "import pandas as pd\n",
      "import networkx as nx\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "import seaborn as sns\n",
      "sns.set(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
      "sns.set_context(\"talk\")\n",
      "sns.set_palette('Set2', 10)\n",
      "57/2:\n",
      "import altair as alt\n",
      "alt.renderers.enable('notebook')\n",
      "57/3: wiki = wa.Wikipedia('he')\n",
      "57/4: zuk_he = wiki.page('מבצע_צוק_איתן')\n",
      "57/5: frl = pd.read_pickle('data/rev_link_df.pkl.gz', compression='gzip')\n",
      "57/6:\n",
      "pd.set_option('display.max_colwidth', -1)\n",
      "frl[frl.link.str.contains('archive.org')].drop_duplicates(subset=['lang', 'link'])\n",
      "57/7: frl.head()\n",
      "57/8:\n",
      "wb = pd.read_pickle('data/links_wayback.pkl.gz', compression='gzip')\n",
      "wb.head()\n",
      "57/9: wb_exp = pd.read_pickle('data/links_wayback_expanded.pkl.gz', compression='gzip')\n",
      "57/10: wb_exp.dropna().head()\n",
      "57/11: wb_exp.head()\n",
      "57/12: wb_exp.shape\n",
      "57/13: wb_exp.dropna().shape\n",
      "57/14: wb_exp.dropna().shape / wb_exp.shape\n",
      "57/15: wb_exp.dropna().shape / wb_exp.shape[0]\n",
      "57/16: wb_exp.dropna().shape[0] / wb_exp.shape[0]\n",
      "56/1:\n",
      "import wikipediaapi as wa\n",
      "import pandas as pd\n",
      "import networkx as nx\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "import seaborn as sns\n",
      "sns.set(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
      "sns.set_context(\"talk\")\n",
      "sns.set_palette('Set2', 10)\n",
      "56/2: wiki = wa.Wikipedia('he')\n",
      "56/3: zuk_he = wiki.page('מבצע_צוק_איתן')\n",
      "56/4:\n",
      "def print_langlinks(page):\n",
      "    langlinks = page.langlinks\n",
      "    for k in sorted(langlinks.keys()):\n",
      "        v = langlinks[k]\n",
      "        print(\"%s: %s - %s: %s\" % (k, v.language, v.title, v.fullurl))\n",
      "\n",
      "print_langlinks(zuk_he)\n",
      "56/5:\n",
      "def print_sections(sections, level=0):\n",
      "        for s in sections:\n",
      "                print(\"%s: %s - %s\" % (\"*\" * (level + 1), s.title, s.text[0:40]))\n",
      "                print_sections(s.sections, level + 1)\n",
      "#print_sections(zuk_he.sections)\n",
      "56/6: zuk_he.links\n",
      "56/7:\n",
      "ll = zuk_he.langlinks\n",
      "ll['he'] = ll['ar'].langlinks['he']\n",
      "56/8:\n",
      "ll = zuk_he.langlinks\n",
      "ll['he'] = ll['ar'].langlinks['he']\n",
      "56/9: zuk_he.langlinks\n",
      "56/10: len(zuk_he.langlinks)\n",
      "56/11: zuk_he.langlinks\n",
      "56/12: rdf = pd.read_csv('data/all_revisions_by_page_lang.csv', index=False)\n",
      "56/13: rdf = pd.read_csv('data/all_revisions_by_page_lang.csv')\n",
      "56/14:\n",
      "creation_dates = rdf[rdf.timestamp==rdf.groupby('lang').timestamp.transform(min)][['lang', 'timestamp']].sort_values('timestamp')\n",
      "creation_dates\n",
      "56/15:\n",
      "creation_dates = rdf[rdf.timestamp==rdf.groupby('lang').timestamp.transform(min)][['lang', 'timestamp']].sort_values('timestamp')\n",
      "creation_dates.sort_values('lang')\n",
      "56/16: zuk_he.langlinks\n",
      "58/1:\n",
      "import wikipediaapi as wa\n",
      "import pandas as pd\n",
      "import networkx as nx\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "import seaborn as sns\n",
      "sns.set(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
      "sns.set_context(\"talk\")\n",
      "sns.set_palette('Set2', 10)\n",
      "59/1:\n",
      "def print_langlinks(page):\n",
      "    langlinks = page.langlinks\n",
      "    for k in sorted(langlinks.keys()):\n",
      "        v = langlinks[k]\n",
      "        print(\"%s: %s - %s: %s\" % (k, v.language, v.title, v.fullurl))\n",
      "\n",
      "print_langlinks(zuk_he)\n",
      "59/2: zuk_he = wiki.page('מבצע_צוק_איתן')\n",
      "59/3:\n",
      "def print_langlinks(page):\n",
      "    langlinks = page.langlinks\n",
      "    for k in sorted(langlinks.keys()):\n",
      "        v = langlinks[k]\n",
      "        print(\"%s: %s - %s: %s\" % (k, v.language, v.title, v.fullurl))\n",
      "\n",
      "print_langlinks(zuk_he)\n",
      "59/4:\n",
      "import wikipediaapi as wa\n",
      "import pandas as pd\n",
      "import networkx as nx\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "import seaborn as sns\n",
      "sns.set(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
      "sns.set_context(\"talk\")\n",
      "sns.set_palette('Set2', 10)\n",
      "59/5: wiki = wa.Wikipedia('he')\n",
      "59/6: zuk_he = wiki.page('מבצע_צוק_איתן')\n",
      "59/7:\n",
      "def print_langlinks(page):\n",
      "    langlinks = page.langlinks\n",
      "    for k in sorted(langlinks.keys()):\n",
      "        v = langlinks[k]\n",
      "        print(\"%s: %s - %s: %s\" % (k, v.language, v.title, v.fullurl))\n",
      "\n",
      "print_langlinks(zuk_he)\n",
      "59/8:\n",
      "def print_sections(sections, level=0):\n",
      "        for s in sections:\n",
      "                print(\"%s: %s - %s\" % (\"*\" * (level + 1), s.title, s.text[0:40]))\n",
      "                print_sections(s.sections, level + 1)\n",
      "#print_sections(zuk_he.sections)\n",
      "59/9: zuk_he.links\n",
      "59/10:\n",
      "ll = zuk_he.langlinks\n",
      "ll['he'] = ll['ar'].langlinks['he']\n",
      "59/11: zuk_he.langlinks\n",
      "59/12: zuk_he.categories\n",
      "59/13:\n",
      "categories = {}\n",
      "for k in sorted(ll.keys()):\n",
      "    categories[k] = ll[k].categories\n",
      "59/14:\n",
      "from collections import Counter\n",
      "print({k: len(v) for k,v in  categories.items()})\n",
      "59/15: import requests\n",
      "59/16:\n",
      "ext_links = {}\n",
      "\n",
      "req = {'action': 'query',\n",
      "       'prop': 'extlinks',\n",
      "       'format': 'json',\n",
      "       'ellimit': 'max',}\n",
      "\n",
      "for k in sorted(ll.keys()):\n",
      "    v = ll[k]\n",
      "    params = req.copy()\n",
      "    params['titles'] = v.title\n",
      "    resp = requests.get(f'https://{k}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        eloffset = resp['continue']['eloffset']\n",
      "        resp = requests.get(f'https://{k}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    ext_links[k] = final_resp\n",
      "59/17: lc = Counter(flatten(l for l in clean_links.values()))\n",
      "59/18: flatten = lambda l: [item for sublist in l for item in sublist]\n",
      "59/19:\n",
      "ext_links = {}\n",
      "\n",
      "req = {'action': 'query',\n",
      "       'prop': 'extlinks',\n",
      "       'format': 'json',\n",
      "       'ellimit': 'max',}\n",
      "\n",
      "for k in sorted(ll.keys()):\n",
      "    v = ll[k]\n",
      "    params = req.copy()\n",
      "    params['titles'] = v.title\n",
      "    resp = requests.get(f'https://{k}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        eloffset = resp['continue']['eloffset']\n",
      "        resp = requests.get(f'https://{k}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    ext_links[k] = final_resp\n",
      "59/20: clean_links['en']\n",
      "59/21:\n",
      "ext_links = {}\n",
      "\n",
      "req = {'action': 'query',\n",
      "       'prop': 'extlinks',\n",
      "       'format': 'json',\n",
      "       'ellimit': 'max',}\n",
      "\n",
      "for k in sorted(ll.keys()):\n",
      "    v = ll[k]\n",
      "    params = req.copy()\n",
      "    params['titles'] = v.title\n",
      "    resp = requests.get(f'https://{k}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        eloffset = resp['continue']['eloffset']\n",
      "        resp = requests.get(f'https://{k}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    ext_links[k] = final_resp\n",
      "59/22:\n",
      "ext_links = {}\n",
      "\n",
      "req = {'action': 'query',\n",
      "       'prop': 'extlinks',\n",
      "       'format': 'json',\n",
      "       'ellimit': 'max',}\n",
      "\n",
      "for k in sorted(ll.keys()):\n",
      "    v = ll[k]\n",
      "    params = req.copy()\n",
      "    params['titles'] = v.title\n",
      "    resp = requests.get(f'https://{k}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        eloffset = resp['continue']['eloffset']\n",
      "        resp = requests.get(f'https://{k}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    ext_links[k] = final_resp\n",
      "59/23:\n",
      "ext_links = {}\n",
      "\n",
      "req = {'action': 'query',\n",
      "       'prop': 'extlinks',\n",
      "       'format': 'json',\n",
      "       'ellimit': 'max',}\n",
      "\n",
      "for k in sorted(ll.keys()):\n",
      "    v = ll[k]\n",
      "    params = req.copy()\n",
      "    params['titles'] = v.title\n",
      "    resp = requests.get(f'https://{k}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        break\n",
      "        eloffset = resp['continue']['eloffset']\n",
      "        resp = requests.get(f'https://{k}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    ext_links[k] = final_resp\n",
      "59/24:\n",
      "ext_links = {}\n",
      "\n",
      "req = {'action': 'query',\n",
      "       'prop': 'extlinks',\n",
      "       'format': 'json',\n",
      "       'ellimit': 'max',}\n",
      "\n",
      "for k in sorted(ll.keys()):\n",
      "    v = ll[k]\n",
      "    params = req.copy()\n",
      "    params['titles'] = v.title\n",
      "    resp = requests.get(f'https://{k}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        #eloffset = resp['continue']['eloffset']\n",
      "        resp = requests.get(f'https://{k}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    ext_links[k] = final_resp\n",
      "59/25: len(final_resp)\n",
      "59/26: ext_links['en']\n",
      "59/27:\n",
      "from collections import defaultdict\n",
      "\n",
      "clean_links = defaultdict(list)\n",
      "for lang in ext_links:\n",
      "    for resp in ext_links[lang]:\n",
      "        for page in resp['query']['pages']:\n",
      "            if 'extlinks' in resp['query']['pages'][page]:\n",
      "                links = resp['query']['pages'][page]['extlinks']\n",
      "                num = len(links)\n",
      "                clean_links[lang].extend(flatten([list(l.values()) for l in links]))\n",
      "            else: \n",
      "                clean_links[lang].extend([])\n",
      "                num=0\n",
      "                \n",
      "            title = resp['query']['pages'][page]['title']\n",
      "\n",
      "            print(f'{lang}: extlinks={num}, title: {title}')\n",
      "59/28: clean_links['en']\n",
      "59/29: lc = Counter(flatten(l for l in clean_links.values()))\n",
      "59/30: lc.most_common()\n",
      "59/31:\n",
      "for_cdf = []\n",
      "for lang in ll:\n",
      "    page = ll[lang]\n",
      "    for_cdf.append({'lang': lang, 'full_url': page.fullurl, 'title': page.title, 'summary': page.summary})\n",
      "    \n",
      "cdf = pd.DataFrame(for_cdf)\n",
      "cdf.head()\n",
      "59/32: cdf.to_csv('data/zuk_wiki_pages.csv', index=False)\n",
      "59/33:\n",
      "cl_for_df = []\n",
      "\n",
      "for lang in clean_links:\n",
      "    for link in clean_links[lang]:\n",
      "        cl_for_df.append({'lang': lang, 'link': link})\n",
      "\n",
      "ldf = pd.DataFrame(cl_for_df)\n",
      "ldf.head()\n",
      "59/34: ldf.to_csv('data/lang_extlinks.csv', index=False)\n",
      "59/35: ldf.lang.value_counts()\n",
      "59/36:\n",
      "g = nx.from_pandas_edgelist(ldf, 'lang', 'link', create_using=nx.DiGraph())\n",
      "\n",
      "nx.write_gexf(g, 'data/extlinks.gexf')\n",
      "59/37:\n",
      "from wikidata.client import Client\n",
      "client = Client()  # doctest: +SKIP\n",
      "entity = client.get('Q17324420', load=True)\n",
      "entity\n",
      "59/38: entity.description\n",
      "59/39: entity.data\n",
      "59/40: entity.data['labels']\n",
      "59/41:\n",
      "labels = pd.DataFrame([{'lang': v['language'], 'label': v['value']} for k, v in entity.data['labels'].items()])\n",
      "labels.head()\n",
      "59/42: labels.to_csv('data/wikidata_labels.csv', index=False)\n",
      "59/43: entity.data['aliases']\n",
      "59/44: flatten([[{'lang': v['language'], 'alias': v['value']} for v in l] for k, l in entity.data['aliases'].items()])\n",
      "59/45:\n",
      "aliases = pd.DataFrame(flatten([[{'lang': v['language'], 'alias': v['value']} for v in l] for k, l in entity.data['aliases'].items()]))\n",
      "aliases.head()\n",
      "59/46: aliases.to_csv('data/wikidata_aliases.csv', index=False)\n",
      "59/47:\n",
      "revisions = {}\n",
      "\n",
      "req = {'action': 'query',\n",
      "       'prop': 'revisions',\n",
      "       'format': 'json',\n",
      "       'rvlimit': 'max',\n",
      "       'rvprop': 'timestamp|user|comment|size|tags|flags|ids|roles',}\n",
      "\n",
      "for k in sorted(ll.keys()):\n",
      "    v = ll[k]\n",
      "    params = req.copy()\n",
      "    params['titles'] = v.title\n",
      "    resp = requests.get(f'https://{k}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        #eloffset = resp['continue']['rvcontinue']\n",
      "        resp = requests.get(f'https://{k}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    revisions[k] = final_resp\n",
      "59/48: revisions['he'][0]\n",
      "59/49:\n",
      "clean_revs = defaultdict(list)\n",
      "for lang in revisions:\n",
      "    for resp in revisions[lang]:\n",
      "        for page in resp['query']['pages']:\n",
      "            if 'revisions' in resp['query']['pages'][page]:\n",
      "                revs = resp['query']['pages'][page]['revisions']\n",
      "                num = len(revs)\n",
      "                clean_revs[lang].extend(revs)\n",
      "            else: \n",
      "                clean_revs[lang].extend([])\n",
      "                num=0\n",
      "                \n",
      "            title = resp['query']['pages'][page]['title']\n",
      "\n",
      "            print(f'{lang}: revs={num}, title: {title}')\n",
      "59/50:\n",
      "cr_for_df = []\n",
      "\n",
      "for lang in clean_revs:\n",
      "    for rev in clean_revs[lang]:\n",
      "        tmp = rev.copy()\n",
      "        tmp['lang'] = lang\n",
      "        tmp['title'] = ll[lang].title\n",
      "        tmp['tags'] = ','.join(rev['tags'])\n",
      "        tmp['roles'] = ','.join(rev['roles'])\n",
      "        cr_for_df.append(tmp)\n",
      "\n",
      "rdf = (pd.DataFrame(cr_for_df)\n",
      "       .assign(timestamp = lambda x: pd.to_datetime(x.timestamp))\n",
      "      )\n",
      "\n",
      "rdf.head()\n",
      "59/51: rdf.to_csv('data/all_revisions_by_page_lang.csv', index=False)\n",
      "59/52: rdf = pd.read_csv('data/all_revisions_by_page_lang.csv')\n",
      "59/53: zuk_he.langlinks\n",
      "59/54:\n",
      "creation_dates = rdf[rdf.timestamp==rdf.groupby('lang').timestamp.transform(min)][['lang', 'timestamp']].sort_values('timestamp')\n",
      "creation_dates.sort_values('lang')\n",
      "59/55: creation_dates.to_csv('data/pages_creation_dates.csv', index=False)\n",
      "59/56: rdf[rdf.lang=='hu']\n",
      "59/57:\n",
      "rev_cont = {}\n",
      "\n",
      "req = {'action': 'query',\n",
      "       'prop': 'revisions',\n",
      "       'format': 'json',\n",
      "       'rvlimit': 'max',\n",
      "       'rvprop': 'timestamp|user|comment|size|tags|content',}\n",
      "\n",
      "#for k in sorted(ll.keys()):\n",
      "for k in sorted(['he']):\n",
      "    v = ll[k]\n",
      "    params = req.copy()\n",
      "    params['titles'] = v.title\n",
      "    resp = requests.get(f'https://{k}.wikipedia.org/w/api.php', params = params).json()\n",
      "    final_resp = [resp, ]\n",
      "    while 'continue' in resp:\n",
      "        params.update(resp['continue'])\n",
      "        print (resp['continue'])\n",
      "        eloffset = resp['continue']['rvcontinue']\n",
      "        resp = requests.get(f'https://{k}.wikipedia.org/w/api.php', params=params).json()\n",
      "        final_resp.append(resp)\n",
      "    rev_cont[k] = final_resp\n",
      "59/58: import gzip, pickle\n",
      "59/59:\n",
      "fp=gzip.open('data/revision_external_links.pkl','rb')\n",
      "pickle.load(rev_ext,fp)\n",
      "59/60:\n",
      "fp=gzip.open('data/revision_external_links.pkl','rb')\n",
      "rev_ext = pickle.load(fp)\n",
      "59/61: sorted_rev_ext = {lang: sorted([rev for rev in revs if 'parse' in rev], key=lambda k: k['parse']['revid']) for lang, revs in rev_ext.items()}\n",
      "59/62:\n",
      "full_df = []\n",
      "for lang in sorted_rev_ext:\n",
      "    for resp in sorted_rev_ext[lang]:\n",
      "        if 'parse' in resp:\n",
      "            for link in resp['parse']['externallinks']:\n",
      "                full_df.append({'lang': lang, 'link': link, 'revid': resp['parse']['revid']})\n",
      "        \n",
      "full_rev_link_df = pd.DataFrame(full_df)\n",
      "59/63: frl = full_rev_link_df.merge(rdf[['lang', 'revid', 'timestamp']], how='left')\n",
      "59/64:\n",
      "import altair as alt\n",
      "alt.renderers.enable('notebook')\n",
      "\n",
      "link_appear = frl.groupby('link').timestamp.min().to_frame().resample('M', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "\n",
      "alt.Chart(link_appear).mark_bar().encode(\n",
      "    x='timestamp:T',\n",
      "    y='links:Q'\n",
      ").properties\n",
      "59/65: frl.head()\n",
      "59/66:\n",
      "import altair as alt\n",
      "alt.renderers.enable('notebook')\n",
      "\n",
      "link_appear = frl.groupby('link').timestamp.min().to_frame().resample('M', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "\n",
      "alt.Chart(link_appear).mark_bar().encode(\n",
      "    x='timestamp:T',\n",
      "    y='links:Q'\n",
      ").properties\n",
      "59/67: frl.groupby('link').timestamp.min().to_frame()\n",
      "59/68: frl.groupby('link').timestamp.min().to_frame().resample('M', on='timestamp').size().reset_index()\n",
      "59/69: frl.groupby('link').timestamp.min().reset_index().resample('M', on='timestamp').size().reset_index()\n",
      "59/70: frl.dtypes\n",
      "59/71: frl['timestamp'] = frl.timestamp.to_datetime()\n",
      "59/72: frl['timestamp'] = pd.to_datetime(frl.timestamp)\n",
      "59/73: frl.groupby('link').timestamp.min().reset_index().resample('M', on='timestamp').size().reset_index()\n",
      "59/74:\n",
      "import altair as alt\n",
      "alt.renderers.enable('notebook')\n",
      "\n",
      "link_appear = frl.groupby('link').timestamp.min().to_frame().resample('M', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "\n",
      "alt.Chart(link_appear).mark_bar().encode(\n",
      "    x='timestamp:T',\n",
      "    y='links:Q'\n",
      ").properties\n",
      "59/75:\n",
      "import altair as alt\n",
      "alt.renderers.enable('notebook')\n",
      "\n",
      "link_appear = frl.groupby('link').timestamp.min().to_frame().resample('M', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "\n",
      "alt.Chart(link_appear).mark_bar().encode(\n",
      "    x='timestamp:T',\n",
      "    y='links:Q'\n",
      ").properties\n",
      "59/76:\n",
      "import altair as alt\n",
      "alt.renderers.enable('notebook')\n",
      "\n",
      "link_appear = frl.groupby('link').timestamp.min().to_frame().resample('M', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "\n",
      "alt.Chart(link_appear).mark_bar().encode(\n",
      "    x='timestamp:T',\n",
      "    y='links:Q'\n",
      ")\n",
      "59/77:\n",
      "creation_dates_day = creation_dates.resample('D', on='timestamp').size().reset_index().rename(columns={0:'pages_created'})\n",
      "creation_dates_day['pages'] = creation_dates_day.pages_created.cumsum()\n",
      "creation_dates_day['pages_pct'] = creation_dates_day.pages / creation_dates_day.pages.max()\n",
      "\n",
      "creation_dates_day\n",
      "59/78: creation_dates.dtypes\n",
      "59/79: creation_dates['timestamp'] = pd.to_datetime(creation_dates.timestamp)\n",
      "59/80:\n",
      "creation_dates_day = creation_dates.resample('D', on='timestamp').size().reset_index().rename(columns={0:'pages_created'})\n",
      "creation_dates_day['pages'] = creation_dates_day.pages_created.cumsum()\n",
      "creation_dates_day['pages_pct'] = creation_dates_day.pages / creation_dates_day.pages.max()\n",
      "\n",
      "creation_dates_day\n",
      "59/81:\n",
      "first_link_appear = frl.groupby('link').timestamp.min().to_frame().resample('D', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "first_link_appear['link_accum'] = first_link_appear.links.cumsum()\n",
      "first_link_appear['link_pct_change'] = first_link_appear.link_accum.pct_change()\n",
      "\n",
      "bar = alt.Chart(first_link_appear[(first_link_appear.timestamp>='2014-07-05') & (first_link_appear.timestamp<'2014-11-01')]).mark_bar().encode(\n",
      "    x='timestamp:T',\n",
      "    y='link_accum:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "line = alt.Chart(creation_dates_day[(creation_dates_day.timestamp>='2014-07-05') & (creation_dates_day.timestamp<'2014-11-01')]).mark_line(color='red').encode(\n",
      "    x='timestamp:T',\n",
      "    y='pages:Q'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "alt.layer(\n",
      "    bar,\n",
      "    line\n",
      ").resolve_scale(\n",
      "    y='independent'\n",
      ").properties(title='Protective Edge Wikipedia pages count and external link apperances')\n",
      "59/82:\n",
      "first_link_appear = frl.groupby('link').timestamp.min().to_frame().resample('D', on='timestamp').size().reset_index().rename(columns={0:'links'})\n",
      "first_link_appear['link_accum'] = first_link_appear.links.cumsum()\n",
      "first_link_appear['link_pct'] = first_link_appear.link_accum / first_link_appear.link_accum.max()\n",
      "\n",
      "\n",
      "bar = alt.Chart(first_link_appear[(first_link_appear.timestamp>='2014-07-05') & (first_link_appear.timestamp<'2018-11-01')]).mark_line(color='blue').encode(\n",
      "    x='timestamp:T',\n",
      "    y='link_pct:Q',\n",
      "    tooltip = 'timestamp'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "line = alt.Chart(creation_dates_day[(creation_dates_day.timestamp>='2014-07-05') & (creation_dates_day.timestamp<'2018-11-01')]).mark_line(color='red').encode(\n",
      "    x='timestamp:T',\n",
      "    y='pages_pct:Q',\n",
      "    tooltip = 'timestamp'\n",
      ").properties(width=800, height=600)\n",
      "\n",
      "alt.layer(\n",
      "    bar,\n",
      "    line\n",
      ").interactive().properties(title='Protective Edge Wikipedia pages count and external link apperances (pct from max number for each of them)')\n",
      "59/83:\n",
      "(frl\n",
      " .groupby(['lang', 'link'])\n",
      " .timestamp.min()\n",
      " .reset_index()\n",
      " .sort_values(by=['lang', 'timestamp'], ascending=False)\n",
      " .to_csv('data/links_creation_date.csv', index=False))\n",
      "59/84:\n",
      "(frl\n",
      " .groupby(['lang', 'link'])\n",
      " .timestamp.agg(['min', 'max'])\n",
      " .reset_index()\n",
      " .sort_values(by=['lang', 'min'], ascending=False)\n",
      ")\n",
      "59/85:\n",
      "pd.set_option('display.max_colwidth', -1)\n",
      "frl[frl.link.str.contains('archive.org')].drop_duplicates(subset=['lang', 'link'])\n",
      "59/86: frl[frl.link.str.contains('Dover.aspx')].drop_duplicates(subset=['lang', 'link'])\n",
      "59/87: frl.link.value_counts()\n",
      "59/88: frl.link.mode()\n",
      "59/89: frl.link.mode().iat[0]\n",
      "59/90:\n",
      "top_link = frl.link.mode().iat[0]\n",
      "frl[frl.link==top_link]\n",
      "59/91:\n",
      "top_link = frl.link.mode().iat[0]\n",
      "frl.timestamp.drop_duplicates().reset_index().merge(frl[frl.link==top_link], on='timestamp', how='left')\n",
      "59/92:\n",
      "top_link = frl.link.mode().iat[0]\n",
      "keys = ['lang', 'revid', 'timestamp']\n",
      "frl.groupby(keys).size().reset_index().drop('size', axis=1).merge(frl[frl.link==top_link], on='timestamp', how='left')\n",
      "59/93:\n",
      "top_link = frl.link.mode().iat[0]\n",
      "keys = ['lang', 'revid', 'timestamp']\n",
      "frl.groupby(keys).size().reset_index().drop('size', axis=1)\n",
      "59/94:\n",
      "top_link = frl.link.mode().iat[0]\n",
      "keys = ['lang', 'revid', 'timestamp']\n",
      "frl.groupby(keys).size().reset_index()\n",
      "59/95:\n",
      "top_link = frl.link.mode().iat[0]\n",
      "keys = ['lang', 'revid', 'timestamp']\n",
      "frl.groupby(keys).size().reset_index().drop(0, axis=1).merge(frl[frl.link==top_link], on='timestamp', how='left')\n",
      "59/96:\n",
      "top_link = frl.link.mode().iat[0]\n",
      "keys = ['lang', 'revid', 'timestamp']\n",
      "frl.groupby(keys).size().reset_index().drop(0, axis=1).merge(frl[frl.link==top_link], on=keys, how='left')\n",
      "59/97:\n",
      "top_link = frl.link.mode().iat[0]\n",
      "keys = ['lang', 'revid', 'timestamp']\n",
      "frl.groupby(keys).size().reset_index().drop(0, axis=1)\n",
      "59/98:\n",
      "top_link = frl.link.mode().iat[0]\n",
      "keys = ['lang', 'revid', 'timestamp']\n",
      "frl.groupby(keys).size().reset_index().drop(0, axis=1).merge(frl[frl.link==top_link], on=keys, how='left')\n",
      "59/99:\n",
      "top_link = frl.link.mode().iat[0]\n",
      "keys = ['lang', 'revid', 'timestamp']\n",
      "top_link_exists = frl.groupby(keys).size().reset_index().drop(0, axis=1).merge(frl[frl.link==top_link], on=keys, how='left')\n",
      "59/100:\n",
      "top_link = frl.link.mode().iat[0]\n",
      "keys = ['lang', 'revid', 'timestamp']\n",
      "top_link_exists = (frl.groupby(keys).size()\n",
      "                   .reset_index()\n",
      "                   .drop(0, axis=1)\n",
      "                   .merge(frl[frl.link==top_link], on=keys, how='left')\n",
      "                   .assign(link = lambda x: x.link.notnull())\n",
      "                   .pivot(index='timestamp', columns='lang', values='link'))\n",
      "59/101:\n",
      "top_link = frl.link.mode().iat[0]\n",
      "keys = ['lang', 'revid', 'timestamp']\n",
      "top_link_exists = (frl.groupby(keys).size()\n",
      "                   .reset_index()\n",
      "                   .drop(0, axis=1)\n",
      "                   .merge(frl[frl.link==top_link], on=keys, how='left')\n",
      "                   .assign(link = lambda x: x.link.notnull())\n",
      "                   .pivot(index=['revid','timestamp'], columns='lang', values='link'))\n",
      "59/102:\n",
      "top_link = frl.link.mode().iat[0]\n",
      "keys = ['lang', 'revid', 'timestamp']\n",
      "top_link_exists = (frl.groupby(keys).size()\n",
      "                   .reset_index()\n",
      "                   .drop(0, axis=1)\n",
      "                   .merge(frl[frl.link==top_link], on=keys, how='left')\n",
      "                   .assign(link = lambda x: x.link.notnull())\n",
      "                   .pivot(index='timestamp', columns='lang', values='link'))\n",
      "59/103:\n",
      "top_link = frl.link.mode().iat[0]\n",
      "keys = ['lang', 'revid', 'timestamp']\n",
      "top_link_exists = (frl.groupby(keys).size()\n",
      "                   .reset_index()\n",
      "                   .drop(0, axis=1)\n",
      "                   .merge(frl[frl.link==top_link], on=keys, how='left')\n",
      "                   .assign(link = lambda x: x.link.notnull())\n",
      "                   .pivot(index='timestamp', columns='lang', values='link', agg_func=max))\n",
      "59/104:\n",
      "top_link = frl.link.mode().iat[0]\n",
      "keys = ['lang', 'revid', 'timestamp']\n",
      "top_link_exists = (frl.groupby(keys).size()\n",
      "                   .reset_index()\n",
      "                   .drop(0, axis=1)\n",
      "                   .merge(frl[frl.link==top_link], on=keys, how='left')\n",
      "                   .assign(link = lambda x: x.link.notnull()))\n",
      "                   #.pivot(index='timestamp', columns='lang', values='link'))\n",
      "59/105: top_link_exists.groupby(keys).size()\n",
      "59/106: top_link_exists.groupby(keys).size()>1\n",
      "59/107: top_link_exists.groupby(keys).size()[top_link_exists.groupby(keys).size()>1]\n",
      "59/108: top_link_exists.groupby(['timestamp', 'lang', 'link']).size()[top_link_exists.groupby(keys).size()>1]\n",
      "59/109: top_link_exists.groupby(['timestamp', 'lang', 'link']).size()[top_link_exists.groupby(['timestamp', 'lang', 'link']).size()>1]\n",
      "59/110: top_links_exist[top_link_exists.timestamp='2016-10-01 10:51:32']\n",
      "59/111: top_links_exist[top_link_exists.timestamp-='2016-10-01 10:51:32']\n",
      "59/112: top_links_exist[top_link_exists.timestamp=='2016-10-01 10:51:32']\n",
      "59/113: top_link_exists[top_link_exists.timestamp=='2016-10-01 10:51:32']\n",
      "59/114:\n",
      "top_link = frl.link.mode().iat[0]\n",
      "keys = ['lang', 'revid', 'timestamp']\n",
      "top_link_exists = (frl.groupby(keys).size()\n",
      "                   .reset_index()\n",
      "                   .drop(0, axis=1)\n",
      "                   .merge(frl[frl.link==top_link], on=keys, how='left')\n",
      "                   .assign(link = lambda x: x.link.notnull()))\n",
      "                   .groupby(['timestamp', 'lang', 'link']).link.max().reset_index()\n",
      "                   #.pivot(index='timestamp', columns='lang', values='link'))\n",
      "59/115:\n",
      "top_link = frl.link.mode().iat[0]\n",
      "keys = ['lang', 'revid', 'timestamp']\n",
      "top_link_exists = (frl.groupby(keys).size()\n",
      "                   .reset_index()\n",
      "                   .drop(0, axis=1)\n",
      "                   .merge(frl[frl.link==top_link], on=keys, how='left')\n",
      "                   .assign(link = lambda x: x.link.notnull())\n",
      "                   .groupby(['timestamp', 'lang', 'link']).link.max().reset_index())\n",
      "                   #.pivot(index='timestamp', columns='lang', values='link'))\n",
      "59/116:\n",
      "top_link = frl.link.mode().iat[0]\n",
      "keys = ['lang', 'revid', 'timestamp']\n",
      "top_link_exists = (frl.groupby(keys).size()\n",
      "                   .reset_index()\n",
      "                   .drop(0, axis=1)\n",
      "                   .merge(frl[frl.link==top_link], on=keys, how='left')\n",
      "                   .assign(link = lambda x: x.link.notnull())\n",
      "                   .groupby(['timestamp', 'lang', 'link']).size().reset_index())\n",
      "                   #.pivot(index='timestamp', columns='lang', values='link'))\n",
      "59/117:\n",
      "top_link = frl.link.mode().iat[0]\n",
      "keys = ['lang', 'revid', 'timestamp']\n",
      "top_link_exists = (frl.groupby(keys).size()\n",
      "                   .reset_index()\n",
      "                   .drop(0, axis=1)\n",
      "                   .merge(frl[frl.link==top_link], on=keys, how='left')\n",
      "                   .assign(link = lambda x: x.link.notnull())\n",
      "                   .groupby(['timestamp', 'lang', 'link']).size().reset_index()\n",
      "                   .pivot(index='timestamp', columns='lang', values='link'))\n",
      "59/118: top_link_exists\n",
      "59/119: import seaborn as sns\n",
      "59/120: sns.heatmap(top_link_exists)\n",
      "59/121:\n",
      "top_link = frl.link.mode().iat[0]\n",
      "keys = ['lang', 'revid', 'timestamp']\n",
      "top_link_exists = (frl.groupby(keys).size()\n",
      "                   .reset_index()\n",
      "                   .drop(0, axis=1)\n",
      "                   .merge(frl[frl.link==top_link], on=keys, how='left')\n",
      "                   .assign(link = lambda x: x.link.notnull().astype(int))\n",
      "                   .groupby(['timestamp', 'lang', 'link']).size().reset_index()\n",
      "                   .pivot(index='timestamp', columns='lang', values='link'))\n",
      "59/122: import seaborn as sns\n",
      "59/123: sns.heatmap(top_link_exists)\n",
      "59/124:\n",
      "top_link = frl.link.mode().iat[0]\n",
      "keys = ['lang', 'revid', 'timestamp']\n",
      "top_link_exists = (frl.groupby(keys).size()\n",
      "                   .reset_index()\n",
      "                   .drop(0, axis=1)\n",
      "                   .merge(frl[frl.link==top_link], on=keys, how='left')\n",
      "                   .assign(link = lambda x: x.link.notnull().astype(int))\n",
      "                   .groupby(['timestamp', 'lang', 'link']).size().reset_index()\n",
      "                   .pivot(index='timestamp', columns='lang', values='link').fillna(0))\n",
      "59/125: sns.heatmap(top_link_exists)\n",
      "59/126:\n",
      "import seaborn as sns\n",
      "sns.set()\n",
      "59/127: sns.heatmap(top_link_exists)\n",
      "59/128: sns.heatmap(top_link_exists, cbar=False)\n",
      "59/129:\n",
      "top_link = frl.link.mode().iat[0]\n",
      "keys = ['lang', 'revid', 'timestamp']\n",
      "top_link_exists = (frl.groupby(keys).size()\n",
      "                   .reset_index()\n",
      "                   .drop(0, axis=1)\n",
      "                   .merge(frl[frl.link==top_link], on=keys, how='left')\n",
      "                   .assign(link = lambda x: x.link.notnull().astype(int))\n",
      "                   .groupby(['timestamp', 'lang', 'link']).size().reset_index()\n",
      "                   .pivot(index='timestamp', columns='lang', values='link').fillna(0).sort_index())\n",
      "59/130:\n",
      "import seaborn as sns\n",
      "sns.set()\n",
      "59/131: sns.heatmap(top_link_exists, cbar=False)\n",
      "59/132:\n",
      "top_link = frl.link.mode().iat[0]\n",
      "keys = ['lang', 'revid', 'timestamp']\n",
      "top_link_exists = (frl.groupby(keys).size()\n",
      "                   .reset_index()\n",
      "                   .drop(0, axis=1)\n",
      "                   .merge(frl[frl.link==top_link], on=keys, how='left')\n",
      "                   .assign(link = lambda x: x.link.notnull().astype(int))\n",
      "                   .groupby(['timestamp', 'lang', 'link']).size().reset_index()\n",
      "                   .pivot(index='timestamp', columns='lang', values='link').fillna(method='ffill').fillna('0'.sort_index())\n",
      "59/133:\n",
      "top_link = frl.link.mode().iat[0]\n",
      "keys = ['lang', 'revid', 'timestamp']\n",
      "top_link_exists = (frl.groupby(keys).size()\n",
      "                   .reset_index()\n",
      "                   .drop(0, axis=1)\n",
      "                   .merge(frl[frl.link==top_link], on=keys, how='left')\n",
      "                   .assign(link = lambda x: x.link.notnull().astype(int))\n",
      "                   .groupby(['timestamp', 'lang', 'link']).size().reset_index()\n",
      "                   .pivot(index='timestamp', columns='lang', values='link').fillna(method='ffill').fillna(0).sort_index())\n",
      "59/134:\n",
      "import seaborn as sns\n",
      "sns.set()\n",
      "59/135: sns.heatmap(top_link_exists, cbar=False)\n",
      "59/136:\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "sns.set()\n",
      "59/137:\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "sns.set_context('poster')\n",
      "59/138: sns.heatmap(top_link_exists, cbar=False)\n",
      "59/139:\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "sns.set_context('talk')\n",
      "59/140: sns.heatmap(top_link_exists, cbar=False)\n",
      "59/141:\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "sns.set_context('paper')\n",
      "59/142: sns.heatmap(top_link_exists, cbar=False)\n",
      "59/143: sns.heatmap(top_link_exists, cbar=False, fmt='y')\n",
      "59/144:\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "sns.set_context('paper')\n",
      "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
      "59/145: sns.heatmap(top_link_exists, cbar=False)\n",
      "59/146:\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "sns.set_context('paper')\n",
      "sns.set(rc={'figure.figsize':(30,20)})\n",
      "59/147: sns.heatmap(top_link_exists, cbar=False)\n",
      "59/148:\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "sns.set_context('paper')\n",
      "sns.set(rc={'figure.figsize':(20,15)})\n",
      "59/149: sns.heatmap(top_link_exists, cbar=False)\n",
      "59/150:\n",
      "top_link = frl.link.mode().iat[0]\n",
      "\n",
      "def get_link_exists_df(df, link):\n",
      "    keys = ['lang', 'revid', 'timestamp']\n",
      "    link_exists = (df.groupby(keys).size()\n",
      "                       .reset_index()\n",
      "                       .drop(0, axis=1)\n",
      "                       .merge(df[df.link==top_link], on=keys, how='left')\n",
      "                       .assign(link = lambda x: x.link.notnull().astype(int))\n",
      "                       .groupby(['timestamp', 'lang', 'link']).size().reset_index()\n",
      "                       .pivot(index='timestamp', columns='lang', values='link')\n",
      "                       .fillna(method='ffill').fillna(0).sort_index())\n",
      "    return link_exists\n",
      "\n",
      "top_link_exists = get_link_exists_df(frl, top_link)\n",
      "59/151:\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "sns.set_context('paper')\n",
      "sns.set(rc={'figure.figsize':(20,15)})\n",
      "59/152: sns.heatmap(top_link_exists, cbar=False)\n",
      "59/153: frl.link.mode().iat[1]\n",
      "59/154: frl.link.value_counts.iat[1]\n",
      "59/155: frl.link.value_counts[1]\n",
      "59/156: frl.link.value_counts()[1]\n",
      "59/157: frl.link.value_counts().iloc[1]\n",
      "59/158: frl.link.value_counts()\n",
      "59/159: frl.link.value_counts().index[1]\n",
      "59/160: second_link_exists = get_link_exists_df(frl, frl.link.value_counts().index[1])\n",
      "59/161: sns.heatmap(second_link_exists, cbar=False)\n",
      "59/162:\n",
      "top_link = frl.link.mode().iat[0]\n",
      "\n",
      "def get_link_exists_df(df, link):\n",
      "    keys = ['lang', 'revid', 'timestamp']\n",
      "    link_exists = (df.groupby(keys).size()\n",
      "                       .reset_index()\n",
      "                       .drop(0, axis=1)\n",
      "                       .merge(df[df.link==link], on=keys, how='left')\n",
      "                       .assign(link = lambda x: x.link.notnull().astype(int))\n",
      "                       .groupby(['timestamp', 'lang', 'link']).size().reset_index()\n",
      "                       .pivot(index='timestamp', columns='lang', values='link')\n",
      "                       .fillna(method='ffill').fillna(0).sort_index())\n",
      "    return link_exists\n",
      "\n",
      "top_link_exists = get_link_exists_df(frl, top_link)\n",
      "59/163: second_link_exists = get_link_exists_df(frl, frl.link.value_counts().index[1])\n",
      "59/164: sns.heatmap(second_link_exists, cbar=False)\n",
      "59/165: sns.heatmap(second_link_exists, cbar=False, title='frl.link.value_counts().index[1]')\n",
      "59/166: sns.heatmap(second_link_exists, cbar=False).set_title(frl.link.value_counts().index[1])\n",
      "59/167:\n",
      "l = frl.link.value_counts().index[2]\n",
      "sns.heatmap(get_link_exists_df(frl, l), cbar=False).set_title(l)\n",
      "59/168:\n",
      "l = frl.link.value_counts().index[3]\n",
      "sns.heatmap(get_link_exists_df(frl, l), cbar=False).set_title(l)\n",
      "59/169:\n",
      "l = frl.link.value_counts().index[4]\n",
      "sns.heatmap(get_link_exists_df(frl, l), cbar=False).set_title(l)\n",
      "59/170:\n",
      "l = frl.link.value_counts().index[5]\n",
      "sns.heatmap(get_link_exists_df(frl, l), cbar=False).set_title(l)\n",
      "59/171:\n",
      "l = frl.link.value_counts().index[2]\n",
      "sns.heatmap(get_link_exists_df(frl, l), cbar=False, linewidths=.5).set_title(l)\n",
      "59/172:\n",
      "l = frl.link.value_counts().index[2]\n",
      "sns.heatmap(get_link_exists_df(frl, frl.link.value_counts().index[2]), cbar=False).set_title(l)\n",
      "59/173:\n",
      "for l in list(frl.link.value_counts().head(10).index):\n",
      "    sns.heatmap(get_link_exists_df(frl, l), cbar=False).set_title(l)\n",
      "59/174:\n",
      "fig, axes = plt.subplots(nrows=10)\n",
      "for ax, l in zip(axes, list(frl.link.value_counts().head(10).index)):\n",
      "    sns.heatmap(get_link_exists_df(frl, l), cbar=False, ax=ax).set_title(l)\n",
      "59/175: sns.heatmap(top_link_exists, cbar=False).set_title(top_link)\n",
      "59/176:\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "sns.set_context('talk')\n",
      "sns.set(rc={'figure.figsize':(20,15)})\n",
      "59/177: sns.heatmap(top_link_exists, cbar=False).set_title(top_link)\n",
      "59/178:\n",
      "fig, axes = plt.subplots(nrows=20, figsize=(20,40))\n",
      "for ax, l in zip(axes, list(frl.link.value_counts().head(20).index)):\n",
      "    sns.heatmap(get_link_exists_df(frl, l), cbar=False, ax=ax).set_title(l)\n",
      "59/179:\n",
      "fig, axes = plt.subplots(nrows=20, figsize=(20,45))\n",
      "for ax, l in zip(axes, list(frl.link.value_counts().head(20).index)):\n",
      "    sns.heatmap(get_link_exists_df(frl, l), cbar=False, ax=ax).set_title(l)\n",
      "59/180:\n",
      "fig, axes = plt.subplots(nrows=20, figsize=(20,55))\n",
      "for ax, l in zip(axes, list(frl.link.value_counts().head(20).index)):\n",
      "    sns.heatmap(get_link_exists_df(frl, l), cbar=False, ax=ax).set_title(l)\n",
      "59/181:\n",
      "fig, axes = plt.subplots(nrows=20, figsize=(20,55))\n",
      "for ax, l in zip(axes, list(frl.link.value_counts().head(20).index)):\n",
      "    sns.heatmap(get_link_exists_df(frl, l), cbar=False, ax=ax).set_title(l)\n",
      "\n",
      "plt.tight_layout()\n",
      "60/1: import pandas as pd\n",
      "60/2:\n",
      "df = pd.read_pickle('lang_uri_time_all.pkl.gz', compression='gzip')\n",
      "df.head()\n",
      "61/1: import pandas as pd\n",
      "62/1: import pandas as pd\n",
      "62/2:\n",
      "df = pd.read_pickle('lang_uri_time_all.pkl.gz', compression='gzip')\n",
      "df.head()\n",
      "62/3: df['uri_unsh_no_clean'] = df.uri_unshrtn_raw.combine_first(df.uri_unshrtn_bitly.combine_first(df.uri_unshrtn.combine_first(df.uri)))\n",
      "62/4: df.uri_unsh_no_clean.head()\n",
      "62/5:\n",
      "unique_uris = pd.concat([df.uri_cln, df.uri_cln_unshrtn]).drop_duplicates()\n",
      "unique_uris = unique_uris[~(unique_uris.str.contains('archive.org').fillna(False))].to_frame().rename(columns={0: 'uri'})\n",
      "unique_uris.head()\n",
      "62/6: import requests\n",
      "62/7:\n",
      "from tqdm import tqdm\n",
      "\n",
      "# Register `pandas.progress_apply` and `pandas.Series.map_apply` with `tqdm`\n",
      "# (can use `tqdm_gui`, `tqdm_notebook`, optional kwargs, etc.)\n",
      "tqdm.pandas(desc=\"my bar!\")\n",
      "\n",
      "# Now you can use `progress_apply` instead of `apply`\n",
      "# and `progress_map` instead of `map`\n",
      "#df.progress_apply(lambda x: x**2)\n",
      "# can also groupby:\n",
      "# df.groupby(0).progress_apply(lambda x: x**2)\n",
      "62/8:\n",
      "from tqdm import tqdm\n",
      "\n",
      "# Register `pandas.progress_apply` and `pandas.Series.map_apply` with `tqdm`\n",
      "# (can use `tqdm_gui`, `tqdm_notebook`, optional kwargs, etc.)\n",
      "tqdm.pandas(desc=\"my bar!\")\n",
      "\n",
      "# Now you can use `progress_apply` instead of `apply`\n",
      "# and `progress_map` instead of `map`\n",
      "#df.progress_apply(lambda x: x**2)\n",
      "# can also groupby:\n",
      "# df.groupby(0).progress_apply(lambda x: x**2)\n",
      "62/9:\n",
      "from functools import wraps\n",
      "\n",
      "def retry(times):\n",
      "\n",
      "    def wrapper_fn(f):\n",
      "\n",
      "        @wraps(f)\n",
      "        def new_wrapper(*args,**kwargs):\n",
      "            for i in range(times):\n",
      "                try:\n",
      "                    #print 'try %s' % (i + 1)\n",
      "                    return f(*args,**kwargs)\n",
      "                except Exception as e:\n",
      "                    error = e\n",
      "            raise error\n",
      "\n",
      "        return new_wrapper\n",
      "\n",
      "    return wrapper_fn\n",
      "\n",
      "\n",
      "import concurrent.futures\n",
      "\n",
      "@retry(3)\n",
      "def get_wb_avail(uri, ts):\n",
      "    resp = requests.get('https://archive.org/wayback/available', params={'url': uri, 'timestamp': ts}).json()\n",
      "    return resp\n",
      "62/10:\n",
      "import requests\n",
      "import concurrent.futures\n",
      "62/11:\n",
      "#base_uri = 'http://memgator.cs.odu.edu/timemap/json/'\n",
      "base_uri = 'http://timetravel.mementoweb.org/timemap/json/'\n",
      "#test_uri = missing_uris8[0]\n",
      "base_uri_time =  'http://timetravel.mementoweb.org/api/json/20140801000000/'\n",
      "test_uri = 'http://english.alarabiya.net/en/News/2014/07/30/Hamas-official-asks-Hezbollah-to-join-fight-against-Israel-.html'\n",
      "requests.get(base_uri+test_uri).json()\n",
      "62/12: requests.get(base_uri_time+test_uri).json()\n",
      "62/13:\n",
      "unique_uris_raw = pd.concat([df.uri, df.uri_unsh_no_clean]).drop_duplicates()\n",
      "unique_uris_raw = unique_uris_raw[~(unique_uris_raw.str.contains('archive.org').fillna(False))].to_frame().rename(columns={0: 'uri'})\n",
      "unique_uris_raw.head()\n",
      "62/14:\n",
      "@retry(3)\n",
      "def get_memento_avail(uri):\n",
      "    try:\n",
      "        resp = requests.get(base_uri+uri).json()\n",
      "    except:\n",
      "        resp = None\n",
      "    return resp\n",
      "62/15: get_memento_avail('http://archive.is/timemap/http://english.alarabiya.net/en/News/2014/07/30/Hamas-official-asks-Hezbollah-to-join-fight-against-Israel-.html')\n",
      "62/16: memento_avail = unique_uris_raw.progress_apply(lambda x: get_memento_avail(x.uri), axis=1)\n",
      "62/17: memento_avail\n",
      "62/18: memento_avail\n",
      "62/19: memento_avail.head()\n",
      "62/20: memento_avail.head()\n",
      "62/21: memento_avail.head()\n",
      "62/22: memento_avail.head()\n",
      "62/23: memento_avail.to_pickle('data/memento_avail.pkl.gz', compression='gzip')\n",
      "62/24: memento_avail.iat[0]\n",
      "62/25: memento_avail.iat[0].keys\n",
      "62/26: memento_avail.iat[0].keys()\n",
      "62/27: [x['archive_id'] for x in memento_avail.iat[0]['timemap_index']]\n",
      "62/28:\n",
      "mad = {ma['original_uri']: [x['archive_id'] \n",
      "                      for x in ma['timemap_index']] for ma in memento_avail.tolist()}\n",
      "62/29:\n",
      "mad = {ma['original_uri']: [x['archive_id'] \n",
      "                      for x in ma['timemap_index']] for ma in memento_avail.tolist() if ma is not None}\n",
      "62/30: memento_avail.isna().sum(), memento_avail.shape[0]\n",
      "62/31:\n",
      "mad = {'original_uri': ma['original_uri'], [x['archive_id'] \n",
      "                      for x in ma['timemap_index']] for ma in memento_avail.tolist() if ma is not None}\n",
      "62/32:\n",
      "mad = {'original_uri': ma['original_uri'], [x['archive_id']: True \n",
      "                      for x in ma['timemap_index']] for ma in memento_avail.tolist() if ma is not None}\n",
      "62/33:\n",
      "mad = {ma['original_uri']: [x['archive_id'] \n",
      "                      for x in ma['timemap_index']] for ma in memento_avail.tolist() if ma is not None}\n",
      "62/34: mad\n",
      "62/35: [len(x) for u, x in mad.items()]\n",
      "62/36: min([len(x) for u, x in mad.items()])\n",
      "62/37: max([len(x) for u, x in mad.items()])\n",
      "62/38:\n",
      "from collections import Counter\n",
      "Counter([len(x) for u, x in mad.items()])\n",
      "62/39: memento_avail.iat[0]\n",
      "62/40:\n",
      "@retry(3)\n",
      "def get_memento_avail(base_uri, uri):\n",
      "    try:\n",
      "        resp = requests.get(base_uri+uri).json()\n",
      "    except:\n",
      "        resp = None\n",
      "    return resp\n",
      "62/41: get_memento_avail(base_uri, 'http://archive.is/timemap/http://english.alarabiya.net/en/News/2014/07/30/Hamas-official-asks-Hezbollah-to-join-fight-against-Israel-.html')\n",
      "62/42:\n",
      "base_uri_cd = 'http://memgator.cs.odu.edu/timemap/json/'\n",
      "base_uri = 'http://timetravel.mementoweb.org/timemap/json/'\n",
      "#test_uri = missing_uris8[0]\n",
      "base_uri_time =  'http://timetravel.mementoweb.org/api/json/20140801000000/'\n",
      "test_uri = 'http://english.alarabiya.net/en/News/2014/07/30/Hamas-official-asks-Hezbollah-to-join-fight-against-Israel-.html'\n",
      "requests.get(base_uri+test_uri).json()\n",
      "62/43: get_memento_avail(base_uri_cd, 'http://archive.is/timemap/http://english.alarabiya.net/en/News/2014/07/30/Hamas-official-asks-Hezbollah-to-join-fight-against-Israel-.html')\n",
      "62/44: get_memento_avail(base_uri_cd, 'walla.co.il')\n",
      "62/45: get_memento_avail(base_uri_cd, 'archive.is/timemap/http://english.alarabiya.net/en/News/2014/07/30/Hamas-official-asks-Hezbollah-to-join-fight-against-Israel-.html')\n",
      "62/46: memento_avail_cd = unique_uris_raw.progress_apply(lambda x: get_memento_avail(base_uri_cd, x.uri), axis=1)\n",
      "63/1:\n",
      "import wikipediaapi as wa\n",
      "import pandas as pd\n",
      "import networkx as nx\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "import seaborn as sns\n",
      "sns.set(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
      "sns.set_context(\"talk\")\n",
      "sns.set_palette('Set2', 10)\n",
      "63/2: import pandas as pd\n",
      "64/1: import pandas as pd\n",
      "65/1: %matplotlib inline\n",
      "65/2:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "66/1: 1==2\n",
      "66/2: import pandas as pd\n",
      "66/3: import pandas as pd\n",
      "66/4: import pandas as pd\n",
      "66/5:\n",
      "for x in range(100000000):\n",
      "    pass\n",
      "66/6: __ver__\n",
      "66/7: import sys\n",
      "66/8: sys.version\n",
      "67/1: import pandas as pd\n",
      "67/2: ma = pd.read_pickle('data/memento_avail.pkl.gz', compression='gzip')\n",
      "67/3:\n",
      "mad = {m['original_uri']: [x['archive_id'] \n",
      "                      for x in m['timemap_index']] for m in ma.tolist() if m is not None}\n",
      "67/4: mad[0]\n",
      "67/5: mad = [({'original_uri': m['original_uri']}, {x['archive_id']: True for x in m['timemap_index']}) for m in ma.tolist() if m is not None}\n",
      "67/6: mad = [({'original_uri': m['original_uri']}}) for m in ma.tolist() if m is not None}\n",
      "67/7: mad = [({'original_uri': m['original_uri']}) for m in ma.tolist() if m is not None}\n",
      "67/8: mad = [({'original_uri': m['original_uri']}, {x['archive_id']: True for x in m['timemap_index']}) for m in ma.tolist() if m is not None]\n",
      "67/9: mad = [{**x, **y} for x, y in mad]\n",
      "67/10: mad[:5]\n",
      "67/11: mad = pd.DataFrame(mad)\n",
      "67/12: mad.head()\n",
      "67/13: mad = pd.DataFrame(mad).fillna(False)\n",
      "67/14: mad.sum(axis=1)\n",
      "67/15: mad = pd.DataFrame(mad).fillna(False).set_index('original_uri')\n",
      "67/16: mad.sum(axis=1)\n",
      "67/17: mad.sum(axis=0)\n",
      "67/18: mad.sum(axis=0)/127200\n",
      "67/19: (mad.sum(axis=0)/127200).sort_values(ascending=False)\n",
      "67/20: wb = pd.read_pickle('data/wb_avail_all_sources_df_FINAL.pkl.gz', compression='gzip')\n",
      "67/21: wb = pd.read_pickle('data/wb_avail_all_sources_df_FINAL.pkl.gz', compression='gzip')\n",
      "67/22: wb.head()\n",
      "67/23: wb[wb.available==False].head()\n",
      "67/24: wb[wb.available==False].uri.iat[0]\n",
      "67/25:\n",
      "@retry(3)\n",
      "def get_cdx_avail(base_uri = 'http://web.archive.org/cdx/search/cdx', uri):\n",
      "    try:\n",
      "        resp = requests.get(base_uri, params={'url': uri}).text()\n",
      "    except:\n",
      "        resp = None\n",
      "    return resp\n",
      "67/26:\n",
      "def retry(times):\n",
      "\n",
      "    def wrapper_fn(f):\n",
      "\n",
      "        @wraps(f)\n",
      "        def new_wrapper(*args,**kwargs):\n",
      "            for i in range(times):\n",
      "                try:\n",
      "                    #print 'try %s' % (i + 1)\n",
      "                    return f(*args,**kwargs)\n",
      "                except Exception as e:\n",
      "                    error = e\n",
      "            raise error\n",
      "\n",
      "        return new_wrapper\n",
      "\n",
      "    return wrapper_fn\n",
      "\n",
      "\n",
      "@retry(3)\n",
      "def get_cdx_avail(base_uri = 'http://web.archive.org/cdx/search/cdx', uri):\n",
      "    try:\n",
      "        resp = requests.get(base_uri, params={'url': uri}).text()\n",
      "    except:\n",
      "        resp = None\n",
      "    return resp\n",
      "67/27:\n",
      "def retry(times):\n",
      "\n",
      "    def wrapper_fn(f):\n",
      "\n",
      "        @wraps(f)\n",
      "        def new_wrapper(*args,**kwargs):\n",
      "            for i in range(times):\n",
      "                try:\n",
      "                    #print 'try %s' % (i + 1)\n",
      "                    return f(*args,**kwargs)\n",
      "                except Exception as e:\n",
      "                    error = e\n",
      "            raise error\n",
      "\n",
      "        return new_wrapper\n",
      "\n",
      "    return wrapper_fn\n",
      "\n",
      "\n",
      "@retry(3)\n",
      "def get_cdx_avail(base_uri, uri):\n",
      "    try:\n",
      "        resp = requests.get(base_uri, params={'url': uri}).text()\n",
      "    except:\n",
      "        resp = None\n",
      "    return resp\n",
      "67/28:\n",
      "from functools import wraps\n",
      "\n",
      "def retry(times):\n",
      "\n",
      "    def wrapper_fn(f):\n",
      "\n",
      "        @wraps(f)\n",
      "        def new_wrapper(*args,**kwargs):\n",
      "            for i in range(times):\n",
      "                try:\n",
      "                    #print 'try %s' % (i + 1)\n",
      "                    return f(*args,**kwargs)\n",
      "                except Exception as e:\n",
      "                    error = e\n",
      "            raise error\n",
      "\n",
      "        return new_wrapper\n",
      "\n",
      "    return wrapper_fn\n",
      "\n",
      "\n",
      "@retry(3)\n",
      "def get_cdx_avail(base_uri, uri):\n",
      "    try:\n",
      "        resp = requests.get(base_uri, params={'url': uri}).text()\n",
      "    except:\n",
      "        resp = None\n",
      "    return resp\n",
      "67/29:\n",
      "base_uri = 'http://web.archive.org/cdx/search/cdx'\n",
      "get_cdx_avail(base_uri, wb[wb.available==False].uri.iat[0])\n",
      "67/30:\n",
      "from functools import wraps\n",
      "\n",
      "def retry(times):\n",
      "\n",
      "    def wrapper_fn(f):\n",
      "\n",
      "        @wraps(f)\n",
      "        def new_wrapper(*args,**kwargs):\n",
      "            for i in range(times):\n",
      "                try:\n",
      "                    #print 'try %s' % (i + 1)\n",
      "                    return f(*args,**kwargs)\n",
      "                except Exception as e:\n",
      "                    error = e\n",
      "            raise error\n",
      "\n",
      "        return new_wrapper\n",
      "\n",
      "    return wrapper_fn\n",
      "\n",
      "\n",
      "@retry(3)\n",
      "def get_cdx_avail(base_uri, uri):\n",
      "    try:\n",
      "        resp = requests.get(base_uri, params={'url': uri})\n",
      "    except:\n",
      "        resp = None\n",
      "    return resp\n",
      "67/31:\n",
      "base_uri = 'http://web.archive.org/cdx/search/cdx'\n",
      "get_cdx_avail(base_uri, wb[wb.available==False].uri.iat[0])\n",
      "67/32: wb[wb.available==False].uri.iat[0]\n",
      "67/33:\n",
      "base_uri = 'http://web.archive.org/cdx/search/cdx'\n",
      "get_cdx_avail(base_uri, wb[wb.available==False].uri.iat[0])\n",
      "67/34: x\n",
      "67/35:\n",
      "base_uri = 'http://web.archive.org/cdx/search/cdx'\n",
      "x = get_cdx_avail(base_uri, wb[wb.available==False].uri.iat[0])\n",
      "67/36: x\n",
      "67/37: print(x)\n",
      "67/38:\n",
      "from functools import wraps\n",
      "\n",
      "def retry(times):\n",
      "\n",
      "    def wrapper_fn(f):\n",
      "\n",
      "        @wraps(f)\n",
      "        def new_wrapper(*args,**kwargs):\n",
      "            for i in range(times):\n",
      "                try:\n",
      "                    #print 'try %s' % (i + 1)\n",
      "                    return f(*args,**kwargs)\n",
      "                except Exception as e:\n",
      "                    error = e\n",
      "            raise error\n",
      "\n",
      "        return new_wrapper\n",
      "\n",
      "    return wrapper_fn\n",
      "\n",
      "\n",
      "@retry(3)\n",
      "def get_cdx_avail(base_uri, uri):\n",
      "    try:\n",
      "        resp = requests.get(base_uri, params={'url': uri})\n",
      "    except:\n",
      "        print('err')\n",
      "        resp = None\n",
      "    return resp\n",
      "67/39:\n",
      "base_uri = 'http://web.archive.org/cdx/search/cdx'\n",
      "get_cdx_avail(base_uri, wb[wb.available==False].uri.iat[0])\n",
      "67/40:\n",
      "from functools import wraps\n",
      "\n",
      "def retry(times):\n",
      "\n",
      "    def wrapper_fn(f):\n",
      "\n",
      "        @wraps(f)\n",
      "        def new_wrapper(*args,**kwargs):\n",
      "            for i in range(times):\n",
      "                try:\n",
      "                    #print 'try %s' % (i + 1)\n",
      "                    return f(*args,**kwargs)\n",
      "                except Exception as e:\n",
      "                    error = e\n",
      "            raise error\n",
      "\n",
      "        return new_wrapper\n",
      "\n",
      "    return wrapper_fn\n",
      "\n",
      "\n",
      "@retry(3)\n",
      "def get_cdx_avail(base_uri, uri):\n",
      "\n",
      "    resp = requests.get(base_uri, params={'url': uri})\n",
      "    return resp\n",
      "67/41:\n",
      "base_uri = 'http://web.archive.org/cdx/search/cdx'\n",
      "get_cdx_avail(base_uri, wb[wb.available==False].uri.iat[0])\n",
      "67/42:\n",
      "import requests\n",
      "from functools import wraps\n",
      "\n",
      "def retry(times):\n",
      "\n",
      "    def wrapper_fn(f):\n",
      "\n",
      "        @wraps(f)\n",
      "        def new_wrapper(*args,**kwargs):\n",
      "            for i in range(times):\n",
      "                try:\n",
      "                    #print 'try %s' % (i + 1)\n",
      "                    return f(*args,**kwargs)\n",
      "                except Exception as e:\n",
      "                    error = e\n",
      "            raise error\n",
      "\n",
      "        return new_wrapper\n",
      "\n",
      "    return wrapper_fn\n",
      "\n",
      "\n",
      "@retry(3)\n",
      "def get_cdx_avail(base_uri, uri):\n",
      "    try:\n",
      "        resp = requests.get(base_uri, params={'url': uri})\n",
      "    except:\n",
      "        resp = None\n",
      "    return resp\n",
      "67/43:\n",
      "base_uri = 'http://web.archive.org/cdx/search/cdx'\n",
      "get_cdx_avail(base_uri, wb[wb.available==False].uri.iat[0])\n",
      "67/44:\n",
      "import requests\n",
      "from functools import wraps\n",
      "\n",
      "def retry(times):\n",
      "\n",
      "    def wrapper_fn(f):\n",
      "\n",
      "        @wraps(f)\n",
      "        def new_wrapper(*args,**kwargs):\n",
      "            for i in range(times):\n",
      "                try:\n",
      "                    #print 'try %s' % (i + 1)\n",
      "                    return f(*args,**kwargs)\n",
      "                except Exception as e:\n",
      "                    error = e\n",
      "            raise error\n",
      "\n",
      "        return new_wrapper\n",
      "\n",
      "    return wrapper_fn\n",
      "\n",
      "\n",
      "@retry(3)\n",
      "def get_cdx_avail(base_uri, uri):\n",
      "    try:\n",
      "        resp = requests.get(base_uri, params={'url': uri}).text()\n",
      "    except:\n",
      "        resp = None\n",
      "    return resp\n",
      "67/45:\n",
      "base_uri = 'http://web.archive.org/cdx/search/cdx'\n",
      "get_cdx_avail(base_uri, wb[wb.available==False].uri.iat[0])\n",
      "67/46:\n",
      "import requests\n",
      "from functools import wraps\n",
      "\n",
      "def retry(times):\n",
      "\n",
      "    def wrapper_fn(f):\n",
      "\n",
      "        @wraps(f)\n",
      "        def new_wrapper(*args,**kwargs):\n",
      "            for i in range(times):\n",
      "                try:\n",
      "                    #print 'try %s' % (i + 1)\n",
      "                    return f(*args,**kwargs)\n",
      "                except Exception as e:\n",
      "                    error = e\n",
      "            raise error\n",
      "\n",
      "        return new_wrapper\n",
      "\n",
      "    return wrapper_fn\n",
      "\n",
      "\n",
      "@retry(3)\n",
      "def get_cdx_avail(base_uri, uri):\n",
      "    try:\n",
      "        resp = requests.get(base_uri, params={'url': uri})\n",
      "    except:\n",
      "        resp = None\n",
      "    return resp\n",
      "67/47:\n",
      "base_uri = 'http://web.archive.org/cdx/search/cdx'\n",
      "x = get_cdx_avail(base_uri, wb[wb.available==False].uri.iat[0])\n",
      "67/48: dir(x)\n",
      "67/49: x.text\n",
      "67/50:\n",
      "import requests\n",
      "from functools import wraps\n",
      "\n",
      "def retry(times):\n",
      "\n",
      "    def wrapper_fn(f):\n",
      "\n",
      "        @wraps(f)\n",
      "        def new_wrapper(*args,**kwargs):\n",
      "            for i in range(times):\n",
      "                try:\n",
      "                    #print 'try %s' % (i + 1)\n",
      "                    return f(*args,**kwargs)\n",
      "                except Exception as e:\n",
      "                    error = e\n",
      "            raise error\n",
      "\n",
      "        return new_wrapper\n",
      "\n",
      "    return wrapper_fn\n",
      "\n",
      "\n",
      "@retry(3)\n",
      "def get_cdx_avail(base_uri, uri):\n",
      "    try:\n",
      "        resp = requests.get(base_uri, params={'url': uri}).text\n",
      "    except:\n",
      "        resp = None\n",
      "    return resp\n",
      "67/51:\n",
      "base_uri = 'http://web.archive.org/cdx/search/cdx'\n",
      "x = get_cdx_avail(base_uri, wb[wb.available==False].uri.iat[0])\n",
      "67/52: x.text\n",
      "67/53:\n",
      "base_uri = 'http://web.archive.org/cdx/search/cdx'\n",
      "get_cdx_avail(base_uri, wb[wb.available==False].uri.iat[0])\n",
      "67/54: x\n",
      "67/55:\n",
      "base_uri = 'http://web.archive.org/cdx/search/cdx'\n",
      "get_cdx_avail(base_uri, wb[wb.available==False].uri.iat[0])\n",
      "67/56: x\n",
      "67/57:\n",
      "import requests\n",
      "from functools import wraps\n",
      "\n",
      "def retry(times):\n",
      "\n",
      "    def wrapper_fn(f):\n",
      "\n",
      "        @wraps(f)\n",
      "        def new_wrapper(*args,**kwargs):\n",
      "            for i in range(times):\n",
      "                try:\n",
      "                    #print 'try %s' % (i + 1)\n",
      "                    return f(*args,**kwargs)\n",
      "                except Exception as e:\n",
      "                    error = e\n",
      "            raise error\n",
      "\n",
      "        return new_wrapper\n",
      "\n",
      "    return wrapper_fn\n",
      "\n",
      "\n",
      "@retry(3)\n",
      "def get_cdx_avail(base_uri, uri):\n",
      "    try:\n",
      "        resp = requests.get(base_uri, params={'url': uri})\n",
      "        #resp = resp.re\n",
      "    except:\n",
      "        resp = None\n",
      "    return resp\n",
      "67/58:\n",
      "base_uri = 'http://web.archive.org/cdx/search/cdx'\n",
      "get_cdx_avail(base_uri, wb[wb.available==False].uri.iat[0])\n",
      "67/59: x\n",
      "67/60: dir(x)\n",
      "67/61:\n",
      "base_uri = 'http://web.archive.org/cdx/search/cdx'\n",
      "x = get_cdx_avail(base_uri, wb[wb.available==False].uri.iat[0])\n",
      "67/62: dir(x)\n",
      "67/63:\n",
      "import requests\n",
      "from functools import wraps\n",
      "\n",
      "def retry(times):\n",
      "\n",
      "    def wrapper_fn(f):\n",
      "\n",
      "        @wraps(f)\n",
      "        def new_wrapper(*args,**kwargs):\n",
      "            for i in range(times):\n",
      "                try:\n",
      "                    #print 'try %s' % (i + 1)\n",
      "                    return f(*args,**kwargs)\n",
      "                except Exception as e:\n",
      "                    error = e\n",
      "            raise error\n",
      "\n",
      "        return new_wrapper\n",
      "\n",
      "    return wrapper_fn\n",
      "\n",
      "\n",
      "@retry(3)\n",
      "def get_cdx_avail(base_uri, uri):\n",
      "    try:\n",
      "        resp = requests.get(base_uri, params={'url': uri})\n",
      "        resp = (resp.url, resp.status_code, resp.text)\n",
      "    except:\n",
      "        resp = None\n",
      "    return resp\n",
      "67/64:\n",
      "base_uri = 'http://web.archive.org/cdx/search/cdx'\n",
      "x = get_cdx_avail(base_uri, wb[wb.available==False].uri.iat[0])\n",
      "67/65:\n",
      "base_uri = 'http://web.archive.org/cdx/search/cdx'\n",
      "x = get_cdx_avail(base_uri, wb[wb.available==False].uri.iat[0])\n",
      "67/66: x\n",
      "67/67:\n",
      "df = pd.read_pickle('lang_uri_time_all.pkl.gz', compression='gzip')\n",
      "df.head()\n",
      "67/68:\n",
      "unique_uris = pd.concat([df.uri_cln, df.uri_cln_unshrtn]).drop_duplicates()\n",
      "unique_uris = unique_uris[~(unique_uris.str.contains('archive.org').fillna(False))].to_frame().rename(columns={0: 'uri'})\n",
      "unique_uris.head()\n",
      "67/69:\n",
      "from tqdm import tqdm\n",
      "\n",
      "# Register `pandas.progress_apply` and `pandas.Series.map_apply` with `tqdm`\n",
      "# (can use `tqdm_gui`, `tqdm_notebook`, optional kwargs, etc.)\n",
      "tqdm.pandas(desc=\"my bar!\")\n",
      "\n",
      "# Now you can use `progress_apply` instead of `apply`\n",
      "# and `progress_map` instead of `map`\n",
      "#df.progress_apply(lambda x: x**2)\n",
      "# can also groupby:\n",
      "# df.groupby(0).progress_apply(lambda x: x**2)\n",
      "67/70: cdx_avail = unique_uris_raw.head().progress_apply(lambda x: get_cdx_avail(base_uri, x.uri), axis=1)\n",
      "67/71:\n",
      "from tqdm import tqdm\n",
      "\n",
      "# Register `pandas.progress_apply` and `pandas.Series.map_apply` with `tqdm`\n",
      "# (can use `tqdm_gui`, `tqdm_notebook`, optional kwargs, etc.)\n",
      "tqdm.pandas(desc=\"my bar!\")\n",
      "\n",
      "# Now you can use `progress_apply` instead of `apply`\n",
      "# and `progress_map` instead of `map`\n",
      "#df.progress_apply(lambda x: x**2)\n",
      "# can also groupby:\n",
      "# df.groupby(0).progress_apply(lambda x: x**2)\n",
      "67/72: cdx_avail = unique_uris.head().progress_apply(lambda x: get_cdx_avail(base_uri, x.uri), axis=1)\n",
      "67/73: cdx_avail\n",
      "67/74: cdx_avail.tolist()\n",
      "67/75: cdx_avail = unique_uris.progress_apply(lambda x: get_cdx_avail(base_uri, x.uri), axis=1)\n",
      "68/1: import pandas as pd\n",
      "68/2: ma = pd.read_pickle('data/memento_avail.pkl.gz', compression='gzip')\n",
      "68/3: mad = [({'original_uri': m['original_uri']}, {x['archive_id']: True for x in m['timemap_index']}) for m in ma.tolist() if m is not None]\n",
      "68/4: mad = [{**x, **y} for x, y in mad]\n",
      "68/5: mad = pd.DataFrame(mad).fillna(False).set_index('original_uri')\n",
      "68/6: (mad.sum(axis=0)/127200).sort_values(ascending=False)\n",
      "68/7: wb = pd.read_pickle('data/wb_avail_all_sources_df_FINAL.pkl.gz', compression='gzip')\n",
      "68/8: wb[wb.available==False].uri.iat[0]\n",
      "68/9:\n",
      "import requests\n",
      "from functools import wraps\n",
      "\n",
      "def retry(times):\n",
      "\n",
      "    def wrapper_fn(f):\n",
      "\n",
      "        @wraps(f)\n",
      "        def new_wrapper(*args,**kwargs):\n",
      "            for i in range(times):\n",
      "                try:\n",
      "                    #print 'try %s' % (i + 1)\n",
      "                    return f(*args,**kwargs)\n",
      "                except Exception as e:\n",
      "                    error = e\n",
      "            raise error\n",
      "\n",
      "        return new_wrapper\n",
      "\n",
      "    return wrapper_fn\n",
      "\n",
      "\n",
      "@retry(3)\n",
      "def get_cdx_avail(base_uri, uri):\n",
      "    try:\n",
      "        resp = requests.get(base_uri, params={'url': uri})\n",
      "        if resp.status_code==200:\n",
      "            text = resp.text\n",
      "        else:\n",
      "            text = None\n",
      "        resp = (resp.url, resp.status_code, resp.text)\n",
      "    except:\n",
      "        resp = None\n",
      "    return resp\n",
      "68/10:\n",
      "base_uri = 'http://web.archive.org/cdx/search/cdx'\n",
      "x = get_cdx_avail(base_uri, wb[wb.available==False].uri.iat[0])\n",
      "68/11: x\n",
      "68/12:\n",
      "import requests\n",
      "from functools import wraps\n",
      "\n",
      "def retry(times):\n",
      "\n",
      "    def wrapper_fn(f):\n",
      "\n",
      "        @wraps(f)\n",
      "        def new_wrapper(*args,**kwargs):\n",
      "            for i in range(times):\n",
      "                try:\n",
      "                    #print 'try %s' % (i + 1)\n",
      "                    return f(*args,**kwargs)\n",
      "                except Exception as e:\n",
      "                    error = e\n",
      "            raise error\n",
      "\n",
      "        return new_wrapper\n",
      "\n",
      "    return wrapper_fn\n",
      "\n",
      "\n",
      "@retry(3)\n",
      "def get_cdx_avail(base_uri, uri):\n",
      "    try:\n",
      "        resp = requests.get(base_uri, params={'url': uri})\n",
      "        if resp.status_code==200:\n",
      "            text = resp.text\n",
      "        else:\n",
      "            text = None\n",
      "        resp = (resp.url, resp.status_code, text)\n",
      "    except:\n",
      "        resp = None\n",
      "    return resp\n",
      "68/13:\n",
      "base_uri = 'http://web.archive.org/cdx/search/cdx'\n",
      "x = get_cdx_avail(base_uri, wb[wb.available==False].uri.iat[0])\n",
      "68/14: x\n",
      "68/15:\n",
      "base_uri = 'http://web.archive.org/cdx/search/cdx'\n",
      "x = get_cdx_avail(base_uri, wb[wb.available==False].uri.iat[0]+'2139023')\n",
      "68/16: x\n",
      "68/17: x[2]\n",
      "68/18:\n",
      "import requests\n",
      "from functools import wraps\n",
      "\n",
      "def retry(times):\n",
      "\n",
      "    def wrapper_fn(f):\n",
      "\n",
      "        @wraps(f)\n",
      "        def new_wrapper(*args,**kwargs):\n",
      "            for i in range(times):\n",
      "                try:\n",
      "                    #print 'try %s' % (i + 1)\n",
      "                    return f(*args,**kwargs)\n",
      "                except Exception as e:\n",
      "                    error = e\n",
      "            raise error\n",
      "\n",
      "        return new_wrapper\n",
      "\n",
      "    return wrapper_fn\n",
      "\n",
      "\n",
      "@retry(3)\n",
      "def get_cdx_avail(base_uri, uri):\n",
      "    try:\n",
      "        resp = requests.get(base_uri, params={'url': uri})\n",
      "        if resp.status_code==200:\n",
      "            text = resp.text\n",
      "        else:\n",
      "            text = None\n",
      "        resp = (resp.url, resp.status_code, text)\n",
      "    except:\n",
      "        resp = None\n",
      "    return resp\n",
      "68/19:\n",
      "base_uri = 'http://web.archive.org/cdx/search/cdx'\n",
      "x = get_cdx_avail(base_uri, wb[wb.available==False].uri.iat[0]+'2139023')\n",
      "68/20: x[2]\n",
      "68/21:\n",
      "import requests\n",
      "from functools import wraps\n",
      "\n",
      "def retry(times):\n",
      "\n",
      "    def wrapper_fn(f):\n",
      "\n",
      "        @wraps(f)\n",
      "        def new_wrapper(*args,**kwargs):\n",
      "            for i in range(times):\n",
      "                try:\n",
      "                    #print 'try %s' % (i + 1)\n",
      "                    return f(*args,**kwargs)\n",
      "                except Exception as e:\n",
      "                    error = e\n",
      "            raise error\n",
      "\n",
      "        return new_wrapper\n",
      "\n",
      "    return wrapper_fn\n",
      "\n",
      "\n",
      "@retry(3)\n",
      "def get_cdx_avail(base_uri, uri):\n",
      "    try:\n",
      "        resp = requests.get(base_uri, params={'url': uri})\n",
      "        if resp.status_code=='200':\n",
      "            text = resp.text\n",
      "        else:\n",
      "            text = None\n",
      "        resp = (resp.url, resp.status_code, text)\n",
      "    except:\n",
      "        resp = None\n",
      "    return resp\n",
      "68/22:\n",
      "base_uri = 'http://web.archive.org/cdx/search/cdx'\n",
      "x = get_cdx_avail(base_uri, wb[wb.available==False].uri.iat[0]+'2139023')\n",
      "68/23: x[2]\n",
      "68/24:\n",
      "import requests\n",
      "from functools import wraps\n",
      "\n",
      "def retry(times):\n",
      "\n",
      "    def wrapper_fn(f):\n",
      "\n",
      "        @wraps(f)\n",
      "        def new_wrapper(*args,**kwargs):\n",
      "            for i in range(times):\n",
      "                try:\n",
      "                    #print 'try %s' % (i + 1)\n",
      "                    return f(*args,**kwargs)\n",
      "                except Exception as e:\n",
      "                    error = e\n",
      "            raise error\n",
      "\n",
      "        return new_wrapper\n",
      "\n",
      "    return wrapper_fn\n",
      "\n",
      "\n",
      "@retry(3)\n",
      "def get_cdx_avail(base_uri, uri):\n",
      "    try:\n",
      "        resp = requests.get(base_uri, params={'url': uri})\n",
      "        resp = (resp.url, resp.status_code, resp.text)\n",
      "    except:\n",
      "        resp = None\n",
      "    return resp\n",
      "68/25:\n",
      "base_uri = 'http://web.archive.org/cdx/search/cdx'\n",
      "x = get_cdx_avail(base_uri, wb[wb.available==False].uri.iat[0]+'2139023')\n",
      "68/26: x[2]\n",
      "68/27:\n",
      "df = pd.read_pickle('lang_uri_time_all.pkl.gz', compression='gzip')\n",
      "df.head()\n",
      "68/28:\n",
      "unique_uris = pd.concat([df.uri_cln, df.uri_cln_unshrtn]).drop_duplicates()\n",
      "unique_uris = unique_uris[~(unique_uris.str.contains('archive.org').fillna(False))].to_frame().rename(columns={0: 'uri'})\n",
      "unique_uris.head()\n",
      "68/29:\n",
      "from tqdm import tqdm\n",
      "\n",
      "# Register `pandas.progress_apply` and `pandas.Series.map_apply` with `tqdm`\n",
      "# (can use `tqdm_gui`, `tqdm_notebook`, optional kwargs, etc.)\n",
      "tqdm.pandas(desc=\"my bar!\")\n",
      "\n",
      "# Now you can use `progress_apply` instead of `apply`\n",
      "# and `progress_map` instead of `map`\n",
      "#df.progress_apply(lambda x: x**2)\n",
      "# can also groupby:\n",
      "# df.groupby(0).progress_apply(lambda x: x**2)\n",
      "68/30: cdx_avail = unique_uris.progress_apply(lambda x: get_cdx_avail(base_uri, x.uri), axis=1)\n",
      "68/31: cdx_avail.head()\n",
      "68/32: cdx_avail.head()\n",
      "68/33: cdx_avail.to_pickle('data/cdx_avail.pkl.gz', compression='gzip')\n",
      "68/34: cdx_avail.head()\n",
      "68/35: cdx_status = [(x[0], x[1]) for x in cdx_avail.tolist()]\n",
      "68/36: cdx_status = pd.DataFrame([(x[0], x[1]) for x in cdx_avail.tolist()], header=['uri', 'status'])\n",
      "68/37: cdx_status.head()\n",
      "68/38: cdx_status.head()\n",
      "68/39: cdx_status = pd.DataFrame([(x[0], x[1], len(x[2].split('\\n'))) for x in cdx_avail.tolist()], header=['uri', 'status', 'lines'])\n",
      "68/40: cdx_status = pd.DataFrame([(x[0], x[1], len(x[2].split('\\n'))) for x in cdx_avail.tolist()], columns=['uri', 'status', 'lines'])\n",
      "68/41: cdx_status.head()\n",
      "68/42: cdx_status.status.value_counts()\n",
      "68/43: cdx_status.lines.value_counts()\n",
      "68/44: cdx_status.status.value_counts()\n",
      "68/45: cdx_avail.iat[0]\n",
      "68/46: cdx_status = pd.DataFrame([(x[0], x[1], len(x[2].split('\\n'))) for x in cdx_avail.tolist()], columns=['uri', 'status', 'lines'] if len(x[2]>0 else (x[0], x[1], 0))\n",
      "68/47: cdx_status = pd.DataFrame([(x[0], x[1], len(x[2].split('\\n'))) for x in cdx_avail.tolist() if len(x[2]>0 else (x[0], x[1], 0))], columns=['uri', 'status', 'lines'])\n",
      "68/48:\n",
      "cdx_status = pd.DataFrame([(x[0], x[1], len(x[2].split('\\n'))) \n",
      "                           for x in cdx_avail.tolist() \n",
      "                           if len(x[2])>0 else (x[0], x[1], 0)], \n",
      "                          columns=['uri', 'status', 'lines'])\n",
      "68/49:\n",
      "cdx_status = pd.DataFrame([(x[0], x[1], len(x[2].split('\\n'))) \n",
      "                           for x in cdx_avail.tolist() \n",
      "                           if: len(x[2])>0 else: (x[0], x[1], 0)], \n",
      "                          columns=['uri', 'status', 'lines'])\n",
      "68/50:\n",
      "cdx_status = pd.DataFrame([(x[0], x[1], len(x[2].split('\\n'))) \n",
      "                           for x in cdx_avail.tolist() \n",
      "                           if len(x[2])>0 else (x[0], x[1], 0)], \n",
      "                          columns=['uri', 'status', 'lines'])\n",
      "68/51:\n",
      "cdx_status = pd.DataFrame([(x[0], x[1], len(x[2].split('\\n')))\n",
      "                            if len(x[2])>0 else (x[0], x[1], 0)\n",
      "                           for x in cdx_avail.tolist() \n",
      "                           ], \n",
      "                          columns=['uri', 'status', 'lines'])\n",
      "68/52: cdx_status.head()\n",
      "68/53: cdx_status.lines.value_counts()\n",
      "68/54: 99636 / unique_uris.shape[0]\n",
      "68/55: cdx_status.to_csv('data/cdx_avail_lines.csv.gz', index=False, compression='gzip')\n",
      "68/56:\n",
      "@retry(3)\n",
      "def get_memento_avail(base_uri, uri):\n",
      "    try:\n",
      "        resp = requests.get(base_uri+uri).json()\n",
      "    except:\n",
      "        resp = None\n",
      "    return resp\n",
      "68/57: base_uri_time =  'http://timetravel.mementoweb.org/api/json/20140801000000/'\n",
      "68/58: get_memento_avail(base_uri_time, 'archive.is/timemap/http://english.alarabiya.net/en/News/2014/07/30/Hamas-official-asks-Hezbollah-to-join-fight-against-Israel-.html')\n",
      "68/59: get_memento_avail(base_uri_time, 'http://archive.is/timemap/http://english.alarabiya.net/en/News/2014/07/30/Hamas-official-asks-Hezbollah-to-join-fight-against-Israel-.html')\n",
      "68/60: get_memento_avail(base_uri_time, 'english.alarabiya.net/en/News/2014/07/30/Hamas-official-asks-Hezbollah-to-join-fight-against-Israel-.html')\n",
      "68/61:\n",
      "@retry(3)\n",
      "def get_memento_avail(base_uri, uri):\n",
      "    try:\n",
      "        resp = requests.get(base_uri+uri).json()\n",
      "    except:\n",
      "        resp = None\n",
      "    return resp\n",
      "68/62: base_uri_time =  'http://timetravel.mementoweb.org/api/json/20140801000000/'\n",
      "68/63: get_memento_avail(base_uri_time, 'english.alarabiya.net/en/News/2014/07/30/Hamas-official-asks-Hezbollah-to-join-fight-against-Israel-.html')\n",
      "68/64: memento_api2_avail = unique_uris.progress_apply(lambda x: get_memento_avail(base_uri_time, x.uri), axis=1)\n",
      "69/1: import pandas as pd\n",
      "69/2: ma = pd.read_pickle('data/memento_avail.pkl.gz', compression='gzip')\n",
      "69/3: mad = [({'original_uri': m['original_uri']}, {x['archive_id']: True for x in m['timemap_index']}) for m in ma.tolist() if m is not None]\n",
      "69/4: mad = [{**x, **y} for x, y in mad]\n",
      "69/5: mad = pd.DataFrame(mad).fillna(False).set_index('original_uri')\n",
      "69/6: (mad.sum(axis=0)/127200).sort_values(ascending=False)\n",
      "69/7: wb = pd.read_pickle('data/wb_avail_all_sources_df_FINAL.pkl.gz', compression='gzip')\n",
      "69/8: wb[wb.available==False].uri.iat[0]\n",
      "69/9:\n",
      "import requests\n",
      "from functools import wraps\n",
      "\n",
      "def retry(times):\n",
      "\n",
      "    def wrapper_fn(f):\n",
      "\n",
      "        @wraps(f)\n",
      "        def new_wrapper(*args,**kwargs):\n",
      "            for i in range(times):\n",
      "                try:\n",
      "                    #print 'try %s' % (i + 1)\n",
      "                    return f(*args,**kwargs)\n",
      "                except Exception as e:\n",
      "                    error = e\n",
      "            raise error\n",
      "\n",
      "        return new_wrapper\n",
      "\n",
      "    return wrapper_fn\n",
      "\n",
      "\n",
      "@retry(3)\n",
      "def get_cdx_avail(base_uri, uri):\n",
      "    try:\n",
      "        resp = requests.get(base_uri, params={'url': uri})\n",
      "        resp = (resp.url, resp.status_code, resp.text)\n",
      "    except:\n",
      "        resp = None\n",
      "    return resp\n",
      "69/10:\n",
      "df = pd.read_pickle('lang_uri_time_all.pkl.gz', compression='gzip')\n",
      "df.head()\n",
      "69/11:\n",
      "unique_uris = pd.concat([df.uri_cln, df.uri_cln_unshrtn]).drop_duplicates()\n",
      "unique_uris = unique_uris[~(unique_uris.str.contains('archive.org').fillna(False))].to_frame().rename(columns={0: 'uri'})\n",
      "unique_uris.head()\n",
      "69/12:\n",
      "from tqdm import tqdm\n",
      "\n",
      "# Register `pandas.progress_apply` and `pandas.Series.map_apply` with `tqdm`\n",
      "# (can use `tqdm_gui`, `tqdm_notebook`, optional kwargs, etc.)\n",
      "tqdm.pandas(desc=\"my bar!\")\n",
      "\n",
      "# Now you can use `progress_apply` instead of `apply`\n",
      "# and `progress_map` instead of `map`\n",
      "#df.progress_apply(lambda x: x**2)\n",
      "# can also groupby:\n",
      "# df.groupby(0).progress_apply(lambda x: x**2)\n",
      "69/13:\n",
      "@retry(3)\n",
      "def get_memento_avail(base_uri, uri):\n",
      "    try:\n",
      "        resp = requests.get(base_uri+uri).json()\n",
      "    except:\n",
      "        resp = None\n",
      "    return resp\n",
      "69/14: base_uri_time =  'http://timetravel.mementoweb.org/api/json/20140801000000/'\n",
      "69/15: get_memento_avail(base_uri_time, 'english.alarabiya.net/en/News/2014/07/30/Hamas-official-asks-Hezbollah-to-join-fight-against-Israel-.html')\n",
      "69/16: base_uri_time =  'http://timetravel.mementoweb.org/api/json/20140801000000/'\n",
      "69/17: get_memento_avail(base_uri_time, 'english.alarabiya.net/en/News/2014/07/30/Hamas-official-asks-Hezbollah-to-join-fight-against-Israel-.html')\n",
      "69/18: cdx_statux = pd.read_csv('data/cdx_avail_lines.csv.gz', compression='gzip')\n",
      "69/19: cdx_status = pd.read_csv('data/cdx_avail_lines.csv.gz', compression='gzip')\n",
      "69/20: cdx_status.head()\n",
      "69/21: cdx_status[cdx_status.lines==0]\n",
      "69/22: cdx_status[cdx_status.lines==1]\n",
      "69/23: cdx_status[cdx_status.lines==2]\n",
      "69/24: cdx_avail.head()\n",
      "69/25: cdx_avail = pd.read_pickle('data/cdx_avail.pkl.gz', compression='gzip')\n",
      "69/26: cdx_avail.head()\n",
      "69/27: import pandas as pd\n",
      "69/28:\n",
      "df = pd.read_pickle('lang_uri_time_all.pkl.gz', compression='gzip')\n",
      "df.head()\n",
      "69/29:\n",
      "unique_uris = pd.concat([df.uri_cln, df.uri_cln_unshrtn]).drop_duplicates()\n",
      "unique_uris = unique_uris[~(unique_uris.str.contains('archive.org').fillna(False))].to_frame().rename(columns={0: 'uri'})\n",
      "unique_uris.head()\n",
      "69/30: cdx_avail = pd.concat([unique_uris, cdx_avail], axis=1)\n",
      "69/31: cdx_avail.head()\n",
      "69/32: cdx_avail.head().tolist()\n",
      "69/33: cdx_avail.head().iteritems()\n",
      "69/34: list(cdx_avail.head().iteritems())\n",
      "69/35: list(cdx_avail.head().iteritems())[0]\n",
      "69/36: list(cdx_avail.head().iteritems(axis=1))[0]\n",
      "69/37: list(zip(cdx_avail.head().iteritems()))[0]\n",
      "69/38: list(zip(cdx_avail.head().iteritems()))\n",
      "69/39:\n",
      "cdx_status = pd.DataFrame([(u, x[0], x[1], len(x[2].split('\\n')))\n",
      "                            if len(x[2])>0 else (x[0], x[1], 0)\n",
      "                           for u, x in zip(cdx_avail.tolist() \n",
      "                           ], \n",
      "                          columns=['uri', 'cdx_url', 'status', 'lines'])\n",
      "69/40:\n",
      "cdx_status = pd.DataFrame([(u, x[0], x[1], len(x[2].split('\\n')))\n",
      "                            if len(x[2])>0 else (x[0], x[1], 0)\n",
      "                           for u, x in zip(cdx_avail.iloc[:, 0].tolist(), cdx_avail.iloc[:, 1])\n",
      "                           ], \n",
      "                          columns=['uri', 'cdx_url', 'status', 'lines'])\n",
      "69/41: cdx_status.head()\n",
      "69/42:\n",
      "cdx_status = pd.DataFrame([(u, x[0], x[1], len(x[2].split('\\n')))\n",
      "                            if len(x[2])>0 else (u, x[0], x[1], 0)\n",
      "                           for u, x in zip(cdx_avail.iloc[:, 0].tolist(), cdx_avail.iloc[:, 1])\n",
      "                           ], \n",
      "                          columns=['uri', 'cdx_url', 'status', 'lines'])\n",
      "69/43: cdx_status.head()\n",
      "69/44: 99636 / unique_uris.shape[0]\n",
      "69/45: cdx_status.to_csv('data/cdx_avail_lines.csv.gz', index=False, compression='gzip')\n",
      "69/46: cdx_status[cdx_status.lines==2]\n",
      "69/47: cdx_status[cdx_status.lines==2].head()\n",
      "69/48: cdx_avail.reset_index()[cdx_status.lines==2].head()\n",
      "69/49: cdx_avail.reset_index()[cdx_status.lines==2].iat[3]\n",
      "69/50: cdx_avail.reset_index()[cdx_status.lines==2].iat[2]\n",
      "69/51: cdx_avail.reset_index()[cdx_status.lines==2].iat[0, 3]\n",
      "69/52: cdx_avail.reset_index()[cdx_status.lines==2].iat[0, 2]\n",
      "69/53: cdx_avail.reset_index()[cdx_status.lines==2].iat[0, 2][2]\n",
      "69/54: print(cdx_avail.reset_index()[cdx_status.lines==2].iat[0, 2][2])\n",
      "69/55:\n",
      "cdx_status = pd.DataFrame([(u, x[0], x[1], len(x[2].strip().split('\\n')))\n",
      "                            if len(x[2])>0 else (u, x[0], x[1], 0)\n",
      "                           for u, x in zip(cdx_avail.iloc[:, 0].tolist(), cdx_avail.iloc[:, 1])\n",
      "                           ], \n",
      "                          columns=['uri', 'cdx_url', 'status', 'lines'])\n",
      "69/56: cdx_status.head()\n",
      "69/57: 99636 / unique_uris.shape[0]\n",
      "69/58: cdx_status = pd.read_csv('data/cdx_avail_lines.csv.gz', compression='gzip')\n",
      "69/59: cdx_status[cdx_status.lines==2].head()\n",
      "69/60: print(cdx_avail.reset_index()[cdx_status.lines==2].iat[0, 2][2])\n",
      "69/61:\n",
      "cdx_status = pd.DataFrame([(u, x[0], x[1], len(x[2].strip('\\n').split('\\n')))\n",
      "                            if len(x[2])>0 else (u, x[0], x[1], 0)\n",
      "                           for u, x in zip(cdx_avail.iloc[:, 0].tolist(), cdx_avail.iloc[:, 1])\n",
      "                           ], \n",
      "                          columns=['uri', 'cdx_url', 'status', 'lines'])\n",
      "69/62: cdx_status.head()\n",
      "69/63: 99636 / unique_uris.shape[0]\n",
      "69/64: cdx_status = pd.read_csv('data/cdx_avail_lines.csv.gz', compression='gzip')\n",
      "69/65: cdx_status[cdx_status.lines==2].head()\n",
      "69/66: print(cdx_avail.reset_index()[cdx_status.lines==2].iat[0, 2][2])\n",
      "69/67: '2132'.split('\\n')\n",
      "69/68: '2132\\n'.strip().split('\\n')\n",
      "69/69:\n",
      "cdx_status = pd.DataFrame([(u, x[0], x[1], len(x[2].strip().split('\\n')))\n",
      "                            if len(x[2])>0 else (u, x[0], x[1], 0)\n",
      "                           for u, x in zip(cdx_avail.iloc[:, 0].tolist(), cdx_avail.iloc[:, 1])\n",
      "                           ], \n",
      "                          columns=['uri', 'cdx_url', 'status', 'lines'])\n",
      "69/70: cdx_status.head()\n",
      "69/71: print(cdx_avail.reset_index().iat[1, 2][2])\n",
      "69/72: cdx_status.head()\n",
      "69/73: cdx_status[cdx_status.lines==2].head()\n",
      "69/74: print(cdx_avail.reset_index()[cdx_status.lines==2].iat[0, 2][2])\n",
      "69/75: cdx_status[cdx_status.lines==1].head()\n",
      "69/76: print(cdx_avail.reset_index()[cdx_status.lines==2].iat[0, 2][2])\n",
      "69/77: cdx_status.lines.value_counts()\n",
      "69/78: wb_df = pd.read_pickle('data/wb_avail_all_sources_df_FINAL.pkl.gz', compression='gzip')\n",
      "69/79: wb_df.head()\n",
      "69/80: wb = cdx_status.merge(wb_df[['uri', 'available']], on='uri', how='left')\n",
      "69/81: wb.head()\n",
      "69/82: wb = cdx_status.merge(wb_df[['uri', 'available']], on='uri', how='left').fillna(False)\n",
      "69/83: wb.head()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/84: wb = cdx_status[['lines', 'uri']].merge(wb_df[['uri', 'available']], on='uri', how='left').fillna(False)\n",
      "69/85: wb[[lines=]]\n",
      "69/86: wb.head\n",
      "69/87: wb = cdx_status[['lines', 'uri']]\n",
      "69/88: wb.head\n",
      "69/89: wb = cdx_status['lines', 'uri']\n",
      "69/90: wb = cdx_status[['lines', 'uri']]\n",
      "69/91: wb.head\n",
      "69/92: wb = cdx_status[['lines', 'uri']].merge(wb_df[['uri', 'available']], on='uri', how='left').fillna(False)\n",
      "69/93: wb.head()\n",
      "69/94: wb = cdx_status[['uri', 'lines']].merge(wb_df[['uri', 'available']], on='uri', how='left').fillna(False)\n",
      "69/95: wb.head()\n",
      "69/96: wb = cdx_status[['uri', 'lines']].rename(columns={'lines': 'cdx_records'}).merge(wb_df[['uri', 'available']], on='uri', how='left').fillna(False)\n",
      "69/97: wb.head()\n",
      "69/98: wb = cdx_status[['uri', 'lines']].rename(columns={'lines': 'cdx'}).merge(wb_df[['uri', 'available']], on='uri', how='left').fillna(False)\n",
      "69/99: wb.head()\n",
      "69/100: wb[(wb.cdx==0) & (wb.available)]\n",
      "69/101: wb[(wb.cdx!=0) & (~wb.available)]\n",
      "69/102: wb = cdx_status[['uri', 'lines']].rename(columns={'lines': 'cdx'}).merge(wb_df[['uri', 'available']], on='uri', how='outer').fillna(False)\n",
      "69/103: wb[(wb.cdx!=0) & (~wb.available)]\n",
      "69/104: wb[(wb.cdx==0) & (wb.available)]\n",
      "69/105: wb = cdx_status[['uri', 'lines']].rename(columns={'lines': 'cdx'}).merge(wb_df[['uri', 'available']], on='uri', how='left').fillna(False)\n",
      "69/106: wb[(wb.cdx==0) & (available)]\n",
      "69/107: wb[(wb.cdx==0) & (wb.available)]\n",
      "69/108: wb[(wb.cdx!=0) & (~wb.available)].shape\n",
      "62/47:\n",
      "from functools import wraps\n",
      "\n",
      "def retry(times):\n",
      "\n",
      "    def wrapper_fn(f):\n",
      "\n",
      "        @wraps(f)\n",
      "        def new_wrapper(*args,**kwargs):\n",
      "            for i in range(times):\n",
      "                try:\n",
      "                    #print 'try %s' % (i + 1)\n",
      "                    return f(*args,**kwargs)\n",
      "                except Exception as e:\n",
      "                    error = e\n",
      "            raise error\n",
      "\n",
      "        return new_wrapper\n",
      "\n",
      "    return wrapper_fn\n",
      "\n",
      "\n",
      "import concurrent.futures\n",
      "\n",
      "@retry(3)\n",
      "def get_wb_avail(uri, ts):\n",
      "    resp = requests.get('https://archive.org/wayback/available', params={'url': uri, 'timestamp': ts}).text\n",
      "    return resp\n",
      "62/48:\n",
      "from functools import wraps\n",
      "\n",
      "def retry(times):\n",
      "\n",
      "    def wrapper_fn(f):\n",
      "\n",
      "        @wraps(f)\n",
      "        def new_wrapper(*args,**kwargs):\n",
      "            for i in range(times):\n",
      "                try:\n",
      "                    #print 'try %s' % (i + 1)\n",
      "                    return f(*args,**kwargs)\n",
      "                except Exception as e:\n",
      "                    error = e\n",
      "            raise error\n",
      "\n",
      "        return new_wrapper\n",
      "\n",
      "    return wrapper_fn\n",
      "\n",
      "\n",
      "import concurrent.futures\n",
      "\n",
      "@retry(3)\n",
      "def get_wb_avail(uri, ts):\n",
      "    resp = requests.get('https://archive.org/wayback/available', params={'url': uri, 'timestamp': ts}).json()\n",
      "    return resp\n",
      "62/49: get_memento_avail(base_uri, 'english.alarabiya.net/en/News/2014/07/30/Hamas-official-asks-Hezbollah-to-join-fight-against-Israel-.html')\n",
      "62/50: memento_avail_cd.to_pickle('data/memento_avail_cd.pkl.gz', compression='gzip')\n",
      "62/51: unique_uris.head()\n",
      "62/52: unique_uris.to_csv('data/unique_uris_for_ia.csv', index=False)\n",
      "62/53: unique_uris.to_csv('data/unique_uris_for_ia.csv', index=False, compression='gzip')\n",
      "62/54: unique_uris.to_csv('data/unique_uris_for_ia.csv.gz', index=False, compression='gzip')\n",
      "62/55: memento_avail_cd.head()\n",
      "62/56: memento_avail_cd.isna()\n",
      "62/57: memento_avail_cd.isna().sum()\n",
      "62/58: memento_avail_cd.iat[3]\n",
      "62/59:\n",
      "macd = [(ma['original_uri'], x['first']['uri'], x['first']['datetime'], x['last']['uri'], x['last']['datetime']) \n",
      "                      for x in ma['timemap_index'] for ma in memento_avail_cd.tolist() if ma is not None]\n",
      "62/60:\n",
      "macd = [(ma['original_uri'], ma['first']['uri'], x['first']['datetime'], x['last']['uri'], x['last']['datetime']) \n",
      "                      for ma in memento_avail_cd.tolist() if ma is not None]\n",
      "62/61: memento_avail_cd.iat[3].keys()\n",
      "62/62: memento_avail_cd.iat[3]['timemap_uri'].keys()\n",
      "62/63: memento_avail_cd.iat[3]['timegate_uri'].keys()\n",
      "62/64: memento_avail_cd.iat[3].keys()\n",
      "62/65: memento_avail_cd.iat[3].mementos.keys()\n",
      "62/66: memento_avail_cd.iat[3]['mementos'].keys()\n",
      "62/67:\n",
      "macd = [(x['original_uri'], x['mementos']['first']['uri'], \n",
      "         x['mementos']['first']['datetime'], x['mementos']['last']['uri'], \n",
      "         x['mementos']['last']['datetime'], len(x['mementos']['list'])) \n",
      "                      for x in memento_avail_cd.tolist() if x is not None]\n",
      "62/68: macd[:5]\n",
      "62/69:\n",
      "macd = pd.DataFrame([(x['original_uri'], x['mementos']['first']['uri'], \n",
      "                      x['mementos']['first']['datetime'], x['mementos']['last']['uri'], \n",
      "                      x['mementos']['last']['datetime'], len(x['mementos']['list'])) \n",
      "                      for x in memento_avail_cd.tolist() if x is not None],\n",
      "                   columns=['original_uri', 'first_snap', 'first_time', 'last_snap', 'last_time', 'num_snaps'])\n",
      "62/70: macd.head()\n",
      "62/71: None*6\n",
      "62/72: [None]*6\n",
      "62/73: (None,)*6\n",
      "62/74: (None)*6\n",
      "62/75:\n",
      "macd = pd.DataFrame([(x['original_uri'], x['mementos']['first']['uri'], \n",
      "                      x['mementos']['first']['datetime'], x['mementos']['last']['uri'], \n",
      "                      x['mementos']['last']['datetime'], len(x['mementos']['list'])) \n",
      "                      for x in memento_avail_cd.tolist() if x is not None else (None,)*6],\n",
      "                   columns=['original_uri', 'first_snap', 'first_time', 'last_snap', 'last_time', 'num_snaps'])\n",
      "62/76:\n",
      "macd = pd.DataFrame([(x['original_uri'], x['mementos']['first']['uri'], \n",
      "                      x['mementos']['first']['datetime'], x['mementos']['last']['uri'], \n",
      "                      x['mementos']['last']['datetime'], len(x['mementos']['list'])) \n",
      "                      if x is not None else (None,)*6 for x in memento_avail_cd.tolist() ],\n",
      "                   columns=['original_uri', 'first_snap', 'first_time', 'last_snap', 'last_time', 'num_snaps'])\n",
      "62/77: macd.head()\n",
      "62/78:\n",
      "macd = pd.DataFrame([(x['original_uri'], x['mementos']['first']['uri'], \n",
      "                      x['mementos']['first']['datetime'], x['mementos']['last']['uri'], \n",
      "                      x['mementos']['last']['datetime'], len(x['mementos']['list'])) \n",
      "                      if x is not None else (None,)*6 for x in memento_avail_cd.tolist() ],\n",
      "                   columns=['original_uri', 'first_snap', 'first_time', 'last_snap', 'last_time', 'num_snaps'])\n",
      "macd = pd.concat([unique_uris, macd])\n",
      "62/79:\n",
      "macd = pd.DataFrame([(x['original_uri'], x['mementos']['first']['uri'], \n",
      "                      x['mementos']['first']['datetime'], x['mementos']['last']['uri'], \n",
      "                      x['mementos']['last']['datetime'], len(x['mementos']['list'])) \n",
      "                      if x is not None else (None,)*6 for x in memento_avail_cd.tolist() ],\n",
      "                   columns=['original_uri', 'first_snap', 'first_time', 'last_snap', 'last_time', 'num_snaps'])\n",
      "macd = pd.concat([unique_uris, macd], axis=1)\n",
      "62/80: unique_uris\n",
      "62/81:\n",
      "macd = pd.DataFrame([(x['original_uri'], x['mementos']['first']['uri'], \n",
      "                      x['mementos']['first']['datetime'], x['mementos']['last']['uri'], \n",
      "                      x['mementos']['last']['datetime'], len(x['mementos']['list'])) \n",
      "                      if x is not None else (None,)*6 for x in memento_avail_cd.tolist() ],\n",
      "                   columns=['original_uri', 'first_snap', 'first_time', 'last_snap', 'last_time', 'num_snaps'])\n",
      "macd = pd.concat([unique_uris.reset_index().drop('0', axis=1), macd], axis=1)\n",
      "62/82:\n",
      "macd = pd.DataFrame([(x['original_uri'], x['mementos']['first']['uri'], \n",
      "                      x['mementos']['first']['datetime'], x['mementos']['last']['uri'], \n",
      "                      x['mementos']['last']['datetime'], len(x['mementos']['list'])) \n",
      "                      if x is not None else (None,)*6 for x in memento_avail_cd.tolist() ],\n",
      "                   columns=['original_uri', 'first_snap', 'first_time', 'last_snap', 'last_time', 'num_snaps'])\n",
      "macd = pd.concat([unique_uris.reset_index().drop(0, axis=1), macd], axis=1)\n",
      "62/83:\n",
      "macd = pd.DataFrame([(x['original_uri'], x['mementos']['first']['uri'], \n",
      "                      x['mementos']['first']['datetime'], x['mementos']['last']['uri'], \n",
      "                      x['mementos']['last']['datetime'], len(x['mementos']['list'])) \n",
      "                      if x is not None else (None,)*6 for x in memento_avail_cd.tolist() ],\n",
      "                   columns=['original_uri', 'first_snap', 'first_time', 'last_snap', 'last_time', 'num_snaps'])\n",
      "macd = pd.concat([unique_uris.reset_index().drop([0], axis=1), macd], axis=1)\n",
      "62/84: unique_uris.reset_index()\n",
      "62/85:\n",
      "macd = pd.DataFrame([(x['original_uri'], x['mementos']['first']['uri'], \n",
      "                      x['mementos']['first']['datetime'], x['mementos']['last']['uri'], \n",
      "                      x['mementos']['last']['datetime'], len(x['mementos']['list'])) \n",
      "                      if x is not None else (None,)*6 for x in memento_avail_cd.tolist() ],\n",
      "                   columns=['original_uri', 'first_snap', 'first_time', 'last_snap', 'last_time', 'num_snaps'])\n",
      "macd = pd.concat([unique_uris.reset_index().drop('index', axis=1), macd], axis=1)\n",
      "62/86: macd.head()\n",
      "62/87: macd.tail(20)\n",
      "62/88: unique_uris.reset_index().tail(20)\n",
      "62/89:\n",
      "macd = pd.DataFrame([(x['original_uri'], x['mementos']['first']['uri'], \n",
      "                      x['mementos']['first']['datetime'], x['mementos']['last']['uri'], \n",
      "                      x['mementos']['last']['datetime'], len(x['mementos']['list'])) \n",
      "                      if x is not None else (None,)*6 for x in memento_avail_cd.tolist() ],\n",
      "                   columns=['original_uri', 'first_snap', 'first_time', 'last_snap', 'last_time', 'num_snaps'])\n",
      "macd = pd.concat([unique_uris_raw.reset_index().drop('index', axis=1), macd], axis=1)\n",
      "62/90: unique_uris.reset_index().tail(20)\n",
      "62/91: macd.tail(20)\n",
      "62/92: macd.to_pickle('data/memento_avail_cd_df.pkl.gz', index=False, compression='gzip')\n",
      "62/93: macd.to_pickle('data/memento_avail_cd_df.pkl.gz', compression='gzip')\n",
      "62/94: missing_uris1 = macd[macd.original_uri.isna()].uri\n",
      "62/95:\n",
      "missing_uris1 = macd[macd.original_uri.isna()].uri\n",
      "missing_uris1.head()\n",
      "62/96: memento_avail_cd = missing_uris1.progress_apply(lambda x: get_memento_avail(base_uri_cd, x.uri), axis=1)\n",
      "62/97: memento_avail_cd.head()\n",
      "62/98: memento_avail_cd_m1 = missing_uris1.progress_apply(lambda x: get_memento_avail(base_uri_cd, x.uri), axis=1)\n",
      "62/99:\n",
      "missing_uris1 = macd[macd.original_uri.isna()].uri.to_frame()\n",
      "missing_uris1.head()\n",
      "62/100: memento_avail_cd_m1 = missing_uris1.progress_apply(lambda x: get_memento_avail(base_uri_cd, x.uri), axis=1)\n",
      "70/1:\n",
      "import wikipediaapi as wa\n",
      "import pandas as pd\n",
      "import networkx as nx\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "import seaborn as sns\n",
      "sns.set(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
      "sns.set_context(\"talk\")\n",
      "sns.set_palette('Set2', 10)\n",
      "70/2:\n",
      "import altair as alt\n",
      "alt.renderers.enable('notebook')\n",
      "70/3: import pandas as pd\n",
      "70/4:\n",
      "df = pd.read_pickle('lang_uri_time_all.pkl.gz', compression='gzip')\n",
      "df.head()\n",
      "70/5: df['uri_unsh_no_clean'] = df.uri_unshrtn_raw.combine_first(df.uri_unshrtn_bitly.combine_first(df.uri_unshrtn.combine_first(df.uri)))\n",
      "70/6: df.uri_unsh_no_clean.head()\n",
      "70/7:\n",
      "unique_uris = pd.concat([df.uri_cln, df.uri_cln_unshrtn]).drop_duplicates()\n",
      "unique_uris = unique_uris[~(unique_uris.str.contains('archive.org').fillna(False))].to_frame().rename(columns={0: 'uri'})\n",
      "unique_uris.head()\n",
      "70/8: import requests\n",
      "70/9:\n",
      "from tqdm import tqdm\n",
      "\n",
      "# Register `pandas.progress_apply` and `pandas.Series.map_apply` with `tqdm`\n",
      "# (can use `tqdm_gui`, `tqdm_notebook`, optional kwargs, etc.)\n",
      "tqdm.pandas(desc=\"my bar!\")\n",
      "\n",
      "# Now you can use `progress_apply` instead of `apply`\n",
      "# and `progress_map` instead of `map`\n",
      "#df.progress_apply(lambda x: x**2)\n",
      "# can also groupby:\n",
      "# df.groupby(0).progress_apply(lambda x: x**2)\n",
      "70/10:\n",
      "from functools import wraps\n",
      "\n",
      "def retry(times):\n",
      "\n",
      "    def wrapper_fn(f):\n",
      "\n",
      "        @wraps(f)\n",
      "        def new_wrapper(*args,**kwargs):\n",
      "            for i in range(times):\n",
      "                try:\n",
      "                    #print 'try %s' % (i + 1)\n",
      "                    return f(*args,**kwargs)\n",
      "                except Exception as e:\n",
      "                    error = e\n",
      "            raise error\n",
      "\n",
      "        return new_wrapper\n",
      "\n",
      "    return wrapper_fn\n",
      "\n",
      "\n",
      "import concurrent.futures\n",
      "\n",
      "@retry(3)\n",
      "def get_wb_avail(uri, ts):\n",
      "    resp = requests.get('https://archive.org/wayback/available', params={'url': uri, 'timestamp': ts}).json()\n",
      "    return resp\n",
      "70/11:\n",
      "import requests\n",
      "import concurrent.futures\n",
      "70/12:\n",
      "base_uri_cd = 'http://memgator.cs.odu.edu/timemap/json/'\n",
      "base_uri = 'http://timetravel.mementoweb.org/timemap/json/'\n",
      "#test_uri = missing_uris8[0]\n",
      "base_uri_time =  'http://timetravel.mementoweb.org/api/json/20140801000000/'\n",
      "test_uri = 'http://english.alarabiya.net/en/News/2014/07/30/Hamas-official-asks-Hezbollah-to-join-fight-against-Israel-.html'\n",
      "requests.get(base_uri+test_uri).json()\n",
      "70/13: requests.get(base_uri_time+test_uri).json()\n",
      "70/14:\n",
      "unique_uris_raw = pd.concat([df.uri, df.uri_unsh_no_clean]).drop_duplicates()\n",
      "unique_uris_raw = unique_uris_raw[~(unique_uris_raw.str.contains('archive.org').fillna(False))].to_frame().rename(columns={0: 'uri'})\n",
      "unique_uris_raw.head()\n",
      "70/15:\n",
      "@retry(3)\n",
      "def get_memento_avail(base_uri, uri):\n",
      "    try:\n",
      "        resp = requests.get(base_uri+uri).json()\n",
      "    except:\n",
      "        resp = None\n",
      "    return resp\n",
      "70/16: macd = pd.read_pickle('data/memento_avail_cd_df.pkl.gz', compression='gzip')\n",
      "70/17:\n",
      "missing_uris1 = macd[macd.original_uri.isna()].uri.to_frame()\n",
      "missing_uris1.head()\n",
      "70/18: memento_avail_cd_m1 = missing_uris1.progress_apply(lambda x: get_memento_avail(base_uri_cd, x.uri), axis=1)\n",
      "71/1:\n",
      "import requests\n",
      "import concurrent.futures\n",
      "71/2:\n",
      "base_uri_cd = 'http://memgator.cs.odu.edu/timemap/json/'\n",
      "base_uri = 'http://timetravel.mementoweb.org/timemap/json/'\n",
      "#test_uri = missing_uris8[0]\n",
      "base_uri_time =  'http://timetravel.mementoweb.org/api/json/20140801000000/'\n",
      "test_uri = 'http://english.alarabiya.net/en/News/2014/07/30/Hamas-official-asks-Hezbollah-to-join-fight-against-Israel-.html'\n",
      "requests.get(base_uri+test_uri).json()\n",
      "71/3: requests.get(base_uri_time+test_uri).json()\n",
      "71/4:\n",
      "import wikipediaapi as wa\n",
      "import pandas as pd\n",
      "import networkx as nx\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "import seaborn as sns\n",
      "sns.set(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
      "sns.set_context(\"talk\")\n",
      "sns.set_palette('Set2', 10)\n",
      "71/5: import pandas as pd\n",
      "71/6:\n",
      "df = pd.read_pickle('lang_uri_time_all.pkl.gz', compression='gzip')\n",
      "df.head()\n",
      "71/7: df['uri_unsh_no_clean'] = df.uri_unshrtn_raw.combine_first(df.uri_unshrtn_bitly.combine_first(df.uri_unshrtn.combine_first(df.uri)))\n",
      "71/8: df.uri_unsh_no_clean.head()\n",
      "71/9:\n",
      "unique_uris = pd.concat([df.uri_cln, df.uri_cln_unshrtn]).drop_duplicates()\n",
      "unique_uris = unique_uris[~(unique_uris.str.contains('archive.org').fillna(False))].to_frame().rename(columns={0: 'uri'})\n",
      "unique_uris.head()\n",
      "71/10: import requests\n",
      "71/11:\n",
      "from tqdm import tqdm\n",
      "\n",
      "# Register `pandas.progress_apply` and `pandas.Series.map_apply` with `tqdm`\n",
      "# (can use `tqdm_gui`, `tqdm_notebook`, optional kwargs, etc.)\n",
      "tqdm.pandas(desc=\"my bar!\")\n",
      "\n",
      "# Now you can use `progress_apply` instead of `apply`\n",
      "# and `progress_map` instead of `map`\n",
      "#df.progress_apply(lambda x: x**2)\n",
      "# can also groupby:\n",
      "# df.groupby(0).progress_apply(lambda x: x**2)\n",
      "71/12:\n",
      "from functools import wraps\n",
      "\n",
      "def retry(times):\n",
      "\n",
      "    def wrapper_fn(f):\n",
      "\n",
      "        @wraps(f)\n",
      "        def new_wrapper(*args,**kwargs):\n",
      "            for i in range(times):\n",
      "                try:\n",
      "                    #print 'try %s' % (i + 1)\n",
      "                    return f(*args,**kwargs)\n",
      "                except Exception as e:\n",
      "                    error = e\n",
      "            raise error\n",
      "\n",
      "        return new_wrapper\n",
      "\n",
      "    return wrapper_fn\n",
      "\n",
      "\n",
      "import concurrent.futures\n",
      "\n",
      "@retry(3)\n",
      "def get_wb_avail(uri, ts):\n",
      "    resp = requests.get('https://archive.org/wayback/available', params={'url': uri, 'timestamp': ts}).json()\n",
      "    return resp\n",
      "71/13:\n",
      "wb_avail = {}\n",
      "\n",
      "# We can use a with statement to ensure threads are cleaned up promptly\n",
      "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
      "    # Start the load operations and mark each future with its URL\n",
      "    future_to_parse = {executor.submit(get_wb_avail, uri, '20140801000000'): uri for idx, uri in unique_uris.itertuples()}\n",
      "    for future in concurrent.futures.as_completed(future_to_parse):\n",
      "        uri = future_to_parse[future]\n",
      "        wb_avail[uri] = future\n",
      "\n",
      "\n",
      "for future in concurrent.futures.as_completed(future_to_parse):\n",
      "    uri = future_to_parse[future]\n",
      "    try:\n",
      "        wb_avail[uri] = future.result()\n",
      "    except:\n",
      "        wb_avail[uri] = None\n",
      "72/1:\n",
      "import wikipediaapi as wa\n",
      "import pandas as pd\n",
      "import networkx as nx\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "import seaborn as sns\n",
      "sns.set(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
      "sns.set_context(\"talk\")\n",
      "sns.set_palette('Set2', 10)\n",
      "72/2: import pandas as pd\n",
      "72/3:\n",
      "df = pd.read_pickle('lang_uri_time_all.pkl.gz', compression='gzip')\n",
      "df.head()\n",
      "72/4: df['uri_unsh_no_clean'] = df.uri_unshrtn_raw.combine_first(df.uri_unshrtn_bitly.combine_first(df.uri_unshrtn.combine_first(df.uri)))\n",
      "72/5: df.uri_unsh_no_clean.head()\n",
      "72/6:\n",
      "unique_uris = pd.concat([df.uri_cln, df.uri_cln_unshrtn]).drop_duplicates()\n",
      "unique_uris = unique_uris[~(unique_uris.str.contains('archive.org').fillna(False))].to_frame().rename(columns={0: 'uri'})\n",
      "unique_uris.head()\n",
      "72/7: import requests\n",
      "72/8:\n",
      "from tqdm import tqdm\n",
      "\n",
      "# Register `pandas.progress_apply` and `pandas.Series.map_apply` with `tqdm`\n",
      "# (can use `tqdm_gui`, `tqdm_notebook`, optional kwargs, etc.)\n",
      "tqdm.pandas(desc=\"my bar!\")\n",
      "\n",
      "# Now you can use `progress_apply` instead of `apply`\n",
      "# and `progress_map` instead of `map`\n",
      "#df.progress_apply(lambda x: x**2)\n",
      "# can also groupby:\n",
      "# df.groupby(0).progress_apply(lambda x: x**2)\n",
      "72/9:\n",
      "from functools import wraps\n",
      "\n",
      "def retry(times):\n",
      "\n",
      "    def wrapper_fn(f):\n",
      "\n",
      "        @wraps(f)\n",
      "        def new_wrapper(*args,**kwargs):\n",
      "            for i in range(times):\n",
      "                try:\n",
      "                    #print 'try %s' % (i + 1)\n",
      "                    return f(*args,**kwargs)\n",
      "                except Exception as e:\n",
      "                    error = e\n",
      "            raise error\n",
      "\n",
      "        return new_wrapper\n",
      "\n",
      "    return wrapper_fn\n",
      "\n",
      "\n",
      "import concurrent.futures\n",
      "\n",
      "@retry(3)\n",
      "def get_wb_avail(uri, ts):\n",
      "    resp = requests.get('https://archive.org/wayback/available', params={'url': uri, 'timestamp': ts}).json()\n",
      "    return resp\n",
      "72/10:\n",
      "import requests\n",
      "import concurrent.futures\n",
      "72/11:\n",
      "base_uri_cd = 'http://memgator.cs.odu.edu/timemap/json/'\n",
      "base_uri = 'http://timetravel.mementoweb.org/timemap/json/'\n",
      "#test_uri = missing_uris8[0]\n",
      "base_uri_time =  'http://timetravel.mementoweb.org/api/json/20140801000000/'\n",
      "test_uri = 'http://english.alarabiya.net/en/News/2014/07/30/Hamas-official-asks-Hezbollah-to-join-fight-against-Israel-.html'\n",
      "requests.get(base_uri+test_uri).json()\n",
      "72/12: requests.get(base_uri_time+test_uri).json()\n",
      "72/13:\n",
      "unique_uris_raw = pd.concat([df.uri, df.uri_unsh_no_clean]).drop_duplicates()\n",
      "unique_uris_raw = unique_uris_raw[~(unique_uris_raw.str.contains('archive.org').fillna(False))].to_frame().rename(columns={0: 'uri'})\n",
      "unique_uris_raw.head()\n",
      "72/14:\n",
      "@retry(3)\n",
      "def get_memento_avail(base_uri, uri):\n",
      "    try:\n",
      "        resp = requests.get(base_uri+uri).json()\n",
      "    except:\n",
      "        resp = None\n",
      "    return resp\n",
      "72/15: get_memento_avail('http://archive.is/timemap/http://english.alarabiya.net/en/News/2014/07/30/Hamas-official-asks-Hezbollah-to-join-fight-against-Israel-.html')\n",
      "72/16: macd = pd.read_pickle('data/memento_avail_cd_df.pkl.gz', compression='gzip')\n",
      "72/17:\n",
      "missing_uris1 = macd[macd.original_uri.isna()].uri.to_frame()\n",
      "missing_uris1.head()\n",
      "72/18: memento_avail_cd_m1 = missing_uris1.head(50).progress_apply(lambda x: get_memento_avail(base_uri_cd, x.uri), axis=1)\n",
      "72/19: memento_avail_cd_m1.value_counts()\n",
      "72/20: memento_avail_cd_m1.value_counts()\n",
      "72/21: memento_avail_cd_m1\n",
      "72/22: requests('http://walla.co.il').json()\n",
      "72/23: requests.get('http://walla.co.il').json()\n",
      "72/24:\n",
      "@retry(3)\n",
      "def get_memento_avail_check_json(base_uri, uri):\n",
      "    try:\n",
      "        resp = requests.get(base_uri+uri)\n",
      "        if resp.status!='200':\n",
      "            resp = {'text': resp.text}\n",
      "        else:\n",
      "            resp = resp.json()\n",
      "    except:\n",
      "        resp = None\n",
      "    return resp\n",
      "72/25:\n",
      "@retry(3)\n",
      "def get_memento_avail_check_json(base_uri, uri):\n",
      "    try:\n",
      "        resp = requests.get(base_uri+uri)\n",
      "        print(resp.status)\n",
      "        if resp.status!='200':\n",
      "            resp = {'text': resp.text}\n",
      "            print(resp)\n",
      "        else:\n",
      "            resp = resp.json()\n",
      "    except:\n",
      "        resp = None\n",
      "    return resp\n",
      "72/26: memento_avail_cd_m1 = missing_uris1.head(10).progress_apply(lambda x: get_memento_avail(base_uri_cd, x.uri), axis=1)\n",
      "72/27: memento_avail_cd_m1 = missing_uris1.head(10).progress_apply(lambda x: get_memento_avail_check_json(base_uri_cd, x.uri), axis=1)\n",
      "72/28: memento_avail_cd_m1\n",
      "72/29:\n",
      "@retry(3)\n",
      "def get_memento_avail_check_json(base_uri, uri):\n",
      "\n",
      "    resp = requests.get(base_uri+uri)\n",
      "    print(resp.status)\n",
      "    if resp.text=='200':\n",
      "        resp = {'text': resp.text}\n",
      "        print(resp)\n",
      "    else:\n",
      "        resp = resp.json()\n",
      "\n",
      "    return resp\n",
      "72/30: memento_avail_cd_m1 = missing_uris1.head(10).progress_apply(lambda x: get_memento_avail_check_json(base_uri_cd, x.uri), axis=1)\n",
      "72/31:\n",
      "@retry(3)\n",
      "def get_memento_avail_check_json(base_uri, uri):\n",
      "    try:\n",
      "        resp = requests.get(base_uri+uri)\n",
      "        print(resp.status_code)\n",
      "        if resp.status_code=='200':\n",
      "            resp = {'text': resp.text}\n",
      "            print(resp)\n",
      "        else:\n",
      "            resp = resp.json()\n",
      "    except:\n",
      "        resp = None\n",
      "    return resp\n",
      "72/32: memento_avail_cd_m1 = missing_uris1.head(10).progress_apply(lambda x: get_memento_avail_check_json(base_uri_cd, x.uri), axis=1)\n",
      "72/33:\n",
      "@retry(3)\n",
      "def get_memento_avail_check_json(base_uri, uri):\n",
      "    try:\n",
      "        resp = requests.get(base_uri+uri)\n",
      "        print(resp.status_code)\n",
      "        if resp.status_code=='200':\n",
      "            resp = {'text': resp.text}\n",
      "            print(resp)\n",
      "        else:\n",
      "            resp = resp.json()\n",
      "    except:\n",
      "        resp = None\n",
      "    return resp\n",
      "72/34: memento_avail_cd_m1 = missing_uris1.head(10).progress_apply(lambda x: get_memento_avail_check_json(base_uri_cd, x.uri), axis=1)\n",
      "72/35: memento_avail_cd_m1\n",
      "72/36:\n",
      "@retry(3)\n",
      "def get_memento_avail_check_json(base_uri, uri):\n",
      "    try:\n",
      "        resp = requests.get(base_uri+uri)\n",
      "        print(resp.status_code)\n",
      "        if resp.status_code!='200':\n",
      "            resp = {'text': resp.text}\n",
      "            print(resp)\n",
      "        else:\n",
      "            resp = resp.json()\n",
      "    except:\n",
      "        resp = None\n",
      "    return resp\n",
      "72/37: memento_avail_cd_m1 = missing_uris1.head(10).progress_apply(lambda x: get_memento_avail_check_json(base_uri_cd, x.uri), axis=1)\n",
      "72/38:\n",
      "@retry(3)\n",
      "def get_memento_avail_check_json(base_uri, uri):\n",
      "    try:\n",
      "        resp = requests.get(base_uri+uri)\n",
      "        print(resp.status_code)\n",
      "        if resp.status_code!=200:\n",
      "            resp = {'text': resp.text}\n",
      "            print(resp)\n",
      "        else:\n",
      "            resp = resp.json()\n",
      "    except:\n",
      "        resp = None\n",
      "    return resp\n",
      "72/39: memento_avail_cd_m1 = missing_uris1.head(10).progress_apply(lambda x: get_memento_avail_check_json(base_uri_cd, x.uri), axis=1)\n",
      "72/40:\n",
      "@retry(3)\n",
      "def get_memento_avail_check_json(base_uri, uri):\n",
      "    try:\n",
      "        resp = requests.get(base_uri+uri)\n",
      "        if resp.status_code!=200:\n",
      "            resp = {'text': resp.text, 'status_code': resp.status_code}\n",
      "        else:\n",
      "            resp = resp.json()\n",
      "    except:\n",
      "        resp = None\n",
      "    return resp\n",
      "72/41: memento_avail_cd_m1 = missing_uris1.head(10).progress_apply(lambda x: get_memento_avail_check_json(base_uri_cd, x.uri), axis=1)\n",
      "72/42: memento_avail_cd_m1\n",
      "73/1: import pandas as pd\n",
      "73/2:\n",
      "df = pd.read_pickle('lang_uri_time_all.pkl.gz', compression='gzip')\n",
      "df.head()\n",
      "73/3: df['uri_unsh_no_clean'] = df.uri_unshrtn_raw.combine_first(df.uri_unshrtn_bitly.combine_first(df.uri_unshrtn.combine_first(df.uri)))\n",
      "73/4: df.uri_unsh_no_clean.head()\n",
      "73/5:\n",
      "unique_uris = pd.concat([df.uri_cln, df.uri_cln_unshrtn]).drop_duplicates()\n",
      "unique_uris = unique_uris[~(unique_uris.str.contains('archive.org').fillna(False))].to_frame().rename(columns={0: 'uri'})\n",
      "unique_uris.head()\n",
      "73/6: import requests\n",
      "73/7:\n",
      "from tqdm import tqdm\n",
      "\n",
      "# Register `pandas.progress_apply` and `pandas.Series.map_apply` with `tqdm`\n",
      "# (can use `tqdm_gui`, `tqdm_notebook`, optional kwargs, etc.)\n",
      "tqdm.pandas(desc=\"my bar!\")\n",
      "\n",
      "# Now you can use `progress_apply` instead of `apply`\n",
      "# and `progress_map` instead of `map`\n",
      "#df.progress_apply(lambda x: x**2)\n",
      "# can also groupby:\n",
      "# df.groupby(0).progress_apply(lambda x: x**2)\n",
      "73/8:\n",
      "from functools import wraps\n",
      "\n",
      "def retry(times):\n",
      "\n",
      "    def wrapper_fn(f):\n",
      "\n",
      "        @wraps(f)\n",
      "        def new_wrapper(*args,**kwargs):\n",
      "            for i in range(times):\n",
      "                try:\n",
      "                    #print 'try %s' % (i + 1)\n",
      "                    return f(*args,**kwargs)\n",
      "                except Exception as e:\n",
      "                    error = e\n",
      "            raise error\n",
      "\n",
      "        return new_wrapper\n",
      "\n",
      "    return wrapper_fn\n",
      "\n",
      "\n",
      "import concurrent.futures\n",
      "\n",
      "@retry(3)\n",
      "def get_wb_avail(uri, ts):\n",
      "    resp = requests.get('https://archive.org/wayback/available', params={'url': uri, 'timestamp': ts}).json()\n",
      "    return resp\n",
      "73/9:\n",
      "import requests\n",
      "import concurrent.futures\n",
      "73/10:\n",
      "base_uri_cd = 'http://memgator.cs.odu.edu/timemap/json/'\n",
      "base_uri = 'http://timetravel.mementoweb.org/timemap/json/'\n",
      "#test_uri = missing_uris8[0]\n",
      "base_uri_time =  'http://timetravel.mementoweb.org/api/json/20140801000000/'\n",
      "test_uri = 'http://english.alarabiya.net/en/News/2014/07/30/Hamas-official-asks-Hezbollah-to-join-fight-against-Israel-.html'\n",
      "requests.get(base_uri+test_uri).json()\n",
      "73/11: macd = pd.read_pickle('data/memento_avail_cd_df.pkl.gz', compression='gzip')\n",
      "73/12:\n",
      "missing_uris1 = macd[macd.original_uri.isna()].uri.to_frame()\n",
      "missing_uris1.head()\n",
      "73/13:\n",
      "@retry(3)\n",
      "def get_memento_avail_check_json(base_uri, uri):\n",
      "    try:\n",
      "        resp = requests.get(base_uri+uri)\n",
      "        if resp.status_code!=200:\n",
      "            resp = {'text': resp.text, 'status_code': resp.status_code}\n",
      "        else:\n",
      "            resp = resp.json()\n",
      "    except:\n",
      "        resp = None\n",
      "    return resp\n",
      "73/14: memento_avail_cd_m1 = missing_uris1.head(10).progress_apply(lambda x: get_memento_avail_check_json(base_uri_cd, x.uri), axis=1)\n",
      "73/15: memento_avail_cd_m1\n",
      "73/16: memento_avail_cd_m1 = missing_uris1.progress_apply(lambda x: get_memento_avail_check_json(base_uri_cd, x.uri), axis=1)\n",
      "74/1: %matplotlib inline\n",
      "74/2:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "74/3: lut = pd.read_pickle('lang_uri_time_all.pkl.gz', compression='gzip')\n",
      "69/109: wb.head()\n",
      "69/110: wb.head()\n",
      "69/111: wb.to_csv('data/wb_avail_cdx_all.csv.gz', index=False, compression='gzip')\n",
      "74/4: wb = pd.read_pickle('data/wb_avail_cdx_all.csv.gz', compression='gzip')\n",
      "74/5: wb = pd.to_csv('data/wb_avail_cdx_all.csv.gz', compression='gzip')\n",
      "74/6: wb = pd.read_csv('data/wb_avail_cdx_all.csv.gz', compression='gzip')\n",
      "74/7: wb.head()\n",
      "74/8: wb.cdx>0 wb.available\n",
      "74/9: wb.cdx>0 and wb.available\n",
      "74/10: pd.concat([wb.cdx>0, wb.available]).any()\n",
      "74/11: pd.concat([wb.cdx>0, wb.available]).any(axis=1)\n",
      "74/12: pd.concat([wb.cdx>0, wb.available])\n",
      "74/13: pd.concat([wb.cdx>0, wb.available],axis=1)\n",
      "74/14: pd.concat([wb.cdx>0, wb.available],axis=1).any()\n",
      "74/15: pd.concat([wb.cdx>0, wb.available],axis=1).any(axis=1)\n",
      "74/16: wb['avail'] = pd.concat([wb.cdx>0, wb.available],axis=1).any(axis=1)\n",
      "74/17: lut['uri_cln_wb_avail'] = lut.uri_cln.map(wb.set_index('uri').avail)\n",
      "74/18:\n",
      "lut['uri_cln_wb_avail'] = lut.uri_cln.map(wb.set_index('uri').avail)\n",
      "lut['uri_cln_unshrtn_wb_avail'] = lut.uri_cln_unshrtn.map(wb.set_index('uri').avail)\n",
      "74/19: lut.uri_cln_unshrtn_wb_avail.value_counts()\n",
      "74/20: lut.uri_cln_wb_avail.value_counts()\n",
      "74/21: lut.[['uri_cln_wb_avail', 'uri_cln_unshrtn_wb_avail']].value_counts()\n",
      "74/22: lut[['uri_cln_wb_avail', 'uri_cln_unshrtn_wb_avail']].value_counts()\n",
      "74/23: lut.uri_cln_wb_avail.value_counts()\n",
      "74/24: lut.uri_cln_unshrtn_wb_avail.value_counts()\n",
      "74/25: lut[(lut.uri_cln_wb_avail) & (~lut.uri_cln_unshrtn_wb_avail)].value_counts()\n",
      "74/26:\n",
      "lut['uri_cln_wb_avail'] = lut.uri_cln.map(wb.set_index('uri').avail).fillna(False)\n",
      "lut['uri_cln_unshrtn_wb_avail'] = lut.uri_cln_unshrtn.map(wb.set_index('uri').avail).fillna(False)\n",
      "74/27: lut.uri_cln_unshrtn_wb_avail.value_counts()\n",
      "74/28: lut.uri_cln_wb_avail.value_counts()\n",
      "74/29: lut[(lut.uri_cln_wb_avail) & (~lut.uri_cln_unshrtn_wb_avail)]\n",
      "74/30: lut[(lut.uri_cln_wb_avail) & (~lut.uri_cln_unshrtn_wb_avail)].shape\n",
      "74/31: lut[(~lut.uri_cln_wb_avail) & (lut.uri_cln_unshrtn_wb_avail)].shape\n",
      "74/32: lut.to_pickle('lang_uri_time_wb_avail_all.pkl.gz', compression='gzip')\n",
      "74/33: lut.uri_cln_unshrtn_wb_avail.sum()/lut.shape[0]\n",
      "74/34: lut.uri_cln_wb_avail.sum()/lut.shape[0]\n",
      "74/35: lut[(lut.uri_cln_wb_avail) | (lut.uri_cln_unshrtn_wb_avail)].shape[0]/lut.shape[0]\n",
      "74/36: lut[(lut.uri_cln_wb_avail) & (~lut.uri_cln_unshrtn_wb_avail)].shape[0]/lut.shape[0]\n",
      "74/37: lut[(~lut.uri_cln_wb_avail) & (lut.uri_cln_unshrtn_wb_avail)].shape[0]/lut.shape[0]\n",
      "74/38: lut.groupby(['source', 'uri_cln_unshrtn_wb_avail']).uri_cln_unshrtn.nunique()\n",
      "74/39: lut.groupby(['source', 'uri_cln_unshrtn_wb_avail']).uri_cln_unshrtn.nunique().unstack()\n",
      "74/40: lut.groupby(['source', 'uri_cln_unshrtn_wb_avail']).uri_cln_unshrtn.nunique().unstack()/lut.groupby('source').nunique()\n",
      "74/41: lut.groupby(['source', 'uri_cln_unshrtn_wb_avail']).uri_cln_unshrtn.nunique().unstack()/lut.groupby('source').uri_cln_unshrtn.nunique()\n",
      "74/42: lut.groupby(['source', 'uri_cln_unshrtn_wb_avail']).uri_cln_unshrtn.nunique().unstack().div(lut.groupby('source').uri_cln_unshrtn.nunique(), axis=1)\n",
      "74/43: lut.groupby(['source', 'uri_cln_unshrtn_wb_avail']).uri_cln_unshrtn.nunique().unstack().div(lut.groupby('source').uri_cln_unshrtn.nunique(), axis=0)\n",
      "74/44: lut.groupby(['lang', 'uri_cln_unshrtn_wb_avail']).uri_cln_unshrtn.nunique().unstack().div(lut.groupby('lang').uri_cln_unshrtn.nunique(), axis=0)\n",
      "74/45: lut.groupby(['source', 'uri_cln_unshrtn_wb_avail']).uri_cln_unshrtn.nunique().unstack().div(lut.groupby('source').uri_cln_unshrtn.nunique(), axis=0).sort_values('True')\n",
      "74/46: lut.groupby(['source', 'uri_cln_unshrtn_wb_avail']).uri_cln_unshrtn.nunique().unstack().div(lut.groupby('source').uri_cln_unshrtn.nunique(), axis=0).sort_values(True)\n",
      "74/47: lut.groupby(['source', 'uri_cln_unshrtn_wb_avail']).uri_cln_unshrtn.nunique().unstack().div(lut.groupby('source').uri_cln_unshrtn.nunique(), axis=0).sort_values(True, ascending=False)\n",
      "74/48: lut.groupby(['lang', 'uri_cln_unshrtn_wb_avail']).uri_cln_unshrtn.nunique().unstack().div(lut.groupby('lang').uri_cln_unshrtn.nunique(), axis=0).sort_values(True, ascending=False)\n",
      "74/49: lut['wb_avail'] = lut[(lut.uri_cln_wb_avail) | (lut.uri_cln_unshrtn_wb_avail)]\n",
      "74/50: lut['wb_avail'] = (lut.uri_cln_wb_avail) | (lut.uri_cln_unshrtn_wb_avail)\n",
      "74/51: lut[lut.wb_avail].shape[0]/lut.shape[0]\n",
      "74/52: lut[(lut.uri_cln_wb_avail) & (~lut.uri_cln_unshrtn_wb_avail)].shape[0]/lut.shape[0]\n",
      "74/53: lut[(~lut.uri_cln_wb_avail) & (lut.uri_cln_unshrtn_wb_avail)].shape[0]/lut.shape[0]\n",
      "74/54: lut.groupby(['is_shortened', 'uri_cln_unshrtn_wb_avail']).uri_cln_unshrtn.nunique().unstack().div(lut.groupby('is_shortened').uri_cln_unshrtn.nunique(), axis=0).sort_values(True, ascending=False)\n",
      "74/55: lut['is_unshrtn'] = lut.uri_cln_unshrtn!=lut.uri_cln\n",
      "74/56: lut.groupby(['is_unshrtn', 'uri_cln_unshrtn_wb_avail']).uri_cln_unshrtn.nunique().unstack().div(lut.groupby('is_unshrtn').uri_cln_unshrtn.nunique(), axis=0).sort_values(True, ascending=False)\n",
      "74/57: lut[lut.is_shortened].groupby(['is_unshrtn', 'uri_cln_unshrtn_wb_avail']).uri_cln_unshrtn.nunique().unstack().div(lut[lut.is_shortened].groupby('is_unshrtn').uri_cln_unshrtn.nunique(), axis=0).sort_values(True, ascending=False)\n",
      "74/58: lut[lut.uri_cln_unshrtn_wb_avail].uri_cln_unshrtn.nunique()/lut.uri_cln_unshrtn.nunique()\n",
      "74/59:\n",
      "unique_uris = pd.concat([lut.uri_cln, lut.uri_cln_unshrtn]).drop_duplicates()\n",
      "unique_uris = unique_uris[~(unique_uris.str.contains('archive.org').fillna(False))].to_frame().rename(columns={0: 'uri'})\n",
      "unique_uris.head()\n",
      "74/60: lut[lut.wb_avail].uri_cln_unshrtn.nunique() / unique_uris.shape[0]\n",
      "74/61: lut.groupby(['lang', 'uri_cln_unshrtn_wb_avail']).uri_cln_unshrtn.nunique().unstack().div(lut.groupby('lang').uri_cln_unshrtn.nunique(), axis=0).sort_values(True, ascending=False).drop(False, axis=1)\n",
      "74/62: lut.groupby(['lang', 'uri_cln_unshrtn_wb_avail']).uri_cln_unshrtn.nunique().unstack().div(lut.groupby('lang').uri_cln_unshrtn.nunique(), axis=0).sort_values(True, ascending=False).drop(False, axis=1).round(2)\n",
      "74/63: lut.groupby(['lang', 'source']).uri_cln_unshrtn.nunique().unstack()\n",
      "74/64:\n",
      "lang_sour = lut.groupby(['lang', 'source']).uri_cln_unshrtn.nunique().unstack() \n",
      "lang_sour = lang_sour.div(lang_sour.sum(axis=1), axis=0)\n",
      "74/65:\n",
      "lang_sour = lut.groupby(['lang', 'source']).uri_cln_unshrtn.nunique().unstack() \n",
      "lang_sour = lang_sour.div(lang_sour.sum(axis=1), axis=0)\n",
      "lang_sour.head()\n",
      "74/66:\n",
      "lang_sour = lut.groupby(['lang', 'source']).uri_cln_unshrtn.nunique().unstack() \n",
      "lang_sour = lang_sour.div(lang_sour.sum(axis=1), axis=0).fillna(0)\n",
      "lang_sour.head()\n",
      "74/67:\n",
      "lang_sour = lut.groupby(['lang', 'source']).uri_cln_unshrtn.nunique().unstack() \n",
      "lang_sour = lang_sour.div(lang_sour.sum(axis=1), axis=0).fillna(0).mul(100).round()\n",
      "lang_sour.head()\n",
      "74/68:\n",
      "lang_sour = lut.groupby(['lang', 'source']).uri_cln_unshrtn.nunique().unstack() \n",
      "lang_sour = lang_sour.div(lang_sour.sum(axis=1), axis=0).fillna(0).mul(100).round().astype(int)\n",
      "lang_sour.head()\n",
      "74/69:\n",
      "lang_sour = lut.groupby(['lang', 'source']).uri_cln_unshrtn.nunique().unstack() \n",
      "lang_sour = lang_sour.div(lang_sour.sum(axis=1), axis=0).fillna(0).mul(100).round().astype(int)\n",
      "lang_sour\n",
      "69/112:\n",
      "unique_uris_raw = pd.concat([df.uri, df.uri_unsh_no_clean]).drop_duplicates()\n",
      "unique_uris_raw = unique_uris_raw[~(unique_uris_raw.str.contains('archive.org').fillna(False))].to_frame().rename(columns={0: 'uri'})\n",
      "unique_uris_raw.head()\n",
      "69/113: df['uri_unsh_no_clean'] = df.uri_unshrtn_raw.combine_first(df.uri_unshrtn_bitly.combine_first(df.uri_unshrtn.combine_first(df.uri)))\n",
      "69/114:\n",
      "unique_uris_raw = pd.concat([df.uri, df.uri_unsh_no_clean]).drop_duplicates()\n",
      "unique_uris_raw = unique_uris_raw[~(unique_uris_raw.str.contains('archive.org').fillna(False))].to_frame().rename(columns={0: 'uri'})\n",
      "unique_uris_raw.head()\n",
      "69/115: requests.get(unique_uris_raw.uri.iat[3])\n",
      "69/116: x = requests.get(unique_uris_raw.uri.iat[3])\n",
      "69/117: dir(x)\n",
      "69/118:\n",
      "from tqdm import tqdm\n",
      "\n",
      "# Register `pandas.progress_apply` and `pandas.Series.map_apply` with `tqdm`\n",
      "# (can use `tqdm_gui`, `tqdm_notebook`, optional kwargs, etc.)\n",
      "tqdm.pandas(desc=\"my bar!\")\n",
      "\n",
      "# Now you can use `progress_apply` instead of `apply`\n",
      "# and `progress_map` instead of `map`\n",
      "#df.progress_apply(lambda x: x**2)\n",
      "# can also groupby:\n",
      "# df.groupby(0).progress_apply(lambda x: x**2)\n",
      "69/119:\n",
      "@retry(3)\n",
      "def get_response(uri):\n",
      "    try:\n",
      "        resp = requests.get(uri)\n",
      "    except:\n",
      "        resp = None\n",
      "    return resp\n",
      "69/120:\n",
      "@retry(3)\n",
      "def get_response(uri):\n",
      "    resp = requests.get(uri)\n",
      "    return resp\n",
      "69/121:\n",
      "unique_uris_raw_only = df.uri_unsh_no_clean.drop_duplicates().to_frame().rename(columns={'uri_unsh_no_clean': 'uri'})\n",
      "unique_uris_raw.head()\n",
      "69/122:\n",
      "unique_uris_raw_only = df.uri_unsh_no_clean.drop_duplicates().to_frame().rename(columns={'uri_unsh_no_clean': 'uri'})\n",
      "unique_uris_raw.shape\n",
      "69/123:\n",
      "unique_uris_raw_only = df.uri_unsh_no_clean.drop_duplicates().to_frame().rename(columns={'uri_unsh_no_clean': 'uri'})\n",
      "unique_uris_raw_only.shape\n",
      "69/124:\n",
      "unique_uris_raw_only_unsh = df.uri_unsh_no_clean.drop_duplicates().to_frame().rename(columns={'uri_unsh_no_clean': 'uri'})\n",
      "unique_uris_raw_only_unsh.shape\n",
      "69/125: all_resp = unique_uris_raw_only_unsh.progress_apply(lambda x: get_response(x.uri), axis=1)\n",
      "69/126: all_resp = unique_uris_raw_only_unsh.head().progress_apply(lambda x: get_response(x.uri), axis=1)\n",
      "69/127:\n",
      "def retry_none(times):\n",
      "\n",
      "    def wrapper_fn(f):\n",
      "\n",
      "        @wraps(f)\n",
      "        def new_wrapper(*args,**kwargs):\n",
      "            for i in range(times):\n",
      "                try:\n",
      "                    #print 'try %s' % (i + 1)\n",
      "                    return f(*args,**kwargs)\n",
      "                except Exception as e:\n",
      "                    error = e\n",
      "            return None\n",
      "\n",
      "        return new_wrapper\n",
      "\n",
      "    return wrapper_fn\n",
      "69/128:\n",
      "@retry_none(3)\n",
      "def get_response(uri):\n",
      "    resp = requests.get(uri)\n",
      "    return resp\n",
      "69/129: df['uri_unsh_no_clean'] = df.uri_unshrtn_raw.combine_first(df.uri_unshrtn_bitly.combine_first(df.uri_unshrtn.combine_first(df.uri)))\n",
      "69/130: all_resp = unique_uris_raw_only_unsh.head().progress_apply(lambda x: get_response(x.uri), axis=1)\n",
      "69/131:\n",
      "all_resp = {}\n",
      "\n",
      "# We can use a with statement to ensure threads are cleaned up promptly\n",
      "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
      "    # Start the load operations and mark each future with its URL\n",
      "    future_to_parse = {executor.submit(get_response, uri): uri for idx, uri in unique_uris_raw_only_unsh.itertuples()}\n",
      "    for future in concurrent.futures.as_completed(future_to_parse):\n",
      "        uri = future_to_parse[future]\n",
      "        all_resp[uri] = future\n",
      "\n",
      "\n",
      "for future in concurrent.futures.as_completed(future_to_parse):\n",
      "    uri = future_to_parse[future]\n",
      "    try:\n",
      "        all_resp[uri] = future.result()\n",
      "    except:\n",
      "        all_resp[uri] = None\n",
      "69/132: import concurrent.futures\n",
      "69/133:\n",
      "all_resp = {}\n",
      "\n",
      "# We can use a with statement to ensure threads are cleaned up promptly\n",
      "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
      "    # Start the load operations and mark each future with its URL\n",
      "    future_to_parse = {executor.submit(get_response, uri): uri for idx, uri in unique_uris_raw_only_unsh.itertuples()}\n",
      "    for future in concurrent.futures.as_completed(future_to_parse):\n",
      "        uri = future_to_parse[future]\n",
      "        all_resp[uri] = future\n",
      "\n",
      "\n",
      "for future in concurrent.futures.as_completed(future_to_parse):\n",
      "    uri = future_to_parse[future]\n",
      "    try:\n",
      "        all_resp[uri] = future.result()\n",
      "    except:\n",
      "        all_resp[uri] = None\n",
      "74/70: os\n",
      "74/71:\n",
      "import os\n",
      "os.getpid()\n",
      "74/72: df['uri_unsh_no_clean'] = df.uri_unshrtn_raw.combine_first(df.uri_unshrtn_bitly.combine_first(df.uri_unshrtn.combine_first(df.uri)))\n",
      "74/73: lut['uri_unsh_no_clean'] = df.uri_unshrtn_raw.combine_first(df.uri_unshrtn_bitly.combine_first(df.uri_unshrtn.combine_first(df.uri)))\n",
      "74/74: lut['uri_unsh_no_clean'] = lut.uri_unshrtn_raw.combine_first(lut.uri_unshrtn_bitly.combine_first(lut.uri_unshrtn.combine_first(lut.uri)))\n",
      "74/75:\n",
      "unique_uris_raw_only_unsh = lut.uri_unsh_no_clean.drop_duplicates().to_frame().rename(columns={'uri_unsh_no_clean': 'uri'})\n",
      "unique_uris_raw_only_unsh.shape\n",
      "74/76: unique_uris_raw_only_unsh.to_csv('data/unique_uris_raw_only_unsh.csv.gz', index=False, compression='gzip')\n",
      "74/77: unique_uris_raw_only_unsh.to_csv('data/unique_uris_raw_only_unsh.csv', index=False)\n",
      "74/78:\n",
      "unique_uris_raw_only_unsh_no_single = lut.unique_uris_raw_only_unsh.str.replace('\\'', '')\n",
      "unique_uris_raw_only_unsh.unique_uris_raw_only_unsh_no_single.value_counts()\n",
      "74/79:\n",
      "unique_uris_raw_only_unsh_no_single = unique_uris_raw_only_unsh.str.replace('\\'', '')\n",
      "unique_uris_raw_only_unsh_no_single.value_counts()\n",
      "74/80:\n",
      "unique_uris_raw_only_unsh_no_single = unique_uris_raw_only_unsh.uri.str.replace('\\'', '').to_frame()\n",
      "unique_uris_raw_only_unsh_no_single.value_counts()\n",
      "74/81:\n",
      "unique_uris_raw_only_unsh_no_single = unique_uris_raw_only_unsh.uri.str.replace('\\'', '').to_frame()\n",
      "unique_uris_raw_only_unsh_no_single.uri.value_counts()\n",
      "74/82:\n",
      "unique_uris_raw_only_unsh_no_single = unique_uris_raw_only_unsh.uri.str.replace('\\'', '').to_frame()\n",
      "unique_uris_raw_only_unsh_no_single.to_csv('data/unique_uris_raw_only_unsh_no_single.csv', index=False)\n",
      "74/83:\n",
      "unique_uris_raw_only_unsh_no_single = unique_uris_raw_only_unsh.uri.str.replace('\\'', '\\\\\\'').to_frame()\n",
      "unique_uris_raw_only_unsh_no_single.to_csv('data/unique_uris_raw_only_unsh_no_single.csv', index=False)\n",
      "74/84: lut[lut.uri.str.contains('\"|<|>|#|%|\\\\{|\\\\}|||\\\\|\\\\^|~|\\\\[|\\\\]|`|\\'', regex=True)].groupby('source').uri.nunique()\n",
      "74/85: lut[lut.uri.str.contains('\"|<|>|#|%|\\\\{|\\\\}|||\\\\|\\\\^|~|\\\\[|\\\\]|`|\\'', regex=True).fillna(False)].groupby('source').uri.nunique()\n",
      "74/86: lut[lut.uri.str.contains('\"|<|>|#|\\\\{|\\\\}|||\\\\|\\\\^|~|\\\\[|\\\\]|`|\\'', regex=True).fillna(False)].groupby('source').uri.nunique()\n",
      "74/87: lut[lut.uri.str.contains('\"|<|>|#|\\\\{|\\\\}|||\\\\|\\\\^|~|\\\\[|\\\\]|`|\\'', regex=True).fillna(False)]\n",
      "74/88: lut[lut.uri.str.contains('\"|<|>|#|\\\\{|\\\\}|||\\\\|\\\\^|~|\\\\[|\\\\]|`|\\'', regex=True).fillna(False)].uri\n",
      "74/89: lut[lut.uri.str.contains('\"|<|>', regex=True).fillna(False)].uri\n",
      "74/90: lut[lut.uri.str.contains('\"|<|>|#', regex=True).fillna(False)].uri\n",
      "74/91: pd.set_option('display.max_colwidth', -1)\n",
      "74/92: lut[lut.uri.str.contains('\"|<|>|#', regex=True).fillna(False)].uri\n",
      "74/93: lut[lut.uri.str.contains('\"|<|>|\\\\{|\\\\}|||\\\\|\\\\^|~|\\\\[|\\\\]|`|\\'', regex=True).fillna(False)].uri\n",
      "74/94: lut[lut.uri.str.contains('\"|<|>|\\{|\\\\}|||\\\\|\\\\^|~|\\\\[|\\\\]|`|\\'', regex=True).fillna(False)].uri\n",
      "74/95: lut[lut.uri.str.contains('\"|<|>|\\\\{', regex=True).fillna(False)].uri\n",
      "74/96: lut[lut.uri.str.contains('\"|<|>|\\{', regex=True).fillna(False)].uri\n",
      "74/97: lut[lut.uri.str.contains('\"|<|>|\\\\{', regex=True).fillna(False)].uri\n",
      "74/98: lut[lut.uri.str.contains('\"|\\\\<|\\\\>|\\\\{', regex=True).fillna(False)].uri\n",
      "74/99: lut[lut.uri.str.contains('\"|<|>|\\\\{', regex=True).fillna(False)].uri\n",
      "74/100: lut[lut.uri.str.contains('\\\\\"|<|>|\\\\{', regex=True).fillna(False)].uri\n",
      "74/101: lut[lut.uri.str.contains('\\\\\"|<|>|{', regex=True).fillna(False)].uri\n",
      "74/102: lut[lut.uri.str.contains('\\\\\"|<|>|{|}', regex=True).fillna(False)].uri\n",
      "74/103: lut[lut.uri.str.contains('\\\\\"|<|>|{|}|\\|', regex=True).fillna(False)].uri\n",
      "74/104: lut[lut.uri.str.contains('\\\\\"|<|>|{|}|\\|\\^|~|\\[|\\]|`|\\'', regex=True).fillna(False)].uri\n",
      "74/105: lut[lut.uri.str.contains('\\\\\"|<|>|{|}|\\|\\^|~|\\[|\\]|`|\\'', regex=True).fillna(False)].groupby('source').uri.nunique()\n",
      "74/106: lut[lut.uri.str.contains('\\\"|<|>|{|}|\\|\\^|~|\\[|\\]|`|\\'', regex=True).fillna(False)].groupby('source').uri.nunique()\n",
      "74/107: lut[lut.uri.str.contains('\\'', regex=True).fillna(False)].groupby('source').uri.nunique()\n",
      "74/108: lut[lut.uri.str.contains('\\\"|<|>|{|}|\\|\\^|~|\\[|\\]|`|\\'', regex=True).fillna(False)].groupby('source').uri.nunique()\n",
      "74/109: lut[lut.uri.str.contains('\\\"', regex=True).fillna(False)].groupby('source').uri.nunique()\n",
      "74/110: lut[lut.uri.str.contains('\"', regex=True).fillna(False)].groupby('source').uri.nunique()\n",
      "74/111: lut[lut.uri.str.contains('\"|<|>|{|}|\\|\\^|~|\\[|\\]|`|\\'', regex=True).fillna(False)].groupby('source').uri.nunique()\n",
      "74/112:\n",
      "testr = '''https://www.rp.pl/Konflikt-izraelsko-palestynski/180529976-USA-Minuta-ciszy-dla-Palestynczykow-w-szkole-Protest-Zydow.html;200;2.348753;0.000330;0.011990;0;0.000;text/html; charset=windows-1250\n",
      "http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.desklight-c3394e44-19a2-45eb-ad63-c71bec715d05;200;0.692087;0.053310;0.086916;0;0.000;text/html;charset=UTF-8\n",
      "https://www.rp.pl/Konflikt-izraelsko-palestynski/305159920-Izrael-zrzuca-wine-na-Hamas.html;200;0.559529;0.000298;0.011882;0;0.000;text/html; charset=windows-1250\n",
      "https://www.tvn24.pl/wiadomosci-ze-swiata,2/konflikt-izraelsko-palestynski-konferencja-w-paryzu,649333.html;200;0.952678;0.000340;0.034502;0;0.000;text/html; charset=UTF-8\n",
      "https://fakty.interia.pl/raporty/raport-konflikt-izraelsko-palestynski/opinie,parametr,fakty_dol,nPack,3;200;0.500981;0.000418;0.038015;0;0.000;text/html; charset=UTF-8\n",
      "http://cejsh.icm.edu.pl/cejsh/element/bwmeta1.element.desklight-c4ec6b2a-e60d-4928-a30f-dc6640296b15;200;0.295232;0.000377;0.034092;0;0.000;text/html;charset=UTF-8\n",
      "'''\n",
      "\n",
      "print (testr)\n",
      "74/113:\n",
      "testr = '''https://www.rp.pl/Konflikt-izraelsko-palestynski/180529976-USA-Minuta-ciszy-dla-Palestynczykow-w-szkole-Protest-Zydow.html;200;2.348753;0.000330;0.011990;0;0.000;text/html; charset=windows-1250\n",
      "http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.desklight-c3394e44-19a2-45eb-ad63-c71bec715d05;200;0.692087;0.053310;0.086916;0;0.000;text/html;charset=UTF-8\n",
      "https://www.rp.pl/Konflikt-izraelsko-palestynski/305159920-Izrael-zrzuca-wine-na-Hamas.html;200;0.559529;0.000298;0.011882;0;0.000;text/html; charset=windows-1250\n",
      "https://www.tvn24.pl/wiadomosci-ze-swiata,2/konflikt-izraelsko-palestynski-konferencja-w-paryzu,649333.html;200;0.952678;0.000340;0.034502;0;0.000;text/html; charset=UTF-8\n",
      "https://fakty.interia.pl/raporty/raport-konflikt-izraelsko-palestynski/opinie,parametr,fakty_dol,nPack,3;200;0.500981;0.000418;0.038015;0;0.000;text/html; charset=UTF-8\n",
      "http://cejsh.icm.edu.pl/cejsh/element/bwmeta1.element.desklight-c4ec6b2a-e60d-4928-a30f-dc6640296b15;200;0.295232;0.000377;0.034092;0;0.000;text/html;charset=UTF-8\n",
      "'''\n",
      "\n",
      "for x in testr.split('\\n'):\n",
      "    print (x)\n",
      "74/114:\n",
      "testr = '''https://www.rp.pl/Konflikt-izraelsko-palestynski/180529976-USA-Minuta-ciszy-dla-Palestynczykow-w-szkole-Protest-Zydow.html;200;2.348753;0.000330;0.011990;0;0.000;text/html; charset=windows-1250\n",
      "http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.desklight-c3394e44-19a2-45eb-ad63-c71bec715d05;200;0.692087;0.053310;0.086916;0;0.000;text/html;charset=UTF-8\n",
      "https://www.rp.pl/Konflikt-izraelsko-palestynski/305159920-Izrael-zrzuca-wine-na-Hamas.html;200;0.559529;0.000298;0.011882;0;0.000;text/html; charset=windows-1250\n",
      "https://www.tvn24.pl/wiadomosci-ze-swiata,2/konflikt-izraelsko-palestynski-konferencja-w-paryzu,649333.html;200;0.952678;0.000340;0.034502;0;0.000;text/html; charset=UTF-8\n",
      "https://fakty.interia.pl/raporty/raport-konflikt-izraelsko-palestynski/opinie,parametr,fakty_dol,nPack,3;200;0.500981;0.000418;0.038015;0;0.000;text/html; charset=UTF-8\n",
      "http://cejsh.icm.edu.pl/cejsh/element/bwmeta1.element.desklight-c4ec6b2a-e60d-4928-a30f-dc6640296b15;200;0.295232;0.000377;0.034092;0;0.000;text/html;charset=UTF-8\n",
      "'''\n",
      "import re\n",
      "\n",
      "curl_re = '(?P<url>.*);\\d+;[\\d\\.]+;[\\d\\.]+;[\\d\\.]+;[\\d\\.]+;(?P<content_type>[^;]+);?(?P<charset>[^;])$'\n",
      "for x in testr.split('\\n'):\n",
      "    re.match(curl_re, x)\n",
      "74/115:\n",
      "testr = '''https://www.rp.pl/Konflikt-izraelsko-palestynski/180529976-USA-Minuta-ciszy-dla-Palestynczykow-w-szkole-Protest-Zydow.html;200;2.348753;0.000330;0.011990;0;0.000;text/html; charset=windows-1250\n",
      "http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.desklight-c3394e44-19a2-45eb-ad63-c71bec715d05;200;0.692087;0.053310;0.086916;0;0.000;text/html;charset=UTF-8\n",
      "https://www.rp.pl/Konflikt-izraelsko-palestynski/305159920-Izrael-zrzuca-wine-na-Hamas.html;200;0.559529;0.000298;0.011882;0;0.000;text/html; charset=windows-1250\n",
      "https://www.tvn24.pl/wiadomosci-ze-swiata,2/konflikt-izraelsko-palestynski-konferencja-w-paryzu,649333.html;200;0.952678;0.000340;0.034502;0;0.000;text/html; charset=UTF-8\n",
      "https://fakty.interia.pl/raporty/raport-konflikt-izraelsko-palestynski/opinie,parametr,fakty_dol,nPack,3;200;0.500981;0.000418;0.038015;0;0.000;text/html; charset=UTF-8\n",
      "http://cejsh.icm.edu.pl/cejsh/element/bwmeta1.element.desklight-c4ec6b2a-e60d-4928-a30f-dc6640296b15;200;0.295232;0.000377;0.034092;0;0.000;text/html;charset=UTF-8\n",
      "'''\n",
      "import re\n",
      "\n",
      "curl_re = '(?P<url>.*);\\d+;[\\d\\.]+;[\\d\\.]+;[\\d\\.]+;[\\d\\.]+;(?P<content_type>[^;]+);?(?P<charset>[^;])$'\n",
      "for x in testr.split('\\n'):\n",
      "    print(re.match(curl_re, x))\n",
      "74/116:\n",
      "testr = '''https://www.rp.pl/Konflikt-izraelsko-palestynski/180529976-USA-Minuta-ciszy-dla-Palestynczykow-w-szkole-Protest-Zydow.html;200;2.348753;0.000330;0.011990;0;0.000;text/html; charset=windows-1250\n",
      "http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.desklight-c3394e44-19a2-45eb-ad63-c71bec715d05;200;0.692087;0.053310;0.086916;0;0.000;text/html;charset=UTF-8\n",
      "https://www.rp.pl/Konflikt-izraelsko-palestynski/305159920-Izrael-zrzuca-wine-na-Hamas.html;200;0.559529;0.000298;0.011882;0;0.000;text/html; charset=windows-1250\n",
      "https://www.tvn24.pl/wiadomosci-ze-swiata,2/konflikt-izraelsko-palestynski-konferencja-w-paryzu,649333.html;200;0.952678;0.000340;0.034502;0;0.000;text/html; charset=UTF-8\n",
      "https://fakty.interia.pl/raporty/raport-konflikt-izraelsko-palestynski/opinie,parametr,fakty_dol,nPack,3;200;0.500981;0.000418;0.038015;0;0.000;text/html; charset=UTF-8\n",
      "http://cejsh.icm.edu.pl/cejsh/element/bwmeta1.element.desklight-c4ec6b2a-e60d-4928-a30f-dc6640296b15;200;0.295232;0.000377;0.034092;0;0.000;text/html;charset=UTF-8\n",
      "'''\n",
      "import re\n",
      "\n",
      "curl_re = '^(?P<url>.*);(?P<status_code>\\d+);(?P<time_total>[\\d\\.]+);(?P<time_namelookup>[\\d\\.]+);(?P<time_connect>[\\d\\.]+);(?P<size_download>[\\d\\.]+);(?P<speed_download>[\\d\\.]+);(?P<content_type>[^;]+);?(?P<charset>[^;]+)$'\n",
      "for x in testr.split('\\n'):\n",
      "    print(re.match(curl_re, x))\n",
      "74/117:\n",
      "testr = '''https://www.rp.pl/Konflikt-izraelsko-palestynski/180529976-USA-Minuta-ciszy-dla-Palestynczykow-w-szkole-Protest-Zydow.html;200;2.348753;0.000330;0.011990;0;0.000;text/html; charset=windows-1250\n",
      "http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.desklight-c3394e44-19a2-45eb-ad63-c71bec715d05;200;0.692087;0.053310;0.086916;0;0.000;text/html;charset=UTF-8\n",
      "https://www.rp.pl/Konflikt-izraelsko-palestynski/305159920-Izrael-zrzuca-wine-na-Hamas.html;200;0.559529;0.000298;0.011882;0;0.000;text/html; charset=windows-1250\n",
      "https://www.tvn24.pl/wiadomosci-ze-swiata,2/konflikt-izraelsko-palestynski-konferencja-w-paryzu,649333.html;200;0.952678;0.000340;0.034502;0;0.000;text/html; charset=UTF-8\n",
      "https://fakty.interia.pl/raporty/raport-konflikt-izraelsko-palestynski/opinie,parametr,fakty_dol,nPack,3;200;0.500981;0.000418;0.038015;0;0.000;text/html; charset=UTF-8\n",
      "http://cejsh.icm.edu.pl/cejsh/element/bwmeta1.element.desklight-c4ec6b2a-e60d-4928-a30f-dc6640296b15;200;0.295232;0.000377;0.034092;0;0.000;text/html;charset=UTF-8\n",
      "'''\n",
      "import re\n",
      "\n",
      "curl_re = '^(?P<url>.*);(?P<status_code>\\d+);(?P<time_total>[\\d\\.]+);(?P<time_namelookup>[\\d\\.]+);(?P<time_connect>[\\d\\.]+);(?P<size_download>[\\d\\.]+);(?P<speed_download>[\\d\\.]+);(?P<content_type>[^;]+);?(?P<charset>[^;]+)$'\n",
      "for x in testr.split('\\n'):\n",
      "    print(re.match(curl_re, x).groups())\n",
      "74/118:\n",
      "testr = '''https://www.rp.pl/Konflikt-izraelsko-palestynski/180529976-USA-Minuta-ciszy-dla-Palestynczykow-w-szkole-Protest-Zydow.html;200;2.348753;0.000330;0.011990;0;0.000;text/html; charset=windows-1250\n",
      "http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.desklight-c3394e44-19a2-45eb-ad63-c71bec715d05;200;0.692087;0.053310;0.086916;0;0.000;text/html;charset=UTF-8\n",
      "https://www.rp.pl/Konflikt-izraelsko-palestynski/305159920-Izrael-zrzuca-wine-na-Hamas.html;200;0.559529;0.000298;0.011882;0;0.000;text/html; charset=windows-1250\n",
      "https://www.tvn24.pl/wiadomosci-ze-swiata,2/konflikt-izraelsko-palestynski-konferencja-w-paryzu,649333.html;200;0.952678;0.000340;0.034502;0;0.000;text/html; charset=UTF-8\n",
      "https://fakty.interia.pl/raporty/raport-konflikt-izraelsko-palestynski/opinie,parametr,fakty_dol,nPack,3;200;0.500981;0.000418;0.038015;0;0.000;text/html; charset=UTF-8\n",
      "http://cejsh.icm.edu.pl/cejsh/element/bwmeta1.element.desklight-c4ec6b2a-e60d-4928-a30f-dc6640296b15;200;0.295232;0.000377;0.034092;0;0.000;text/html;charset=UTF-8'''\n",
      "import re\n",
      "\n",
      "curl_re = '^(?P<url>.*);(?P<status_code>\\d+);(?P<time_total>[\\d\\.]+);(?P<time_namelookup>[\\d\\.]+);(?P<time_connect>[\\d\\.]+);(?P<size_download>[\\d\\.]+);(?P<speed_download>[\\d\\.]+);(?P<content_type>[^;]+);?(?P<charset>[^;]+)$'\n",
      "for x in testr.split('\\n'):\n",
      "    print(re.match(curl_re, x).groups())\n",
      "74/119:\n",
      "testr = '''https://www.rp.pl/Konflikt-izraelsko-palestynski/180529976-USA-Minuta-ciszy-dla-Palestynczykow-w-szkole-Protest-Zydow.html;200;2.348753;0.000330;0.011990;0;0.000;text/html; charset=windows-1250\n",
      "http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.desklight-c3394e44-19a2-45eb-ad63-c71bec715d05;200;0.692087;0.053310;0.086916;0;0.000;text/html;charset=UTF-8\n",
      "https://www.rp.pl/Konflikt-izraelsko-palestynski/305159920-Izrael-zrzuca-wine-na-Hamas.html;200;0.559529;0.000298;0.011882;0;0.000;text/html; charset=windows-1250\n",
      "https://www.tvn24.pl/wiadomosci-ze-swiata,2/konflikt-izraelsko-palestynski-konferencja-w-paryzu,649333.html;200;0.952678;0.000340;0.034502;0;0.000;text/html; charset=UTF-8\n",
      "https://fakty.interia.pl/raporty/raport-konflikt-izraelsko-palestynski/opinie,parametr,fakty_dol,nPack,3;200;0.500981;0.000418;0.038015;0;0.000;text/html; charset=UTF-8\n",
      "http://cejsh.icm.edu.pl/cejsh/element/bwmeta1.element.desklight-c4ec6b2a-e60d-4928-a30f-dc6640296b15;200;0.295232;0.000377;0.034092;0;0.000;text/html;charset=UTF-8'''\n",
      "import re\n",
      "\n",
      "curl_re = '^(?P<url>.*);(?P<status_code>\\d+);(?P<time_total>[\\d\\.]+);(?P<time_namelookup>[\\d\\.]+);(?P<time_connect>[\\d\\.]+);(?P<size_download>[\\d\\.]+);(?P<speed_download>[\\d\\.]+);(?P<content_type>[^;]+)(?:;?(?P<charset>[^;]+))?$'\n",
      "for x in testr.split('\\n'):\n",
      "    print(re.match(curl_re, x).groups())\n",
      "74/120:\n",
      "testr = '''https://www.rp.pl/Konflikt-izraelsko-palestynski/180529976-USA-Minuta-ciszy-dla-Palestynczykow-w-szkole-Protest-Zydow.html;200;2.348753;0.000330;0.011990;0;0.000;text/html; charset=windows-1250\n",
      "http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.desklight-c3394e44-19a2-45eb-ad63-c71bec715d05;200;0.692087;0.053310;0.086916;0;0.000;text/html;charset=UTF-8\n",
      "https://www.rp.pl/Konflikt-izraelsko-palestynski/305159920-Izrael-zrzuca-wine-na-Hamas.html;200;0.559529;0.000298;0.011882;0;0.000;text/html; charset=windows-1250\n",
      "https://www.tvn24.pl/wiadomosci-ze-swiata,2/konflikt-izraelsko-palestynski-konferencja-w-paryzu,649333.html;200;0.952678;0.000340;0.034502;0;0.000;text/html; charset=UTF-8\n",
      "https://fakty.interia.pl/raporty/raport-konflikt-izraelsko-palestynski/opinie,parametr,fakty_dol,nPack,3;200;0.500981;0.000418;0.038015;0;0.000;text/html; charset=UTF-8\n",
      "http://cejsh.icm.edu.pl/cejsh/element/bwmeta1.element.desklight-c4ec6b2a-e60d-4928-a30f-dc6640296b15;200;0.295232;0.000377;0.034092;0;0.000;text/html;charset=UTF-8'''\n",
      "import re\n",
      "\n",
      "curl_re = '^(?P<url>.*);(?P<status_code>\\d+);(?P<time_total>[\\d\\.]+);(?P<time_namelookup>[\\d\\.]+);(?P<time_connect>[\\d\\.]+);(?P<size_download>[\\d\\.]+);(?P<speed_download>[\\d\\.]+);(?P<content_type>[^;]+)(?:;?\\s+?(?P<charset>[^;]+))?$'\n",
      "for x in testr.split('\\n'):\n",
      "    print(re.match(curl_re, x).groups())\n",
      "74/121:\n",
      "testr = '''https://www.rp.pl/Konflikt-izraelsko-palestynski/180529976-USA-Minuta-ciszy-dla-Palestynczykow-w-szkole-Protest-Zydow.html;200;2.348753;0.000330;0.011990;0;0.000;text/html; charset=windows-1250\n",
      "http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.desklight-c3394e44-19a2-45eb-ad63-c71bec715d05;200;0.692087;0.053310;0.086916;0;0.000;text/html;charset=UTF-8\n",
      "https://www.rp.pl/Konflikt-izraelsko-palestynski/305159920-Izrael-zrzuca-wine-na-Hamas.html;200;0.559529;0.000298;0.011882;0;0.000;text/html; charset=windows-1250\n",
      "https://www.tvn24.pl/wiadomosci-ze-swiata,2/konflikt-izraelsko-palestynski-konferencja-w-paryzu,649333.html;200;0.952678;0.000340;0.034502;0;0.000;text/html; charset=UTF-8\n",
      "https://fakty.interia.pl/raporty/raport-konflikt-izraelsko-palestynski/opinie,parametr,fakty_dol,nPack,3;200;0.500981;0.000418;0.038015;0;0.000;text/html; charset=UTF-8\n",
      "http://cejsh.icm.edu.pl/cejsh/element/bwmeta1.element.desklight-c4ec6b2a-e60d-4928-a30f-dc6640296b15;200;0.295232;0.000377;0.034092;0;0.000;text/html;charset=UTF-8'''\n",
      "import re\n",
      "\n",
      "curl_re = '^(?P<url>.*);(?P<status_code>\\d+);(?P<time_total>[\\d\\.]+);(?P<time_namelookup>[\\d\\.]+);(?P<time_connect>[\\d\\.]+);(?P<size_download>[\\d\\.]+);(?P<speed_download>[\\d\\.]+);(?P<content_type>[^;]+)(?:;?\\s*(?P<charset>[^;]+))?$'\n",
      "for x in testr.split('\\n'):\n",
      "    print(re.match(curl_re, x).groups())\n",
      "74/122:\n",
      "testr = '''https://www.rp.pl/Konflikt-izraelsko-palestynski/180529976-USA-Minuta-ciszy-dla-Palestynczykow-w-szkole-Protest-Zydow.html;200;2.348753;0.000330;0.011990;0;0.000;text/html; charset=windows-1250\n",
      "http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.desklight-c3394e44-19a2-45eb-ad63-c71bec715d05;200;0.692087;0.053310;0.086916;0;0.000;text/html;charset=UTF-8\n",
      "https://www.rp.pl/Konflikt-izraelsko-palestynski/305159920-Izrael-zrzuca-wine-na-Hamas.html;200;0.559529;0.000298;0.011882;0;0.000;text/html; charset=windows-1250\n",
      "https://www.tvn24.pl/wiadomosci-ze-swiata,2/konflikt-izraelsko-palestynski-konferencja-w-paryzu,649333.html;200;0.952678;0.000340;0.034502;0;0.000;text/html; charset=UTF-8\n",
      "https://fakty.interia.pl/raporty/raport-konflikt-izraelsko-palestynski/opinie,parametr,fakty_dol,nPack,3;200;0.500981;0.000418;0.038015;0;0.000;text/html; charset=UTF-8\n",
      "http://cejsh.icm.edu.pl/cejsh/element/bwmeta1.element.desklight-c4ec6b2a-e60d-4928-a30f-dc6640296b15;200;0.295232;0.000377;0.034092;0;0.000;text/html;charset=UTF-8'''\n",
      "import re\n",
      "\n",
      "curl_re = '^(?P<url>.*);(?P<status_code>\\d+);(?P<time_total>[\\d\\.]+);(?P<time_namelookup>[\\d\\.]+);(?P<time_connect>[\\d\\.]+);(?P<size_download>[\\d\\.]+);(?P<speed_download>[\\d\\.]+);(?P<content_type>[^;]+)(?:;?\\s*(?P<charset>[^;]+))?$'\n",
      "for x in testr.split('\\n'):\n",
      "    print(re.match(curl_re, x).groups())\n",
      "    dir(re.match(curl_re, x))\n",
      "74/123:\n",
      "testr = '''https://www.rp.pl/Konflikt-izraelsko-palestynski/180529976-USA-Minuta-ciszy-dla-Palestynczykow-w-szkole-Protest-Zydow.html;200;2.348753;0.000330;0.011990;0;0.000;text/html; charset=windows-1250\n",
      "http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.desklight-c3394e44-19a2-45eb-ad63-c71bec715d05;200;0.692087;0.053310;0.086916;0;0.000;text/html;charset=UTF-8\n",
      "https://www.rp.pl/Konflikt-izraelsko-palestynski/305159920-Izrael-zrzuca-wine-na-Hamas.html;200;0.559529;0.000298;0.011882;0;0.000;text/html; charset=windows-1250\n",
      "https://www.tvn24.pl/wiadomosci-ze-swiata,2/konflikt-izraelsko-palestynski-konferencja-w-paryzu,649333.html;200;0.952678;0.000340;0.034502;0;0.000;text/html; charset=UTF-8\n",
      "https://fakty.interia.pl/raporty/raport-konflikt-izraelsko-palestynski/opinie,parametr,fakty_dol,nPack,3;200;0.500981;0.000418;0.038015;0;0.000;text/html; charset=UTF-8\n",
      "http://cejsh.icm.edu.pl/cejsh/element/bwmeta1.element.desklight-c4ec6b2a-e60d-4928-a30f-dc6640296b15;200;0.295232;0.000377;0.034092;0;0.000;text/html;charset=UTF-8'''\n",
      "import re\n",
      "\n",
      "curl_re = '^(?P<url>.*);(?P<status_code>\\d+);(?P<time_total>[\\d\\.]+);(?P<time_namelookup>[\\d\\.]+);(?P<time_connect>[\\d\\.]+);(?P<size_download>[\\d\\.]+);(?P<speed_download>[\\d\\.]+);(?P<content_type>[^;]+)(?:;?\\s*(?P<charset>[^;]+))?$'\n",
      "for x in testr.split('\\n'):\n",
      "    print(re.match(curl_re, x).groups())\n",
      "    print(dir(re.match(curl_re, x)))\n",
      "74/124:\n",
      "testr = '''https://www.rp.pl/Konflikt-izraelsko-palestynski/180529976-USA-Minuta-ciszy-dla-Palestynczykow-w-szkole-Protest-Zydow.html;200;2.348753;0.000330;0.011990;0;0.000;text/html; charset=windows-1250\n",
      "http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.desklight-c3394e44-19a2-45eb-ad63-c71bec715d05;200;0.692087;0.053310;0.086916;0;0.000;text/html;charset=UTF-8\n",
      "https://www.rp.pl/Konflikt-izraelsko-palestynski/305159920-Izrael-zrzuca-wine-na-Hamas.html;200;0.559529;0.000298;0.011882;0;0.000;text/html; charset=windows-1250\n",
      "https://www.tvn24.pl/wiadomosci-ze-swiata,2/konflikt-izraelsko-palestynski-konferencja-w-paryzu,649333.html;200;0.952678;0.000340;0.034502;0;0.000;text/html; charset=UTF-8\n",
      "https://fakty.interia.pl/raporty/raport-konflikt-izraelsko-palestynski/opinie,parametr,fakty_dol,nPack,3;200;0.500981;0.000418;0.038015;0;0.000;text/html; charset=UTF-8\n",
      "http://cejsh.icm.edu.pl/cejsh/element/bwmeta1.element.desklight-c4ec6b2a-e60d-4928-a30f-dc6640296b15;200;0.295232;0.000377;0.034092;0;0.000;text/html;charset=UTF-8'''\n",
      "import re\n",
      "\n",
      "curl_re = '^(?P<url>.*);(?P<status_code>\\d+);(?P<time_total>[\\d\\.]+);(?P<time_namelookup>[\\d\\.]+);(?P<time_connect>[\\d\\.]+);(?P<size_download>[\\d\\.]+);(?P<speed_download>[\\d\\.]+);(?P<content_type>[^;]+)(?:;?\\s*(?P<charset>[^;]+))?$'\n",
      "for x in testr.split('\\n'):\n",
      "    print(re.match(curl_re, x).groupdict())\n",
      "74/125:\n",
      "curl = '../../curl_results.csv'\n",
      "with open(curl, 'r') as f:\n",
      "    res = [re.match(curl_re, line.strip('\\n')).groupdict() for line in f.readlines()]\n",
      "        \n",
      "len(res)\n",
      "74/126:\n",
      "curl = '../../curl_results.csv'\n",
      "with open(curl, 'r') as f:\n",
      "    [print(line) for line in f.readlines()]\n",
      "    res = [re.match(curl_re, line.strip('\\n')).groupdict() for line in f.readlines()]\n",
      "        \n",
      "len(res)\n",
      "74/127:\n",
      "curl = '../../curl_results.csv'\n",
      "with open(curl, 'r') as f:\n",
      "    [print(line) for line in f.readlines()]\n",
      "    res = [re.match(curl_re, line.strip('\\\\n')).groupdict() for line in f.readlines()]\n",
      "        \n",
      "len(res)\n",
      "74/128:\n",
      "curl = '../../curl_results.csv'\n",
      "with open(curl, 'r') as f:\n",
      "    for line in f.readlines():\n",
      "        m = re.match(curl_re, line.strip('\\\\n'))\n",
      "            if m is None:\n",
      "                print line\n",
      "        \n",
      "len(res)\n",
      "74/129:\n",
      "curl = '../../curl_results.csv'\n",
      "with open(curl, 'r') as f:\n",
      "    for line in f.readlines():\n",
      "        m = re.match(curl_re, line.strip('\\\\n'))\n",
      "        if m is None:\n",
      "            print line\n",
      "        \n",
      "len(res)\n",
      "74/130:\n",
      "curl = '../../curl_results.csv'\n",
      "with open(curl, 'r') as f:\n",
      "    for line in f.readlines():\n",
      "        m = re.match(curl_re, line.strip('\\\\n'))\n",
      "        if m is None:\n",
      "            print( line)\n",
      "        \n",
      "len(res)\n",
      "74/131:\n",
      "testr = '''https://www.rp.pl/Konflikt-izraelsko-palestynski/180529976-USA-Minuta-ciszy-dla-Palestynczykow-w-szkole-Protest-Zydow.html;200;2.348753;0.000330;0.011990;0;0.000;text/html; charset=windows-1250\n",
      "http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.desklight-c3394e44-19a2-45eb-ad63-c71bec715d05;200;0.692087;0.053310;0.086916;0;0.000;text/html;charset=UTF-8\n",
      "https://www.rp.pl/Konflikt-izraelsko-palestynski/305159920-Izrael-zrzuca-wine-na-Hamas.html;200;0.559529;0.000298;0.011882;0;0.000;text/html; charset=windows-1250\n",
      "https://www.tvn24.pl/wiadomosci-ze-swiata,2/konflikt-izraelsko-palestynski-konferencja-w-paryzu,649333.html;200;0.952678;0.000340;0.034502;0;0.000;text/html; charset=UTF-8\n",
      "https://fakty.interia.pl/raporty/raport-konflikt-izraelsko-palestynski/opinie,parametr,fakty_dol,nPack,3;200;0.500981;0.000418;0.038015;0;0.000;text/html; charset=UTF-8\n",
      "http://cejsh.icm.edu.pl/cejsh/element/bwmeta1.element.desklight-c4ec6b2a-e60d-4928-a30f-dc6640296b15;200;0.295232;0.000377;0.034092;0;0.000;text/html;charset=UTF-8'''\n",
      "import re\n",
      "\n",
      "curl_re = '^(?P<url>.*);(?P<status_code>\\d+);(?P<time_total>[\\d\\.]+);(?P<time_namelookup>[\\d\\.]+);(?P<time_connect>[\\d\\.]+);(?P<size_download>[\\d\\.]+);(?P<speed_download>[\\d\\.]+);(?P<content_type>[^;]+)(?:;?\\s*(?P<charset>c|Charset=[^;]+))*;?$'\n",
      "for x in testr.split('\\n'):\n",
      "    print(re.match(curl_re, x).groupdict())\n",
      "74/132:\n",
      "testr = '''https://www.rp.pl/Konflikt-izraelsko-palestynski/180529976-USA-Minuta-ciszy-dla-Palestynczykow-w-szkole-Protest-Zydow.html;200;2.348753;0.000330;0.011990;0;0.000;text/html; charset=windows-1250\n",
      "http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.desklight-c3394e44-19a2-45eb-ad63-c71bec715d05;200;0.692087;0.053310;0.086916;0;0.000;text/html;charset=UTF-8\n",
      "https://www.rp.pl/Konflikt-izraelsko-palestynski/305159920-Izrael-zrzuca-wine-na-Hamas.html;200;0.559529;0.000298;0.011882;0;0.000;text/html; charset=windows-1250\n",
      "https://www.tvn24.pl/wiadomosci-ze-swiata,2/konflikt-izraelsko-palestynski-konferencja-w-paryzu,649333.html;200;0.952678;0.000340;0.034502;0;0.000;text/html; charset=UTF-8\n",
      "https://fakty.interia.pl/raporty/raport-konflikt-izraelsko-palestynski/opinie,parametr,fakty_dol,nPack,3;200;0.500981;0.000418;0.038015;0;0.000;text/html; charset=UTF-8\n",
      "http://cejsh.icm.edu.pl/cejsh/element/bwmeta1.element.desklight-c4ec6b2a-e60d-4928-a30f-dc6640296b15;200;0.295232;0.000377;0.034092;0;0.000;text/html;charset=UTF-8'''\n",
      "import re\n",
      "\n",
      "curl_re = '^(?P<url>.*);(?P<status_code>\\d+);(?P<time_total>[\\d\\.]+);(?P<time_namelookup>[\\d\\.]+);(?P<time_connect>[\\d\\.]+);(?P<size_download>[\\d\\.]+);(?P<speed_download>[\\d\\.]+);(?P<content_type>[^;]+)(?:;?\\s*(?P<charset>[^;]+))*;?$'\n",
      "for x in testr.split('\\n'):\n",
      "    print(re.match(curl_re, x).groupdict())\n",
      "74/133:\n",
      "testr = '''https://www.rp.pl/Konflikt-izraelsko-palestynski/180529976-USA-Minuta-ciszy-dla-Palestynczykow-w-szkole-Protest-Zydow.html;200;2.348753;0.000330;0.011990;0;0.000;text/html; charset=windows-1250\n",
      "http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.desklight-c3394e44-19a2-45eb-ad63-c71bec715d05;200;0.692087;0.053310;0.086916;0;0.000;text/html;charset=UTF-8\n",
      "https://www.rp.pl/Konflikt-izraelsko-palestynski/305159920-Izrael-zrzuca-wine-na-Hamas.html;200;0.559529;0.000298;0.011882;0;0.000;text/html; charset=windows-1250\n",
      "https://www.tvn24.pl/wiadomosci-ze-swiata,2/konflikt-izraelsko-palestynski-konferencja-w-paryzu,649333.html;200;0.952678;0.000340;0.034502;0;0.000;text/html; charset=UTF-8\n",
      "https://fakty.interia.pl/raporty/raport-konflikt-izraelsko-palestynski/opinie,parametr,fakty_dol,nPack,3;200;0.500981;0.000418;0.038015;0;0.000;text/html; charset=UTF-8\n",
      "http://cejsh.icm.edu.pl/cejsh/element/bwmeta1.element.desklight-c4ec6b2a-e60d-4928-a30f-dc6640296b15;200;0.295232;0.000377;0.034092;0;0.000;text/html;charset=UTF-8'''\n",
      "import re\n",
      "\n",
      "curl_re = '^(?P<url>.*);(?P<status_code>\\d+);(?P<time_total>[\\d\\.]+);(?P<time_namelookup>[\\d\\.]+);(?P<time_connect>[\\d\\.]+);(?P<size_download>[\\d\\.]+);(?P<speed_download>[\\d\\.]+);(?P<content_type>[^;]+)(?:;?\\s*(?P<charset>(c|C)harset=[^;]+))*;?$'\n",
      "for x in testr.split('\\n'):\n",
      "    print(re.match(curl_re, x).groupdict())\n",
      "74/134:\n",
      "curl = '../../curl_results.csv'\n",
      "with open(curl, 'r') as f:\n",
      "    for line in f.readlines():\n",
      "        m = re.match(curl_re, line.strip('\\\\n'))\n",
      "        if m is None:\n",
      "            print( line)\n",
      "        \n",
      "len(res)\n",
      "74/135:\n",
      "testr = '''https://www.rp.pl/Konflikt-izraelsko-palestynski/180529976-USA-Minuta-ciszy-dla-Palestynczykow-w-szkole-Protest-Zydow.html;200;2.348753;0.000330;0.011990;0;0.000;text/html; charset=windows-1250\n",
      "http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.desklight-c3394e44-19a2-45eb-ad63-c71bec715d05;200;0.692087;0.053310;0.086916;0;0.000;text/html;charset=UTF-8\n",
      "https://www.rp.pl/Konflikt-izraelsko-palestynski/305159920-Izrael-zrzuca-wine-na-Hamas.html;200;0.559529;0.000298;0.011882;0;0.000;text/html; charset=windows-1250\n",
      "https://www.tvn24.pl/wiadomosci-ze-swiata,2/konflikt-izraelsko-palestynski-konferencja-w-paryzu,649333.html;200;0.952678;0.000340;0.034502;0;0.000;text/html; charset=UTF-8\n",
      "https://fakty.interia.pl/raporty/raport-konflikt-izraelsko-palestynski/opinie,parametr,fakty_dol,nPack,3;200;0.500981;0.000418;0.038015;0;0.000;text/html; charset=UTF-8\n",
      "http://cejsh.icm.edu.pl/cejsh/element/bwmeta1.element.desklight-c4ec6b2a-e60d-4928-a30f-dc6640296b15;200;0.295232;0.000377;0.034092;0;0.000;text/html;charset=UTF-8'''\n",
      "import re\n",
      "\n",
      "#curl_re = '^(?P<url>.*);(?P<status_code>\\d+);(?P<time_total>[\\d\\.]+);(?P<time_namelookup>[\\d\\.]+);(?P<time_connect>[\\d\\.]+);(?P<size_download>[\\d\\.]+);(?P<speed_download>[\\d\\.]+);(?P<content_type>[^;]+)(?:;?\\s*(?P<charset>(c|C)harset=[^;]+))*;?$'\n",
      "curl_re = '^(?P<url>.*);(?P<status_code>\\d+);(?P<time_total>[\\d\\.]+);(?P<time_namelookup>[\\d\\.]+);(?P<time_connect>[\\d\\.]+);(?P<size_download>[\\d\\.]+);(?P<speed_download>[\\d\\.]+);(?P<content_type>([^;]+)|(text/html.*)$'\n",
      "\n",
      "for x in testr.split('\\n'):\n",
      "    print(re.match(curl_re, x).groupdict())\n",
      "74/136:\n",
      "testr = '''https://www.rp.pl/Konflikt-izraelsko-palestynski/180529976-USA-Minuta-ciszy-dla-Palestynczykow-w-szkole-Protest-Zydow.html;200;2.348753;0.000330;0.011990;0;0.000;text/html; charset=windows-1250\n",
      "http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.desklight-c3394e44-19a2-45eb-ad63-c71bec715d05;200;0.692087;0.053310;0.086916;0;0.000;text/html;charset=UTF-8\n",
      "https://www.rp.pl/Konflikt-izraelsko-palestynski/305159920-Izrael-zrzuca-wine-na-Hamas.html;200;0.559529;0.000298;0.011882;0;0.000;text/html; charset=windows-1250\n",
      "https://www.tvn24.pl/wiadomosci-ze-swiata,2/konflikt-izraelsko-palestynski-konferencja-w-paryzu,649333.html;200;0.952678;0.000340;0.034502;0;0.000;text/html; charset=UTF-8\n",
      "https://fakty.interia.pl/raporty/raport-konflikt-izraelsko-palestynski/opinie,parametr,fakty_dol,nPack,3;200;0.500981;0.000418;0.038015;0;0.000;text/html; charset=UTF-8\n",
      "http://cejsh.icm.edu.pl/cejsh/element/bwmeta1.element.desklight-c4ec6b2a-e60d-4928-a30f-dc6640296b15;200;0.295232;0.000377;0.034092;0;0.000;text/html;charset=UTF-8'''\n",
      "import re\n",
      "\n",
      "#curl_re = '^(?P<url>.*);(?P<status_code>\\d+);(?P<time_total>[\\d\\.]+);(?P<time_namelookup>[\\d\\.]+);(?P<time_connect>[\\d\\.]+);(?P<size_download>[\\d\\.]+);(?P<speed_download>[\\d\\.]+);(?P<content_type>[^;]+)(?:;?\\s*(?P<charset>(c|C)harset=[^;]+))*;?$'\n",
      "curl_re = '^(?P<url>.*);(?P<status_code>\\d+);(?P<time_total>[\\d\\.]+);(?P<time_namelookup>[\\d\\.]+);(?P<time_connect>[\\d\\.]+);(?P<size_download>[\\d\\.]+);(?P<speed_download>[\\d\\.]+);(?P<content_type>([^;]+)|(text/html.*))$'\n",
      "\n",
      "for x in testr.split('\\n'):\n",
      "    print(re.match(curl_re, x).groupdict())\n",
      "74/137:\n",
      "curl = '../../curl_results.csv'\n",
      "with open(curl, 'r') as f:\n",
      "    for line in f.readlines():\n",
      "        m = re.match(curl_re, line.strip('\\\\n'))\n",
      "        if m is None:\n",
      "            print( line)\n",
      "        \n",
      "len(res)\n",
      "74/138:\n",
      "testr = '''https://www.rp.pl/Konflikt-izraelsko-palestynski/180529976-USA-Minuta-ciszy-dla-Palestynczykow-w-szkole-Protest-Zydow.html;200;2.348753;0.000330;0.011990;0;0.000;text/html; charset=windows-1250\n",
      "http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.desklight-c3394e44-19a2-45eb-ad63-c71bec715d05;200;0.692087;0.053310;0.086916;0;0.000;text/html;charset=UTF-8\n",
      "https://www.rp.pl/Konflikt-izraelsko-palestynski/305159920-Izrael-zrzuca-wine-na-Hamas.html;200;0.559529;0.000298;0.011882;0;0.000;text/html; charset=windows-1250\n",
      "https://www.tvn24.pl/wiadomosci-ze-swiata,2/konflikt-izraelsko-palestynski-konferencja-w-paryzu,649333.html;200;0.952678;0.000340;0.034502;0;0.000;text/html; charset=UTF-8\n",
      "https://fakty.interia.pl/raporty/raport-konflikt-izraelsko-palestynski/opinie,parametr,fakty_dol,nPack,3;200;0.500981;0.000418;0.038015;0;0.000;text/html; charset=UTF-8\n",
      "http://cejsh.icm.edu.pl/cejsh/element/bwmeta1.element.desklight-c4ec6b2a-e60d-4928-a30f-dc6640296b15;200;0.295232;0.000377;0.034092;0;0.000;text/html;charset=UTF-8'''\n",
      "import re\n",
      "\n",
      "#curl_re = '^(?P<url>.*);(?P<status_code>\\d+);(?P<time_total>[\\d\\.]+);(?P<time_namelookup>[\\d\\.]+);(?P<time_connect>[\\d\\.]+);(?P<size_download>[\\d\\.]+);(?P<speed_download>[\\d\\.]+);(?P<content_type>[^;]+)(?:;?\\s*(?P<charset>(c|C)harset=[^;]+))*;?$'\n",
      "curl_re = '^(?P<url>.*);(?P<status_code>\\d+);(?P<time_total>[\\d\\.]+);(?P<time_namelookup>[\\d\\.]+);(?P<time_connect>[\\d\\.]+);(?P<size_download>[\\d\\.]+);(?P<speed_download>[\\d\\.]+);(?P<content_type>([^;]+)|(text/html.*)|(text/plain.*))$'\n",
      "\n",
      "for x in testr.split('\\n'):\n",
      "    print(re.match(curl_re, x).groupdict())\n",
      "74/139:\n",
      "curl = '../../curl_results.csv'\n",
      "with open(curl, 'r') as f:\n",
      "    for line in f.readlines():\n",
      "        m = re.match(curl_re, line.strip('\\\\n'))\n",
      "        if m is None:\n",
      "            print( line)\n",
      "        \n",
      "len(res)\n",
      "74/140:\n",
      "testr = '''https://www.rp.pl/Konflikt-izraelsko-palestynski/180529976-USA-Minuta-ciszy-dla-Palestynczykow-w-szkole-Protest-Zydow.html;200;2.348753;0.000330;0.011990;0;0.000;text/html; charset=windows-1250\n",
      "http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.desklight-c3394e44-19a2-45eb-ad63-c71bec715d05;200;0.692087;0.053310;0.086916;0;0.000;text/html;charset=UTF-8\n",
      "https://www.rp.pl/Konflikt-izraelsko-palestynski/305159920-Izrael-zrzuca-wine-na-Hamas.html;200;0.559529;0.000298;0.011882;0;0.000;text/html; charset=windows-1250\n",
      "https://www.tvn24.pl/wiadomosci-ze-swiata,2/konflikt-izraelsko-palestynski-konferencja-w-paryzu,649333.html;200;0.952678;0.000340;0.034502;0;0.000;text/html; charset=UTF-8\n",
      "https://fakty.interia.pl/raporty/raport-konflikt-izraelsko-palestynski/opinie,parametr,fakty_dol,nPack,3;200;0.500981;0.000418;0.038015;0;0.000;text/html; charset=UTF-8\n",
      "http://cejsh.icm.edu.pl/cejsh/element/bwmeta1.element.desklight-c4ec6b2a-e60d-4928-a30f-dc6640296b15;200;0.295232;0.000377;0.034092;0;0.000;text/html;charset=UTF-8'''\n",
      "import re\n",
      "\n",
      "#curl_re = '^(?P<url>.*);(?P<status_code>\\d+);(?P<time_total>[\\d\\.]+);(?P<time_namelookup>[\\d\\.]+);(?P<time_connect>[\\d\\.]+);(?P<size_download>[\\d\\.]+);(?P<speed_download>[\\d\\.]+);(?P<content_type>[^;]+)(?:;?\\s*(?P<charset>(c|C)harset=[^;]+))*;?$'\n",
      "curl_re = '^(?P<url>.*);(?P<status_code>\\d+);(?P<time_total>[\\d\\.]+);(?P<time_namelookup>[\\d\\.]+);(?P<time_connect>[\\d\\.]+);(?P<size_download>[\\d\\.]+);(?P<speed_download>[\\d\\.]+);(?P<content_type>([^;]+)|(text/html.*)|(text/plain.*)|(text/xml)|(application/json))$'\n",
      "\n",
      "for x in testr.split('\\n'):\n",
      "    print(re.match(curl_re, x).groupdict())\n",
      "74/141:\n",
      "curl = '../../curl_results.csv'\n",
      "with open(curl, 'r') as f:\n",
      "    for line in f.readlines():\n",
      "        m = re.match(curl_re, line.strip('\\\\n'))\n",
      "        if m is None:\n",
      "            print( line)\n",
      "        \n",
      "len(res)\n",
      "74/142:\n",
      "testr = '''https://www.rp.pl/Konflikt-izraelsko-palestynski/180529976-USA-Minuta-ciszy-dla-Palestynczykow-w-szkole-Protest-Zydow.html;200;2.348753;0.000330;0.011990;0;0.000;text/html; charset=windows-1250\n",
      "http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.desklight-c3394e44-19a2-45eb-ad63-c71bec715d05;200;0.692087;0.053310;0.086916;0;0.000;text/html;charset=UTF-8\n",
      "https://www.rp.pl/Konflikt-izraelsko-palestynski/305159920-Izrael-zrzuca-wine-na-Hamas.html;200;0.559529;0.000298;0.011882;0;0.000;text/html; charset=windows-1250\n",
      "https://www.tvn24.pl/wiadomosci-ze-swiata,2/konflikt-izraelsko-palestynski-konferencja-w-paryzu,649333.html;200;0.952678;0.000340;0.034502;0;0.000;text/html; charset=UTF-8\n",
      "https://fakty.interia.pl/raporty/raport-konflikt-izraelsko-palestynski/opinie,parametr,fakty_dol,nPack,3;200;0.500981;0.000418;0.038015;0;0.000;text/html; charset=UTF-8\n",
      "http://cejsh.icm.edu.pl/cejsh/element/bwmeta1.element.desklight-c4ec6b2a-e60d-4928-a30f-dc6640296b15;200;0.295232;0.000377;0.034092;0;0.000;text/html;charset=UTF-8'''\n",
      "import re\n",
      "\n",
      "#curl_re = '^(?P<url>.*);(?P<status_code>\\d+);(?P<time_total>[\\d\\.]+);(?P<time_namelookup>[\\d\\.]+);(?P<time_connect>[\\d\\.]+);(?P<size_download>[\\d\\.]+);(?P<speed_download>[\\d\\.]+);(?P<content_type>[^;]+)(?:;?\\s*(?P<charset>(c|C)harset=[^;]+))*;?$'\n",
      "curl_re = '^(?P<url>.*);(?P<status_code>\\d+);(?P<time_total>[\\d\\.]+);(?P<time_namelookup>[\\d\\.]+);(?P<time_connect>[\\d\\.]+);(?P<size_download>[\\d\\.]+);(?P<speed_download>[\\d\\.]+);(?P<content_type>([^;]+)|(text/html.*)|(text/plain.*)|(text/xml.*)|(application/json.*))$'\n",
      "\n",
      "for x in testr.split('\\n'):\n",
      "    print(re.match(curl_re, x).groupdict())\n",
      "74/143:\n",
      "curl = '../../curl_results.csv'\n",
      "with open(curl, 'r') as f:\n",
      "    for line in f.readlines():\n",
      "        m = re.match(curl_re, line.strip('\\\\n'))\n",
      "        if m is None:\n",
      "            print( line)\n",
      "        \n",
      "len(res)\n",
      "74/144:\n",
      "testr = '''https://www.rp.pl/Konflikt-izraelsko-palestynski/180529976-USA-Minuta-ciszy-dla-Palestynczykow-w-szkole-Protest-Zydow.html;200;2.348753;0.000330;0.011990;0;0.000;text/html; charset=windows-1250\n",
      "http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.desklight-c3394e44-19a2-45eb-ad63-c71bec715d05;200;0.692087;0.053310;0.086916;0;0.000;text/html;charset=UTF-8\n",
      "https://www.rp.pl/Konflikt-izraelsko-palestynski/305159920-Izrael-zrzuca-wine-na-Hamas.html;200;0.559529;0.000298;0.011882;0;0.000;text/html; charset=windows-1250\n",
      "https://www.tvn24.pl/wiadomosci-ze-swiata,2/konflikt-izraelsko-palestynski-konferencja-w-paryzu,649333.html;200;0.952678;0.000340;0.034502;0;0.000;text/html; charset=UTF-8\n",
      "https://fakty.interia.pl/raporty/raport-konflikt-izraelsko-palestynski/opinie,parametr,fakty_dol,nPack,3;200;0.500981;0.000418;0.038015;0;0.000;text/html; charset=UTF-8\n",
      "http://cejsh.icm.edu.pl/cejsh/element/bwmeta1.element.desklight-c4ec6b2a-e60d-4928-a30f-dc6640296b15;200;0.295232;0.000377;0.034092;0;0.000;text/html;charset=UTF-8'''\n",
      "import re\n",
      "\n",
      "#curl_re = '^(?P<url>.*);(?P<status_code>\\d+);(?P<time_total>[\\d\\.]+);(?P<time_namelookup>[\\d\\.]+);(?P<time_connect>[\\d\\.]+);(?P<size_download>[\\d\\.]+);(?P<speed_download>[\\d\\.]+);(?P<content_type>[^;]+)(?:;?\\s*(?P<charset>(c|C)harset=[^;]+))*;?$'\n",
      "curl_re = '^(?P<url>.*);(?P<status_code>\\d+);(?P<time_total>[\\d\\.]+);(?P<time_namelookup>[\\d\\.]+);(?P<time_connect>[\\d\\.]+);(?P<size_download>[\\d\\.]+);(?P<speed_download>[\\d\\.]+);(?P<content_type>([^;]+)|(text/html.*)|(text/plain.*)|(text/xml.*)|(application/json.*)|(application/rss+xml.*))$'\n",
      "\n",
      "for x in testr.split('\\n'):\n",
      "    print(re.match(curl_re, x).groupdict())\n",
      "74/145:\n",
      "curl = '../../curl_results.csv'\n",
      "with open(curl, 'r') as f:\n",
      "    for line in f.readlines():\n",
      "        m = re.match(curl_re, line.strip('\\\\n'))\n",
      "        if m is None:\n",
      "            print( line)\n",
      "        \n",
      "len(res)\n",
      "74/146:\n",
      "testr = '''https://www.rp.pl/Konflikt-izraelsko-palestynski/180529976-USA-Minuta-ciszy-dla-Palestynczykow-w-szkole-Protest-Zydow.html;200;2.348753;0.000330;0.011990;0;0.000;text/html; charset=windows-1250\n",
      "http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.desklight-c3394e44-19a2-45eb-ad63-c71bec715d05;200;0.692087;0.053310;0.086916;0;0.000;text/html;charset=UTF-8\n",
      "https://www.rp.pl/Konflikt-izraelsko-palestynski/305159920-Izrael-zrzuca-wine-na-Hamas.html;200;0.559529;0.000298;0.011882;0;0.000;text/html; charset=windows-1250\n",
      "https://www.tvn24.pl/wiadomosci-ze-swiata,2/konflikt-izraelsko-palestynski-konferencja-w-paryzu,649333.html;200;0.952678;0.000340;0.034502;0;0.000;text/html; charset=UTF-8\n",
      "https://fakty.interia.pl/raporty/raport-konflikt-izraelsko-palestynski/opinie,parametr,fakty_dol,nPack,3;200;0.500981;0.000418;0.038015;0;0.000;text/html; charset=UTF-8\n",
      "http://cejsh.icm.edu.pl/cejsh/element/bwmeta1.element.desklight-c4ec6b2a-e60d-4928-a30f-dc6640296b15;200;0.295232;0.000377;0.034092;0;0.000;text/html;charset=UTF-8'''\n",
      "import re\n",
      "\n",
      "#curl_re = '^(?P<url>.*);(?P<status_code>\\d+);(?P<time_total>[\\d\\.]+);(?P<time_namelookup>[\\d\\.]+);(?P<time_connect>[\\d\\.]+);(?P<size_download>[\\d\\.]+);(?P<speed_download>[\\d\\.]+);(?P<content_type>[^;]+)(?:;?\\s*(?P<charset>(c|C)harset=[^;]+))*;?$'\n",
      "curl_re = '^(?P<url>.*);(?P<status_code>\\d+);(?P<time_total>[\\d\\.]+);(?P<time_namelookup>[\\d\\.]+);(?P<time_connect>[\\d\\.]+);(?P<size_download>[\\d\\.]+);(?P<speed_download>[\\d\\.]+);(?P<content_type>([^;]+)|(text/html.*)|(text/plain.*)|(text/xml.*)|(application/json.*)|(application/rss\\+xml.*))$'\n",
      "\n",
      "for x in testr.split('\\n'):\n",
      "    print(re.match(curl_re, x).groupdict())\n",
      "74/147:\n",
      "curl = '../../curl_results.csv'\n",
      "with open(curl, 'r') as f:\n",
      "    for line in f.readlines():\n",
      "        m = re.match(curl_re, line.strip('\\\\n'))\n",
      "        if m is None:\n",
      "            print( line)\n",
      "        \n",
      "len(res)\n",
      "74/148:\n",
      "testr = '''https://www.rp.pl/Konflikt-izraelsko-palestynski/180529976-USA-Minuta-ciszy-dla-Palestynczykow-w-szkole-Protest-Zydow.html;200;2.348753;0.000330;0.011990;0;0.000;text/html; charset=windows-1250\n",
      "http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.desklight-c3394e44-19a2-45eb-ad63-c71bec715d05;200;0.692087;0.053310;0.086916;0;0.000;text/html;charset=UTF-8\n",
      "https://www.rp.pl/Konflikt-izraelsko-palestynski/305159920-Izrael-zrzuca-wine-na-Hamas.html;200;0.559529;0.000298;0.011882;0;0.000;text/html; charset=windows-1250\n",
      "https://www.tvn24.pl/wiadomosci-ze-swiata,2/konflikt-izraelsko-palestynski-konferencja-w-paryzu,649333.html;200;0.952678;0.000340;0.034502;0;0.000;text/html; charset=UTF-8\n",
      "https://fakty.interia.pl/raporty/raport-konflikt-izraelsko-palestynski/opinie,parametr,fakty_dol,nPack,3;200;0.500981;0.000418;0.038015;0;0.000;text/html; charset=UTF-8\n",
      "http://cejsh.icm.edu.pl/cejsh/element/bwmeta1.element.desklight-c4ec6b2a-e60d-4928-a30f-dc6640296b15;200;0.295232;0.000377;0.034092;0;0.000;text/html;charset=UTF-8'''\n",
      "import re\n",
      "\n",
      "#curl_re = '^(?P<url>.*);(?P<status_code>\\d+);(?P<time_total>[\\d\\.]+);(?P<time_namelookup>[\\d\\.]+);(?P<time_connect>[\\d\\.]+);(?P<size_download>[\\d\\.]+);(?P<speed_download>[\\d\\.]+);(?P<content_type>[^;]+)(?:;?\\s*(?P<charset>(c|C)harset=[^;]+))*;?$'\n",
      "curl_re = '^(?P<url>.*);(?P<status_code>\\d+);(?P<time_total>[\\d\\.]+);(?P<time_namelookup>[\\d\\.]+);(?P<time_connect>[\\d\\.]+);(?P<size_download>[\\d\\.]+);(?P<speed_download>[\\d\\.]+);(?P<content_type>([^;]+)|(text/html.*)|(text/plain.*)|(text/xml.*)|(application/json.*)|(application/rss\\+xml.*)|(\\*/\\*.*))$'\n",
      "\n",
      "for x in testr.split('\\n'):\n",
      "    print(re.match(curl_re, x).groupdict())\n",
      "74/149:\n",
      "curl = '../../curl_results.csv'\n",
      "with open(curl, 'r') as f:\n",
      "    for line in f.readlines():\n",
      "        m = re.match(curl_re, line.strip('\\\\n'))\n",
      "        if m is None:\n",
      "            print( line)\n",
      "        \n",
      "len(res)\n",
      "74/150:\n",
      "testr = '''https://www.rp.pl/Konflikt-izraelsko-palestynski/180529976-USA-Minuta-ciszy-dla-Palestynczykow-w-szkole-Protest-Zydow.html;200;2.348753;0.000330;0.011990;0;0.000;text/html; charset=windows-1250\n",
      "http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.desklight-c3394e44-19a2-45eb-ad63-c71bec715d05;200;0.692087;0.053310;0.086916;0;0.000;text/html;charset=UTF-8\n",
      "https://www.rp.pl/Konflikt-izraelsko-palestynski/305159920-Izrael-zrzuca-wine-na-Hamas.html;200;0.559529;0.000298;0.011882;0;0.000;text/html; charset=windows-1250\n",
      "https://www.tvn24.pl/wiadomosci-ze-swiata,2/konflikt-izraelsko-palestynski-konferencja-w-paryzu,649333.html;200;0.952678;0.000340;0.034502;0;0.000;text/html; charset=UTF-8\n",
      "https://fakty.interia.pl/raporty/raport-konflikt-izraelsko-palestynski/opinie,parametr,fakty_dol,nPack,3;200;0.500981;0.000418;0.038015;0;0.000;text/html; charset=UTF-8\n",
      "http://cejsh.icm.edu.pl/cejsh/element/bwmeta1.element.desklight-c4ec6b2a-e60d-4928-a30f-dc6640296b15;200;0.295232;0.000377;0.034092;0;0.000;text/html;charset=UTF-8'''\n",
      "import re\n",
      "\n",
      "#curl_re = '^(?P<url>.*);(?P<status_code>\\d+);(?P<time_total>[\\d\\.]+);(?P<time_namelookup>[\\d\\.]+);(?P<time_connect>[\\d\\.]+);(?P<size_download>[\\d\\.]+);(?P<speed_download>[\\d\\.]+);(?P<content_type>[^;]+)(?:;?\\s*(?P<charset>(c|C)harset=[^;]+))*;?$'\n",
      "curl_re = '^(?P<url>.*);(?P<status_code>\\d+);(?P<time_total>[\\d\\.]+);(?P<time_namelookup>[\\d\\.]+);(?P<time_connect>[\\d\\.]+);(?P<size_download>[\\d\\.]+);(?P<speed_download>[\\d\\.]+);(?P<content_type>([^;]+)|(text/html.*)|(text/plain.*)|(text/xml.*)|(application/json.*)|(application/rss\\+xml.*)|(\\*/\\*.*)|(image/pdf.*)|(application/pdf.*))$'\n",
      "\n",
      "for x in testr.split('\\n'):\n",
      "    print(re.match(curl_re, x).groupdict())\n",
      "74/151:\n",
      "curl = '../../curl_results.csv'\n",
      "with open(curl, 'r') as f:\n",
      "    for line in f.readlines():\n",
      "        m = re.match(curl_re, line.strip('\\\\n'))\n",
      "        if m is None:\n",
      "            print( line)\n",
      "        \n",
      "len(res)\n",
      "74/152:\n",
      "curl = '../../curl_results.csv'\n",
      "res = []\n",
      "with open(curl, 'r') as f:\n",
      "    for line in f.readlines():\n",
      "        m = re.match(curl_re, line.strip('\\n'))\n",
      "        if m is None:\n",
      "            m = {url=line.strip('\\n')}\n",
      "        res.append(m)\n",
      "        \n",
      "len(res)\n",
      "74/153:\n",
      "curl = '../../curl_results.csv'\n",
      "res = []\n",
      "with open(curl, 'r') as f:\n",
      "    for line in f.readlines():\n",
      "        m = re.match(curl_re, line.strip('\\n'))\n",
      "        if m is None:\n",
      "            m = {'url':line.strip('\\n')}\n",
      "        res.append(m)\n",
      "        \n",
      "len(res)\n",
      "74/154: res = pd.DataFrame(res)\n",
      "74/155:\n",
      "curl = '../../curl_results.csv'\n",
      "res = []\n",
      "with open(curl, 'r') as f:\n",
      "    for line in f.readlines():\n",
      "        m = re.match(curl_re, line.strip('\\n'))\n",
      "        if m is None:\n",
      "            m = {'url':line.strip('\\n')}\n",
      "        else:\n",
      "            m = m.groupdict()\n",
      "        res.append(m)\n",
      "        \n",
      "len(res)\n",
      "74/156: res = pd.DataFrame(res)\n",
      "74/157:\n",
      "curl = '../../curl_results.csv'\n",
      "res = []\n",
      "with open(curl, 'r') as f:\n",
      "    for line in f.readlines():\n",
      "        m = re.match(curl_re, line.strip('\\n'))\n",
      "        if m is None:\n",
      "            m = {'url':line.strip('\\n')}\n",
      "        else:\n",
      "            m = m.groupdict()\n",
      "        res.append(m)\n",
      "        \n",
      "len(res)\n",
      "74/158:\n",
      "res = pd.DataFrame(res)\n",
      "res.head()\n",
      "74/159:\n",
      "curl = '../../curl_results.csv'\n",
      "res = []\n",
      "with open(curl, 'r') as f:\n",
      "    for line in f.readlines():\n",
      "        m = re.match(curl_re, line.strip('\\\\n'))\n",
      "        if m is None:\n",
      "            m = {'url':line.strip('\\\\n')}\n",
      "        else:\n",
      "            m = m.groupdict()\n",
      "        res.append(m)\n",
      "        \n",
      "len(res)\n",
      "74/160:\n",
      "res = pd.DataFrame(res)\n",
      "res.head()\n",
      "74/161:\n",
      "curl = '../../curl_results.csv'\n",
      "res = []\n",
      "with open(curl, 'r') as f:\n",
      "    for line in f:\n",
      "        m = re.match(curl_re, line)\n",
      "        if m is None:\n",
      "            m = {'url':line}\n",
      "        else:\n",
      "            m = m.groupdict()\n",
      "        res.append(m)\n",
      "        \n",
      "len(res)\n",
      "74/162:\n",
      "res = pd.DataFrame(res)\n",
      "res.head()\n",
      "74/163:\n",
      "testr = '''https://www.rp.pl/Konflikt-izraelsko-palestynski/180529976-USA-Minuta-ciszy-dla-Palestynczykow-w-szkole-Protest-Zydow.html;200;2.348753;0.000330;0.011990;0;0.000;text/html; charset=windows-1250\n",
      "http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.desklight-c3394e44-19a2-45eb-ad63-c71bec715d05;200;0.692087;0.053310;0.086916;0;0.000;text/html;charset=UTF-8\n",
      "https://www.rp.pl/Konflikt-izraelsko-palestynski/305159920-Izrael-zrzuca-wine-na-Hamas.html;200;0.559529;0.000298;0.011882;0;0.000;text/html; charset=windows-1250\n",
      "https://www.tvn24.pl/wiadomosci-ze-swiata,2/konflikt-izraelsko-palestynski-konferencja-w-paryzu,649333.html;200;0.952678;0.000340;0.034502;0;0.000;text/html; charset=UTF-8\n",
      "https://fakty.interia.pl/raporty/raport-konflikt-izraelsko-palestynski/opinie,parametr,fakty_dol,nPack,3;200;0.500981;0.000418;0.038015;0;0.000;text/html; charset=UTF-8\n",
      "http://cejsh.icm.edu.pl/cejsh/element/bwmeta1.element.desklight-c4ec6b2a-e60d-4928-a30f-dc6640296b15;200;0.295232;0.000377;0.034092;0;0.000;text/html;charset=UTF-8'''\n",
      "import re\n",
      "\n",
      "#curl_re = '^(?P<url>.*);(?P<status_code>\\d+);(?P<time_total>[\\d\\.]+);(?P<time_namelookup>[\\d\\.]+);(?P<time_connect>[\\d\\.]+);(?P<size_download>[\\d\\.]+);(?P<speed_download>[\\d\\.]+);(?P<content_type>[^;]+)(?:;?\\s*(?P<charset>(c|C)harset=[^;]+))*;?$'\n",
      "curl_re = '^(?P<url>.*);(?P<status_code>\\d+);(?P<time_total>[\\d\\.]+);(?P<time_namelookup>[\\d\\.]+);(?P<time_connect>[\\d\\.]+);(?P<size_download>[\\d\\.]+);(?P<speed_download>[\\d\\.]+);(?P<content_type>(text/html.*)|(text/plain.*)|(text/xml.*)|(application/json.*)|(application/rss\\+xml.*)|(\\*/\\*.*)|(image/pdf.*)|(application/pdf.*)|([^;]+))$'\n",
      "\n",
      "for x in testr.split('\\n'):\n",
      "    print(re.match(curl_re, x).groupdict())\n",
      "74/164:\n",
      "curl = '../../curl_results.csv'\n",
      "res = []\n",
      "with open(curl, 'r') as f:\n",
      "    for line in f:\n",
      "        m = re.match(curl_re, line)\n",
      "        if m is None:\n",
      "            m = {'url':line}\n",
      "        else:\n",
      "            m = m.groupdict()\n",
      "        res.append(m)\n",
      "        \n",
      "len(res)\n",
      "74/165:\n",
      "res = pd.DataFrame(res)\n",
      "res.head()\n",
      "74/166:\n",
      "curl = '../../curl_results.csv'\n",
      "res = []\n",
      "with open(curl, 'r') as f:\n",
      "    for line in f:\n",
      "        print(line)\n",
      "        m = re.match(curl_re, line)\n",
      "        if m is None:\n",
      "            m = {'url':line}\n",
      "        else:\n",
      "            m = m.groupdict()\n",
      "        res.append(m)\n",
      "        \n",
      "len(res)\n",
      "74/167:\n",
      "curl = '../../curl_results.csv'\n",
      "res = []\n",
      "with open(curl, 'r') as f:\n",
      "    for line in f:\n",
      "        line = line.strip('\\n')\n",
      "        m = re.match(curl_re, line)\n",
      "        if m is None:\n",
      "            m = {'url':line}\n",
      "        else:\n",
      "            m = m.groupdict()\n",
      "        res.append(m)\n",
      "        \n",
      "len(res)\n",
      "74/168:\n",
      "res = pd.DataFrame(res)\n",
      "res.head()\n",
      "74/169:\n",
      "testr = '''https://www.rp.pl/Konflikt-izraelsko-palestynski/180529976-USA-Minuta-ciszy-dla-Palestynczykow-w-szkole-Protest-Zydow.html;200;2.348753;0.000330;0.011990;0;0.000;text/html; charset=windows-1250\n",
      "http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.desklight-c3394e44-19a2-45eb-ad63-c71bec715d05;200;0.692087;0.053310;0.086916;0;0.000;text/html;charset=UTF-8\n",
      "https://www.rp.pl/Konflikt-izraelsko-palestynski/305159920-Izrael-zrzuca-wine-na-Hamas.html;200;0.559529;0.000298;0.011882;0;0.000;text/html; charset=windows-1250\n",
      "https://www.tvn24.pl/wiadomosci-ze-swiata,2/konflikt-izraelsko-palestynski-konferencja-w-paryzu,649333.html;200;0.952678;0.000340;0.034502;0;0.000;text/html; charset=UTF-8\n",
      "https://fakty.interia.pl/raporty/raport-konflikt-izraelsko-palestynski/opinie,parametr,fakty_dol,nPack,3;200;0.500981;0.000418;0.038015;0;0.000;text/html; charset=UTF-8\n",
      "http://cejsh.icm.edu.pl/cejsh/element/bwmeta1.element.desklight-c4ec6b2a-e60d-4928-a30f-dc6640296b15;200;0.295232;0.000377;0.034092;0;0.000;text/html;charset=UTF-8'''\n",
      "import re\n",
      "\n",
      "#curl_re = '^(?P<url>.*);(?P<status_code>\\d+);(?P<time_total>[\\d\\.]+);(?P<time_namelookup>[\\d\\.]+);(?P<time_connect>[\\d\\.]+);(?P<size_download>[\\d\\.]+);(?P<speed_download>[\\d\\.]+);(?P<content_type>[^;]+)(?:;?\\s*(?P<charset>(c|C)harset=[^;]+))*;?$'\n",
      "curl_re = '^(?P<url>.*);(?P<status_code>\\d+);(?P<time_total>[\\d\\.]+);(?P<time_namelookup>[\\d\\.]+);(?P<time_connect>[\\d\\.]+);(?P<size_download>[\\d\\.]+);(?P<speed_download>[\\d\\.]+);(?P<content_type>(text/html.*)|(text/plain.*)|(text/xml.*)|(application/json.*)|(application/rss\\+xml.*)|(\\*/\\*.*)|(image/pdf.*)|(application/pdf.*)|([^;]+))?$'\n",
      "\n",
      "for x in testr.split('\\n'):\n",
      "    print(re.match(curl_re, x).groupdict())\n",
      "74/170:\n",
      "curl = '../../curl_results.csv'\n",
      "res = []\n",
      "with open(curl, 'r') as f:\n",
      "    for line in f:\n",
      "        line = line.strip('\\n')\n",
      "        m = re.match(curl_re, line)\n",
      "        if m is None:\n",
      "            m = {'url':line}\n",
      "        else:\n",
      "            m = m.groupdict()\n",
      "        res.append(m)\n",
      "        \n",
      "len(res)\n",
      "74/171:\n",
      "res = pd.DataFrame(res)\n",
      "res.head()\n",
      "74/172: res.status_code.value_counts()\n",
      "74/173: res[res.status_code=='000']\n",
      "75/1: 1\n",
      "75/2: 1\n",
      "75/3: 1\n",
      "75/4: len(all_resp)\n",
      "74/174: os.getpid()\n",
      "75/5: import os; os.getpid()\n",
      "74/175:\n",
      "bad_urls = res[(res.status_code=='000') & (res.time_total==0)].shape[0]\n",
      "no_dns = res[(res.status_code=='000') & (res.time_total!=0) & (res.time_namelookup==0)].shape[0]\n",
      "bad_server = res[(res.status_code=='000') & (res.time_total!=0) & (res.time_namelookup!=0) & (res.time_connect==0)].shape[0]\n",
      "not_found = res[res.status_code=='404'].shape[0]\n",
      "print(f'404={not_found}, bad urls={bad_urls}, no_dns={no_dns}, bad_server={bad_server}')\n",
      "74/176:\n",
      "def to_numeric(df):\n",
      "    '''\n",
      "    Turn timedelta columns into numeric dtype\n",
      "    '''\n",
      "    cols = [\"size_download\", \"speed_download\", \"status_code\", \"time_connect\", \"time_namelookup\", \"time_total\"]\n",
      "    numeric = df[cols].apply(pd.to_numeric)\n",
      "    df = df.copy()\n",
      "    df[cols] = numeric\n",
      "    return df\n",
      "74/177:\n",
      "def to_numeric(df):\n",
      "    '''\n",
      "    Turn timedelta columns into numeric dtype\n",
      "    '''\n",
      "    cols = [\"size_download\", \"speed_download\", \"status_code\", \"time_connect\", \"time_namelookup\", \"time_total\"]\n",
      "    numeric = df[cols].apply(pd.to_numeric)\n",
      "    df = df.copy()\n",
      "    df[cols] = numeric\n",
      "    return df\n",
      "\n",
      "res = res.pipe(to_numeric)\n",
      "74/178:\n",
      "bad_urls = res[(res.status_code=='000') & (res.time_total==0)].shape[0]\n",
      "no_dns = res[(res.status_code=='000') & (res.time_total!=0) & (res.time_namelookup==0)].shape[0]\n",
      "bad_server = res[(res.status_code=='000') & (res.time_total!=0) & (res.time_namelookup!=0) & (res.time_connect==0)].shape[0]\n",
      "not_found = res[res.status_code=='404'].shape[0]\n",
      "print(f'404={not_found}, bad urls={bad_urls}, no_dns={no_dns}, bad_server={bad_server}')\n",
      "74/179:\n",
      "def to_numeric(df):\n",
      "    '''\n",
      "    Turn timedelta columns into numeric dtype\n",
      "    '''\n",
      "    cols = [\"size_download\", \"speed_download\", \"status_code\", \"time_connect\", \"time_namelookup\", \"time_total\"]\n",
      "    numeric = df[cols].apply(pd.to_numeric)\n",
      "    df = df.copy()\n",
      "    df[cols] = numeric\n",
      "    return df\n",
      "\n",
      "res = res.pipe(to_numeric)\n",
      "74/180:\n",
      "bad_urls = res[(res.status_code=='000') & (res.time_total==0)].shape[0]\n",
      "no_dns = res[(res.status_code=='000') & (res.time_total!=0) & (res.time_namelookup==0)].shape[0]\n",
      "bad_server = res[(res.status_code=='000') & (res.time_total!=0) & (res.time_namelookup!=0) & (res.time_connect==0)].shape[0]\n",
      "not_found = res[res.status_code=='404'].shape[0]\n",
      "print(f'404={not_found}, bad urls={bad_urls}, no_dns={no_dns}, bad_server={bad_server}')\n",
      "74/181: res.head()\n",
      "74/182: res.dtypes()\n",
      "74/183: res.dtypes\n",
      "74/184:\n",
      "bad_urls = res[(res.status_code==0) & (res.time_total==0)].shape[0]\n",
      "no_dns = res[(res.status_code==0) & (res.time_total!=0) & (res.time_namelookup==0)].shape[0]\n",
      "bad_server = res[(res.status_code==0) & (res.time_total!=0) & (res.time_namelookup!=0) & (res.time_connect==0)].shape[0]\n",
      "not_found = res[res.status_code=='404'].shape[0]\n",
      "print(f'404={not_found}, bad urls={bad_urls}, no_dns={no_dns}, bad_server={bad_server}')\n",
      "74/185:\n",
      "bad_urls = res[(res.status_code==0) & (res.time_total==0)].shape[0]\n",
      "no_dns = res[(res.status_code==0) & (res.time_total!=0) & (res.time_namelookup==0)].shape[0]\n",
      "bad_server = res[(res.status_code==0) & (res.time_total!=0) & (res.time_namelookup!=0) & (res.time_connect==0)].shape[0]\n",
      "not_found = res[res.status_code==404].shape[0]\n",
      "print(f'404={not_found}, bad urls={bad_urls}, no_dns={no_dns}, bad_server={bad_server}')\n",
      "74/186:\n",
      "import http\n",
      "http.HTTPStatus\n",
      "74/187:\n",
      "import http\n",
      "dict(http.HTTPStatus)\n",
      "74/188:\n",
      "import http\n",
      "list(http.HTTPStatus)\n",
      "74/189:\n",
      "import http\n",
      "http.HTTPStatus.200\n",
      "74/190:\n",
      "import http\n",
      "http.HTTPStatus.OK\n",
      "74/191:\n",
      "import http\n",
      "http.HTTPStatus==200\n",
      "74/192:\n",
      "import http\n",
      "http.HTTPStatus(2)\n",
      "74/193:\n",
      "import http\n",
      "http.HTTPStatus(200)\n",
      "74/194: res.status_code.value_counts().to_frame().apply(lambda x: http.HTTPStatus(x))\n",
      "74/195: res.status_code.value_counts().to_frame()\n",
      "74/196: res.status_code.value_counts().to_frame().applymap(lambda x: http.HTTPStatus(x))\n",
      "74/197: res.status_code.value_counts().to_frame()\n",
      "74/198: res.status_code.value_counts().reset_index().apply(lambda x: http.HTTPStatus(x))\n",
      "74/199: res.status_code.value_counts().reset_index().apply(lambda x: http.HTTPStatus(x.status_code))\n",
      "74/200: res.status_code.value_counts().reset_index()\n",
      "74/201: res.status_code.value_counts().reset_index().apply(lambda x: http.HTTPStatus(x.status_code), axis=1)\n",
      "74/202: res.status_code.value_counts().reset_index()\n",
      "74/203: res.status_code.value_counts().reset_index().apply(lambda x: http.HTTPStatus(x.index), axis=1)\n",
      "74/204: res.status_code.value_counts().reset_index().apply(lambda x: http.HTTPStatus(x['index']), axis=1)\n",
      "74/205: res.status_code.value_counts().reset_index().apply(lambda x: str(int(x['index'])) + ' ' +http.HTTPStatus(x['index']).phrase + ': ' +http.HTTPStatus(x['index']).description if x['index']!=0 else 'CURL_ERROR', axis=1 )\n",
      "74/206: res.status_code.value_counts().reset_index().apply(lambda x: str(int(x['index'])) + ' ' +http.HTTPStatus(int(x['index'])).phrase + ': ' +http.HTTPStatus(int(x['index'])).description if x['index']!=0 else 'CURL_ERROR', axis=1 )\n",
      "74/207:\n",
      "def get_status_desc(code):\n",
      "    code = int(code)\n",
      "    try:\n",
      "        status = http.HTTPStatus(code)\n",
      "        msg = str(code) + ' ' + status.phrase + ': ' + status.description\n",
      "    except ValueError:\n",
      "        msg = str(code) + ' STATUS_PHRASE_NOT_FOUND'\n",
      "    return msg\n",
      "res.status_code.value_counts().reset_index().apply(get_status_desc, axis=1 )\n",
      "74/208:\n",
      "def get_status_desc(s):\n",
      "    code = int(s['index'])\n",
      "    try:\n",
      "        status = http.HTTPStatus(code)\n",
      "        msg = str(code) + ' ' + status.phrase + ': ' + status.description\n",
      "    except ValueError:\n",
      "        msg = str(code) + ' STATUS_PHRASE_NOT_FOUND'\n",
      "    return msg\n",
      "res.status_code.value_counts().reset_index().apply(get_status_desc, axis=1 )\n",
      "74/209:\n",
      "from functools import lru_cache\n",
      "\n",
      "@lru_cache()\n",
      "def get_status_desc(s):\n",
      "    code = int(s['index'])\n",
      "    try:\n",
      "        status = http.HTTPStatus(code)\n",
      "        msg = str(code) + ' ' + status.phrase + ': ' + status.description\n",
      "    except ValueError:\n",
      "        msg = str(code) + ' STATUS_PHRASE_NOT_FOUND'\n",
      "    return msg\n",
      "\n",
      "res['status_phrase'] = res.status_code.apply(get_status_desc, axis=1 )\n",
      "74/210:\n",
      "from functools import lru_cache\n",
      "\n",
      "@lru_cache()\n",
      "def get_status_desc(s):\n",
      "    code = int(s['index'])\n",
      "    try:\n",
      "        status = http.HTTPStatus(code)\n",
      "        msg = str(code) + ' ' + status.phrase + ': ' + status.description\n",
      "    except ValueError:\n",
      "        msg = str(code) + ' STATUS_PHRASE_NOT_FOUND'\n",
      "    return msg\n",
      "\n",
      "res['status_phrase'] = res.apply(get_status_desc, axis=1 )\n",
      "74/211:\n",
      "from functools import lru_cache\n",
      "\n",
      "@lru_cache()\n",
      "def get_status_desc(code):\n",
      "    code = int(code)\n",
      "    try:\n",
      "        status = http.HTTPStatus(code)\n",
      "        msg = str(code) + ' ' + status.phrase + ': ' + status.description\n",
      "    except ValueError:\n",
      "        msg = str(code) + ' STATUS_PHRASE_NOT_FOUND'\n",
      "    return msg\n",
      "\n",
      "res['status_phrase'] = res.status_code.apply(get_status_desc, axis=1 )\n",
      "74/212:\n",
      "from functools import lru_cache\n",
      "\n",
      "@lru_cache()\n",
      "def get_status_desc(code):\n",
      "    code = int(code)\n",
      "    try:\n",
      "        status = http.HTTPStatus(code)\n",
      "        msg = str(code) + ' ' + status.phrase + ': ' + status.description\n",
      "    except ValueError:\n",
      "        msg = str(code) + ' STATUS_PHRASE_NOT_FOUND'\n",
      "    return msg\n",
      "\n",
      "res['status_phrase'] = res.status_code.apply(get_status_desc)\n",
      "74/213: res[res.status_code.isna()]\n",
      "74/214:\n",
      "from functools import lru_cache\n",
      "\n",
      "@lru_cache()\n",
      "def get_status_desc(code):\n",
      "    code = int(code)\n",
      "    try:\n",
      "        status = http.HTTPStatus(code)\n",
      "        msg = str(code) + ' ' + status.phrase + ': ' + status.description\n",
      "    except ValueError:\n",
      "        msg = str(code) + ' STATUS_PHRASE_NOT_FOUND'\n",
      "    return msg\n",
      "\n",
      "res['status_phrase'] = res.status_code.fillna(0.apply(get_status_desc)\n",
      "74/215:\n",
      "from functools import lru_cache\n",
      "\n",
      "@lru_cache()\n",
      "def get_status_desc(code):\n",
      "    code = int(code)\n",
      "    try:\n",
      "        status = http.HTTPStatus(code)\n",
      "        msg = str(code) + ' ' + status.phrase + ': ' + status.description\n",
      "    except ValueError:\n",
      "        msg = str(code) + ' STATUS_PHRASE_NOT_FOUND'\n",
      "    return msg\n",
      "\n",
      "res['status_phrase'] = res.status_code.fillna(0).apply(get_status_desc)\n",
      "74/216: res[res.status_code.isna()]\n",
      "74/217: res.head\n",
      "74/218:\n",
      "from functools import lru_cache\n",
      "\n",
      "@lru_cache()\n",
      "def get_status_desc(code):\n",
      "    code = int(code)\n",
      "    try:\n",
      "        status = http.HTTPStatus(code)\n",
      "        msg = str(code) + ' ' + status.phrase + ': ' + status.description\n",
      "    except ValueError:\n",
      "        msg = str(code) + ' STATUS_PHRASE_NOT_FOUND' if code!=0 else: str(code) + ' CURL_ERROR'\n",
      "    return msg\n",
      "\n",
      "res['status_phrase'] = res.status_code.fillna(0).apply(get_status_desc)\n",
      "74/219:\n",
      "from functools import lru_cache\n",
      "\n",
      "@lru_cache()\n",
      "def get_status_desc(code):\n",
      "    code = int(code)\n",
      "    try:\n",
      "        status = http.HTTPStatus(code)\n",
      "        msg = str(code) + ' ' + status.phrase + ': ' + status.description\n",
      "    except ValueError:\n",
      "        msg = str(code) + ' STATUS_PHRASE_NOT_FOUND' if code!=0 else str(code) + ' CURL_ERROR'\n",
      "    return msg\n",
      "\n",
      "res['status_phrase'] = res.status_code.fillna(0).apply(get_status_desc)\n",
      "74/220: res.head\n",
      "74/221: res.head()\n",
      "74/222: res[res.status_code=='000']\n",
      "74/223: res[res.status_code==0]\n",
      "74/224: res.status_phrase.value_counts()\n",
      "74/225: res.status_phrase.value_counts().to_frame()\n",
      "74/226:\n",
      "bad_urls = res[(res.status_code==0) & (res.time_total==0)].shape[0]\n",
      "no_dns = res[(res.status_code==0) & (res.time_total!=0) & (res.time_namelookup==0)].shape[0]\n",
      "bad_server = res[(res.status_code==0) & (res.time_total!=0) & (res.time_namelookup!=0) & (res.time_connect==0)].shape[0]\n",
      "not_found = res[res.status_code==404].shape[0]\n",
      "print(f'bad urls={bad_urls}, no_dns={no_dns}, bad_server={bad_server}')\n",
      "74/227:\n",
      "bad_urls = res[(res.status_code==0) & (res.time_total==0)].shape[0]\n",
      "no_dns = res[(res.status_code==0) & (res.time_total!=0) & (res.time_namelookup==0)].shape[0]\n",
      "bad_server = res[(res.status_code==0) & (res.time_total!=0) & (res.time_namelookup!=0) & (res.time_connect==0)].shape[0]\n",
      "other = res[(res.status_code==0) & (res.time_total!=0) & (res.time_namelookup!=0) & (res.time_connect!=0)].shape[0]\n",
      "not_found = res[res.status_code==404].shape[0]\n",
      "print(f'bad urls={bad_urls}, no_dns={no_dns}, bad_server={bad_server}')\n",
      "74/228:\n",
      "bad_urls = res[(res.status_code==0) & (res.time_total==0)].shape[0]\n",
      "no_dns = res[(res.status_code==0) & (res.time_total!=0) & (res.time_namelookup==0)].shape[0]\n",
      "bad_server = res[(res.status_code==0) & (res.time_total!=0) & (res.time_namelookup!=0) & (res.time_connect==0)].shape[0]\n",
      "other = res[(res.status_code==0) & (res.time_total!=0) & (res.time_namelookup!=0) & (res.time_connect!=0)].shape[0]\n",
      "not_found = res[res.status_code==404].shape[0]\n",
      "print(f'bad urls={bad_urls}, no_dns={no_dns}, bad_server={bad_server}, other={other}')\n",
      "74/229: lut[(source=='wikipedia') & (~lut.uri_cln_unshrtn_wb_avail)].head()\n",
      "74/230: lut[(lut.source=='wikipedia') & (~lut.uri_cln_unshrtn_wb_avail)].head()\n",
      "74/231: lut[(lut.source=='wikipedia') & (~lut.uri_cln_unshrtn_wb_avail)].tld.value_counts()\n",
      "74/232: lut[(lut.source=='wikipedia') & (~lut.uri_cln_unshrtn_wb_avail)].uri_cln_unshrtn.nunique()\n",
      "74/233:\n",
      "unarch_wiki = lut[(lut.source=='wikipedia') & (~lut.uri_cln_unshrtn_wb_avail)]\n",
      "unarch_wiki.uri_cln_unshrtn.nunique()\n",
      "74/234: unarch_wiki.groupby('tld').uri.nunique()\n",
      "74/235: unarch_wiki.groupby('tld').uri.nunique().sort_values(ascending=False)\n",
      "74/236: unarch_wiki.groupby('tld').uri.nunique().sort_values(ascending=False)\n",
      "74/237: unarch_wiki.groupby('tld').uri.nunique().sort_values(ascending=False) / 788\n",
      "74/238: unarch_wiki.groupby('tld').uri.nunique().sort_values(ascending=False) / 788.head(10)\n",
      "74/239: (unarch_wiki.groupby('tld').uri.nunique().sort_values(ascending=False) / 788).head(10)\n",
      "74/240: unarch_wiki.shape[)]\n",
      "74/241: unarch_wiki.shape[0]\n",
      "74/242: (unarch_wiki.groupby('tld_unshrtn').uri.nunique().sort_values(ascending=False) / 788).head(10)\n",
      "74/243:\n",
      "unarch = lut[(~lut.uri_cln_unshrtn_wb_avail)]\n",
      "\n",
      "def get_top_tld(df):\n",
      "    return df.tld_unshrtn.value_counts().head(10)\n",
      "unarch.groupby('source').apply(get_top_tld)\n",
      "74/244:\n",
      "unarch = lut[(~lut.uri_cln_unshrtn_wb_avail)]\n",
      "\n",
      "def get_top_tld(df):\n",
      "    return df.tld_unshrtn.value_counts().head(10)\n",
      "unarch.groupby('source').apply(get_top_tld).to_frame()\n",
      "74/245: (unarch_wiki.groupby('tld_unshrtn').uri_cln_unshrtn.nunique().sort_values(ascending=False) / 788).head(10)\n",
      "74/246:\n",
      "unarch = lut[(~lut.uri_cln_unshrtn_wb_avail)]\n",
      "\n",
      "def get_top_tld(df):\n",
      "    return df.groupby('tld_unshrtn').uri_cln_unshrtn.nunique().sort_values(ascending=False).head(10)\n",
      "unarch.groupby('source').apply(get_top_tld).to_frame()\n",
      "74/247:\n",
      "unarch = lut[(~lut.uri_cln_unshrtn_wb_avail)]\n",
      "\n",
      "def get_top_tld(df):\n",
      "    return df.groupby('tld_unshrtn').uri_cln_unshrtn.nunique().sort_values(ascending=False).head(10)\n",
      "unarch.groupby('source').apply(get_top_tld).to_frame().reset_index()\n",
      "74/248:\n",
      "unarch = lut[(~lut.uri_cln_unshrtn_wb_avail)]\n",
      "\n",
      "def get_top_tld(df):\n",
      "    return df.groupby('tld_unshrtn').uri_cln_unshrtn.nunique().sort_values(ascending=False).head(10)\n",
      "unarch.groupby('source').apply(get_top_tld).to_frame()\n",
      "74/249: lut.groupby('lang').apply(get_top_tld).to_frame()\n",
      "74/250: lut[lut.lang=='en,sco'] = 'en'\n",
      "74/251:\n",
      "big_langs = lut[lut.lang.isin(['he', 'ar', 'en', 'de', 'ru', 'fr'])]\n",
      "big_langs.groupby('lang')lut.apply(get_top_tld).to_frame()\n",
      "74/252:\n",
      "big_langs = lut[lut.lang.isin(['he', 'ar', 'en', 'de', 'ru', 'fr'])]\n",
      "big_langs.groupby('lang').apply(get_top_tld).to_frame()\n",
      "74/253:\n",
      "unarch = lut[(~lut.uri_cln_unshrtn_wb_avail)]\n",
      "\n",
      "def get_top_tld(df):\n",
      "    num_uniq_uri = df.uri_cln_unshrtn.nunique()\n",
      "    return df.groupby('tld_unshrtn').uri_cln_unshrtn.nunique().sort_values(ascending=False).head(10)/num_uniq_uri\n",
      "unarch.groupby('source').apply(get_top_tld).to_frame()\n",
      "74/254: lut = pd.read_pickle('lang_uri_time_wb_avail_all.pkl.gz', compression='gzip')\n",
      "74/255:\n",
      "unarch = lut[(~lut.uri_cln_unshrtn_wb_avail)]\n",
      "\n",
      "def get_top_tld(df):\n",
      "    num_uniq_uri = df.uri_cln_unshrtn.nunique()\n",
      "    return df.groupby('tld_unshrtn').uri_cln_unshrtn.nunique().sort_values(ascending=False).head(10)/num_uniq_uri\n",
      "unarch.groupby('source').apply(get_top_tld).to_frame()\n",
      "74/256:\n",
      "unarch = lut[(~lut.uri_cln_unshrtn_wb_avail)]\n",
      "\n",
      "def get_top_tld(df):\n",
      "    num_uniq_uri = df.uri_cln_unshrtn.nunique()\n",
      "    return (df.groupby('tld_unshrtn').uri_cln_unshrtn.nunique().sort_values(ascending=False).head(10)/num_uniq_uri).round(2)\n",
      "unarch.groupby('source').apply(get_top_tld).to_frame()\n",
      "74/257:\n",
      "big_langs = lut[lut.lang.isin(['he', 'ar', 'en', 'de', 'ru', 'fr'])]\n",
      "big_langs.groupby('lang').apply(get_top_tld).to_frame()\n",
      "73/17: memento_avail_cd_m1.head()\n",
      "73/18: memento_avail_cd_m1.head()\n",
      "73/19: memento_avail_cd_m1.head()\n",
      "73/20: memento_avail_cd_m1.head()\n",
      "73/21: memento_avail_cd_m1.head()\n",
      "73/22: memento_avail_cd_m1.head()\n",
      "73/23: 1\n",
      "73/24: memento_avail_cd_m1.head()\n",
      "73/25: memento_avail_cd_m1.head()\n",
      "73/26: memento_avail_cd_m1.head()\n",
      "73/27: memento_avail_cd_m1.head()\n",
      "73/28: memento_avail_cd_m1.to_pickle('data/memento_avail_cd_missing1_df.pkl.gz', compression='gzip')\n",
      "73/29: memento_avail_cd_m1.isna().sum()\n",
      "73/30:\n",
      "macd_m1 = []\n",
      "for x in memento_avail_cd_m1.tolist():\n",
      "    if x is None:\n",
      "        tup = (None,)*6 \n",
      "    elif 'text' in x:\n",
      "        tup = (x['text'], None, None, None, None, None) \n",
      "    else:\n",
      "        tup = (x['original_uri'], x['mementos']['first']['uri'], \n",
      "              x['mementos']['first']['datetime'], x['mementos']['last']['uri'], \n",
      "              x['mementos']['last']['datetime'], len(x['mementos']['list'])) \n",
      "    macd_m1.append(tup)      \n",
      "            \n",
      "macd_m1 = pd.DataFrame(macd_m1, columns=['original_uri', 'first_snap', 'first_time', \n",
      "                                         'last_snap', 'last_time', 'num_snaps'])\n",
      "macd_m1 = pd.concat([missing_uris1.reset_index().drop('index', axis=1), macd], axis=1)\n",
      "73/31: macd_m1.head()\n",
      "73/32:\n",
      "macd_m1 = []\n",
      "for x in memento_avail_cd_m1.tolist():\n",
      "    if x is None:\n",
      "        tup = (None,)*6 \n",
      "    elif 'text' in x:\n",
      "        tup = (x['text'], None, None, None, None, None) \n",
      "    else:\n",
      "        tup = (x['original_uri'], x['mementos']['first']['uri'], \n",
      "              x['mementos']['first']['datetime'], x['mementos']['last']['uri'], \n",
      "              x['mementos']['last']['datetime'], len(x['mementos']['list'])) \n",
      "    macd_m1.append(tup)      \n",
      "            \n",
      "macd_m1 = pd.DataFrame(macd_m1, columns=['original_uri', 'first_snap', 'first_time', \n",
      "                                         'last_snap', 'last_time', 'num_snaps'])\n",
      "macd_m1 = pd.concat([missing_uris1.reset_index().drop('index', axis=1), macd], axis=1)\n",
      "73/33: macd_m1.head()\n",
      "73/34: missing_uris1.head()\n",
      "73/35:\n",
      "macd_m1 = []\n",
      "for x in memento_avail_cd_m1.tolist():\n",
      "    if x is None:\n",
      "        tup = (None,)*6 \n",
      "    elif 'text' in x:\n",
      "        tup = (x['text'], None, None, None, None, None) \n",
      "    else:\n",
      "        tup = (x['original_uri'], x['mementos']['first']['uri'], \n",
      "              x['mementos']['first']['datetime'], x['mementos']['last']['uri'], \n",
      "              x['mementos']['last']['datetime'], len(x['mementos']['list'])) \n",
      "    macd_m1.append(tup)      \n",
      "            \n",
      "macd_m1 = pd.DataFrame(macd_m1, columns=['original_uri', 'first_snap', 'first_time', \n",
      "                                         'last_snap', 'last_time', 'num_snaps'])\n",
      "macd_m1 = pd.concat([missing_uris1.reset_index().drop('index', axis=1), macd_m1], axis=1)\n",
      "73/36: missing_uris1.head()\n",
      "73/37: macd_m1.head()\n",
      "73/38: macd_m1.to_pickle('data/memento_avail_cd_m1_df.pkl.gz', compression='gzip')\n",
      "73/39: macd.head()\n",
      "73/40: macd_all = macd_m1.combine_first(macd)\n",
      "73/41: macd_all.original_uri.value_counts()\n",
      "73/42: macd_all[macd_all.original_uri!='404 page not found\\n']\n",
      "73/43: macd_all[macd_all.original_uri!='404 page not found\\n'].shape[0]\n",
      "73/44: macd_all[macd_all.original_uri!='404 page not found\\n'].shape[0] / macd_all.shape[0]\n",
      "73/45: macd_all.to_pickle('data/macd_all.pkl.gz', compression='gzip')\n",
      "74/258: macd_all = pd.read_pickle('data/macd_all.pkl.gz', compression='gzip')\n",
      "74/259:\n",
      "macd_all = pd.read_pickle('data/macd_all.pkl.gz', compression='gzip')\n",
      "macd_all.head()\n",
      "74/260:\n",
      "lut['uri_mem_avail'] = lut.uri.map(macd_all.set_index('uri').first_snap.isna()).fillna(False)\n",
      "lut['uri_unsh_no_clean_mem_avail'] = lut.uri_unsh_no_clean.map(macd_all.set_index('uri').first_snap.isna()).fillna(False)\n",
      "74/261:\n",
      "lut['uri_mem_avail'] = lut.uri.map(macd_all.drop_duplicates(subset='uri').set_index('uri').first_snap.isna()).fillna(False)\n",
      "lut['uri_unsh_no_clean_mem_avail'] = lut.uri_unsh_no_clean.map(macd_all.drop_duplicates(subset='uri').set_index('uri').first_snap.isna()).fillna(False)\n",
      "74/262: lut['uri_unsh_no_clean'] = lut.uri_unshrtn_raw.combine_first(lut.uri_unshrtn_bitly.combine_first(lut.uri_unshrtn.combine_first(lut.uri)))\n",
      "74/263:\n",
      "lut['uri_mem_avail'] = lut.uri.map(macd_all.drop_duplicates(subset='uri').set_index('uri').first_snap.isna()).fillna(False)\n",
      "lut['uri_unsh_no_clean_mem_avail'] = lut.uri_unsh_no_clean.map(macd_all.drop_duplicates(subset='uri').set_index('uri').first_snap.isna()).fillna(False)\n",
      "74/264:\n",
      "lut['uri_mem_avail'] = lut.uri.map(macd_all.drop_duplicates(subset='uri').set_index('uri').first_snap==None).fillna(False)\n",
      "lut['uri_unsh_no_clean_mem_avail'] = lut.uri_unsh_no_clean.map(macd_all.drop_duplicates(subset='uri').set_index('uri').first_snap==None).fillna(False)\n",
      "74/265: lut.dtypes\n",
      "74/266: lut.uri_unsh_no_clean_mem_avail.sum()/lut.shape[0]\n",
      "74/267: lut.uri_unsh_no_clean_mem_avail\n",
      "74/268:\n",
      "lut['uri_mem_avail'] = lut.uri.map(macd_all.drop_duplicates(subset='uri').set_index('uri').first_snap.isna()==False).fillna(False)\n",
      "lut['uri_unsh_no_clean_mem_avail'] = lut.uri_unsh_no_clean.map(macd_all.drop_duplicates(subset='uri').set_index('uri').first_snap.isna()==False).fillna(False)\n",
      "74/269: lut.dtypes\n",
      "74/270: lut.uri_unsh_no_clean_mem_avail.sum()/lut.shape[0]\n",
      "74/271: lut.uri_cln_unshrtn_wb_avail.sum()/lut.shape[0]\n",
      "74/272: lut.uri_unsh_no_clean_mem_avail.sum()/lut.shape[0]\n",
      "74/273: lut.uri_mem_avail.sum()/lut.shape[0]\n",
      "74/274: lut['wb_avail'] = (lut.uri_unsh_no_clean_mem_avail) | (lut.uri_mem_avail)\n",
      "74/275: lut[lut.wb_avail].shape[0]/lut.shape[0]\n",
      "74/276: lut['mem_avail'] = (lut.uri_unsh_no_clean_mem_avail) | (lut.uri_mem_avail)\n",
      "74/277: lut[lut.mem_avail].shape[0]/lut.shape[0]\n",
      "74/278: lut[(lut.uri_mem_avail) & (~lut.uri_unsh_no_clean_mem_avail)].shape[0]/lut.shape[0]\n",
      "74/279: lut[(lut.uri_cln_wb_avail) & (~lut.uri_unsh_no_clean_mem_avail)].shape[0]/lut.shape[0]\n",
      "74/280: lut[(lut.uri_mem_avail) & (~lut.uri_unsh_no_clean_mem_avail)].shape[0]/lut.shape[0]\n",
      "74/281: lut[(~lut.uri_mem_avail) & (lut.uri_unsh_no_clean_mem_avail)].shape[0]/lut.shape[0]\n",
      "74/282: lut[lut.mem_avail].uri_unsh_no_clean_mem_avail.nunique() / unique_uris_raw_only_unsh_no_single.shape[0]\n",
      "74/283: lut[lut.mem_avail].uri_unsh_no_clean_mem_avail.nunique()\n",
      "74/284: lut['mem_avail'] = (lut.uri_unsh_no_clean_mem_avail) | (lut.uri_mem_avail)\n",
      "74/285: lut[lut.mem_avail].uri_unsh_no_clean_mem_avail.nunique()\n",
      "74/286: lut[lut.mem_avail].shape[0]/lut.shape[0]\n",
      "74/287: unique_uris_raw_only_unsh_no_single.shape[0]\n",
      "74/288: lut[lut.mem_avail].uri_unsh_no_clean.nunique() / unique_uris_raw_only_unsh_no_single.shape[0]\n",
      "74/289: lut[lut.uri_unsh_no_clean_mem_avail].uri_unsh_no_clean.nunique()/lut.uri_unsh_no_clean.nunique()\n",
      "74/290: lut.groupby(['source', 'uri_unsh_no_clean_mem_avail']).uri_unsh_no_clean.nunique().unstack().div(lut.groupby('source').uri_unsh_no_clean.nunique(), axis=0).sort_values(True, ascending=False)\n",
      "74/291: lut.groupby(['lang', 'uri_unsh_no_clean_mem_avail']).uri_unsh_no_clean.nunique().unstack().div(lut.groupby('lang').uri_unsh_no_clean.nunique(), axis=0).sort_values(True, ascending=False).drop(False, axis=1).round(2)\n",
      "74/292:\n",
      "lut['uri_mem_avail'] = lut.uri.map(~macd_all.drop_duplicates(subset='uri').set_index('uri').first_snap.isna()).fillna(False)\n",
      "lut['uri_unsh_no_clean_mem_avail'] = lut.uri_unsh_no_clean.map(~macd_all.drop_duplicates(subset='uri').set_index('uri').first_snap.isna()).fillna(False)\n",
      "74/293: lut[lut.mem_avail].uri_unsh_no_clean.nunique() / unique_uris_raw_only_unsh_no_single.shape[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/294: lut[lut.uri_unsh_no_clean_mem_avail].uri_unsh_no_clean.nunique()/lut.uri_unsh_no_clean.nunique()\n",
      "74/295: lut.groupby(['source', 'uri_unsh_no_clean_mem_avail']).uri_unsh_no_clean.nunique().unstack().div(lut.groupby('source').uri_unsh_no_clean.nunique(), axis=0).sort_values(True, ascending=False)\n",
      "74/296: ~macd_all.drop_duplicates(subset='uri').set_index('uri').first_snap.isna()\n",
      "74/297: lut.uri_unsh_no_clean\n",
      "74/298: lut[lut.uri_unsh_no_clean=='http://www.jpost.com/printarticle.aspx?id=362859']\n",
      "74/299:\n",
      "unarch_wiki = lut[(lut.source=='wikipedia') & (~lut.uri_unsh_no_clean_mem_avail)]\n",
      "unarch_wiki.uri_unsh_no_clean.nunique()\n",
      "74/300: unarch_wiki.shape[0]\n",
      "74/301: (unarch_wiki.groupby('tld_unshrtn').uri_cln_unshrtn.nunique().sort_values(ascending=False) / 788).head(10)\n",
      "74/302: lut[lut.tld=='newsru.co.il']\n",
      "74/303: macd_all[macd_all.uri=='http://newsru.co.il/mideast/23aug2014/47day_103.html']\n",
      "74/304: macd_all[macd_all.uri=='http://newsru.co.il/mideast/21aug2014/likv_130.html']\n",
      "74/305: macd_all[macd_all.original_uri=='http://newsru.co.il/mideast/21aug2014/likv_130.html']\n",
      "74/306: macd_all[macd_all.uri=='http://newsru.co.il/mideast/21aug2014/likv_130.html']\n",
      "74/307: macd_all.shape\n",
      "74/308: macd_all.uri.isna.sum()\n",
      "74/309: macd_all.uri.isna().sum()\n",
      "74/310: macd_all[macd_all.uri.str.contains['newsru.co.il']]\n",
      "74/311: macd_all[macd_all.uri.str.contains('newsru.co.il')]\n",
      "74/312: macd_all[macd_all.uri=='http://newsru.co.il/mideast/26aug2014/gaz_601.html']\n",
      "74/313: macd_all.uri.nunique()\n",
      "73/46: macd_all = macd_m1.set_index('uri').combine_first(macd.set_index('uri'))\n",
      "73/47: macd_all.to_pickle('data/macd_all.pkl.gz', compression='gzip')\n",
      "73/48: macd_all = macd_m1.set_index('uri').combine_first(macd.set_index('uri')).reset_index()\n",
      "73/49: macd_all.to_pickle('data/macd_all.pkl.gz', compression='gzip')\n",
      "73/50: macd_all.original_uri.value_counts()\n",
      "73/51: macd_all[macd_all.original_uri!='404 page not found\\n'].shape[0] / macd_all.shape[0]\n",
      "73/52: macd_all.uri.value_counts()\n",
      "73/53: macd_all.original_uri.value_counts()\n",
      "73/54: macd_all[macd_all.original_uri=='http://www.elmogaz.com/node/169244']\n",
      "73/55: macd_all[macd_all.original_uri!='404 page not found\\n'].shape[0] / macd_all.shape[0]\n",
      "74/314:\n",
      "macd_all = pd.read_pickle('data/macd_all.pkl.gz', compression='gzip')\n",
      "macd_all.head()\n",
      "74/315: lut['uri_unsh_no_clean'] = lut.uri_unshrtn_raw.combine_first(lut.uri_unshrtn_bitly.combine_first(lut.uri_unshrtn.combine_first(lut.uri)))\n",
      "74/316:\n",
      "lut['uri_mem_avail'] = lut.uri.map(~macd_all.drop_duplicates(subset='uri').set_index('uri').first_snap.isna()).fillna(False)\n",
      "lut['uri_unsh_no_clean_mem_avail'] = lut.uri_unsh_no_clean.map(~macd_all.drop_duplicates(subset='uri').set_index('uri').first_snap.isna()).fillna(False)\n",
      "74/317: lut.dtypes\n",
      "74/318: lut.uri_unsh_no_clean_mem_avail.sum()/lut.shape[0]\n",
      "74/319: lut.uri_mem_avail.sum()/lut.shape[0]\n",
      "74/320: lut['mem_avail'] = (lut.uri_unsh_no_clean_mem_avail) | (lut.uri_mem_avail)\n",
      "74/321: lut[lut.mem_avail].shape[0]/lut.shape[0]\n",
      "74/322: lut[(lut.uri_mem_avail) & (~lut.uri_unsh_no_clean_mem_avail)].shape[0]/lut.shape[0]\n",
      "74/323: lut[(~lut.uri_mem_avail) & (lut.uri_unsh_no_clean_mem_avail)].shape[0]/lut.shape[0]\n",
      "74/324: lut[lut.mem_avail].uri_unsh_no_clean.nunique() / unique_uris_raw_only_unsh_no_single.shape[0]\n",
      "74/325: lut[lut.uri_unsh_no_clean_mem_avail].uri_unsh_no_clean.nunique()/lut.uri_unsh_no_clean.nunique()\n",
      "74/326: lut.groupby(['source', 'uri_unsh_no_clean_mem_avail']).uri_unsh_no_clean.nunique().unstack().div(lut.groupby('source').uri_unsh_no_clean.nunique(), axis=0).sort_values(True, ascending=False)\n",
      "74/327: lut.groupby(['lang', 'uri_unsh_no_clean_mem_avail']).uri_unsh_no_clean.nunique().unstack().div(lut.groupby('lang').uri_unsh_no_clean.nunique(), axis=0).sort_values(True, ascending=False).drop(False, axis=1).round(2)\n",
      "74/328: lut.groupby(['is_shortened', 'uri_cln_unshrtn_wb_avail']).uri_cln_unshrtn.nunique().unstack().div(lut.groupby('is_shortened').uri_cln_unshrtn.nunique(), axis=0).sort_values(True, ascending=False)\n",
      "74/329: lut.groupby(['is_shortened', 'uri_unsh_no_clean_mem_avail']).uri_unsh_no_clean.nunique().unstack().div(lut.groupby('is_shortened').uri_unsh_no_clean.nunique(), axis=0).sort_values(True, ascending=False)\n",
      "74/330: lut[lut.is_shortened].groupby(['is_unshrtn', 'uri_unsh_no_clean_mem_avail']).uri_unsh_no_clean.nunique().unstack().div(lut[lut.is_shortened].groupby('is_unshrtn').uri_unsh_no_clean.nunique(), axis=0).sort_values(True, ascending=False)\n",
      "74/331: lut['is_unshrtn'] = lut.uri_cln_unshrtn!=lut.uri_cln\n",
      "74/332: lut[lut.is_shortened].groupby(['is_unshrtn', 'uri_unsh_no_clean_mem_avail']).uri_unsh_no_clean.nunique().unstack().div(lut[lut.is_shortened].groupby('is_unshrtn').uri_unsh_no_clean.nunique(), axis=0).sort_values(True, ascending=False)\n",
      "74/333:\n",
      "lang_sour = lut.groupby(['lang', 'source']).uri_unsh_no_clean.nunique().unstack() \n",
      "lang_sour = lang_sour.div(lang_sour.sum(axis=1), axis=0).fillna(0).mul(100).round().astype(int)\n",
      "lang_sour\n",
      "74/334:\n",
      "unarch_wiki = lut[(lut.source=='wikipedia') & (~lut.uri_unsh_no_clean_mem_avail)]\n",
      "unarch_wiki.uri_unsh_no_clean.nunique()\n",
      "74/335: unarch_wiki.shape[0]\n",
      "74/336: (unarch_wiki.groupby('tld_unshrtn').uri_cln_unshrtn.nunique().sort_values(ascending=False) / 788).head(10)\n",
      "74/337: lut[lut.tld=='newsru.co.il']\n",
      "74/338:\n",
      "unarch = lut[(~lut.uri_unsh_no_clean_mem_avail)]\n",
      "\n",
      "def get_top_tld(df):\n",
      "    num_uniq_uri = df.uri_unsh_no_clean.nunique()\n",
      "    return (df.groupby('tld_unshrtn').uri_unsh_no_clean.nunique().sort_values(ascending=False).head(10)/num_uniq_uri).round(2)\n",
      "unarch.groupby('source').apply(get_top_tld).to_frame()\n",
      "74/339: lang_sour.loc['id', 'google']\n",
      "74/340:\n",
      "lang_sour = lut.groupby(['lang', 'source']).uri_unsh_no_clean.nunique().unstack() \n",
      "lang_sour.loc['id', 'google'] = lang_sour.loc['id,jv,ms']\n",
      "lang_sour.loc['jv', 'google'] = lang_sour.loc['id,jv,ms']\n",
      "lang_sour.loc['ms', 'google'] = lang_sour.loc['id,jv,ms']\n",
      "lang_sour.loc['en', 'google'] = lang_sour.loc['en,sco']\n",
      "lang_sour = lang_sour.drop(['sco', '38'], axis=0)\n",
      "\n",
      "lang_sour = lang_sour.div(lang_sour.sum(axis=1), axis=0).fillna(0).round(2).astype(int)\n",
      "lang_sour\n",
      "74/341:\n",
      "lang_sour = lut.groupby(['lang', 'source']).uri_unsh_no_clean.nunique().unstack() \n",
      "lang_sour.loc['id', 'google'] = lang_sour.loc['id,jv,ms', 'google']\n",
      "lang_sour.loc['jv', 'google'] = lang_sour.loc['id,jv,ms', 'google']\n",
      "lang_sour.loc['ms', 'google'] = lang_sour.loc['id,jv,ms', 'google']\n",
      "lang_sour.loc['en', 'google'] = lang_sour.loc['en,sco', 'google']\n",
      "lang_sour = lang_sour.drop(['sco', '38'], axis=0)\n",
      "\n",
      "lang_sour = lang_sour.div(lang_sour.sum(axis=1), axis=0).fillna(0).round(2).astype(int)\n",
      "lang_sour\n",
      "74/342:\n",
      "lang_sour = lut.groupby(['lang', 'source']).uri_unsh_no_clean.nunique().unstack() \n",
      "lang_sour.loc['id', 'google'] = lang_sour.loc['id,jv,ms', 'google']\n",
      "lang_sour.loc['jv', 'google'] = lang_sour.loc['id,jv,ms', 'google']\n",
      "lang_sour.loc['ms', 'google'] = lang_sour.loc['id,jv,ms', 'google']\n",
      "lang_sour.loc['en', 'google'] = lang_sour.loc['en,sco', 'google']\n",
      "lang_sour = lang_sour.drop(['sco', '38'], axis=0)\n",
      "\n",
      "lang_sour\n",
      "74/343:\n",
      "lang_sour = lut.groupby(['lang', 'source']).uri_unsh_no_clean.nunique().unstack() \n",
      "lang_sour.loc['id', 'google'] = lang_sour.loc['id,jv,ms', 'google']\n",
      "lang_sour.loc['jv', 'google'] = lang_sour.loc['id,jv,ms', 'google']\n",
      "lang_sour.loc['ms', 'google'] = lang_sour.loc['id,jv,ms', 'google']\n",
      "lang_sour.loc['en', 'google'] = lang_sour.loc['en,sco', 'google']\n",
      "lang_sour = lang_sour.drop(['sco', '38'], axis=0)\n",
      "\n",
      "lang_sour = lang_sour.div(lang_sour.sum(axis=1), axis=0).fillna(0).round(2)\n",
      "lang_sour\n",
      "74/344: lut.groupby(['lang']).uri_unsh_no_clean_mem_avail.sum()\n",
      "74/345:\n",
      "lang_ma = lut.groupby(['lang']).uri_unsh_no_clean_mem_avail.sum()\n",
      "lang_ma = lang_ma.div(lang_ma.sum(axis=1), axis=0).fillna(0).round(2)\n",
      "74/346:\n",
      "lang_ma = lut.groupby(['lang']).uri_unsh_no_clean_mem_avail.sum()\n",
      "lang_ma = lang_ma.div(lut.groupby('lang').uri_unsh_no_clean.nunique()).fillna(0).round(2)\n",
      "74/347:\n",
      "lang_ma = lut.groupby(['lang']).uri_unsh_no_clean_mem_avail.sum()\n",
      "lang_ma = lang_ma.div(lut.groupby('lang').uri_unsh_no_clean.nunique()).fillna(0).round(2)\n",
      "lang_ma\n",
      "74/348:\n",
      "lang_ma = lut.groupby(['lang']).uri_unsh_no_clean_mem_avail.sum()\n",
      "lang_ma = lang_ma.div(lut.groupby('lang').uri_unsh_no_clean.nunique(), axis=0).fillna(0).round(2)\n",
      "lang_ma\n",
      "74/349: lut.groupby('lang').uri_unsh_no_clean.nunique()\n",
      "74/350:\n",
      "lang_ma = lut.groupby(['lang']).uri_unsh_no_clean_mem_avail.sum()\n",
      "#lang_ma = lang_ma.div(lut.groupby('lang').uri_unsh_no_clean.nunique(), axis=0).fillna(0).round(2)\n",
      "lang_ma\n",
      "74/351: lut.groupby(['lang', 'uri_unsh_no_clean_mem_avail']).uri_unsh_no_clean.nunique().unstack().div(lut.groupby('lang').uri_unsh_no_clean.nunique(), axis=0).sort_values(True, ascending=False).drop(False, axis=1).round(2)\n",
      "74/352: lut.shape\n",
      "74/353: clut = lut.copy()\n",
      "74/354: clut[clut.lang=='en,sco']\n",
      "74/355: clut.loc[clut.lang=='en,sco', 'lang']\n",
      "74/356: clut[clut.loc[clut.lang=='en,sco', 'lang']]\n",
      "74/357: clut.loc[clut.lang=='en,sco', 'lang'] = 'en'\n",
      "74/358: clut.loc[clut.lang=='id,jv,ms', :]\n",
      "74/359: clut.loc[clut.lang=='id,jv,ms', :].assign(lang='id')\n",
      "74/360: clut = lut.copy()\n",
      "74/361:\n",
      "clut.loc[clut.lang=='en,sco', 'lang'] = 'en'\n",
      "clut = (pd.concat([clut, clut.loc[clut.lang=='id,jv,ms', :].assign(lang='id'), \n",
      "                   clut.loc[clut.lang=='id,jv,ms', :].assign(lang='jv'), \n",
      "                   clut.loc[clut.lang=='id,jv,ms', :].assign(lang='ms')], axis=0)\n",
      "       .reset_index().drop('index', axis=1))\n",
      "clut = clut.drop(clut)\n",
      "74/362: clut = lut.copy()\n",
      "74/363:\n",
      "clut.loc[clut.lang=='en,sco', 'lang'] = 'en'\n",
      "clut = (pd.concat([clut, clut.loc[clut.lang=='id,jv,ms', :].assign(lang='id'), \n",
      "                   clut.loc[clut.lang=='id,jv,ms', :].assign(lang='jv'), \n",
      "                   clut.loc[clut.lang=='id,jv,ms', :].assign(lang='ms')], axis=0)\n",
      "       .reset_index().drop('index', axis=1))\n",
      "74/364:\n",
      "clut = lut.copy()clut.loc[clut.lang=='en,sco', 'lang'] = 'en'\n",
      "clut = (pd.concat([clut, clut.loc[clut.lang=='id,jv,ms', :].assign(lang='id'), \n",
      "                   clut.loc[clut.lang=='id,jv,ms', :].assign(lang='jv'), \n",
      "                   clut.loc[clut.lang=='id,jv,ms', :].assign(lang='ms')], axis=0)\n",
      "       .reset_index().drop('index', axis=1))\n",
      "clut = clut[~clut.lang=='id,jv,ms']\n",
      "74/365:\n",
      "clut = lut.copy()\n",
      "clut.loc[clut.lang=='en,sco', 'lang'] = 'en'\n",
      "clut = (pd.concat([clut, clut.loc[clut.lang=='id,jv,ms', :].assign(lang='id'), \n",
      "                   clut.loc[clut.lang=='id,jv,ms', :].assign(lang='jv'), \n",
      "                   clut.loc[clut.lang=='id,jv,ms', :].assign(lang='ms')], axis=0)\n",
      "       .reset_index().drop('index', axis=1))\n",
      "clut = clut[~clut.lang=='id,jv,ms']\n",
      "74/366:\n",
      "clut = lut.copy()\n",
      "clut.loc[clut.lang=='en,sco', 'lang'] = 'en'\n",
      "clut = (pd.concat([clut, clut.loc[clut.lang=='id,jv,ms', :].assign(lang='id'), \n",
      "                   clut.loc[clut.lang=='id,jv,ms', :].assign(lang='jv'), \n",
      "                   clut.loc[clut.lang=='id,jv,ms', :].assign(lang='ms')], axis=0)\n",
      "       .reset_index().drop('index', axis=1))\n",
      "clut = clut[clut.lang!='id,jv,ms']\n",
      "74/367: clut.groupby(['lang', 'uri_unsh_no_clean_mem_avail']).uri_unsh_no_clean.nunique().unstack().div(lut.groupby('lang').uri_unsh_no_clean.nunique(), axis=0).sort_values(True, ascending=False).drop(False, axis=1).round(2)\n",
      "74/368: clut.dtypes\n",
      "74/369: clut.groupby(['lang', 'uri_unsh_no_clean_mem_avail']).uri_unsh_no_clean.nunique().unstack().div(clut.groupby('lang').uri_unsh_no_clean.nunique(), axis=0).sort_values(True, ascending=False).drop(False, axis=1).round(2)\n",
      "74/370:\n",
      "lang_sour = clut.groupby(['lang', 'source']).uri_unsh_no_clean.nunique().unstack() \n",
      "lang_sour = lang_sour.div(lang_sour.sum(axis=1), axis=0).fillna(0).round(2)\n",
      "lang_sour\n",
      "74/371:\n",
      "clut = lut.copy()\n",
      "clut.loc[clut.lang=='en,sco', 'lang'] = 'en'\n",
      "clut = (pd.concat([clut, clut.loc[clut.lang=='id,jv,ms', :].assign(lang='id'), \n",
      "                   clut.loc[clut.lang=='id,jv,ms', :].assign(lang='jv'), \n",
      "                   clut.loc[clut.lang=='id,jv,ms', :].assign(lang='ms')], axis=0)\n",
      "       .reset_index().drop('index', axis=1))\n",
      "clut = clut[~clut.lang.isin(['id,jv,ms', '38'])]\n",
      "74/372: clut.groupby(['lang', 'uri_unsh_no_clean_mem_avail']).uri_unsh_no_clean.nunique().unstack().div(clut.groupby('lang').uri_unsh_no_clean.nunique(), axis=0).sort_values(True, ascending=False).drop(False, axis=1).round(2)\n",
      "74/373:\n",
      "lang_ma = (clut.groupby(['lang', 'uri_unsh_no_clean_mem_avail'])\n",
      "           .uri_unsh_no_clean.nunique()\n",
      "           .unstack()\n",
      "           .div(clut.groupby('lang').uri_unsh_no_clean.nunique(), axis=0)\n",
      "           .sort_values(True, ascending=False)\n",
      "           .drop(False, axis=1).round(2)\n",
      "           .rename(columns={True: 'lang_ma'}))\n",
      "74/374:\n",
      "lang_ma = (clut.groupby(['lang', 'uri_unsh_no_clean_mem_avail'])\n",
      "           .uri_unsh_no_clean.nunique()\n",
      "           .unstack()\n",
      "           .div(clut.groupby('lang').uri_unsh_no_clean.nunique(), axis=0)\n",
      "           .sort_values(True, ascending=False)\n",
      "           .drop(False, axis=1).round(2)\n",
      "           .rename(columns={True: 'uri_unsh_no_clean_mem_avail'}))\n",
      "74/375:\n",
      "lang_ma = (clut.groupby(['lang', 'uri_unsh_no_clean_mem_avail'])\n",
      "           .uri_unsh_no_clean.nunique()\n",
      "           .unstack()\n",
      "           .div(clut.groupby('lang').uri_unsh_no_clean.nunique(), axis=0)\n",
      "           .sort_values(True, ascending=False)\n",
      "           .drop(False, axis=1).round(2)\n",
      "           .rename(columns={True: 'uri_unsh_no_clean_mem_avail'}))\n",
      "lang_sour = clut.groupby(['lang', 'source']).uri_unsh_no_clean.nunique().unstack() \n",
      "lang_sour = lang_sour.div(lang_sour.sum(axis=1), axis=0).fillna(0).round(2)\n",
      "\n",
      "lsma = pd.concat([lang_sour, lang_ma], axis=1)\n",
      "74/376:\n",
      "lang_ma = (clut.groupby(['lang', 'uri_unsh_no_clean_mem_avail'])\n",
      "           .uri_unsh_no_clean.nunique()\n",
      "           .unstack()\n",
      "           .div(clut.groupby('lang').uri_unsh_no_clean.nunique(), axis=0)\n",
      "           .sort_values(True, ascending=False)\n",
      "           .drop(False, axis=1).round(2)\n",
      "           .rename(columns={True: 'uri_unsh_no_clean_mem_avail'}))\n",
      "lang_sour = clut.groupby(['lang', 'source']).uri_unsh_no_clean.nunique().unstack() \n",
      "lang_sour = lang_sour.div(lang_sour.sum(axis=1), axis=0).fillna(0).round(2)\n",
      "\n",
      "lsma = pd.concat([lang_sour, lang_ma], axis=1)\n",
      "lsma\n",
      "74/377:\n",
      "clut = lut.copy()\n",
      "clut.loc[clut.lang=='en,sco', 'lang'] = 'en'\n",
      "clut = (pd.concat([clut, clut.loc[clut.lang=='id,jv,ms', :].assign(lang='id'), \n",
      "                   clut.loc[clut.lang=='id,jv,ms', :].assign(lang='jv'), \n",
      "                   clut.loc[clut.lang=='id,jv,ms', :].assign(lang='ms')], axis=0)\n",
      "       .reset_index().drop('index', axis=1))\n",
      "clut = clut[~clut.lang.isin(['id,jv,ms', 'sco', '38'])]\n",
      "74/378:\n",
      "lang_ma = (clut.groupby(['lang', 'uri_unsh_no_clean_mem_avail'])\n",
      "           .uri_unsh_no_clean.nunique()\n",
      "           .unstack()\n",
      "           .div(clut.groupby('lang').uri_unsh_no_clean.nunique(), axis=0)\n",
      "           .sort_values(True, ascending=False)\n",
      "           .drop(False, axis=1).round(2)\n",
      "           .rename(columns={True: 'uri_unsh_no_clean_mem_avail'}))\n",
      "lang_sour = clut.groupby(['lang', 'source']).uri_unsh_no_clean.nunique().unstack() \n",
      "lang_sour = lang_sour.div(lang_sour.sum(axis=1), axis=0).fillna(0).round(2)\n",
      "\n",
      "lsma = pd.concat([lang_sour, lang_ma], axis=1)\n",
      "lsma\n",
      "74/379: plt.scatter(lsma['google'], lsma.uri_unsh_no_clean_mem_avail)\n",
      "74/380: plt.scatter(lsma['twitter'], lsma.uri_unsh_no_clean_mem_avail)\n",
      "74/381: plt.scatter(lsma['wikipedia'], lsma.uri_unsh_no_clean_mem_avail)\n",
      "74/382: plt.scatter(lsma['youtube'], lsma.uri_unsh_no_clean_mem_avail)\n",
      "74/383: plt.scatter(lsma['twitter'], lsma.uri_unsh_no_clean_mem_avail)\n",
      "74/384: (lsma==0).sum(axis=1)\n",
      "74/385: (lsma==0).sum(axis=0)\n",
      "74/386:\n",
      "for s in clut.source.unique():\n",
      "    plt.scatter(lsma[s], lsma.uri_unsh_no_clean_mem_avail)\n",
      "74/387:\n",
      "fig, axes = plt.subplots(4, 1)\n",
      "for s, ax in zip(clut.source.unique(), axes):\n",
      "    plt.scatter(lsma[s], lsma.uri_unsh_no_clean_mem_avail, ax=ax)\n",
      "74/388:\n",
      "fig, axes = plt.subplots(4, 1)\n",
      "for s, ax in zip(clut.source.unique(), axes):\n",
      "    print ax\n",
      "    plt.scatter(lsma[s], lsma.uri_unsh_no_clean_mem_avail)\n",
      "74/389:\n",
      "fig, axes = plt.subplots(4, 1)\n",
      "for s, ax in zip(clut.source.unique(), axes):\n",
      "    print (ax)\n",
      "    plt.scatter(lsma[s], lsma.uri_unsh_no_clean_mem_avail)\n",
      "74/390:\n",
      "fig, axes = plt.subplots(4, 1)\n",
      "for s, ax in zip(clut.source.unique(), axes):\n",
      "    ax.scatter(lsma[s], lsma.uri_unsh_no_clean_mem_avail)\n",
      "74/391:\n",
      "fig, axes = plt.subplots(4, 1, figsize=(9, 3))\n",
      "for s, ax in zip(clut.source.unique(), axes):\n",
      "    ax.scatter(lsma[s], lsma.uri_unsh_no_clean_mem_avail)\n",
      "74/392:\n",
      "fig, axes = plt.subplots(4, 1, figsize=(5, 20))\n",
      "for s, ax in zip(clut.source.unique(), axes):\n",
      "    ax.scatter(lsma[s], lsma.uri_unsh_no_clean_mem_avail)\n",
      "74/393:\n",
      "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
      "for s, ax in zip(clut.source.unique(), axes):\n",
      "    ax.scatter(lsma[s], lsma.uri_unsh_no_clean_mem_avail)\n",
      "74/394:\n",
      "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
      "for s, row in zip(clut.source.unique(), axes):\n",
      "    for ax in row:\n",
      "        ax.scatter(lsma[s], lsma.uri_unsh_no_clean_mem_avail)\n",
      "74/395:\n",
      "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
      "for s, row in zip(clut.source.unique(), axes):\n",
      "    for ax in row:\n",
      "        ax.scatter(lsma[s], lsma.uri_unsh_no_clean_mem_avail)\n",
      "        ax.set_title(source)\n",
      "74/396:\n",
      "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
      "for s, row in zip(clut.source.unique(), axes):\n",
      "    for ax in row:\n",
      "        ax.scatter(lsma[s], lsma.uri_unsh_no_clean_mem_avail)\n",
      "        ax.set_title(s)\n",
      "74/397:\n",
      "fig, axes = plt.subplots(2, 2, figsize=(5, 20))\n",
      "for ax, row in zip(clut.source.unique(), axes):\n",
      "        ax.scatter(lsma[s], lsma.uri_unsh_no_clean_mem_avail)\n",
      "        ax.set_title(s)\n",
      "74/398:\n",
      "fig, axes = plt.subplots(1, 4, figsize=(5, 20))\n",
      "for ax, row in zip(clut.source.unique(), axes):\n",
      "        ax.scatter(lsma[s], lsma.uri_unsh_no_clean_mem_avail)\n",
      "        ax.set_title(s)\n",
      "74/399:\n",
      "fig, axes = plt.subplots(1, 4, figsize=(5, 20))\n",
      "for s, ax in zip(clut.source.unique(), axes):\n",
      "        ax.scatter(lsma[s], lsma.uri_unsh_no_clean_mem_avail)\n",
      "        ax.set_title(s)\n",
      "74/400:\n",
      "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
      "for s, ax in zip(clut.source.unique(), axes):\n",
      "        ax.scatter(lsma[s], lsma.uri_unsh_no_clean_mem_avail)\n",
      "        ax.set_title(s)\n",
      "74/401:\n",
      "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
      "for s, ax in zip(clut.source.unique(), axes):\n",
      "        ax.scatter(lsma[s], lsma.uri_unsh_no_clean_mem_avail)\n",
      "        ax.set_title(s)\n",
      "        ax.set_ylabel('mem_avail')\n",
      "74/402:\n",
      "fig, axes = plt.subplots(1, 4, figsize=(20, 5), sharey=True)\n",
      "for s, ax in zip(clut.source.unique(), axes):\n",
      "        ax.scatter(lsma[s], lsma.uri_unsh_no_clean_mem_avail)\n",
      "        ax.set_title(s)\n",
      "        ax.set_ylabel('mem_avail')\n",
      "74/403:\n",
      "fig, axes = plt.subplots(1, 4, figsize=(20, 5), sharey=True)\n",
      "for s, ax in zip(clut.source.unique(), axes):\n",
      "        print ax\n",
      "        ax.scatter(lsma[s], lsma.uri_unsh_no_clean_mem_avail)\n",
      "        ax.set_title(s)\n",
      "        ax.set_ylabel('mem_avail')\n",
      "74/404:\n",
      "fig, axes = plt.subplots(1, 4, figsize=(20, 5), sharey=True)\n",
      "for s, ax in zip(clut.source.unique(), axes):\n",
      "        ax.scatter(lsma[s], lsma.uri_unsh_no_clean_mem_avail)\n",
      "        ax.set_title(s)\n",
      "\n",
      "axes[0].set_ylabel('mem_avail')\n",
      "74/405:\n",
      "import statsmodels as sm\n",
      "\n",
      "X = lsma[['wikipedia', 'youtube', 'twitter', 'google']]\n",
      "\n",
      "y = lsma['uri_unsh_no_clean_mem_avail']\n",
      "\n",
      "## fit a OLS model with intercept on TV and Radio\n",
      "X = sm.add_constant(X)\n",
      "est = sm.OLS(y, X).fit()\n",
      "\n",
      "est.summary()\n",
      "74/406:\n",
      "import statsmodels.api as sm\n",
      "\n",
      "X = lsma[['wikipedia', 'youtube', 'twitter', 'google']]\n",
      "\n",
      "y = lsma['uri_unsh_no_clean_mem_avail']\n",
      "\n",
      "## fit a OLS model with intercept on TV and Radio\n",
      "X = sm.add_constant(X)\n",
      "est = sm.OLS(y, X).fit()\n",
      "\n",
      "est.summary()\n",
      "74/407:\n",
      "fig, axes = plt.subplots(1, 4, figsize=(20, 5), sharey=True)\n",
      "for s, ax in zip(clut.source.unique(), axes):\n",
      "        ax.hist(lsma[s])\n",
      "        ax.set_title(s)\n",
      "\n",
      "axes[0].set_ylabel('mem_avail')\n",
      "74/408:\n",
      "fig, axes = plt.subplots(1, 4, figsize=(20, 5), sharey=True)\n",
      "for s, ax in zip(clut.source.unique(), axes):\n",
      "        ax.hist(lsma[s])\n",
      "        ax.set_title(s)\n",
      "\n",
      "axes[0].set_ylabel('langs')\n",
      "74/409:\n",
      "fig, axes = plt.subplots(1, 4, figsize=(20, 5), sharey=True)\n",
      "for s, ax in zip(clut.source.unique(), axes):\n",
      "        ax.hist(lsma[s])\n",
      "        ax.set_title(s)\n",
      "fig.set_title('Histogram of ratios for each source')\n",
      "axes[0].set_ylabel('langs')\n",
      "74/410:\n",
      "fig, axes = plt.subplots(1, 4, figsize=(20, 5), sharey=True)\n",
      "for s, ax in zip(clut.source.unique(), axes):\n",
      "        ax.hist(lsma[s])\n",
      "        ax.set_title(s)\n",
      "axes[0].set_ylabel('langs')\n",
      "76/1:\n",
      "import pandas as pd\n",
      "import geopandas as gpd\n",
      "from shapely.geometry import Point\n",
      "76/2:\n",
      "import pandas as pd\n",
      "import geopandas as gpd\n",
      "from shapely.geometry import Point\n",
      "77/1: %matplotlib inline\n",
      "77/2:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "77/3: lut = pd.read_pickle('lang_uri_time_all.pkl.gz', compression='gzip')\n",
      "77/4: wb = pd.read_csv('data/wb_avail_cdx_all.csv.gz', compression='gzip')\n",
      "77/5: wb.head()\n",
      "77/6: lut = pd.read_pickle('lang_uri_time_wb_avail_all.pkl.gz', compression='gzip')\n",
      "77/7: dict(lut.dtypes)\n",
      "77/8:\n",
      "macd_all = pd.read_pickle('data/macd_all.pkl.gz', compression='gzip')\n",
      "macd_all.head()\n",
      "77/9: lut['uri_unsh_no_clean'] = lut.uri_unshrtn_raw.combine_first(lut.uri_unshrtn_bitly.combine_first(lut.uri_unshrtn.combine_first(lut.uri)))\n",
      "77/10:\n",
      "lut['uri_mem_avail'] = lut.uri.map(~macd_all.drop_duplicates(subset='uri').set_index('uri').first_snap.isna()).fillna(False)\n",
      "lut['uri_unsh_no_clean_mem_avail'] = lut.uri_unsh_no_clean.map(~macd_all.drop_duplicates(subset='uri').set_index('uri').first_snap.isna()).fillna(False)\n",
      "77/11:\n",
      "curl = '../../curl_results.csv'\n",
      "res = []\n",
      "with open(curl, 'r') as f:\n",
      "    for line in f:\n",
      "        line = line.strip('\\n')\n",
      "        m = re.match(curl_re, line)\n",
      "        if m is None:\n",
      "            m = {'url':line}\n",
      "        else:\n",
      "            m = m.groupdict()\n",
      "        res.append(m)\n",
      "        \n",
      "len(res)\n",
      "77/12:\n",
      "res = pd.DataFrame(res)\n",
      "res.head()\n",
      "77/13:\n",
      "testr = '''https://www.rp.pl/Konflikt-izraelsko-palestynski/180529976-USA-Minuta-ciszy-dla-Palestynczykow-w-szkole-Protest-Zydow.html;200;2.348753;0.000330;0.011990;0;0.000;text/html; charset=windows-1250\n",
      "http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.desklight-c3394e44-19a2-45eb-ad63-c71bec715d05;200;0.692087;0.053310;0.086916;0;0.000;text/html;charset=UTF-8\n",
      "https://www.rp.pl/Konflikt-izraelsko-palestynski/305159920-Izrael-zrzuca-wine-na-Hamas.html;200;0.559529;0.000298;0.011882;0;0.000;text/html; charset=windows-1250\n",
      "https://www.tvn24.pl/wiadomosci-ze-swiata,2/konflikt-izraelsko-palestynski-konferencja-w-paryzu,649333.html;200;0.952678;0.000340;0.034502;0;0.000;text/html; charset=UTF-8\n",
      "https://fakty.interia.pl/raporty/raport-konflikt-izraelsko-palestynski/opinie,parametr,fakty_dol,nPack,3;200;0.500981;0.000418;0.038015;0;0.000;text/html; charset=UTF-8\n",
      "http://cejsh.icm.edu.pl/cejsh/element/bwmeta1.element.desklight-c4ec6b2a-e60d-4928-a30f-dc6640296b15;200;0.295232;0.000377;0.034092;0;0.000;text/html;charset=UTF-8'''\n",
      "import re\n",
      "\n",
      "#curl_re = '^(?P<url>.*);(?P<status_code>\\d+);(?P<time_total>[\\d\\.]+);(?P<time_namelookup>[\\d\\.]+);(?P<time_connect>[\\d\\.]+);(?P<size_download>[\\d\\.]+);(?P<speed_download>[\\d\\.]+);(?P<content_type>[^;]+)(?:;?\\s*(?P<charset>(c|C)harset=[^;]+))*;?$'\n",
      "curl_re = '^(?P<url>.*);(?P<status_code>\\d+);(?P<time_total>[\\d\\.]+);(?P<time_namelookup>[\\d\\.]+);(?P<time_connect>[\\d\\.]+);(?P<size_download>[\\d\\.]+);(?P<speed_download>[\\d\\.]+);(?P<content_type>(text/html.*)|(text/plain.*)|(text/xml.*)|(application/json.*)|(application/rss\\+xml.*)|(\\*/\\*.*)|(image/pdf.*)|(application/pdf.*)|([^;]+))?$'\n",
      "\n",
      "for x in testr.split('\\n'):\n",
      "    print(re.match(curl_re, x).groupdict())\n",
      "77/14:\n",
      "curl = '../../curl_results.csv'\n",
      "res = []\n",
      "with open(curl, 'r') as f:\n",
      "    for line in f:\n",
      "        line = line.strip('\\n')\n",
      "        m = re.match(curl_re, line)\n",
      "        if m is None:\n",
      "            m = {'url':line}\n",
      "        else:\n",
      "            m = m.groupdict()\n",
      "        res.append(m)\n",
      "        \n",
      "len(res)\n",
      "77/15:\n",
      "res = pd.DataFrame(res)\n",
      "res.head()\n",
      "77/16:\n",
      "import http\n",
      "http.HTTPStatus(200)\n",
      "77/17:\n",
      "from functools import lru_cache\n",
      "\n",
      "@lru_cache()\n",
      "def get_status_desc(code):\n",
      "    code = int(code)\n",
      "    try:\n",
      "        status = http.HTTPStatus(code)\n",
      "        msg = str(code) + ' ' + status.phrase + ': ' + status.description\n",
      "    except ValueError:\n",
      "        msg = str(code) + ' STATUS_PHRASE_NOT_FOUND' if code!=0 else str(code) + ' CURL_ERROR'\n",
      "    return msg\n",
      "\n",
      "res['status_phrase'] = res.status_code.fillna(0).apply(get_status_desc)\n",
      "77/18: res.status_phrase.value_counts().to_frame()\n",
      "77/19: res[res.status_code==0]\n",
      "77/20:\n",
      "def to_numeric(df):\n",
      "    '''\n",
      "    Turn timedelta columns into numeric dtype\n",
      "    '''\n",
      "    cols = [\"size_download\", \"speed_download\", \"status_code\", \"time_connect\", \"time_namelookup\", \"time_total\"]\n",
      "    numeric = df[cols].apply(pd.to_numeric)\n",
      "    df = df.copy()\n",
      "    df[cols] = numeric\n",
      "    return df\n",
      "\n",
      "res = res.pipe(to_numeric)\n",
      "77/21: res[res.status_code==0]\n",
      "77/22:\n",
      "bad_urls = res[(res.status_code==0) & (res.time_total==0)].shape[0]\n",
      "no_dns = res[(res.status_code==0) & (res.time_total!=0) & (res.time_namelookup==0)].shape[0]\n",
      "bad_server = res[(res.status_code==0) & (res.time_total!=0) & (res.time_namelookup!=0) & (res.time_connect==0)].shape[0]\n",
      "other = res[(res.status_code==0) & (res.time_total!=0) & (res.time_namelookup!=0) & (res.time_connect!=0)].shape[0]\n",
      "not_found = res[res.status_code==404].shape[0]\n",
      "print(f'bad urls={bad_urls}, no_dns={no_dns}, bad_server={bad_server}, other={other}')\n",
      "77/23: res['status_code'] = res.status_code.astype(int).astype(str)\n",
      "77/24: res['status_code'] = res.status_code.fillna(0).astype(int).astype(str)\n",
      "77/25: res = res[res.url!='uri']\n",
      "77/26: res.head()\n",
      "77/27: res.to_csv('data/curl_results_parsed.csv.gz', index=False, compression='gzip')\n",
      "77/28: res = pd.read_csv('data/curl_results_parsed.csv.gz', compression='gzip')\n",
      "77/29:\n",
      "curl = pd.read_csv('data/curl_results_parsed.csv.gz', compression='gzip')\n",
      "lut['status_code'] = lut.uri_unsh_no_clean.map(~curl.drop_duplicates(subset='uri').set_index('url').status_code)\n",
      "lut['content_type'] = lut.uri_unsh_no_clean.map(~curl.drop_duplicates(subset='uri').set_index('url').content_type)\n",
      "77/30:\n",
      "curl = pd.read_csv('data/curl_results_parsed.csv.gz', compression='gzip')\n",
      "lut['status_code'] = lut.uri_unsh_no_clean.map(~curl.drop_duplicates(subset='url').set_index('url').status_code)\n",
      "lut['content_type'] = lut.uri_unsh_no_clean.map(~curl.drop_duplicates(subset='url').set_index('url').content_type)\n",
      "77/31:\n",
      "curl = pd.read_csv('data/curl_results_parsed.csv.gz', compression='gzip')\n",
      "lut['status_code'] = lut.uri_unsh_no_clean.map(curl.drop_duplicates(subset='url').set_index('url').status_code)\n",
      "lut['content_type'] = lut.uri_unsh_no_clean.map(curl.drop_duplicates(subset='url').set_index('url').content_type)\n",
      "77/32: lut.head().T\n",
      "77/33: lut.shape\n",
      "77/34: lut[(lu)]\n",
      "77/35: lut.head().T\n",
      "77/36: lut['archived'] = lut[['uri_cln_wb_avail', 'uri_cln_unshrtn_wb_avail', 'uri_mem_avail', 'uri_unsh_no_clean_mem_avail']].any(axis=1)\n",
      "77/37:\n",
      "lut['archived'] = lut[['uri_cln_wb_avail', 'uri_cln_unshrtn_wb_avail', 'uri_mem_avail', 'uri_unsh_no_clean_mem_avail']].any(axis=1)\n",
      "\n",
      "lut.head().T\n",
      "77/38: lut['archived'].value_counts()\n",
      "77/39: lut.groupby(['uri_cln_unshrtn']).archived.max()\n",
      "77/40: lut.groupby(['uri_cln_unshrtn']).archived.max().reset_index()\n",
      "77/41: lut.groupby(['uri_unsh_no_clean']).archived.max().reset_index()\n",
      "77/42: lut['uri_unsh_no_clean'].isna().sum()\n",
      "77/43: lut.groupby(['uri_unsh_no_clean', 'tld_unshrtn']).archived.max().reset_index()\n",
      "77/44: pd.set_option('display.max_colwidth', -1)\n",
      "77/45: lut.groupby(['uri_unsh_no_clean', 'tld_unshrtn']).archived.max().reset_index()\n",
      "77/46: lut.groupby(['uri_unsh_no_clean', 'tld_unshrtn']).archived.max()\n",
      "77/47: lut.groupby(['uri_unsh_no_clean', 'tld_unshrtn']).archived.max().head(100).tail(50)\n",
      "77/48: lut.groupby(['uri_unsh_no_clean', 'tld_unshrtn']).archived.max().head(1000).tail(50)\n",
      "77/49: lut[lut.status_code==200].groupby(['uri_unsh_no_clean', 'tld_unshrtn']).archived.max().head(1000).tail(50)\n",
      "77/50: lut[lut.status_code==200].groupby(['uri_unsh_no_clean', 'tld_unshrtn', 'source']).archived.max().head(1000).tail(50)\n",
      "77/51: lut[lut.status_code==200].groupby(['uri_unsh_no_clean', 'tld_unshrtn', 'source']).archived.max().sort_index(level=0).head(20)\n",
      "77/52: lut[lut.status_code==200].groupby(['uri_unsh_no_clean', 'tld_unshrtn', 'source']).archived.max().sort_index(level=0)\n",
      "77/53: lut[lut.status_code==200].groupby(['uri_unsh_no_clean', 'source']).archived.max().sort_index(levels=[0, 1])\n",
      "77/54: lut[lut.status_code==200].groupby(['uri_unsh_no_clean', 'source']).archived.max().sort_index(level=[0, 1])\n",
      "77/55: lut[lut.status_code==200].groupby(['uri_unsh_no_clean', 'source']).archived.max().sort_index(level=[1, 2])\n",
      "77/56: lut[lut.status_code==200].groupby(['uri_unsh_no_clean', 'source']).archived.max()\n",
      "77/57:\n",
      "def get_values(g):\n",
      "    return ','.join(list(g.unique()))\n",
      "lut[lut.status_code==200].groupby(['uri_unsh_no_clean']).agg({'archived': np.max, 'source': get_values()})\n",
      "77/58:\n",
      "def get_values(g):\n",
      "    return ','.join(list(g.unique()))\n",
      "lut[lut.status_code==200].groupby(['uri_unsh_no_clean']).agg({'archived': np.max, 'source': get_values)\n",
      "77/59:\n",
      "def get_values(g):\n",
      "    return ','.join(list(g.unique()))\n",
      "lut[lut.status_code==200].groupby(['uri_unsh_no_clean']).agg({'archived': np.max, 'source': get_values})\n",
      "77/60:\n",
      "def get_values(g):\n",
      "    return ','.join(list(g.unique()))\n",
      "\n",
      "uris_for_appr = lut[lut.status_code==200].groupby(['uri_unsh_no_clean']).agg({'archived': np.max, 'source': get_values}).\n",
      "77/61:\n",
      "def get_values(g):\n",
      "    return ','.join(list(g.unique()))\n",
      "\n",
      "uris_for_appr = lut[lut.status_code==200].groupby(['uri_unsh_no_clean']).agg({'archived': np.max, 'source': get_values})\n",
      "77/62: uris_for_appr.head()\n",
      "77/63: uris_for_appr.source.value_counts()\n",
      "77/64: uris_for_appr[(uris_for_appr.source.str.contains(',')) & (uris_for_appr.archived==False)]\n",
      "77/65:\n",
      "def get_values(g):\n",
      "    return ','.join(list(g.unique()))\n",
      "\n",
      "uris_for_appr = lut[lut.status_code==200].groupby(['uri_unsh_no_clean']).agg({'archived': np.max, 'source': get_values}).reset_index()\n",
      "77/66: uris_for_appr.head()\n",
      "77/67: uris_for_appr.source.value_counts()\n",
      "77/68: uris_for_appr[(uris_for_appr.source.str.contains(',')) & (uris_for_appr.archived==False)]\n",
      "77/69: uris_for_appr\n",
      "77/70: uris_for_appr.shape\n",
      "77/71: uris_for_appr.archived.value_counts()\n",
      "77/72: res.shape\n",
      "77/73: lut[lut.status_code==301].groupby(['uri_unsh_no_clean']).agg({'archived': np.max, 'source': get_values}).reset_index()\n",
      "77/74:\n",
      "def get_values(g):\n",
      "    return ','.join(list(g.unique()))\n",
      "\n",
      "uris_for_appr_200 = lut[lut.status_code==200].groupby(['uri_unsh_no_clean']).agg({'archived': np.max, 'source': get_values}).reset_index()\n",
      "77/75: uris_for_appr_200.head()\n",
      "77/76: uris_for_appr_200.shape\n",
      "77/77: uris_for_appr_200.archived.value_counts()\n",
      "77/78: uris_for_appr_200.source.value_counts()\n",
      "77/79: uris_for_appr_200[(uris_for_appr.source.str.contains(',')) & (uris_for_appr.archived==False)]\n",
      "77/80: uris_for_appr_300 = lut[lut.status_code.isin([301,302])].groupby(['uri_unsh_no_clean']).agg({'archived': np.max, 'source': get_values}).reset_index()\n",
      "77/81: uris_for_appr_300.archived.value_counts()\n",
      "77/82: uris_for_appr_300.source.value_counts()\n",
      "77/83:\n",
      "def get_values(g):\n",
      "    return ','.join(list(g.unique()))\n",
      "\n",
      "uris_for_appr_200 = lut[lut.status_code==200].groupby(['uri_unsh_no_clean', 'uri_unshrtn_tld']).agg({'archived': np.max, 'source': get_values}).reset_index()\n",
      "77/84:\n",
      "def get_values(g):\n",
      "    return ','.join(list(g.unique()))\n",
      "\n",
      "uris_for_appr_200 = lut[lut.status_code==200].groupby(['uri_unsh_no_clean', 'uri_tld_unshrtn']).agg({'archived': np.max, 'source': get_values}).reset_index()\n",
      "77/85:\n",
      "def get_values(g):\n",
      "    return ','.join(list(g.unique()))\n",
      "\n",
      "uris_for_appr_200 = lut[lut.status_code==200].groupby(['uri_unsh_no_clean', 'tld_unshrtn']).agg({'archived': np.max, 'source': get_values}).reset_index()\n",
      "77/86: uris_for_appr_200.head()\n",
      "77/87: uris_for_appr_200.shape\n",
      "77/88: uris_for_appr_200.archived.value_counts()\n",
      "77/89: uris_for_appr_200.source.value_counts()\n",
      "77/90: uris_for_appr_200.tld_unshrtn.value_counts()\n",
      "77/91: uris_for_appr_200[uris_for_appr_200.tld_unshrtn=='facebook.com'].archived.value_counts()\n",
      "77/92: uris_for_appr_300 = lut[lut.status_code.isin([301,302])].groupby(['uri_unsh_no_clean', 'tld_unshrtn']).agg({'archived': np.max, 'source': get_values}).reset_index()\n",
      "77/93: uris_for_appr_300.archived.value_counts()\n",
      "77/94: uris_for_appr_300.source.value_counts()\n",
      "77/95: uris_for_appr_200.tld_unshrtn.value_counts().head()\n",
      "77/96: uris_for_appr_200[uris_for_appr_200.tld_unshrtn=='facebook.com'].archived.value_counts().head()\n",
      "77/97: uris_for_appr_300[uris_for_appr_200.tld_unshrtn=='facebook.com'].archived.value_counts().head()\n",
      "77/98: uris_for_appr_300[uris_for_appr_300.tld_unshrtn=='facebook.com'].archived.value_counts().head()\n",
      "77/99: uris_for_appr_300.tld_unshrtn.value_counts().head()\n",
      "77/100: uris_for_appr_300[uris_for_appr_300.tld_unshrtn=='facebook.com'].archived.value_counts()\n",
      "77/101: uris_for_appr_300.tld_unshrtn.value_counts().head()\n",
      "77/102: uris_for_appr_300[uris_for_appr_300.tld_unshrtn=='fb.me'].archived.value_counts()\n",
      "77/103: uris_for_appr_200[uris_for_appr_200.tld_unshrtn=='youtube.com'].archived.value_counts()\n",
      "77/104: uris_for_appr_200[uris_for_appr_200.tld_unshrtn=='youtube.com'].head()\n",
      "77/105:\n",
      "def get_values(g):\n",
      "    return ','.join(list(g.unique()))\n",
      "tlds_to_remove = ['fb.me', 'facebook.com', 'youtu.be', 'youtube.com']\n",
      "uris_for_appr_200 = lut[(lut.status_code==200) & (~lut.tld_unshrtn_isin(tlds_to_remove))].groupby(['uri_unsh_no_clean', 'tld_unshrtn']).agg({'archived': np.max, 'source': get_values}).reset_index()\n",
      "77/106:\n",
      "def get_values(g):\n",
      "    return ','.join(list(g.unique()))\n",
      "tlds_to_remove = ['fb.me', 'facebook.com', 'youtu.be', 'youtube.com']\n",
      "uris_for_appr_200 = lut[(lut.status_code==200) & (~lut.tld_unshrtn.isin(tlds_to_remove))].groupby(['uri_unsh_no_clean', 'tld_unshrtn']).agg({'archived': np.max, 'source': get_values}).reset_index()\n",
      "77/107: uris_for_appr_200[`uris_for_appr_200.tld_unshrtn_isin('')]\n",
      "77/108: uris_for_appr_200[uris_for_appr_200.tld_unshrtn=='facebook.com'].archived.value_counts()\n",
      "77/109: uris_for_appr_200.shape\n",
      "77/110: uris_for_appr_200.archived.value_counts()\n",
      "77/111: uris_for_appr_200.source.value_counts()\n",
      "77/112: uris_for_appr_200.tld_unshrtn.value_counts().head()\n",
      "77/113: uris_for_appr_200[uris_for_appr_200.tld_unshrtn=='facebook.com'].archived.value_counts()\n",
      "77/114: uris_for_appr_200[(uris_for_appr.source.str.contains(',')) & (uris_for_appr.archived==False)]\n",
      "77/115: uris_for_appr_300 = lut[(lut.status_code.isin([301,302])) & (~lut.tld_unshrtn.isin(tlds_to_remove))].groupby(['uri_unsh_no_clean', 'tld_unshrtn']).agg({'archived': np.max, 'source': get_values}).reset_index()\n",
      "77/116: uris_for_appr_300.archived.value_counts()\n",
      "77/117: uris_for_appr_300 = lut[(lut.status_code.isin([301,302])) & (~lut.tld_unshrtn.isin(tlds_to_remove))].groupby(['uri_unsh_no_clean', 'tld_unshrtn']).agg({'archived': np.max, 'source': get_values}).reset_index()\n",
      "77/118: uris_for_appr_300.archived.value_counts()\n",
      "77/119: uris_for_appr_300.source.value_counts()\n",
      "77/120: uris_for_appr_300.tld_unshrtn.value_counts().head()\n",
      "77/121: uris_for_appr_300[uris_for_appr_300.tld_unshrtn=='fb.me'].archived.value_counts()\n",
      "77/122: uris_for_appr_200[uris_for_appr_200.archived].sample(n=200, )\n",
      "77/123:\n",
      "all_samples = uris_for_appr_200[uris_for_appr_200.archived].sample(n=1000, )\n",
      "samples = [all_samples.iloc[200*i:200*i+200] for i in range(5)]\n",
      "77/124: samples\n",
      "77/125: [s.shape for s in samples]\n",
      "77/126:\n",
      "archived_200 = uris_for_appr_200[uris_for_appr_200.archived].sample(n=1000, )\n",
      "archived_200 = [all_samples.iloc[200*i:200*i+200] for i in range(5)]\n",
      "\n",
      "not_archived_200 = uris_for_appr_200[~uris_for_appr_200.archived].sample(n=1000, )\n",
      "not_archived_200 = [all_samples.iloc[200*i:200*i+200] for i in range(5)]\n",
      "\n",
      "archived_300 = uris_for_appr_300[uris_for_appr_300.archived].sample(n=1000, )\n",
      "archived_300 = [all_samples.iloc[200*i:200*i+200] for i in range(5)]\n",
      "\n",
      "not_archived_300 = uris_for_appr_300[~uris_for_appr_300.archived].sample(n=1000, )\n",
      "not_archived_300 = [all_samples.iloc[200*i:200*i+200] for i in range(5)]\n",
      "77/127:\n",
      "samples = {}\n",
      "archived_200 = uris_for_appr_200[uris_for_appr_200.archived].sample(n=1000, )\n",
      "samples['archived_200'] = [archived_200.iloc[200*i:200*i+200] for i in range(5)]\n",
      "\n",
      "not_archived_200 = uris_for_appr_200[~uris_for_appr_200.archived].sample(n=1000, )\n",
      "samples['not_archived_200'] = [not_archived_200.iloc[200*i:200*i+200] for i in range(5)]\n",
      "\n",
      "archived_300 = uris_for_appr_300[uris_for_appr_300.archived].sample(n=1000, )\n",
      "samples['archived_30X'] = [archived_300.iloc[200*i:200*i+200] for i in range(5)]\n",
      "\n",
      "not_archived_300 = uris_for_appr_300[~uris_for_appr_300.archived].sample(n=1000, )\n",
      "samples['not_archived_30X'] = [not_archived_300.iloc[200*i:200*i+200] for i in range(5)]\n",
      "77/128:\n",
      "n_samp = 10\n",
      "n_rows = 200\n",
      "samples = {}\n",
      "def get_samples(df, arch, n_rows=n_rows, n_samp=n_samp):\n",
      "    all_samples = df[df.archived==].sample(n=n_rows*n_samp, )\n",
      "    return [all_samples.iloc[n_rows*i:n_rows*i+n_rows] for i in range(n_rows)]\n",
      "\n",
      "samples['archived_200'] = get_samples(uris_for_appr_200, True)\n",
      "samples['not_archived_200'] = get_samples(uris_for_appr_200, False)\n",
      "samples['archived_30X'] = get_samples(uris_for_appr_300, True)\n",
      "samples['archived_30X'] = get_samples(uris_for_appr_300, False)\n",
      "77/129:\n",
      "n_samp = 10\n",
      "n_rows = 200\n",
      "samples = {}\n",
      "def get_samples(df, arch, n_rows=n_rows, n_samp=n_samp):\n",
      "    all_samples = df[df.archived==arch].sample(n=n_rows*n_samp )\n",
      "    return [all_samples.iloc[n_rows*i:n_rows*i+n_rows] for i in range(n_rows)]\n",
      "\n",
      "samples['archived_200'] = get_samples(uris_for_appr_200, True)\n",
      "samples['not_archived_200'] = get_samples(uris_for_appr_200, False)\n",
      "samples['archived_30X'] = get_samples(uris_for_appr_300, True)\n",
      "samples['archived_30X'] = get_samples(uris_for_appr_300, False)\n",
      "77/130:\n",
      "import os\n",
      "os.mkdir('data/appraisal')\n",
      "77/131:\n",
      "for key, g in samples.items():\n",
      "    for i, samp in enumerate(g):\n",
      "        samp.to_csv(f'data/appraisal/{key}_{i}.csv', index=False)\n",
      "77/132:\n",
      "for key, g in samples.items():\n",
      "    for i, samp in enumerate(g):\n",
      "        print (samp.shape, samp.dtypes)\n",
      "        \n",
      "        samp.to_csv(f'data/appraisal/{key}_{i}.csv', index=False)\n",
      "77/133:\n",
      "n_samp = 10\n",
      "n_rows = 200\n",
      "samples = {}\n",
      "uris_for_appr_200['appraisal'] = ''\n",
      "uris_for_appr_300['appraisal'] = ''\n",
      "\n",
      "def get_samples(df, arch, n_rows=n_rows, n_samp=n_samp):\n",
      "    all_samples = df[df.archived==arch].sample(n=n_rows*n_samp )\n",
      "    return [all_samples.iloc[n_rows*i:n_rows*i+n_rows] for i in range(n_rows)]\n",
      "\n",
      "samples['archived_200'] = get_samples(uris_for_appr_200, True)\n",
      "samples['not_archived_200'] = get_samples(uris_for_appr_200, False)\n",
      "samples['archived_30X'] = get_samples(uris_for_appr_300, True)\n",
      "samples['archived_30X'] = get_samples(uris_for_appr_300, False)\n",
      "77/134:\n",
      "for key, g in samples.items():\n",
      "    for i, samp in enumerate(g):\n",
      "        print (samp.shape, samp.dtypes)\n",
      "        \n",
      "        samp.to_csv(f'data/appraisal/{key}_{i}.csv', index=False)\n",
      "77/135:\n",
      "for key, g in samples.items():\n",
      "    for i, samp in enumerate(g):\n",
      "        print (key, i, samp.shape)\n",
      "        \n",
      "        samp.to_csv(f'data/appraisal/{key}_{i}.csv', index=False)\n",
      "77/136:\n",
      "n_samp = 10\n",
      "n_rows = 200\n",
      "samples = {}\n",
      "uris_for_appr_200['appraisal'] = ''\n",
      "uris_for_appr_300['appraisal'] = ''\n",
      "\n",
      "def get_samples(df, arch, n_rows=n_rows, n_samp=n_samp):\n",
      "    all_samples = df[df.archived==arch].sample(n=n_rows*n_samp )\n",
      "    return [all_samples.iloc[n_rows*i:n_rows*i+n_rows] for i in range(n_samp)]\n",
      "\n",
      "samples['archived_200'] = get_samples(uris_for_appr_200, True)\n",
      "samples['not_archived_200'] = get_samples(uris_for_appr_200, False)\n",
      "samples['archived_30X'] = get_samples(uris_for_appr_300, True)\n",
      "samples['archived_30X'] = get_samples(uris_for_appr_300, False)\n",
      "77/137:\n",
      "for key, g in samples.items():\n",
      "    for i, samp in enumerate(g):\n",
      "        print (key, i, samp.shape)\n",
      "        \n",
      "        samp.to_csv(f'data/appraisal/{key}_{i+1}.csv', index=False)\n",
      "77/138:\n",
      "n_samp = 10\n",
      "n_rows = 200\n",
      "samples = {}\n",
      "uris_for_appr_200['appraisal'] = ''\n",
      "uris_for_appr_300['appraisal'] = ''\n",
      "\n",
      "def get_samples(df, arch, n_rows=n_rows, n_samp=n_samp):\n",
      "    all_samples = df[df.archived==arch].sample(n=n_rows*n_samp )\n",
      "    return [all_samples.iloc[n_rows*i:n_rows*i+n_rows] for i in range(n_samp)]\n",
      "\n",
      "samples['archived_200'] = get_samples(uris_for_appr_200, True)\n",
      "samples['not_archived_200'] = get_samples(uris_for_appr_200, False)\n",
      "samples['archived_30X'] = get_samples(uris_for_appr_300, True)\n",
      "samples['not_archived_30X'] = get_samples(uris_for_appr_300, False)\n",
      "77/139:\n",
      "for key, g in samples.items():\n",
      "    for i, samp in enumerate(g):\n",
      "        print (key, i, samp.shape)\n",
      "        \n",
      "        samp.to_csv(f'data/appraisal/{key}_{i+1}.csv', index=False)\n",
      "77/140:\n",
      "n_samp = 10\n",
      "n_rows = 200\n",
      "samples = {}\n",
      "uris_for_appr_200['appraisal'] = ''\n",
      "uris_for_appr_300['appraisal'] = ''\n",
      "\n",
      "def get_samples(df, arch, n_rows=n_rows, n_samp=n_samp):\n",
      "    all_samples = df[df.archived==arch].sample(n=n_rows*n_samp, seed=42)\n",
      "    return [all_samples.iloc[n_rows*i:n_rows*i+n_rows] for i in range(n_samp)]\n",
      "\n",
      "samples['archived_200'] = get_samples(uris_for_appr_200, True)\n",
      "samples['not_archived_200'] = get_samples(uris_for_appr_200, False)\n",
      "samples['archived_30X'] = get_samples(uris_for_appr_300, True)\n",
      "samples['not_archived_30X'] = get_samples(uris_for_appr_300, False)\n",
      "77/141:\n",
      "n_samp = 10\n",
      "n_rows = 200\n",
      "samples = {}\n",
      "uris_for_appr_200['appraisal'] = ''\n",
      "uris_for_appr_300['appraisal'] = ''\n",
      "\n",
      "def get_samples(df, arch, n_rows=n_rows, n_samp=n_samp):\n",
      "    all_samples = df[df.archived==arch].sample(n=n_rows*n_samp, rand_seed=42)\n",
      "    return [all_samples.iloc[n_rows*i:n_rows*i+n_rows] for i in range(n_samp)]\n",
      "\n",
      "samples['archived_200'] = get_samples(uris_for_appr_200, True)\n",
      "samples['not_archived_200'] = get_samples(uris_for_appr_200, False)\n",
      "samples['archived_30X'] = get_samples(uris_for_appr_300, True)\n",
      "samples['not_archived_30X'] = get_samples(uris_for_appr_300, False)\n",
      "77/142:\n",
      "n_samp = 10\n",
      "n_rows = 200\n",
      "samples = {}\n",
      "uris_for_appr_200['appraisal'] = ''\n",
      "uris_for_appr_300['appraisal'] = ''\n",
      "\n",
      "def get_samples(df, arch, n_rows=n_rows, n_samp=n_samp):\n",
      "    all_samples = df[df.archived==arch].sample(n=n_rows*n_samp, random_seed=42)\n",
      "    return [all_samples.iloc[n_rows*i:n_rows*i+n_rows] for i in range(n_samp)]\n",
      "\n",
      "samples['archived_200'] = get_samples(uris_for_appr_200, True)\n",
      "samples['not_archived_200'] = get_samples(uris_for_appr_200, False)\n",
      "samples['archived_30X'] = get_samples(uris_for_appr_300, True)\n",
      "samples['not_archived_30X'] = get_samples(uris_for_appr_300, False)\n",
      "77/143:\n",
      "n_samp = 10\n",
      "n_rows = 200\n",
      "samples = {}\n",
      "uris_for_appr_200['appraisal'] = ''\n",
      "uris_for_appr_300['appraisal'] = ''\n",
      "\n",
      "def get_samples(df, arch, n_rows=n_rows, n_samp=n_samp):\n",
      "    all_samples = df[df.archived==arch].sample(n=n_rows*n_samp, random_state=42)\n",
      "    return [all_samples.iloc[n_rows*i:n_rows*i+n_rows] for i in range(n_samp)]\n",
      "\n",
      "samples['archived_200'] = get_samples(uris_for_appr_200, True)\n",
      "samples['not_archived_200'] = get_samples(uris_for_appr_200, False)\n",
      "samples['archived_30X'] = get_samples(uris_for_appr_300, True)\n",
      "samples['not_archived_30X'] = get_samples(uris_for_appr_300, False)\n",
      "77/144:\n",
      "for key, g in samples.items():\n",
      "    for i, samp in enumerate(g):\n",
      "        print (key, i, samp.shape)\n",
      "        \n",
      "        samp.to_csv(f'data/appraisal/{key}_{i+1}.csv', index=False)\n",
      "77/145:\n",
      "n_samp = 10\n",
      "n_rows = 200\n",
      "samples = {}\n",
      "uris_for_appr_200['appraisal'] = ''\n",
      "uris_for_appr_300['appraisal'] = ''\n",
      "\n",
      "def get_samples(df, arch, n_rows=n_rows, n_samp=n_samp):\n",
      "    all_samples = df[df.archived==arch].sample(n=n_rows*n_samp, random_state=42)\n",
      "    return [all_samples.iloc[n_rows*i:n_rows*i+n_rows] for i in range(n_samp)]\n",
      "\n",
      "samples['archived_200'] = get_samples(uris_for_appr_200, True)\n",
      "samples['not_archived_200'] = get_samples(uris_for_appr_200, False)\n",
      "samples['archived_30X'] = get_samples(uris_for_appr_300, True)\n",
      "samples['not_archived_30X'] = get_samples(uris_for_appr_300, False)\n",
      "77/146:\n",
      "for key, g in samples.items():\n",
      "    for i, samp in enumerate(g):\n",
      "        print (key, i, samp.shape)\n",
      "        \n",
      "        samp.to_csv(f'data/appraisal/{key}_{i+1}.csv', index=False)\n",
      "77/147:\n",
      "n_samp = 10\n",
      "n_rows = 200\n",
      "samples = {}\n",
      "uris_for_appr_200['appraisal'] = ''\n",
      "uris_for_appr_300['appraisal'] = ''\n",
      "\n",
      "def get_samples(df, arch, n_rows=n_rows, n_samp=n_samp):\n",
      "    all_samples = df[(df.archived==arch) & (~df.source.str.contains(','))].sample(n=n_rows*n_samp, random_state=42)\n",
      "    return [all_samples.iloc[n_rows*i:n_rows*i+n_rows] for i in range(n_samp)]\n",
      "\n",
      "samples['archived_200'] = get_samples(uris_for_appr_200, True)\n",
      "samples['not_archived_200'] = get_samples(uris_for_appr_200, False)\n",
      "samples['archived_30X'] = get_samples(uris_for_appr_300, True)\n",
      "samples['not_archived_30X'] = get_samples(uris_for_appr_300, False)\n",
      "77/148:\n",
      "for key, g in samples.items():\n",
      "    for i, samp in enumerate(g):\n",
      "        print (key, i, samp.shape)\n",
      "        \n",
      "        samp.to_csv(f'data/appraisal/{key}_{i+1}.csv', index=False)\n",
      "77/149: uris_for_appr_200[uris_for_appr_200.source.str.contains(',')].(f'data/appraisal/cross_200.csv', index=False)\n",
      "77/150: uris_for_appr_200[uris_for_appr_200.source.str.contains(',')].to_csv(f'data/appraisal/cross_200.csv', index=False)\n",
      "77/151:\n",
      "uris_for_appr_200[uris_for_appr_200.source.str.contains(',')].to_csv(f'data/appraisal/cross_200.csv', index=False)\n",
      "uris_for_appr_300[uris_for_appr_300.source.str.contains(',')].to_csv(f'data/appraisal/cross_200.csv', index=False)\n",
      "77/152:\n",
      "uris_for_appr_200[uris_for_appr_200.source.str.contains(',')].to_csv(f'data/appraisal/cross_200.csv', index=False)\n",
      "uris_for_appr_300[uris_for_appr_300.source.str.contains(',')].to_csv(f'data/appraisal/cross_300.csv', index=False)\n",
      "77/153:\n",
      "for key, g in samples.items():\n",
      "    for i, samp in enumerate(g):\n",
      "        print (key, i, samp.shape)\n",
      "        print(samp.source.value_counts)\n",
      "        \n",
      "        #samp.to_csv(f'data/appraisal/{key}_{i+1}.csv', index=False)\n",
      "77/154:\n",
      "for key, g in samples.items():\n",
      "    for i, samp in enumerate(g):\n",
      "        print (key, i, samp.shape)\n",
      "        print(samp.source.value_counts())\n",
      "        \n",
      "        #samp.to_csv(f'data/appraisal/{key}_{i+1}.csv', index=False)\n",
      "77/155:\n",
      "for key, g in samples.items():\n",
      "    for i, samp in enumerate(g):\n",
      "        print (key, i, samp.shape)\n",
      "        print(samp.columns)\n",
      "        print(samp.source.value_counts())\n",
      "        \n",
      "        samp[['']].to_csv(f'data/appraisal/{key}_{i+1}.csv', index=False)\n",
      "77/156:\n",
      "for key, g in samples.items():\n",
      "    for i, samp in enumerate(g):\n",
      "        print (key, i, samp.shape)\n",
      "        print(samp.columns)\n",
      "        print(samp.source.value_counts())\n",
      "        \n",
      "        #samp[['']].to_csv(f'data/appraisal/{key}_{i+1}.csv', index=False)\n",
      "77/157:\n",
      "srcs = pd.DataFrame()\n",
      "for key, g in samples.items():\n",
      "    for i, samp in enumerate(g):\n",
      "        print (key, i, samp.shape)\n",
      "        print(samp.source.value_counts())\n",
      "        srcs.append(samp.source.value_counts())\n",
      "        samp[['uri_unsh_no_clean', 'appraisal']].to_csv(f'data/appraisal/{key}_{i+1}.csv', index=False)\n",
      "77/158: srcs\n",
      "77/159:\n",
      "srcs = []\n",
      "for key, g in samples.items():\n",
      "    for i, samp in enumerate(g):\n",
      "        print (key, i, samp.shape)\n",
      "        print(samp.source.value_counts())\n",
      "        srcs.append(samp.source.value_counts())\n",
      "        samp[['uri_unsh_no_clean', 'appraisal']].to_csv(f'data/appraisal/{key}_{i+1}.csv', index=False)\n",
      "77/160: pd.concat(srcs).reset_index().groupby('source').sum()\n",
      "77/161: srcs\n",
      "77/162:\n",
      "srcs = []\n",
      "for key, g in samples.items():\n",
      "    for i, samp in enumerate(g):\n",
      "        print (key, i, samp.shape)\n",
      "        print(samp.source.value_counts())\n",
      "        srcs.append(samp.source.value_counts().reset_index())\n",
      "        samp[['uri_unsh_no_clean', 'appraisal']].to_csv(f'data/appraisal/{key}_{i+1}.csv', index=False)\n",
      "77/163: pd.concat(srcs).reset_index().groupby('source').sum()\n",
      "77/164: pd.concat(srcs).groupby('source').sum()\n",
      "77/165: srcs\n",
      "77/166: pd.concat(srcs).groupby('index').sum()\n",
      "77/167:\n",
      "srcs = []\n",
      "ks = ['a', 'b', 'c', 'd']\n",
      "for k, (key, g) in zip(ks, samples.items()):\n",
      "    for i, samp in enumerate(g):\n",
      "        print (key, i, samp.shape)\n",
      "        print(samp.source.value_counts())\n",
      "        srcs.append(samp.source.value_counts().reset_index())\n",
      "        samp[['uri_unsh_no_clean', 'appraisal']].to_csv(f'data/appraisal/{k}_{i+1}.csv', index=False)\n",
      "77/168:\n",
      "n_samp = 5\n",
      "n_rows = 200\n",
      "samples = {}\n",
      "uris_for_appr_200['appraisal'] = ''\n",
      "uris_for_appr_300['appraisal'] = ''\n",
      "\n",
      "def get_samples(df, arch, n_rows=n_rows, n_samp=n_samp):\n",
      "    all_samples = df[(df.archived==arch) & (~df.source.str.contains(','))].sample(n=n_rows*n_samp, random_state=42)\n",
      "    return [all_samples.iloc[n_rows*i:n_rows*i+n_rows] for i in range(n_samp)]\n",
      "\n",
      "samples['archived_200'] = get_samples(uris_for_appr_200, True)\n",
      "samples['not_archived_200'] = get_samples(uris_for_appr_200, False)\n",
      "samples['archived_30X'] = get_samples(uris_for_appr_300, True)\n",
      "samples['not_archived_30X'] = get_samples(uris_for_appr_300, False)\n",
      "77/169:\n",
      "srcs = []\n",
      "ks = ['a', 'b', 'c', 'd']\n",
      "for k, (key, g) in zip(ks, samples.items()):\n",
      "    for i, samp in enumerate(g):\n",
      "        print (key, i, samp.shape)\n",
      "        print(samp.source.value_counts())\n",
      "        srcs.append(samp.source.value_counts().reset_index())\n",
      "        samp[['uri_unsh_no_clean', 'appraisal']].to_csv(f'data/appraisal/{k}_{i+1}.csv', index=False)\n",
      "77/170: pd.concat(srcs).groupby('index').sum()\n",
      "77/171:\n",
      "uris_for_appr_200.loc[uris_for_appr_200.source.str.contains(',')].to_csv(f'data/appraisal/e.csv', index=False)\n",
      "uris_for_appr_300.loc[uris_for_appr_300.source.str.contains(',')].to_csv(f'data/appraisal/f.csv', index=False)\n",
      "77/172:\n",
      "uris_for_appr_200.loc[uris_for_appr_200.source.str.contains(',')].shape\n",
      "uris_for_appr_300.loc[uris_for_appr_300.source.str.contains(',')].shape\n",
      "77/173: uris_for_appr_200.loc[uris_for_appr_200.source.str.contains(',')].shape\n",
      "77/174: uris_for_appr_300.loc[uris_for_appr_300.source.str.contains(',')].shape\n",
      "77/175:\n",
      "cross2 = uris_for_appr_200.loc[uris_for_appr_200.source.str.contains(','), ['uri_unsh_no_clean', 'appraisal']]\n",
      "cross3 = uris_for_appr_300.loc[uris_for_appr_300.source.str.contains(','), ['uri_unsh_no_clean', 'appraisal']]\n",
      "77/176:\n",
      "for g, df in cross2.groupby(np.arange(len(cross2)) // (len(cross2)//5)):\n",
      "    print(df.shape)\n",
      "77/177:\n",
      "for g, df in cross2.groupby(np.arange(len(cross2)) // (len(cross2)//4)):\n",
      "    print(df.shape)\n",
      "77/178:\n",
      "for g, df in cross2.groupby(np.arange(len(cross2)) // (len(cross2)//5)):\n",
      "    print(df.shape)\n",
      "77/179:\n",
      "for g, df in cross2.groupby(np.arange(len(cross2)) // (len(cross2)//5 - 4)):\n",
      "    print(df.shape)\n",
      "77/180:\n",
      "for g, df in cross2.groupby(np.arange(len(cross2)) // (len(cross2)//5 +1)):\n",
      "    print(df.shape)\n",
      "77/181:\n",
      "for g, df in cross2.groupby(np.arange(len(cross2)) // (len(cross2)//5 +1)):\n",
      "    print(df.shape)\n",
      "    \n",
      "for g, df in cross3.groupby(np.arange(len(cross2)) // (len(cross3)//5 +1)):\n",
      "    print(df.shape)\n",
      "77/182:\n",
      "for g, df in cross2.groupby(np.arange(len(cross2)) // (len(cross2)//5 +1)):\n",
      "    print(df.shape)\n",
      "    \n",
      "for g, df in cross3.groupby(np.arange(len(cross3)) // (len(cross3)//5 +1)):\n",
      "    print(df.shape)\n",
      "77/183:\n",
      "for g, df in cross2.groupby(np.arange(len(cross2)) // (len(cross2)//5 +1)):\n",
      "    print(df.shape)\n",
      "    print(g)\n",
      "    \n",
      "for g, df in cross3.groupby(np.arange(len(cross3)) // (len(cross3)//5 +1)):\n",
      "    print(df.shape)\n",
      "77/184:\n",
      "for g, df in cross2.groupby(np.arange(len(cross2)) // (len(cross2)//5 +1)):\n",
      "    print(df.shape)\n",
      "    df.to_csv(f'data/appraisal/e_{g+1}.csv', index=False)\n",
      "    \n",
      "for g, df in cross3.groupby(np.arange(len(cross3)) // (len(cross3)//5 +1)):\n",
      "    print(df.shape)\n",
      "    df.to_csv(f'data/appraisal/e_{g+1}.csv', index=False)\n",
      "77/185:\n",
      "for g, df in cross2.groupby(np.arange(len(cross2)) // (len(cross2)//5 +1)):\n",
      "    print(df.shape)\n",
      "    df.to_csv(f'data/appraisal/e_{g+1}.csv', index=False)\n",
      "    \n",
      "for g, df in cross3.groupby(np.arange(len(cross3)) // (len(cross3)//5 +1)):\n",
      "    print(df.shape)\n",
      "    df.to_csv(f'data/appraisal/f_{g+1}.csv', index=False)\n",
      "77/186:\n",
      "for g, df in cross2.groupby(np.arange(len(cross2)) // (len(cross2)//5 +1)):\n",
      "    print(df.shape)\n",
      "    df.to_csv(f'data/appraisal/{g+1}_e.csv', index=False)\n",
      "    \n",
      "for g, df in cross3.groupby(np.arange(len(cross3)) // (len(cross3)//5 +1)):\n",
      "    print(df.shape)\n",
      "    df.to_csv(f'data/appraisal/{g+1}_f.csv', index=False)\n",
      "77/187:\n",
      "srcs = []\n",
      "ks = ['a', 'b', 'c', 'd']\n",
      "for k, (key, g) in zip(ks, samples.items()):\n",
      "    for i, samp in enumerate(g):\n",
      "        print (key, i, samp.shape)\n",
      "        print(samp.source.value_counts())\n",
      "        srcs.append(samp.source.value_counts().reset_index())\n",
      "        samp[['uri_unsh_no_clean', 'appraisal']].to_csv(f'data/appraisal/{i+1}_{k}.csv', index=False)\n",
      "77/188:\n",
      "for g, df in cross2.groupby(np.arange(len(cross2)) // (len(cross2)//5 +1)):\n",
      "    print(df.shape)\n",
      "    df.to_csv(f'data/appraisal/{g+1}_e.csv', index=False)\n",
      "    \n",
      "for g, df in cross3.groupby(np.arange(len(cross3)) // (len(cross3)//5 +1)):\n",
      "    print(df.shape)\n",
      "    df.to_csv(f'data/appraisal/{g+1}_f.csv', index=False)\n",
      "77/189:\n",
      "srcs = []\n",
      "ks = ['a', 'b', 'c', 'd']\n",
      "annot = []\n",
      "for k, (key, g) in zip(ks, samples.items()):\n",
      "    s = []\n",
      "    for i, samp in enumerate(g):\n",
      "        print (key, i, samp.shape)\n",
      "        print(samp.source.value_counts())\n",
      "        srcs.append(samp.source.value_counts().reset_index())\n",
      "        s.append(samp[['uri_unsh_no_clean', 'appraisal']])\n",
      "    annot.append(pd.concat(s))\n",
      "77/190: pd.concat(srcs).groupby('index').sum()\n",
      "77/191:\n",
      "n_samp = 5\n",
      "n_rows = 100\n",
      "samples = {}\n",
      "uris_for_appr_200['appraisal'] = ''\n",
      "uris_for_appr_300['appraisal'] = ''\n",
      "\n",
      "def get_samples(df, arch, n_rows=n_rows, n_samp=n_samp):\n",
      "    all_samples = df[(df.archived==arch) & (~df.source.str.contains(','))].sample(n=n_rows*n_samp, random_state=42)\n",
      "    return [all_samples.iloc[n_rows*i:n_rows*i+n_rows] for i in range(n_samp)]\n",
      "\n",
      "samples['archived_200'] = get_samples(uris_for_appr_200, True)\n",
      "samples['not_archived_200'] = get_samples(uris_for_appr_200, False)\n",
      "samples['archived_30X'] = get_samples(uris_for_appr_300, True)\n",
      "samples['not_archived_30X'] = get_samples(uris_for_appr_300, False)\n",
      "77/192:\n",
      "srcs = []\n",
      "ks = ['a', 'b', 'c', 'd']\n",
      "annot = []\n",
      "for k, (key, g) in zip(ks, samples.items()):\n",
      "    s = []\n",
      "    for i, samp in enumerate(g):\n",
      "        print (key, i, samp.shape)\n",
      "        print(samp.source.value_counts())\n",
      "        srcs.append(samp.source.value_counts().reset_index())\n",
      "        s.append(samp[['uri_unsh_no_clean', 'appraisal']])\n",
      "    annot.append(pd.concat(s))\n",
      "77/193: pd.concat(srcs).groupby('index').sum()\n",
      "77/194:\n",
      "cross2 = uris_for_appr_200.loc[uris_for_appr_200.source.str.contains(','), ['uri_unsh_no_clean', 'appraisal']]\n",
      "cross3 = uris_for_appr_300.loc[uris_for_appr_300.source.str.contains(','), ['uri_unsh_no_clean', 'appraisal']]\n",
      "77/195:\n",
      "new_annots = []\n",
      "for s, (g, df) in zip(annots, cross2.groupby(np.arange(len(cross2)) // (len(cross2)//5 +1))):\n",
      "    print(df.shape)\n",
      "    new_annots.append(pd.concat([s, df]))\n",
      "\n",
      "new_new_annots = []\n",
      "for s, (g, df) in zip(new_annots, cross3.groupby(np.arange(len(cross3)) // (len(cross3)//5 +1))):\n",
      "    print(df.shape)\n",
      "    new_annots.append(pd.concat([s, df]))\n",
      "    \n",
      "df.to_csv(f'data/appraisal/{g+1}.csv', index=False)\n",
      "77/196:\n",
      "new_annots = []\n",
      "for s, (g, df) in zip(annot, cross2.groupby(np.arange(len(cross2)) // (len(cross2)//5 +1))):\n",
      "    print(df.shape)\n",
      "    new_annots.append(pd.concat([s, df]))\n",
      "\n",
      "new_new_annots = []\n",
      "for s, (g, df) in zip(new_annots, cross3.groupby(np.arange(len(cross3)) // (len(cross3)//5 +1))):\n",
      "    print(df.shape)\n",
      "    new_annots.append(pd.concat([s, df]))\n",
      "    \n",
      "df.to_csv(f'data/appraisal/{g+1}.csv', index=False)\n",
      "77/197:\n",
      "new_annots = []\n",
      "for s, (g, df) in zip(annot, cross2.groupby(np.arange(len(cross2)) // (len(cross2)//5 +1))):\n",
      "    print(df.shape)\n",
      "    new_annots.append(pd.concat([s, df]))\n",
      "\n",
      "new_new_annots = []\n",
      "for s, (g, df) in zip(new_annots, cross3.groupby(np.arange(len(cross3)) // (len(cross3)//5 +1))):\n",
      "    print(df.shape)\n",
      "    new_annots.append(pd.concat([s, df]))\n",
      "    \n",
      "for i, df in enumerate(new_new_annots):\n",
      "    print (i, df.shape)\n",
      "    #df.to_csv(f'data/appraisal/{g+1}.csv', index=False)\n",
      "77/198:\n",
      "n_samp = 5\n",
      "n_rows = 100\n",
      "samples = {}\n",
      "uris_for_appr_200['appraisal'] = ''\n",
      "uris_for_appr_300['appraisal'] = ''\n",
      "\n",
      "def get_samples(df, arch, n_rows=n_rows, n_samp=n_samp):\n",
      "    all_samples = df[(df.archived==arch) & (~df.source.str.contains(','))].sample(n=n_rows*n_samp, random_state=42)\n",
      "    return [all_samples.iloc[n_rows*i:n_rows*i+n_rows] for i in range(n_samp)]\n",
      "\n",
      "samples['archived_200'] = get_samples(uris_for_appr_200, True)\n",
      "samples['not_archived_200'] = get_samples(uris_for_appr_200, False)\n",
      "samples['archived_30X'] = get_samples(uris_for_appr_300, True)\n",
      "samples['not_archived_30X'] = get_samples(uris_for_appr_300, False)\n",
      "77/199:\n",
      "srcs = []\n",
      "ks = ['a', 'b', 'c', 'd']\n",
      "annot = []\n",
      "for k, (key, g) in zip(ks, samples.items()):\n",
      "    s = []\n",
      "    for i, samp in enumerate(g):\n",
      "        print (key, i, samp.shape)\n",
      "        print(samp.source.value_counts())\n",
      "        srcs.append(samp.source.value_counts().reset_index())\n",
      "        s.append(samp[['uri_unsh_no_clean', 'appraisal']])\n",
      "    annot.append(pd.concat(s))\n",
      "77/200: pd.concat(srcs).groupby('index').sum()\n",
      "77/201:\n",
      "cross2 = uris_for_appr_200.loc[uris_for_appr_200.source.str.contains(','), ['uri_unsh_no_clean', 'appraisal']]\n",
      "cross3 = uris_for_appr_300.loc[uris_for_appr_300.source.str.contains(','), ['uri_unsh_no_clean', 'appraisal']]\n",
      "77/202:\n",
      "new_annots = []\n",
      "for s, (g, df) in zip(annot, cross2.groupby(np.arange(len(cross2)) // (len(cross2)//5 +1))):\n",
      "    print(df.shape)\n",
      "    new_annots.append(pd.concat([s, df]))\n",
      "\n",
      "new_new_annots = []\n",
      "for s, (g, df) in zip(new_annots, cross3.groupby(np.arange(len(cross3)) // (len(cross3)//5 +1))):\n",
      "    print(df.shape)\n",
      "    new_annots.append(pd.concat([s, df]))\n",
      "    \n",
      "for i, df in enumerate(new_new_annots):\n",
      "    print (i, df.shape)\n",
      "    #df.to_csv(f'data/appraisal/{g+1}.csv', index=False)\n",
      "77/203:\n",
      "new_annots = []\n",
      "for s, (g, df) in zip(annot, cross2.groupby(np.arange(len(cross2)) // (len(cross2)//5 +1))):\n",
      "    print(df.shape)\n",
      "    new_annots.append(pd.concat([s, df]))\n",
      "\n",
      "new_new_annots = []\n",
      "for s, (g, df) in zip(new_annots, cross3.groupby(np.arange(len(cross3)) // (len(cross3)//5 +1))):\n",
      "    print(df.shape)\n",
      "    new_new_annots.append(pd.concat([s, df]))\n",
      "    \n",
      "for i, df in enumerate(new_new_annots):\n",
      "    print (i, df.shape)\n",
      "    #df.to_csv(f'data/appraisal/{g+1}.csv', index=False)\n",
      "77/204:\n",
      "n_samp = 5\n",
      "n_rows = 100\n",
      "samples = {}\n",
      "uris_for_appr_200['appraisal'] = ''\n",
      "uris_for_appr_300['appraisal'] = ''\n",
      "\n",
      "def get_samples(df, arch, n_rows=n_rows, n_samp=n_samp):\n",
      "    all_samples = df[(df.archived==arch) & (~df.source.str.contains(','))].sample(n=n_rows*n_samp, random_state=42)\n",
      "    return [all_samples.iloc[n_rows*i:n_rows*i+n_rows] for i in range(n_samp)]\n",
      "\n",
      "samples['archived_200'] = get_samples(uris_for_appr_200, True)\n",
      "samples['not_archived_200'] = get_samples(uris_for_appr_200, False)\n",
      "samples['archived_30X'] = get_samples(uris_for_appr_300, True)\n",
      "samples['not_archived_30X'] = get_samples(uris_for_appr_300, False)\n",
      "77/205:\n",
      "srcs = []\n",
      "ks = ['a', 'b', 'c', 'd']\n",
      "annot = {}\n",
      "for k, (key, g) in zip(ks, samples.items()):\n",
      "    s = []\n",
      "    for i, samp in enumerate(g):\n",
      "        print (key, i, samp.shape)\n",
      "        srcs.append(samp.source.value_counts().reset_index())\n",
      "        s.append(samp[['uri_unsh_no_clean', 'appraisal']])\n",
      "    annot.append(pd.concat(s))\n",
      "77/206:\n",
      "srcs = []\n",
      "ks = ['a', 'b', 'c', 'd']\n",
      "annot = []\n",
      "for k, (key, g) in zip(ks, samples.items()):\n",
      "    s = []\n",
      "    for i, samp in enumerate(g):\n",
      "        print (key, i, samp.shape)\n",
      "        srcs.append(samp.source.value_counts().reset_index())\n",
      "        s.append(samp[['uri_unsh_no_clean', 'appraisal']])\n",
      "    annot.append(pd.concat(s))\n",
      "77/207: pd.concat(srcs).groupby('index').sum()\n",
      "77/208:\n",
      "cross2 = uris_for_appr_200.loc[uris_for_appr_200.source.str.contains(','), ['uri_unsh_no_clean', 'appraisal']]\n",
      "cross3 = uris_for_appr_300.loc[uris_for_appr_300.source.str.contains(','), ['uri_unsh_no_clean', 'appraisal']]\n",
      "77/209: annot\n",
      "77/210: len(annot)\n",
      "77/211:\n",
      "from collections import defaultdict\n",
      "srcs = []\n",
      "ks = ['a', 'b', 'c', 'd']\n",
      "annot = defaultdict(list)\n",
      "for k, (key, g) in zip(ks, samples.items()):\n",
      "    s = []\n",
      "    for i, samp in enumerate(g):\n",
      "        print (key, i, samp.shape)\n",
      "        srcs.append(samp.source.value_counts().reset_index())\n",
      "        s.append(samp[['uri_unsh_no_clean', 'appraisal']])\n",
      "        annot[k+1].append(pd.concat(s))\n",
      "77/212:\n",
      "from collections import defaultdict\n",
      "srcs = []\n",
      "ks = ['a', 'b', 'c', 'd']\n",
      "annot = defaultdict(list)\n",
      "for k, (key, g) in zip(ks, samples.items()):\n",
      "    s = []\n",
      "    for i, samp in enumerate(g):\n",
      "        print (key, i, samp.shape)\n",
      "        srcs.append(samp.source.value_counts().reset_index())\n",
      "        s.append(samp[['uri_unsh_no_clean', 'appraisal']])\n",
      "        annot[str(k+1)].append(pd.concat(s))\n",
      "77/213:\n",
      "from collections import defaultdict\n",
      "srcs = []\n",
      "ks = ['a', 'b', 'c', 'd']\n",
      "annot = defaultdict(list)\n",
      "for k, (key, g) in zip(ks, samples.items()):\n",
      "    s = []\n",
      "    for i, samp in enumerate(g):\n",
      "        print (key, i, samp.shape)\n",
      "        srcs.append(samp.source.value_counts().reset_index())\n",
      "        annot[k].append(samp[['uri_unsh_no_clean', 'appraisal']])\n",
      "77/214: len(annot)\n",
      "77/215:\n",
      "from collections import defaultdict\n",
      "srcs = []\n",
      "ks = ['a', 'b', 'c', 'd']\n",
      "annot = defaultdict(list)\n",
      "for k, (key, g) in zip(ks, samples.items()):\n",
      "    s = []\n",
      "    for i, samp in enumerate(g):\n",
      "        print (key, i, samp.shape)\n",
      "        srcs.append(samp.source.value_counts().reset_index())\n",
      "        annot[i].append(samp[['uri_unsh_no_clean', 'appraisal']])\n",
      "77/216: len(annot)\n",
      "77/217: pd.concat(srcs).groupby('index').sum()\n",
      "77/218:\n",
      "cross2 = uris_for_appr_200.loc[uris_for_appr_200.source.str.contains(','), ['uri_unsh_no_clean', 'appraisal']]\n",
      "cross3 = uris_for_appr_300.loc[uris_for_appr_300.source.str.contains(','), ['uri_unsh_no_clean', 'appraisal']]\n",
      "77/219:\n",
      "for s, (g, df) in zip(annot, cross2.groupby(np.arange(len(cross2)) // (len(cross2)//5 +1))):\n",
      "    print(df.shape)\n",
      "    annot[g].append(df)\n",
      "\n",
      "for s, (g, df) in zip(new_annots, cross3.groupby(np.arange(len(cross3)) // (len(cross3)//5 +1))):\n",
      "    print(df.shape)\n",
      "    annot[g].append(df)\n",
      "    \n",
      "for i, df in enumerate(annot):\n",
      "    print (i, df.shape)\n",
      "    #df.to_csv(f'data/appraisal/{g+1}.csv', index=False)\n",
      "77/220:\n",
      "for s, (g, df) in zip(annot, cross2.groupby(np.arange(len(cross2)) // (len(cross2)//5 +1))):\n",
      "    print(df.shape)\n",
      "    annot[g].append(df)\n",
      "\n",
      "for s, (g, df) in zip(new_annots, cross3.groupby(np.arange(len(cross3)) // (len(cross3)//5 +1))):\n",
      "    print(df.shape)\n",
      "    annot[g].append(df)\n",
      "    \n",
      "for i, df in enumerate(annot):\n",
      "    print (i, len(df))\n",
      "    #df.to_csv(f'data/appraisal/{g+1}.csv', index=False)\n",
      "77/221:\n",
      "for s, (g, df) in zip(annot, cross2.groupby(np.arange(len(cross2)) // (len(cross2)//5 +1))):\n",
      "    print(df.shape)\n",
      "    annot[g].append(df)\n",
      "\n",
      "for s, (g, df) in zip(new_annots, cross3.groupby(np.arange(len(cross3)) // (len(cross3)//5 +1))):\n",
      "    print(df.shape)\n",
      "    annot[g].append(df)\n",
      "    \n",
      "for i, df in enumerate(annot):\n",
      "    print (i, len(annot[df]))\n",
      "    #df.to_csv(f'data/appraisal/{g+1}.csv', index=False)\n",
      "77/222:\n",
      "n_samp = 5\n",
      "n_rows = 100\n",
      "samples = {}\n",
      "uris_for_appr_200['appraisal'] = ''\n",
      "uris_for_appr_300['appraisal'] = ''\n",
      "\n",
      "def get_samples(df, arch, n_rows=n_rows, n_samp=n_samp):\n",
      "    all_samples = df[(df.archived==arch) & (~df.source.str.contains(','))].sample(n=n_rows*n_samp, random_state=42)\n",
      "    return [all_samples.iloc[n_rows*i:n_rows*i+n_rows] for i in range(n_samp)]\n",
      "\n",
      "samples['archived_200'] = get_samples(uris_for_appr_200, True)\n",
      "samples['not_archived_200'] = get_samples(uris_for_appr_200, False)\n",
      "samples['archived_30X'] = get_samples(uris_for_appr_300, True)\n",
      "samples['not_archived_30X'] = get_samples(uris_for_appr_300, False)\n",
      "77/223:\n",
      "from collections import defaultdict\n",
      "srcs = []\n",
      "ks = ['a', 'b', 'c', 'd']\n",
      "annot = defaultdict(list)\n",
      "for k, (key, g) in zip(ks, samples.items()):\n",
      "    s = []\n",
      "    for i, samp in enumerate(g):\n",
      "        print (key, i, samp.shape)\n",
      "        srcs.append(samp.source.value_counts().reset_index())\n",
      "        annot[i].append(samp[['uri_unsh_no_clean', 'appraisal']])\n",
      "77/224: len(annot)\n",
      "77/225: pd.concat(srcs).groupby('index').sum()\n",
      "77/226:\n",
      "cross2 = uris_for_appr_200.loc[uris_for_appr_200.source.str.contains(','), ['uri_unsh_no_clean', 'appraisal']]\n",
      "cross3 = uris_for_appr_300.loc[uris_for_appr_300.source.str.contains(','), ['uri_unsh_no_clean', 'appraisal']]\n",
      "77/227:\n",
      "for s, (g, df) in zip(annot, cross2.groupby(np.arange(len(cross2)) // (len(cross2)//5 +1))):\n",
      "    print(df.shape)\n",
      "    annot[g].append(df)\n",
      "\n",
      "for s, (g, df) in zip(new_annots, cross3.groupby(np.arange(len(cross3)) // (len(cross3)//5 +1))):\n",
      "    print(df.shape)\n",
      "    annot[g].append(df)\n",
      "    \n",
      "for i, df in enumerate(annot):\n",
      "    print (i, len(annot[df]))\n",
      "    #df.to_csv(f'data/appraisal/{g+1}.csv', index=False)\n",
      "77/228:\n",
      "for s, (g, df) in zip(annot, cross2.groupby(np.arange(len(cross2)) // (len(cross2)//5 +1))):\n",
      "    print(df.shape)\n",
      "    annot[g].append(df)\n",
      "\n",
      "for s, (g, df) in zip(new_annots, cross3.groupby(np.arange(len(cross3)) // (len(cross3)//5 -1))):\n",
      "    print(df.shape)\n",
      "    annot[g].append(df)\n",
      "    \n",
      "for i, df in enumerate(annot):\n",
      "    print (i, len(annot[df]))\n",
      "    #df.to_csv(f'data/appraisal/{g+1}.csv', index=False)\n",
      "77/229:\n",
      "for s, (g, df) in zip(annot, cross2.groupby(np.arange(len(cross2)) // (len(cross2)//5 +1))):\n",
      "    print(df.shape)\n",
      "    annot[g].append(df)\n",
      "\n",
      "for s, (g, df) in zip(new_annots, cross3.groupby(np.arange(len(cross3)) // (len(cross3)//5 +10))):\n",
      "    print(df.shape)\n",
      "    annot[g].append(df)\n",
      "    \n",
      "for i, df in enumerate(annot):\n",
      "    print (i, len(annot[df]))\n",
      "    #df.to_csv(f'data/appraisal/{g+1}.csv', index=False)\n",
      "77/230:\n",
      "for s, (g, df) in zip(annot, cross2.groupby(np.arange(len(cross2)) // (len(cross2)//5 +1))):\n",
      "    print(df.shape)\n",
      "    annot[g].append(df)\n",
      "\n",
      "for s, (g, df) in zip(new_annots, cross3.groupby(np.arange(len(cross3)) // (len(cross3)//5 ))):\n",
      "    print(df.shape)\n",
      "    annot[g].append(df)\n",
      "    \n",
      "for i, df in enumerate(annot):\n",
      "    print (i, len(annot[df]))\n",
      "    #df.to_csv(f'data/appraisal/{g+1}.csv', index=False)\n",
      "77/231:\n",
      "for s, (g, df) in zip(annot, cross2.groupby(np.arange(len(cross2)) // (len(cross2)//5 +1))):\n",
      "    print(df.shape)\n",
      "    annot[g].append(df)\n",
      "\n",
      "for s, (g, df) in zip(new_annots, cross3.groupby(np.arange(len(cross3)) // (len(cross3)//5 -10))):\n",
      "    print(df.shape)\n",
      "    annot[g].append(df)\n",
      "    \n",
      "for i, df in enumerate(annot):\n",
      "    print (i, len(annot[df]))\n",
      "    #df.to_csv(f'data/appraisal/{g+1}.csv', index=False)\n",
      "77/232:\n",
      "n_samp = 5\n",
      "n_rows = 100\n",
      "samples = {}\n",
      "uris_for_appr_200['appraisal'] = ''\n",
      "uris_for_appr_300['appraisal'] = ''\n",
      "\n",
      "def get_samples(df, arch, n_rows=n_rows, n_samp=n_samp):\n",
      "    all_samples = df[(df.archived==arch) & (~df.source.str.contains(','))].sample(n=n_rows*n_samp, random_state=42)\n",
      "    return [all_samples.iloc[n_rows*i:n_rows*i+n_rows] for i in range(n_samp)]\n",
      "\n",
      "samples['archived_200'] = get_samples(uris_for_appr_200, True)\n",
      "samples['not_archived_200'] = get_samples(uris_for_appr_200, False)\n",
      "samples['archived_30X'] = get_samples(uris_for_appr_300, True)\n",
      "samples['not_archived_30X'] = get_samples(uris_for_appr_300, False)\n",
      "77/233:\n",
      "from collections import defaultdict\n",
      "srcs = []\n",
      "ks = ['a', 'b', 'c', 'd']\n",
      "annot = defaultdict(list)\n",
      "for k, (key, g) in zip(ks, samples.items()):\n",
      "    s = []\n",
      "    for i, samp in enumerate(g):\n",
      "        print (key, i, samp.shape)\n",
      "        srcs.append(samp.source.value_counts().reset_index())\n",
      "        annot[i].append(samp[['uri_unsh_no_clean', 'appraisal']])\n",
      "77/234: len(annot)\n",
      "77/235: pd.concat(srcs).groupby('index').sum()\n",
      "77/236:\n",
      "cross2 = uris_for_appr_200.loc[uris_for_appr_200.source.str.contains(','), ['uri_unsh_no_clean', 'appraisal']]\n",
      "cross3 = uris_for_appr_300.loc[uris_for_appr_300.source.str.contains(','), ['uri_unsh_no_clean', 'appraisal']]\n",
      "77/237:\n",
      "for s, (g, df) in zip(annot, cross2.groupby(np.arange(len(cross2)) // (len(cross2)//5 +1))):\n",
      "    print(df.shape)\n",
      "    annot[g].append(df)\n",
      "\n",
      "for s, (g, df) in zip(annot, cross3.groupby(np.arange(len(cross3)) // (len(cross3)//5 +1))):\n",
      "    print(df.shape)\n",
      "    annot[g].append(df)\n",
      "    \n",
      "for i, df in enumerate(annot):\n",
      "    print (i, len(annot[df]))\n",
      "    #df.to_csv(f'data/appraisal/{g+1}.csv', index=False)\n",
      "77/238:\n",
      "for key, dfs in enumerate(annot):\n",
      "    print (i, len(dfs))\n",
      "    pd.concat(dfs).to_csv(f'data/appraisal/{key+1}.csv', index=False)\n",
      "77/239:\n",
      "for key, dfs in annot.items():\n",
      "    print (i, len(dfs))\n",
      "    pd.concat(dfs).to_csv(f'data/appraisal/{key+1}.csv', index=False)\n",
      "77/240:\n",
      "n_samp = 5\n",
      "n_rows = 50\n",
      "samples = {}\n",
      "uris_for_appr_200['appraisal'] = ''\n",
      "uris_for_appr_300['appraisal'] = ''\n",
      "\n",
      "def get_samples(df, arch, n_rows=n_rows, n_samp=n_samp):\n",
      "    all_samples = df[(df.archived==arch) & (~df.source.str.contains(','))].sample(n=n_rows*n_samp, random_state=42)\n",
      "    return [all_samples.iloc[n_rows*i:n_rows*i+n_rows] for i in range(n_samp)]\n",
      "\n",
      "samples['archived_200'] = get_samples(uris_for_appr_200, True)\n",
      "samples['not_archived_200'] = get_samples(uris_for_appr_200, False)\n",
      "samples['archived_30X'] = get_samples(uris_for_appr_300, True)\n",
      "samples['not_archived_30X'] = get_samples(uris_for_appr_300, False)\n",
      "77/241:\n",
      "from collections import defaultdict\n",
      "srcs = []\n",
      "ks = ['a', 'b', 'c', 'd']\n",
      "annot = defaultdict(list)\n",
      "for k, (key, g) in zip(ks, samples.items()):\n",
      "    s = []\n",
      "    for i, samp in enumerate(g):\n",
      "        print (key, i, samp.shape)\n",
      "        srcs.append(samp.source.value_counts().reset_index())\n",
      "        annot[i].append(samp[['uri_unsh_no_clean', 'appraisal']])\n",
      "77/242: len(annot)\n",
      "77/243: pd.concat(srcs).groupby('index').sum()\n",
      "77/244:\n",
      "cross2 = uris_for_appr_200.loc[uris_for_appr_200.source.str.contains(','), ['uri_unsh_no_clean', 'appraisal']]\n",
      "cross3 = uris_for_appr_300.loc[uris_for_appr_300.source.str.contains(','), ['uri_unsh_no_clean', 'appraisal']]\n",
      "77/245:\n",
      "for s, (g, df) in zip(annot, cross2.groupby(np.arange(len(cross2)) // (len(cross2)//5 +1))):\n",
      "    print(df.shape)\n",
      "    annot[g].append(df)\n",
      "\n",
      "for s, (g, df) in zip(annot, cross3.groupby(np.arange(len(cross3)) // (len(cross3)//5 +1))):\n",
      "    print(df.shape)\n",
      "    annot[g].append(df)\n",
      "77/246:\n",
      "for key, dfs in annot.items():\n",
      "    print (i, len(dfs))\n",
      "    pd.concat(dfs).to_csv(f'data/appraisal/{key+1}.csv', index=False)\n",
      "77/247:\n",
      "n_samp = 5\n",
      "n_rows = 50\n",
      "samples = {}\n",
      "uris_for_appr_200['appraisal'] = ''\n",
      "uris_for_appr_300['appraisal'] = ''\n",
      "\n",
      "def get_samples(df, arch, n_rows=n_rows, n_samp=n_samp):\n",
      "    all_samples = df[(df.archived==arch) & (~df.source.str.contains(','))].sample(n=n_rows*n_samp, random_state=42)\n",
      "    return [all_samples.iloc[n_rows*i:n_rows*i+n_rows] for i in range(n_samp)]\n",
      "\n",
      "samples['archived_200'] = get_samples(uris_for_appr_200, True)\n",
      "samples['not_archived_200'] = get_samples(uris_for_appr_200, False)\n",
      "samples['archived_30X'] = get_samples(uris_for_appr_300, True)\n",
      "samples['not_archived_30X'] = get_samples(uris_for_appr_300, False)\n",
      "77/248:\n",
      "from collections import defaultdict\n",
      "srcs = []\n",
      "ks = ['a', 'b', 'c', 'd']\n",
      "annot = defaultdict(list)\n",
      "for k, (key, g) in zip(ks, samples.items()):\n",
      "    s = []\n",
      "    for i, samp in enumerate(g):\n",
      "        print (key, i, samp.shape)\n",
      "        srcs.append(samp.source.value_counts().reset_index())\n",
      "        annot[i].append(samp[['uri_unsh_no_clean', 'appraisal']])\n",
      "77/249: len(annot)\n",
      "77/250: pd.concat(srcs).groupby('index').sum()\n",
      "77/251:\n",
      "cross2 = uris_for_appr_200.loc[uris_for_appr_200.source.str.contains(','), ['uri_unsh_no_clean', 'appraisal']].shuffle(frac=1)\n",
      "cross3 = uris_for_appr_300.loc[uris_for_appr_300.source.str.contains(','), ['uri_unsh_no_clean', 'appraisal']].shuffle(frac=1)\n",
      "77/252:\n",
      "for s, (g, df) in zip(annot, cross2.groupby(np.arange(len(cross2)) // (len(cross2)//5 +1))):\n",
      "    print(df.shape)\n",
      "    annot[g].append(df)\n",
      "\n",
      "for s, (g, df) in zip(annot, cross3.groupby(np.arange(len(cross3)) // (len(cross3)//5 +1))):\n",
      "    print(df.shape)\n",
      "    annot[g].append(df)\n",
      "77/253:\n",
      "n_samp = 5\n",
      "n_rows = 50\n",
      "samples = {}\n",
      "uris_for_appr_200['appraisal'] = ''\n",
      "uris_for_appr_300['appraisal'] = ''\n",
      "\n",
      "def get_samples(df, arch, n_rows=n_rows, n_samp=n_samp):\n",
      "    all_samples = df[(df.archived==arch) & (~df.source.str.contains(','))].sample(n=n_rows*n_samp, random_state=42)\n",
      "    return [all_samples.iloc[n_rows*i:n_rows*i+n_rows] for i in range(n_samp)]\n",
      "\n",
      "samples['archived_200'] = get_samples(uris_for_appr_200, True)\n",
      "samples['not_archived_200'] = get_samples(uris_for_appr_200, False)\n",
      "samples['archived_30X'] = get_samples(uris_for_appr_300, True)\n",
      "samples['not_archived_30X'] = get_samples(uris_for_appr_300, False)\n",
      "77/254:\n",
      "from collections import defaultdict\n",
      "srcs = []\n",
      "ks = ['a', 'b', 'c', 'd']\n",
      "annot = defaultdict(list)\n",
      "for k, (key, g) in zip(ks, samples.items()):\n",
      "    s = []\n",
      "    for i, samp in enumerate(g):\n",
      "        print (key, i, samp.shape)\n",
      "        srcs.append(samp.source.value_counts().reset_index())\n",
      "        annot[i].append(samp[['uri_unsh_no_clean', 'appraisal']])\n",
      "77/255: len(annot)\n",
      "77/256: pd.concat(srcs).groupby('index').sum()\n",
      "77/257:\n",
      "cross2 = uris_for_appr_200.loc[uris_for_appr_200.source.str.contains(','), ['uri_unsh_no_clean', 'appraisal']].sample(frac=1, random_state=42)\n",
      "cross3 = uris_for_appr_300.loc[uris_for_appr_300.source.str.contains(','), ['uri_unsh_no_clean', 'appraisal']].sample(frac=1, random_state=42)\n",
      "77/258:\n",
      "for s, (g, df) in zip(annot, cross2.groupby(np.arange(len(cross2)) // (len(cross2)//5 +1))):\n",
      "    print(df.shape)\n",
      "    annot[g].append(df)\n",
      "\n",
      "for s, (g, df) in zip(annot, cross3.groupby(np.arange(len(cross3)) // (len(cross3)//5 +1))):\n",
      "    print(df.shape)\n",
      "    annot[g].append(df)\n",
      "77/259:\n",
      "for key, dfs in annot.items():\n",
      "    print (i, len(dfs))\n",
      "    pd.concat(dfs).to_csv(f'data/appraisal/{key+1}.csv', index=False)\n",
      "77/260:\n",
      "names = ['Anat', 'Dan', 'Nehoray', 'Nurit', 'Tzipy']\n",
      "for name, (key, dfs) in zip(names, annot.items()):\n",
      "    print (i, len(dfs))\n",
      "    pd.concat(dfs).to_csv(f'data/appraisal/{Name}.csv', index=False)\n",
      "77/261:\n",
      "names = ['Anat', 'Dan', 'Nehoray', 'Nurit', 'Tzipy']\n",
      "for name, (key, dfs) in zip(names, annot.items()):\n",
      "    print (i, len(dfs))\n",
      "    pd.concat(dfs).to_csv(f'data/appraisal/{name}.csv', index=False)\n",
      "77/262:\n",
      "names = ['Anat', 'Dan', 'Nehoray', 'Nurit', 'Tzipy']\n",
      "for name, (key, dfs) in zip(names, annot.items()):\n",
      "    print (i, len(dfs))\n",
      "    pd.concat(dfs).sample(frac=1, random_state=42).to_csv(f'data/appraisal/{name}.csv', index=False)\n",
      "77/263:\n",
      "for uri in lut.uri_unsh_no_clean.head().unique():\n",
      "    print uri\n",
      "77/264:\n",
      "for uri in lut.uri_unsh_no_clean.head().unique():\n",
      "    print (uri)\n",
      "77/265: import savepagenow as spn\n",
      "77/266:\n",
      "cached = []\n",
      "for uri in lut.uri_unsh_no_clean.head().unique():\n",
      "    cached.append(spn.capture_or_cache(uri))\n",
      "77/267: to_cache = list(lut.uri_unsh_no_clean.head().unique())\n",
      "77/268:\n",
      "cached = []\n",
      "i=0\n",
      "77/269:\n",
      "for uri in to_cache[i:]:\n",
      "    try:\n",
      "        cached.append(spn.capture_or_cache(uri))\n",
      "        i+=1\n",
      "    except WaybackRuntimeError:\n",
      "        cached.append((None, 'WaybackRuntimeError'))\n",
      "77/270:\n",
      "for uri in to_cache[i:]:\n",
      "    try:\n",
      "        cached.append(spn.capture_or_cache(uri))\n",
      "        i+=1\n",
      "    except spn.WaybackRuntimeError:\n",
      "        cached.append((None, 'WaybackRuntimeError'))\n",
      "77/271:\n",
      "for uri in to_cache[i:]:\n",
      "    try:\n",
      "        cached.append(spn.capture_or_cache(uri))\n",
      "        i+=1\n",
      "    except spn.api.WaybackRuntimeError:\n",
      "        cached.append((None, 'WaybackRuntimeError'))\n",
      "77/272: i\n",
      "77/273:\n",
      "cached = []\n",
      "i=0\n",
      "77/274: to_cache = list(lut.uri_unsh_no_clean).unique())\n",
      "77/275: to_cache = list(lut.uri_unsh_no_clean).unique()\n",
      "77/276: to_cache = list(lut.uri_unsh_no_clean.unique())\n",
      "77/277:\n",
      "for uri in to_cache[i:]:\n",
      "    try:\n",
      "        cached.append(spn.capture_or_cache(uri))\n",
      "        i+=1\n",
      "    except spn.api.WaybackRuntimeError:\n",
      "        cached.append((None, 'WaybackRuntimeError'))\n",
      "77/278:\n",
      "for uri in to_cache[i:]:\n",
      "    try:\n",
      "        cached.append(spn.capture_or_cache(uri))\n",
      "        i+=1\n",
      "    except spn.api.WaybackRuntimeError:\n",
      "        cached.append((None, 'WaybackRuntimeError'))\n",
      "77/279:\n",
      "import requests\n",
      "requests.TooManyRedirects\n",
      "77/280: from requests import TooManyRedirects\n",
      "77/281:\n",
      "for uri in to_cache[i:]:\n",
      "    try:\n",
      "        cached.append(spn.capture_or_cache(uri))\n",
      "        i+=1\n",
      "    except spn.api.WaybackRuntimeError:\n",
      "        cached.append((None, 'WaybackRuntimeError'))\n",
      "    except TooManyRedirects:\n",
      "        cached.append((None, 'TooManyRedirects'))\n",
      "81/1: import pandas as pd\n",
      "81/2:\n",
      "from os import listdir\n",
      "\n",
      "listdir('data/appraisal/filled')\n",
      "81/3: apps = [pd.read_excel(join('data/appraisal/filled', f)) for f in listdir listdir('data/appraisal/filled')]\n",
      "81/4: apps = [pd.read_excel(join('data/appraisal/filled', f)) for f in listdir('data/appraisal/filled')]\n",
      "81/5:\n",
      "from os import listdir\n",
      "from os.path import join\n",
      "\n",
      "listdir('data/appraisal/filled')\n",
      "81/6: apps = [pd.read_excel(join('data/appraisal/filled', f)) for f in listdir('data/appraisal/filled')]\n",
      "81/7: apps[1]\n",
      "81/8: apps[0].head()\n",
      "81/9: apps = [pd.read_excel(join('data/appraisal/filled', f), usecols=(0,1,2), names=('uri_unsh_no_clean','appraisal','comments') for f in listdir('data/appraisal/filled')]\n",
      "81/10: apps = [pd.read_excel(join('data/appraisal/filled', f), usecols=(0,1,2), names=('uri_unsh_no_clean','appraisal','comments')) for f in listdir('data/appraisal/filled')]\n",
      "81/11: apps[0].head()\n",
      "81/12: apps = pd.concat([pd.read_excel(join('data/appraisal/filled', f), usecols=(0,1,2), names=('uri_unsh_no_clean','appraisal','comments')) for f in listdir('data/appraisal/filled')])\n",
      "81/13: apps[0].head()\n",
      "81/14: apps\n",
      "81/15: apps['appraisal'] = apps['appraisal'].astyle(bool)\n",
      "81/16: apps['appraisal'] = apps['appraisal'].astype(bool)\n",
      "81/17: apps.appraisal.value_counts()\n",
      "81/18: apps.appraisal.value_counts() / apps.shape[0]\n",
      "81/19: apps.comments.value_counts()\n",
      "81/20:\n",
      "lut = pd.read_pickle('lang_uri_time_wb_avail_all.pkl.gz', compression='gzip')\n",
      "macd_all = pd.read_pickle('data/macd_all.pkl.gz', compression='gzip')\n",
      "lut['uri_unsh_no_clean'] = lut.uri_unshrtn_raw.combine_first(lut.uri_unshrtn_bitly.combine_first(lut.uri_unshrtn.combine_first(lut.uri)))\n",
      "lut['uri_mem_avail'] = lut.uri.map(~macd_all.drop_duplicates(subset='uri').set_index('uri').first_snap.isna()).fillna(False)\n",
      "lut['uri_unsh_no_clean_mem_avail'] = lut.uri_unsh_no_clean.map(~macd_all.drop_duplicates(subset='uri').set_index('uri').first_snap.isna()).fillna(False)\n",
      "curl = pd.read_csv('data/curl_results_parsed.csv.gz', compression='gzip')\n",
      "lut['status_code'] = lut.uri_unsh_no_clean.map(curl.drop_duplicates(subset='url').set_index('url').status_code)\n",
      "lut['content_type'] = lut.uri_unsh_no_clean.map(curl.drop_duplicates(subset='url').set_index('url').content_type)\n",
      "lut.head().T\n",
      "81/21:\n",
      "lap = lut.merge(apps, how='inner')\n",
      "lap.uri_unsh_no_clean.value_counts()\n",
      "81/22: lap.groupby(['source', 'appraisal']).uri_unsh_no_clean.nunique()\n",
      "81/23: lap.groupby(['source', 'appraisal']).uri_unsh_no_clean.nunique().unstack()\n",
      "81/24: lap.groupby(['source', 'appraisal']).uri_unsh_no_clean.nunique().unstack().assiagn(ratio = lambda x: x[False], x[True])\n",
      "81/25: lap.groupby(['source', 'appraisal']).uri_unsh_no_clean.nunique().unstack().assiagn(ratio = lambda x: x[True] /(x[False]+x[True]))\n",
      "81/26: lap.groupby(['source', 'appraisal']).uri_unsh_no_clean.nunique().unstack().assign(ratio = lambda x: x[True] /(x[False]+x[True]))\n",
      "81/27:\n",
      "def get_values(g):\n",
      "    return ','.join(list(g.unique()))\n",
      "tlds_to_remove = ['fb.me', 'facebook.com', 'youtu.be', 'youtube.com']\n",
      "uris_for_appr = lut.groupby(['uri_unsh_no_clean', 'tld_unshrtn', 'status_code']).agg({'archived': np.max, 'source': get_values}).reset_index()\n",
      "81/28: import numpy as np\n",
      "81/29:\n",
      "def get_values(g):\n",
      "    return ','.join(list(g.unique()))\n",
      "tlds_to_remove = ['fb.me', 'facebook.com', 'youtu.be', 'youtube.com']\n",
      "uris_for_appr = lut.groupby(['uri_unsh_no_clean', 'tld_unshrtn', 'status_code']).agg({'archived': np.max, 'source': get_values}).reset_index()\n",
      "81/30: lut['archived'] = lut[['uri_cln_wb_avail', 'uri_cln_unshrtn_wb_avail', 'uri_mem_avail', 'uri_unsh_no_clean_mem_avail']].any(axis=1)\n",
      "81/31:\n",
      "def get_values(g):\n",
      "    return ','.join(list(g.unique()))\n",
      "tlds_to_remove = ['fb.me', 'facebook.com', 'youtu.be', 'youtube.com']\n",
      "uris_for_appr = lut.groupby(['uri_unsh_no_clean', 'tld_unshrtn', 'status_code']).agg({'archived': np.max, 'source': get_values}).reset_index()\n",
      "81/32:\n",
      "def get_values(g):\n",
      "    return ','.join(list(g.unique()))\n",
      "tlds_to_remove = ['fb.me', 'facebook.com', 'youtu.be', 'youtube.com']\n",
      "uris_for_appr = lut[(~lut.tld_unshrtn.isin(tlds_to_remove))].groupby(['uri_unsh_no_clean', 'tld_unshrtn', 'status_code']).agg({'archived': np.max, 'source': get_values}).reset_index()\n",
      "81/33: uris_for_appr = uris_for_appr[uris_for_appr.uri_unsh_no_clean.isin(apps.uri_unsh_no_clean)]\n",
      "81/34: uris_for_appr\n",
      "81/35: uris_for_appr = uris_for_appr.merge(apps)\n",
      "81/36:\n",
      "def get_values(g):\n",
      "    return ','.join(list(g.unique()))\n",
      "tlds_to_remove = ['fb.me', 'facebook.com', 'youtu.be', 'youtube.com']\n",
      "uris_for_appr = lut[(lut.uri_unsh_no_clean.isin(apps.uri_unsh_no_clean)) & (~lut.tld_unshrtn.isin(tlds_to_remove))].groupby(['uri_unsh_no_clean', 'tld_unshrtn', 'status_code']).agg({'archived': np.max, 'source': get_values}).reset_index()\n",
      "81/37: uris_for_appr = uris_for_appr.merge(apps)\n",
      "81/38: uris_for_appr.head()\n",
      "81/39: uris_for_appr.groupby('source').head()\n",
      "81/40: uris_for_appr.groupby('source').appraisal.sum()\n",
      "81/41: uris_for_appr.groupby(['source', 'appraisal']).unstack().assign(ratio = lambda x: x[True] /(x[False]+x[True]))\n",
      "81/42: uris_for_appr.groupby(['source', 'appraisal'])size().unstack().assign(ratio = lambda x: x[True] /(x[False]+x[True]))\n",
      "81/43: uris_for_appr.groupby(['source', 'appraisal']).size().unstack().assign(ratio = lambda x: x[True] /(x[False]+x[True]))\n",
      "81/44: uris_for_appr.groupby(['source', 'appraisal']).size().unstack().assign(ratio = lambda x: x[True] /(x[False]+x[True])).fillna(0)\n",
      "81/45: uris_for_appr.to_csv('data/appraisa/appraisal_join.csv', index=False)\n",
      "81/46: uris_for_appr.to_csv('data/appraisal/appraisal_join.csv', index=False)\n",
      "81/47:\n",
      "(uris_for_appr\n",
      " .groupby(['source', 'appraisal']).size().unstack().fillna(0)\n",
      " .astype(int)\n",
      " .assign(ratio = lambda x: (x[True] /(x[False]+x[True]).round(2)))\n",
      "81/48:\n",
      "(uris_for_appr\n",
      " .groupby(['source', 'appraisal']).size().unstack().fillna(0)\n",
      " .astype(int)\n",
      " .assign(ratio = lambda x: (x[True] /(x[False]+x[True]).round(2)))\n",
      ")\n",
      "81/49:\n",
      "(uris_for_appr\n",
      " .groupby(['source', 'appraisal']).size().unstack().fillna(0)\n",
      " .astype(int)\n",
      " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
      ")\n",
      "81/50:\n",
      "(lap.groupby(['source', 'appraisal']).uri_unsh_no_clean.nunique()\n",
      " .unstack().astype(int)\n",
      " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
      ")\n",
      "81/51:\n",
      "(lap.groupby(['shortened', 'appraisal']).uri_unsh_no_clean.nunique()\n",
      " .unstack().astype(int)\n",
      " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
      ")\n",
      "81/52:\n",
      "(lap.groupby(['is_shortened', 'appraisal']).uri_unsh_no_clean.nunique()\n",
      " .unstack().astype(int)\n",
      " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
      ")\n",
      "81/53:\n",
      "(lap.groupby(['status_code', 'appraisal']).uri_unsh_no_clean.nunique()\n",
      " .unstack().astype(int)\n",
      " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
      ")\n",
      "81/54:\n",
      "(uris_for_appr\n",
      " .groupby(['status_code', 'appraisal']).size().unstack().fillna(0)\n",
      " .astype(int)\n",
      " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
      ")\n",
      "81/55:\n",
      "(uris_for_appr[uris_for_appr.str.contains(',')]\n",
      " .groupby(['appraisal']).size().unstack()\n",
      ")\n",
      "81/56:\n",
      "(uris_for_appr[uris_for_appr.source.str.contains(',')]\n",
      " .groupby(['appraisal']).size().unstack()\n",
      ")\n",
      "81/57:\n",
      "(uris_for_appr[uris_for_appr.source.str.contains(',')]\n",
      " .groupby(['appraisal']).size()\n",
      ")\n",
      "81/58: ((uris_for_appr.source.str.contains(',')) & (uris_for_appr.appraisal)).sum() / uris_for_appr.source.str.contains(',').sum()\n",
      "81/59:\n",
      "((uris_for_appr.source.str.contains(',')) & (uris_for_appr.appraisal)).sum() / uris_for_appr.source.str.contains(',').sum()\n",
      "((~uris_for_appr.source.str.contains(',')) & (uris_for_appr.appraisal)).sum() / (~uris_for_appr.source.str.contains(',')).sum()\n",
      "81/60:\n",
      "shared = ((uris_for_appr.source.str.contains(',')) & (uris_for_appr.appraisal)).sum() / uris_for_appr.source.str.contains(',').sum()\n",
      "not_shared = ((~uris_for_appr.source.str.contains(',')) & (uris_for_appr.appraisal)).sum() / (~uris_for_appr.source.str.contains(',')).sum()\n",
      "shared, not_shared\n",
      "81/61:\n",
      "lut['is_unshrtn'] = lut.uri_cln_unshrtn!=lut.uri_cln\n",
      "(lap.groupby(['is_unshrtn', 'appraisal']).uri_unsh_no_clean.nunique()\n",
      " .unstack().astype(int)\n",
      " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
      ")\n",
      "81/62:\n",
      "lap['is_unshrtn'] = lap.uri_cln_unshrtn!=lap.uri_cln\n",
      "(lap.groupby(['is_unshrtn', 'appraisal']).uri_unsh_no_clean.nunique()\n",
      " .unstack().astype(int)\n",
      " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
      ")\n",
      "81/63:\n",
      "(uris_for_appr\n",
      " .groupby(['source', 'comment']).size().unstack().fillna(0)\n",
      " .astype(int)\n",
      " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
      ")\n",
      "81/64:\n",
      "(uris_for_appr\n",
      " .groupby(['source', 'comment']).size().unstack().fillna(0)\n",
      ")\n",
      "81/65: uris_for_appr\n",
      "81/66:\n",
      "(uris_for_appr\n",
      " .groupby(['source', 'comments']).size().unstack().fillna(0)\n",
      ")\n",
      "81/67: apps['comments'] = apps.comments.str.lower()\n",
      "81/68:\n",
      "(lap.groupby(['source', 'appraisal']).uri_unsh_no_clean.nunique()\n",
      " .unstack().astype(int)\n",
      " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
      ")\n",
      "81/69:\n",
      "(lap.groupby(['is_shortened', 'appraisal']).uri_unsh_no_clean.nunique()\n",
      " .unstack().astype(int)\n",
      " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
      ")\n",
      "81/70:\n",
      "lap['is_unshrtn'] = lap.uri_cln_unshrtn!=lap.uri_cln\n",
      "(lap.groupby(['is_unshrtn', 'appraisal']).uri_unsh_no_clean.nunique()\n",
      " .unstack().astype(int)\n",
      " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
      ")\n",
      "81/71: import numpy as np\n",
      "81/72: lut['archived'] = lut[['uri_cln_wb_avail', 'uri_cln_unshrtn_wb_avail', 'uri_mem_avail', 'uri_unsh_no_clean_mem_avail']].any(axis=1)\n",
      "81/73:\n",
      "def get_values(g):\n",
      "    return ','.join(list(g.unique()))\n",
      "tlds_to_remove = ['fb.me', 'facebook.com', 'youtu.be', 'youtube.com']\n",
      "uris_for_appr = lut[(lut.uri_unsh_no_clean.isin(apps.uri_unsh_no_clean)) & (~lut.tld_unshrtn.isin(tlds_to_remove))].groupby(['uri_unsh_no_clean', 'tld_unshrtn', 'status_code']).agg({'archived': np.max, 'source': get_values}).reset_index()\n",
      "81/74: uris_for_appr = uris_for_appr.merge(apps)\n",
      "81/75: uris_for_appr.head()\n",
      "81/76: uris_for_appr.to_csv('data/appraisal/appraisal_join.csv', index=False)\n",
      "81/77:\n",
      "(uris_for_appr\n",
      " .groupby(['source', 'appraisal']).size().unstack().fillna(0)\n",
      " .astype(int)\n",
      " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
      ")\n",
      "81/78:\n",
      "shared = ((uris_for_appr.source.str.contains(',')) & (uris_for_appr.appraisal)).sum() / uris_for_appr.source.str.contains(',').sum()\n",
      "not_shared = ((~uris_for_appr.source.str.contains(',')) & (uris_for_appr.appraisal)).sum() / (~uris_for_appr.source.str.contains(',')).sum()\n",
      "shared, not_shared\n",
      "81/79:\n",
      "(uris_for_appr\n",
      " .groupby(['status_code', 'appraisal']).size().unstack().fillna(0)\n",
      " .astype(int)\n",
      " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
      ")\n",
      "81/80:\n",
      "(uris_for_appr\n",
      " .groupby(['source', 'comments']).size().unstack().fillna(0)\n",
      ")\n",
      "81/81:\n",
      "(uris_for_appr\n",
      " .groupby(['source', 'comments']).size().unstack().fillna(0).astype(int)\n",
      ")\n",
      "81/82:\n",
      "apps['comments'] = apps.comments.str.lower()\n",
      "apps.loc[apps.comments=='walled garden', 'comments']\n",
      "81/83:\n",
      "apps['comments'] = apps.comments.str.lower()\n",
      "apps.loc[apps.comments=='wall garden', 'comments'] = 'walled_garden'\n",
      "apps.loc[apps.comments=='early date', 'comments'] = 'earlier date'\n",
      "apps.loc[apps.comments=='irrelrvant', 'comments'] = 'irrelevant'\n",
      "81/84:\n",
      "lut = pd.read_pickle('lang_uri_time_wb_avail_all.pkl.gz', compression='gzip')\n",
      "macd_all = pd.read_pickle('data/macd_all.pkl.gz', compression='gzip')\n",
      "lut['uri_unsh_no_clean'] = lut.uri_unshrtn_raw.combine_first(lut.uri_unshrtn_bitly.combine_first(lut.uri_unshrtn.combine_first(lut.uri)))\n",
      "lut['uri_mem_avail'] = lut.uri.map(~macd_all.drop_duplicates(subset='uri').set_index('uri').first_snap.isna()).fillna(False)\n",
      "lut['uri_unsh_no_clean_mem_avail'] = lut.uri_unsh_no_clean.map(~macd_all.drop_duplicates(subset='uri').set_index('uri').first_snap.isna()).fillna(False)\n",
      "curl = pd.read_csv('data/curl_results_parsed.csv.gz', compression='gzip')\n",
      "lut['status_code'] = lut.uri_unsh_no_clean.map(curl.drop_duplicates(subset='url').set_index('url').status_code)\n",
      "lut['content_type'] = lut.uri_unsh_no_clean.map(curl.drop_duplicates(subset='url').set_index('url').content_type)\n",
      "lut.head().T\n",
      "81/85:\n",
      "lap = lut.merge(apps, how='inner')\n",
      "lap.uri_unsh_no_clean.value_counts()\n",
      "81/86:\n",
      "(lap.groupby(['source', 'appraisal']).uri_unsh_no_clean.nunique()\n",
      " .unstack().astype(int)\n",
      " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
      ")\n",
      "81/87:\n",
      "(lap.groupby(['is_shortened', 'appraisal']).uri_unsh_no_clean.nunique()\n",
      " .unstack().astype(int)\n",
      " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
      ")\n",
      "81/88:\n",
      "lap['is_unshrtn'] = lap.uri_cln_unshrtn!=lap.uri_cln\n",
      "(lap.groupby(['is_unshrtn', 'appraisal']).uri_unsh_no_clean.nunique()\n",
      " .unstack().astype(int)\n",
      " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
      ")\n",
      "81/89: import numpy as np\n",
      "81/90: lut['archived'] = lut[['uri_cln_wb_avail', 'uri_cln_unshrtn_wb_avail', 'uri_mem_avail', 'uri_unsh_no_clean_mem_avail']].any(axis=1)\n",
      "81/91:\n",
      "def get_values(g):\n",
      "    return ','.join(list(g.unique()))\n",
      "tlds_to_remove = ['fb.me', 'facebook.com', 'youtu.be', 'youtube.com']\n",
      "uris_for_appr = lut[(lut.uri_unsh_no_clean.isin(apps.uri_unsh_no_clean)) & (~lut.tld_unshrtn.isin(tlds_to_remove))].groupby(['uri_unsh_no_clean', 'tld_unshrtn', 'status_code']).agg({'archived': np.max, 'source': get_values}).reset_index()\n",
      "81/92: uris_for_appr = uris_for_appr.merge(apps)\n",
      "81/93: uris_for_appr.head()\n",
      "81/94: uris_for_appr.to_csv('data/appraisal/appraisal_join.csv', index=False)\n",
      "81/95:\n",
      "(uris_for_appr\n",
      " .groupby(['source', 'appraisal']).size().unstack().fillna(0)\n",
      " .astype(int)\n",
      " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
      ")\n",
      "81/96:\n",
      "shared = ((uris_for_appr.source.str.contains(',')) & (uris_for_appr.appraisal)).sum() / uris_for_appr.source.str.contains(',').sum()\n",
      "not_shared = ((~uris_for_appr.source.str.contains(',')) & (uris_for_appr.appraisal)).sum() / (~uris_for_appr.source.str.contains(',')).sum()\n",
      "shared, not_shared\n",
      "81/97:\n",
      "(uris_for_appr\n",
      " .groupby(['status_code', 'appraisal']).size().unstack().fillna(0)\n",
      " .astype(int)\n",
      " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
      ")\n",
      "81/98:\n",
      "(uris_for_appr\n",
      " .groupby(['source', 'comments']).size().unstack().fillna(0).astype(int)\n",
      ")\n",
      "81/99:\n",
      "apps['comments'] = apps.comments.str.lower()\n",
      "apps.loc[apps.comments=='wall garden', 'comments'] = 'walled garden'\n",
      "apps.loc[apps.comments=='early date', 'comments'] = 'earlier date'\n",
      "apps.loc[apps.comments=='irrelrvant', 'comments'] = 'irrelevant'\n",
      "81/100:\n",
      "lut = pd.read_pickle('lang_uri_time_wb_avail_all.pkl.gz', compression='gzip')\n",
      "macd_all = pd.read_pickle('data/macd_all.pkl.gz', compression='gzip')\n",
      "lut['uri_unsh_no_clean'] = lut.uri_unshrtn_raw.combine_first(lut.uri_unshrtn_bitly.combine_first(lut.uri_unshrtn.combine_first(lut.uri)))\n",
      "lut['uri_mem_avail'] = lut.uri.map(~macd_all.drop_duplicates(subset='uri').set_index('uri').first_snap.isna()).fillna(False)\n",
      "lut['uri_unsh_no_clean_mem_avail'] = lut.uri_unsh_no_clean.map(~macd_all.drop_duplicates(subset='uri').set_index('uri').first_snap.isna()).fillna(False)\n",
      "curl = pd.read_csv('data/curl_results_parsed.csv.gz', compression='gzip')\n",
      "lut['status_code'] = lut.uri_unsh_no_clean.map(curl.drop_duplicates(subset='url').set_index('url').status_code)\n",
      "lut['content_type'] = lut.uri_unsh_no_clean.map(curl.drop_duplicates(subset='url').set_index('url').content_type)\n",
      "lut.head().T\n",
      "81/101:\n",
      "lap = lut.merge(apps, how='inner')\n",
      "lap.uri_unsh_no_clean.value_counts()\n",
      "81/102:\n",
      "(lap.groupby(['source', 'appraisal']).uri_unsh_no_clean.nunique()\n",
      " .unstack().astype(int)\n",
      " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
      ")\n",
      "81/103:\n",
      "(lap.groupby(['is_shortened', 'appraisal']).uri_unsh_no_clean.nunique()\n",
      " .unstack().astype(int)\n",
      " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
      ")\n",
      "81/104:\n",
      "lap['is_unshrtn'] = lap.uri_cln_unshrtn!=lap.uri_cln\n",
      "(lap.groupby(['is_unshrtn', 'appraisal']).uri_unsh_no_clean.nunique()\n",
      " .unstack().astype(int)\n",
      " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
      ")\n",
      "81/105: import numpy as np\n",
      "81/106: lut['archived'] = lut[['uri_cln_wb_avail', 'uri_cln_unshrtn_wb_avail', 'uri_mem_avail', 'uri_unsh_no_clean_mem_avail']].any(axis=1)\n",
      "81/107:\n",
      "def get_values(g):\n",
      "    return ','.join(list(g.unique()))\n",
      "tlds_to_remove = ['fb.me', 'facebook.com', 'youtu.be', 'youtube.com']\n",
      "uris_for_appr = lut[(lut.uri_unsh_no_clean.isin(apps.uri_unsh_no_clean)) & (~lut.tld_unshrtn.isin(tlds_to_remove))].groupby(['uri_unsh_no_clean', 'tld_unshrtn', 'status_code']).agg({'archived': np.max, 'source': get_values}).reset_index()\n",
      "81/108: uris_for_appr = uris_for_appr.merge(apps)\n",
      "81/109: uris_for_appr.head()\n",
      "81/110: uris_for_appr.to_csv('data/appraisal/appraisal_join.csv', index=False)\n",
      "81/111:\n",
      "(uris_for_appr\n",
      " .groupby(['source', 'appraisal']).size().unstack().fillna(0)\n",
      " .astype(int)\n",
      " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
      ")\n",
      "81/112:\n",
      "shared = ((uris_for_appr.source.str.contains(',')) & (uris_for_appr.appraisal)).sum() / uris_for_appr.source.str.contains(',').sum()\n",
      "not_shared = ((~uris_for_appr.source.str.contains(',')) & (uris_for_appr.appraisal)).sum() / (~uris_for_appr.source.str.contains(',')).sum()\n",
      "shared, not_shared\n",
      "81/113:\n",
      "(uris_for_appr\n",
      " .groupby(['status_code', 'appraisal']).size().unstack().fillna(0)\n",
      " .astype(int)\n",
      " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
      ")\n",
      "81/114:\n",
      "(uris_for_appr\n",
      " .groupby(['source', 'comments']).size().unstack().fillna(0).astype(int)\n",
      ")\n",
      "81/115: apps.comments.value_counts()\n",
      "81/116: apps = pd.concat([pd.read_excel(join('data/appraisal/filled', f), usecols=(0,1,2), names=('uri_unsh_no_clean','appraisal','comments')) for f in listdir('data/appraisal/filled')])\n",
      "81/117: apps['appraisal'] = apps['appraisal'].astype(bool)\n",
      "81/118: apps.appraisal.value_counts() / apps.shape[0]\n",
      "81/119: apps.comments.value_counts()\n",
      "81/120:\n",
      "apps['comments'] = apps.comments.str.lower()\n",
      "apps.loc[apps.comments=='wall garden', 'comments'] = 'walled garden'\n",
      "apps.loc[apps.comments=='early date', 'comments'] = 'earlier date'\n",
      "apps.loc[apps.comments=='irrelrvant', 'comments'] = 'irrelevant'\n",
      "81/121: apps.comments.value_counts()\n",
      "81/122:\n",
      "lut = pd.read_pickle('lang_uri_time_wb_avail_all.pkl.gz', compression='gzip')\n",
      "macd_all = pd.read_pickle('data/macd_all.pkl.gz', compression='gzip')\n",
      "lut['uri_unsh_no_clean'] = lut.uri_unshrtn_raw.combine_first(lut.uri_unshrtn_bitly.combine_first(lut.uri_unshrtn.combine_first(lut.uri)))\n",
      "lut['uri_mem_avail'] = lut.uri.map(~macd_all.drop_duplicates(subset='uri').set_index('uri').first_snap.isna()).fillna(False)\n",
      "lut['uri_unsh_no_clean_mem_avail'] = lut.uri_unsh_no_clean.map(~macd_all.drop_duplicates(subset='uri').set_index('uri').first_snap.isna()).fillna(False)\n",
      "curl = pd.read_csv('data/curl_results_parsed.csv.gz', compression='gzip')\n",
      "lut['status_code'] = lut.uri_unsh_no_clean.map(curl.drop_duplicates(subset='url').set_index('url').status_code)\n",
      "lut['content_type'] = lut.uri_unsh_no_clean.map(curl.drop_duplicates(subset='url').set_index('url').content_type)\n",
      "lut.head().T\n",
      "81/123:\n",
      "lap = lut.merge(apps, how='inner')\n",
      "lap.uri_unsh_no_clean.value_counts()\n",
      "81/124:\n",
      "(lap.groupby(['source', 'appraisal']).uri_unsh_no_clean.nunique()\n",
      " .unstack().astype(int)\n",
      " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
      ")\n",
      "81/125:\n",
      "(lap.groupby(['is_shortened', 'appraisal']).uri_unsh_no_clean.nunique()\n",
      " .unstack().astype(int)\n",
      " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
      ")\n",
      "81/126:\n",
      "lap['is_unshrtn'] = lap.uri_cln_unshrtn!=lap.uri_cln\n",
      "(lap.groupby(['is_unshrtn', 'appraisal']).uri_unsh_no_clean.nunique()\n",
      " .unstack().astype(int)\n",
      " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/127: import numpy as np\n",
      "81/128: lut['archived'] = lut[['uri_cln_wb_avail', 'uri_cln_unshrtn_wb_avail', 'uri_mem_avail', 'uri_unsh_no_clean_mem_avail']].any(axis=1)\n",
      "81/129:\n",
      "def get_values(g):\n",
      "    return ','.join(list(g.unique()))\n",
      "tlds_to_remove = ['fb.me', 'facebook.com', 'youtu.be', 'youtube.com']\n",
      "uris_for_appr = lut[(lut.uri_unsh_no_clean.isin(apps.uri_unsh_no_clean)) & (~lut.tld_unshrtn.isin(tlds_to_remove))].groupby(['uri_unsh_no_clean', 'tld_unshrtn', 'status_code']).agg({'archived': np.max, 'source': get_values}).reset_index()\n",
      "81/130: uris_for_appr = uris_for_appr.merge(apps)\n",
      "81/131: uris_for_appr.head()\n",
      "81/132: uris_for_appr.to_csv('data/appraisal/appraisal_join.csv', index=False)\n",
      "81/133:\n",
      "(uris_for_appr\n",
      " .groupby(['source', 'appraisal']).size().unstack().fillna(0)\n",
      " .astype(int)\n",
      " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
      ")\n",
      "81/134:\n",
      "shared = ((uris_for_appr.source.str.contains(',')) & (uris_for_appr.appraisal)).sum() / uris_for_appr.source.str.contains(',').sum()\n",
      "not_shared = ((~uris_for_appr.source.str.contains(',')) & (uris_for_appr.appraisal)).sum() / (~uris_for_appr.source.str.contains(',')).sum()\n",
      "shared, not_shared\n",
      "81/135:\n",
      "(uris_for_appr\n",
      " .groupby(['status_code', 'appraisal']).size().unstack().fillna(0)\n",
      " .astype(int)\n",
      " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
      ")\n",
      "81/136:\n",
      "(uris_for_appr\n",
      " .groupby(['source', 'comments']).size().unstack().fillna(0).astype(int)\n",
      ")\n",
      "81/137:\n",
      "apps['comments'] = apps.comments.str.lower().fillna('irrelevant')\n",
      "apps.loc[apps.comments=='wall garden', 'comments'] = 'walled garden'\n",
      "apps.loc[apps.comments=='early date', 'comments'] = 'earlier date'\n",
      "apps.loc[apps.comments=='irrelrvant', 'comments'] = 'irrelevant'\n",
      "81/138: apps.comments.value_counts()\n",
      "81/139: apps = pd.concat([pd.read_excel(join('data/appraisal/filled', f), usecols=(0,1,2), names=('uri_unsh_no_clean','appraisal','comments')) for f in listdir('data/appraisal/filled')])\n",
      "81/140: apps['appraisal'] = apps['appraisal'].astype(bool)\n",
      "81/141: apps.appraisal.value_counts() / apps.shape[0]\n",
      "81/142: apps.comments.value_counts()\n",
      "81/143:\n",
      "apps['comments'] = apps.comments.str.lower()\n",
      "apps.loc[apps.comments=='wall garden', 'comments'] = 'walled garden'\n",
      "apps.loc[apps.comments=='early date', 'comments'] = 'earlier date'\n",
      "apps.loc[apps.comments=='irrelrvant', 'comments'] = 'irrelevant'\n",
      "81/144: apps.comments.value_counts()\n",
      "81/145:\n",
      "apps['comments'] = apps.comments.str.lower()\n",
      "apps.loc[apps.comments=='wall garden', 'comments'] = 'walled garden'\n",
      "apps.loc[apps.comments=='early date', 'comments'] = 'earlier date'\n",
      "apps.loc[apps.comments=='irrelrvant', 'comments'] = 'irrelevant'\n",
      "apps.loc[(((~apps.appraisal)) & (apps.comments.isna()) | (apps.comments=='')) , 'comments'] = 'irrelevant'\n",
      "81/146: apps.comments.value_counts()\n",
      "81/147: apps.comments.value_counts()\n",
      "81/148:\n",
      "lut = pd.read_pickle('lang_uri_time_wb_avail_all.pkl.gz', compression='gzip')\n",
      "macd_all = pd.read_pickle('data/macd_all.pkl.gz', compression='gzip')\n",
      "lut['uri_unsh_no_clean'] = lut.uri_unshrtn_raw.combine_first(lut.uri_unshrtn_bitly.combine_first(lut.uri_unshrtn.combine_first(lut.uri)))\n",
      "lut['uri_mem_avail'] = lut.uri.map(~macd_all.drop_duplicates(subset='uri').set_index('uri').first_snap.isna()).fillna(False)\n",
      "lut['uri_unsh_no_clean_mem_avail'] = lut.uri_unsh_no_clean.map(~macd_all.drop_duplicates(subset='uri').set_index('uri').first_snap.isna()).fillna(False)\n",
      "curl = pd.read_csv('data/curl_results_parsed.csv.gz', compression='gzip')\n",
      "lut['status_code'] = lut.uri_unsh_no_clean.map(curl.drop_duplicates(subset='url').set_index('url').status_code)\n",
      "lut['content_type'] = lut.uri_unsh_no_clean.map(curl.drop_duplicates(subset='url').set_index('url').content_type)\n",
      "lut.head().T\n",
      "81/149:\n",
      "lap = lut.merge(apps, how='inner')\n",
      "lap.uri_unsh_no_clean.value_counts()\n",
      "81/150:\n",
      "(lap.groupby(['source', 'appraisal']).uri_unsh_no_clean.nunique()\n",
      " .unstack().astype(int)\n",
      " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
      ")\n",
      "81/151:\n",
      "(lap.groupby(['is_shortened', 'appraisal']).uri_unsh_no_clean.nunique()\n",
      " .unstack().astype(int)\n",
      " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
      ")\n",
      "81/152:\n",
      "lap['is_unshrtn'] = lap.uri_cln_unshrtn!=lap.uri_cln\n",
      "(lap.groupby(['is_unshrtn', 'appraisal']).uri_unsh_no_clean.nunique()\n",
      " .unstack().astype(int)\n",
      " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
      ")\n",
      "81/153: import numpy as np\n",
      "81/154: lut['archived'] = lut[['uri_cln_wb_avail', 'uri_cln_unshrtn_wb_avail', 'uri_mem_avail', 'uri_unsh_no_clean_mem_avail']].any(axis=1)\n",
      "81/155:\n",
      "def get_values(g):\n",
      "    return ','.join(list(g.unique()))\n",
      "tlds_to_remove = ['fb.me', 'facebook.com', 'youtu.be', 'youtube.com']\n",
      "uris_for_appr = lut[(lut.uri_unsh_no_clean.isin(apps.uri_unsh_no_clean)) & (~lut.tld_unshrtn.isin(tlds_to_remove))].groupby(['uri_unsh_no_clean', 'tld_unshrtn', 'status_code']).agg({'archived': np.max, 'source': get_values}).reset_index()\n",
      "81/156: uris_for_appr = uris_for_appr.merge(apps)\n",
      "81/157: uris_for_appr.head()\n",
      "81/158: uris_for_appr.to_csv('data/appraisal/appraisal_join.csv', index=False)\n",
      "81/159:\n",
      "(uris_for_appr\n",
      " .groupby(['source', 'appraisal']).size().unstack().fillna(0)\n",
      " .astype(int)\n",
      " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
      ")\n",
      "81/160:\n",
      "shared = ((uris_for_appr.source.str.contains(',')) & (uris_for_appr.appraisal)).sum() / uris_for_appr.source.str.contains(',').sum()\n",
      "not_shared = ((~uris_for_appr.source.str.contains(',')) & (uris_for_appr.appraisal)).sum() / (~uris_for_appr.source.str.contains(',')).sum()\n",
      "shared, not_shared\n",
      "81/161:\n",
      "(uris_for_appr\n",
      " .groupby(['status_code', 'appraisal']).size().unstack().fillna(0)\n",
      " .astype(int)\n",
      " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
      ")\n",
      "81/162:\n",
      "(uris_for_appr\n",
      " .groupby(['source', 'comments']).size().unstack().fillna(0).astype(int)\n",
      ")\n",
      "81/163:\n",
      "(lap.groupby(['source', 'is_unshrtn', 'appraisal']).uri_unsh_no_clean.nunique()\n",
      " .unstack().astype(int)\n",
      " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
      ")\n",
      "81/164:\n",
      "(lap.groupby(['source', 'is_unshrtn', 'appraisal']).uri_unsh_no_clean.nunique()\n",
      " .unstack()\n",
      ")\n",
      "81/165:\n",
      "(lap.groupby(['source', 'is_unshrtn', 'appraisal']).uri_unsh_no_clean.nunique()\n",
      " .unstack().astype(int).fillna(0)\n",
      " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
      ")\n",
      "81/166:\n",
      "(lap.groupby(['source', 'is_unshrtn', 'appraisal']).uri_unsh_no_clean.nunique()\n",
      " .unstack().astype(int)\n",
      " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
      ")\n",
      "81/167:\n",
      "(lap.groupby(['source', 'is_unshrtn', 'appraisal']).uri_unsh_no_clean.nunique()\n",
      " .unstack().astype(int)\n",
      ")\n",
      "81/168:\n",
      "(lap.groupby(['source', 'is_unshrtn', 'appraisal']).uri_unsh_no_clean.nunique()\n",
      " .unstack().astype(int)\n",
      ")\n",
      "81/169:\n",
      "(lap.groupby(['source', 'is_unshrtn', 'appraisal']).uri_unsh_no_clean.nunique()\n",
      " .unstack().fillna(0.astype(int)\n",
      " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
      ")\n",
      "81/170:\n",
      "(lap.groupby(['source', 'is_unshrtn', 'appraisal']).uri_unsh_no_clean.nunique()\n",
      " .unstack().fillna(0).astype(int)\n",
      " .assign(ratio = lambda x: np.round((x[True] /(x[False]+x[True])), 2))\n",
      ")\n",
      "77/282: i\n",
      "77/283:\n",
      "for uri in to_cache[i:]:\n",
      "    try:\n",
      "        cached.append(spn.capture_or_cache(uri))\n",
      "        i+=1\n",
      "    except spn.api.WaybackRuntimeError:\n",
      "        cached.append((None, 'WaybackRuntimeError'))\n",
      "    except TooManyRedirects:\n",
      "        cached.append((None, 'TooManyRedirects'))\n",
      "77/284: i\n",
      "77/285:\n",
      "for uri in to_cache[i:]:\n",
      "    try:\n",
      "        cached.append(spn.capture_or_cache(uri))\n",
      "        i+=1\n",
      "    except spn.api.WaybackRuntimeError:\n",
      "        cached.append((None, 'WaybackRuntimeError'))\n",
      "    except TooManyRedirects:\n",
      "        cached.append((None, 'TooManyRedirects'))\n",
      "    except TypeError:\n",
      "        cached.append((None, 'TypeError'))\n",
      "77/286: i\n",
      "77/287: i\n",
      "77/288: i\n",
      "77/289: i\n",
      "77/290: i\n",
      "77/291: len(to_cache)\n",
      "77/292:\n",
      "for uri in to_cache[i:]:\n",
      "    try:\n",
      "        cached.append(spn.capture_or_cache(uri))\n",
      "        i+=1\n",
      "    except spn.api.WaybackRuntimeError:\n",
      "        cached.append((None, 'WaybackRuntimeError'))\n",
      "    except TooManyRedirects:\n",
      "        cached.append((None, 'TooManyRedirects'))\n",
      "    except TypeError:\n",
      "        cached.append((None, 'TypeError'))\n",
      "77/293: i\n",
      "77/294: len(to_cache)\n",
      "77/295:\n",
      "for uri in to_cache[i:]:\n",
      "    try:\n",
      "        cached.append(spn.capture_or_cache(uri))\n",
      "    except spn.api.WaybackRuntimeError:\n",
      "        cached.append((None, 'WaybackRuntimeError'))\n",
      "    except TooManyRedirects:\n",
      "        cached.append((None, 'TooManyRedirects'))\n",
      "    except TypeError:\n",
      "        cached.append((None, 'TypeError'))\n",
      "    i+=1\n",
      "77/296: i\n",
      "77/297: cached[:5]\n",
      "77/298: cached[10:15]\n",
      "77/299: zip(to_cache, cached)[10:15]\n",
      "77/300: list(zip(to_cache, cached))[10:15]\n",
      "77/301: list(zip(to_cache, cached))[80000:80005]\n",
      "77/302:\n",
      "cached2 = []\n",
      "i=0\n",
      "for uri in to_cache[i:]:\n",
      "    try:\n",
      "        cached2.append((uri, spn.capture_or_cache(uri)))\n",
      "    except spn.api.WaybackRuntimeError:\n",
      "        cached2.append((uri,(None, 'WaybackRuntimeError')))\n",
      "    except TooManyRedirects:\n",
      "        cached2.append((uri, (None, 'TooManyRedirects')))\n",
      "    except TypeError:\n",
      "        cached2.append((uri, (None, 'TypeError')))\n",
      "    i+=1\n",
      "77/303:\n",
      "cached2 = []\n",
      "i=0\n",
      "for uri in to_cache[i:]:\n",
      "    print i\n",
      "    try:\n",
      "        cached2.append((uri, spn.capture_or_cache(uri)))\n",
      "    except spn.api.WaybackRuntimeError:\n",
      "        cached2.append((uri,(None, 'WaybackRuntimeError')))\n",
      "    except TooManyRedirects:\n",
      "        cached2.append((uri, (None, 'TooManyRedirects')))\n",
      "    except TypeError:\n",
      "        cached2.append((uri, (None, 'TypeError')))\n",
      "    i+=1\n",
      "77/304:\n",
      "cached2 = []\n",
      "i=0\n",
      "for uri in to_cache[i:]:\n",
      "    print (i)\n",
      "    try:\n",
      "        cached2.append((uri, spn.capture_or_cache(uri)))\n",
      "    except spn.api.WaybackRuntimeError:\n",
      "        cached2.append((uri,(None, 'WaybackRuntimeError')))\n",
      "    except TooManyRedirects:\n",
      "        cached2.append((uri, (None, 'TooManyRedirects')))\n",
      "    except TypeError:\n",
      "        cached2.append((uri, (None, 'TypeError')))\n",
      "    i+=1\n",
      "82/1: import pandas as pd\n",
      "82/2:\n",
      "nl = pd.read_csv('data/collscan-daan-nlil_2014.cdx.gz', sep=' ')\n",
      "nl.head()\n",
      "82/3:\n",
      "nl = pd.read_csv('data/collscan-daan-nlil_2014.cdx.gz', sep=' ', header=None, names=['surt', 'timestamp', 'url'], usecols=[1,2])\n",
      "nl.head()\n",
      "82/4:\n",
      "nl = pd.read_csv('data/collscan-daan-nlil_2014.cdx.gz', sep=' ', header=None, names=['timestamp', 'url'], usecols=[1,2])\n",
      "nl.head()\n",
      "82/5:\n",
      "nl = (pd.read_csv('data/collscan-daan-nlil_2014.cdx.gz', sep=' ', header=None, names=['timestamp', 'url'], usecols=[1,2])\n",
      "      .assign(timestamp=lambda x: x.timestamp.astype(int)))\n",
      "nl.head()\n",
      "82/6: nl.timestamp.agg(min, max)\n",
      "82/7: nl.timestamp.agg([min, max])\n",
      "82/8: nl.isna()\n",
      "82/9: nl.isna().sum()\n",
      "82/10: nl>0\n",
      "82/11: (nl.timestamp<=0).sum()\n",
      "83/1: import pandas as pd\n",
      "83/2:\n",
      "lut = pd.read_pickle('lang_uri_time_wb_avail_all.pkl.gz', compression='gzip')\n",
      "macd_all = pd.read_pickle('data/macd_all.pkl.gz', compression='gzip')\n",
      "lut['uri_unsh_no_clean'] = lut.uri_unshrtn_raw.combine_first(lut.uri_unshrtn_bitly.combine_first(lut.uri_unshrtn.combine_first(lut.uri)))\n",
      "lut['uri_mem_avail'] = lut.uri.map(~macd_all.drop_duplicates(subset='uri').set_index('uri').first_snap.isna()).fillna(False)\n",
      "lut['uri_unsh_no_clean_mem_avail'] = lut.uri_unsh_no_clean.map(~macd_all.drop_duplicates(subset='uri').set_index('uri').first_snap.isna()).fillna(False)\n",
      "curl = pd.read_csv('data/curl_results_parsed.csv.gz', compression='gzip')\n",
      "lut['status_code'] = lut.uri_unsh_no_clean.map(curl.drop_duplicates(subset='url').set_index('url').status_code)\n",
      "lut['content_type'] = lut.uri_unsh_no_clean.map(curl.drop_duplicates(subset='url').set_index('url').content_type)\n",
      "lut.head().T\n",
      "83/3:\n",
      "def get_values(g):\n",
      "    return ','.join(list(g.unique()))\n",
      "tlds_to_remove = ['fb.me', 'facebook.com', 'youtu.be', 'youtube.com']\n",
      "uris_for_appr = lut[(lut.uri_unsh_no_clean.isin(apps.uri_unsh_no_clean)) & (~lut.tld_unshrtn.isin(tlds_to_remove))].groupby(['uri_unsh_no_clean', 'tld_unshrtn', 'status_code']).agg({'archived': np.max, 'source': get_values}).reset_index()\n",
      "83/4:\n",
      "def get_values(g):\n",
      "    return ','.join(list(g.unique()))\n",
      "tlds_to_remove = ['fb.me', 'facebook.com', 'youtu.be', 'youtube.com']\n",
      "uris_for_appr = lut[ (~lut.tld_unshrtn.isin(tlds_to_remove))].groupby(['uri_unsh_no_clean', 'tld_unshrtn', 'status_code']).agg({'archived': np.max, 'source': get_values}).reset_index()\n",
      "83/5: import numpy as np\n",
      "83/6:\n",
      "def get_values(g):\n",
      "    return ','.join(list(g.unique()))\n",
      "tlds_to_remove = ['fb.me', 'facebook.com', 'youtu.be', 'youtube.com']\n",
      "uris_for_appr = lut[ (~lut.tld_unshrtn.isin(tlds_to_remove))].groupby(['uri_unsh_no_clean', 'tld_unshrtn', 'status_code']).agg({'archived': np.max, 'source': get_values}).reset_index()\n",
      "83/7: lut['archived'] = lut[['uri_cln_wb_avail', 'uri_cln_unshrtn_wb_avail', 'uri_mem_avail', 'uri_unsh_no_clean_mem_avail']].any(axis=1)\n",
      "83/8:\n",
      "def get_values(g):\n",
      "    return ','.join(list(g.unique()))\n",
      "tlds_to_remove = ['fb.me', 'facebook.com', 'youtu.be', 'youtube.com']\n",
      "uris_for_appr = lut[ (~lut.tld_unshrtn.isin(tlds_to_remove))].groupby(['uri_unsh_no_clean', 'tld_unshrtn', 'status_code']).agg({'archived': np.max, 'source': get_values}).reset_index()\n",
      "83/9: uris_for_appr\n",
      "83/10: uris_for_appr.groupby('source')\n",
      "83/11: uris_for_appr.groupby('source').size()\n",
      "83/12: uris_for_appr[uris_for_appr.source=='twitter,google']\n",
      "83/13: uris_for_appr[(uris_for_appr.source=='twitter,google') & (uris_for_appr.tld_unshrtn=='embassies.gov.il')]\n",
      "83/14: uris_for_appr[(uris_for_appr.source=='twitter,google') & (uris_for_appr.tld_unshrtn=='embassies.gov.il')].iat[0]\n",
      "83/15: uris_for_appr[(uris_for_appr.source=='twitter,google') & (uris_for_appr.tld_unshrtn=='embassies.gov.il')].iat[:, 0]\n",
      "83/16: uris_for_appr[(uris_for_appr.source=='twitter,google') & (uris_for_appr.tld_unshrtn=='embassies.gov.il')].iat[0, 0]\n",
      "83/17: lut[lut.uri_unsh_no_clean=='http://embassies.gov.il/houston/News/CurrentAffairs/Pages/Operation-Protective-Edge-QA.aspx']\n",
      "83/18: uris_for_appr[(uris_for_appr.source=='twitter,google')]\n",
      "83/19: uris_for_appr[(uris_for_appr.source=='twitter,google') & (~uris_for_appr.archived)]\n",
      "83/20: lut[lut.uri_unsh_no_cleann.isin(uris_for_appr[(uris_for_appr.source=='twitter,google') & (~uris_for_appr.archived)].uri_unsh_no_clean)].groupby('uri_unsh_no_clean').lang.nunique().sort_values(ascending=False)\n",
      "83/21: lut[lut.uri_unsh_no_clean.isin(uris_for_appr[(uris_for_appr.source=='twitter,google') & (~uris_for_appr.archived)].uri_unsh_no_clean)].groupby('uri_unsh_no_clean').lang.nunique().sort_values(ascending=False)\n",
      "83/22: lut[lut.uri_unsh_no_clean.isin(uris_for_appr[(uris_for_appr.source=='twitter,google') & (~uris_for_appr.archived)].uri_unsh_no_clean)].groupby('uri_unsh_no_clean').lang.nunique().sort_values(ascending=False).index[0]\n",
      "83/23: lut[lut.uri_unsh_no_clean.isin(uris_for_appr[(uris_for_appr.source=='twitter,google') & (~uris_for_appr.archived)].uri_unsh_no_clean)].groupby('uri_unsh_no_clean').lang.nunique().sort_values(ascending=False).index\n",
      "83/24: lut[lut.uri_unsh_no_clean.isin(uris_for_appr[(uris_for_appr.source=='twitter,google') & (~uris_for_appr.archived)].uri_unsh_no_clean)].groupby('uri_unsh_no_clean').lang.nunique().sort_values(ascending=False)\n",
      "83/25: lut[lut.uri_unsh_no_clean=='https://www.independent.co.uk/news/world/middle-east/israel-gaza-conflict-death-toll-in-gaza-tops-120-after-overnight-raids-as-operation-protective-edge-9601820.html']\n",
      "83/26: lut[lut.uri_unsh_no_clean=='https://www.haaretz.com/5-must-read-articles-on-gaza-conflict-1.5256297']\n",
      "83/27: lut[lut.uri_unsh_no_clean=='https://lumeacarnavalului.com/2014/09/06/operatiunea-protective-edge-ceea-ce-ei-nu-doresc-sa-stii/']\n",
      "83/28: lut[lut.uri_unsh_no_clean.isin(uris_for_appr[(uris_for_appr.source=='youtube,twitter') & (~uris_for_appr.archived)].uri_unsh_no_clean)].groupby('uri_unsh_no_clean').lang.nunique().sort_values(ascending=False)\n",
      "83/29: lut[lut.uri_unsh_no_clean.isin(uris_for_appr[(uris_for_appr.source=='youtube,twitter')].uri_unsh_no_clean)].groupby('uri_unsh_no_clean').lang.nunique().sort_values(ascending=False)\n",
      "83/30: lut[lut.uri_unsh_no_clean=='http://rt.com/news/172384-israel-comandoes-raid-gaza/']\n",
      "83/31: lut[lut.uri_unsh_no_clean=='http://on.rt.com/13u57f']\n",
      "83/32: lut[lut.uri_unsh_no_clean=='http://www.ibtimes.com/timeline-events-gaza-israel-shows-sudden-rapid-escalation-1636264']\n",
      "83/33: lut[(~lut.lang.contains('sco')) & (lut.uri_unsh_no_clean.isin(uris_for_appr[(uris_for_appr.source=='youtube,twitter')].uri_unsh_no_clean))].groupby('uri_unsh_no_clean').lang.nunique().sort_values(ascending=False)\n",
      "83/34: lut[(~lut.lang.str.contains('sco')) & (lut.uri_unsh_no_clean.isin(uris_for_appr[(uris_for_appr.source=='youtube,twitter')].uri_unsh_no_clean))].groupby('uri_unsh_no_clean').lang.nunique().sort_values(ascending=False)\n",
      "83/35: lut[(~lut.lang.str.contains('sco').fillna(False)) & (lut.uri_unsh_no_clean.isin(uris_for_appr[(uris_for_appr.source=='youtube,twitter')].uri_unsh_no_clean))].groupby('uri_unsh_no_clean').lang.nunique().sort_values(ascending=False)\n",
      "83/36:\n",
      "lut[(~lut.lang.contains('sco'))\n",
      "    (lut.uri_unsh_no_clean=='http://www.dailymail.co.uk/news/article-2697194/Outrage-France-country-world-ban-pro-Palestine-demos.html')]\n",
      "83/37:\n",
      "lut[(~lut.lang.str.contains('sco'))\n",
      "    (lut.uri_unsh_no_clean=='http://www.dailymail.co.uk/news/article-2697194/Outrage-France-country-world-ban-pro-Palestine-demos.html')]\n",
      "83/38:\n",
      "lut[(~lut.lang.str.contains('sco').fillna(False))\n",
      "    (lut.uri_unsh_no_clean=='http://www.dailymail.co.uk/news/article-2697194/Outrage-France-country-world-ban-pro-Palestine-demos.html')]\n",
      "83/39:\n",
      "lut[(~lut.lang.str.contains('sco').fillna(False)) &\n",
      "    (lut.uri_unsh_no_clean=='http://www.dailymail.co.uk/news/article-2697194/Outrage-France-country-world-ban-pro-Palestine-demos.html')]\n",
      "83/40: lut[(~lut.lang.str.contains('sco').fillna(False)) & (lut.uri_unsh_no_clean.isin(uris_for_appr[(uris_for_appr.source=='wikipedia,youtube,twitter')].uri_unsh_no_clean))].groupby('uri_unsh_no_clean').lang.nunique().sort_values(ascending=False)\n",
      "83/41:\n",
      "lut[(~lut.lang.str.contains('sco').fillna(False)) &\n",
      "    (lut.uri_unsh_no_clean=='http://nymag.com/daily/intelligencer/2014/07/hamas-didnt-kidnap-the-israeli-teens-after-all.html')]\n",
      "83/42:\n",
      "lut[(~lut.lang.str.contains('sco').fillna(False)) &\n",
      "    (lut.uri_unsh_no_clean=='http://nymag.com/daily/intelligencer/2014/07/hamas-didnt-kidnap-the-israeli-teens-after-all.html')[['lang','uri', 'source','uri_unsh_no_clean', 'timeststamp','archived']]\n",
      "83/43:\n",
      "lut[(~lut.lang.str.contains('sco').fillna(False)) &\n",
      "    (lut.uri_unsh_no_clean=='http://nymag.com/daily/intelligencer/2014/07/hamas-didnt-kidnap-the-israeli-teens-after-all.html')][['lang','uri', 'source','uri_unsh_no_clean', 'timeststamp','archived']]\n",
      "83/44:\n",
      "lut[(~lut.lang.str.contains('sco').fillna(False)) &\n",
      "    (lut.uri_unsh_no_clean=='http://nymag.com/daily/intelligencer/2014/07/hamas-didnt-kidnap-the-israeli-teens-after-all.html')][['lang','uri', 'source','uri_unsh_no_clean', 'timestamp','archived']]\n",
      "83/45: pd.set_option('display.max_colwidth', -1)\n",
      "83/46:\n",
      "lut[(~lut.lang.str.contains('sco').fillna(False)) &\n",
      "    (lut.uri_unsh_no_clean=='http://nymag.com/daily/intelligencer/2014/07/hamas-didnt-kidnap-the-israeli-teens-after-all.html')][['lang','uri', 'source','uri_unsh_no_clean', 'timestamp','archived']]\n",
      "83/47:\n",
      "url_example = 'http://nymag.com/daily/intelligencer/2014/07/hamas-didnt-kidnap-the-israeli-teens-after-all.html'\n",
      "lut[(~lut.lang.str.contains('sco').fillna(False)) &\n",
      "    (lut.uri_unsh_no_clean==url_example)][['lang','uri', 'source','uri_unsh_no_clean', 'timestamp','archived']]\n",
      "83/48:\n",
      "url_example = 'http://www.theguardian.com/world/2014/jul/08/israel-pounds-gaza-against-hamas'\n",
      "lut[(~lut.lang.str.contains('sco').fillna(False)) &\n",
      "    (lut.uri_unsh_no_clean==url_example)][['lang','uri', 'source','uri_unsh_no_clean', 'timestamp','archived']]\n",
      "83/49:\n",
      "url_example = 'http://www.jpost.com/Operation-Protective-Edge/IDF-intensifies-Gaza-attacks-with-artillery-fire-air-strikes-363289'\n",
      "lut[(~lut.lang.str.contains('sco').fillna(False)) &\n",
      "    (lut.uri_unsh_no_clean==url_example)][['lang','uri', 'source','uri_unsh_no_clean', 'timestamp','archived']]\n",
      "83/50: lut[(~lut.lang.str.contains('sco').fillna(False)) & (lut.uri_unsh_no_clean.isin(uris_for_appr[(uris_for_appr.source=='wikipedia,twitter,google')].uri_unsh_no_clean))].groupby('uri_unsh_no_clean').lang.nunique().sort_values(ascending=False)\n",
      "83/51:\n",
      "url_example = 'https://www.ynetnews.com/articles/0,7340,L-4564678,00.html'\n",
      "lut[(~lut.lang.str.contains('sco').fillna(False)) &\n",
      "    (lut.uri_unsh_no_clean==url_example)][['lang','uri', 'source','uri_unsh_no_clean', 'timestamp','archived']]\n",
      "83/52: lut.groupby(['uri_unsh_no_clean','archived']).size()\n",
      "83/53: lut.groupby('archived').uri_unsh_no_clean.nunique()\n",
      "83/54: lut.groupby('archived').uri_unsh_no_clean.nunique()/lut.uri_unsh_no_clean.nunique()\n",
      "83/55: lut.groupby('archived').uri_cln.nunique()/lut.uri_cln.nunique()\n",
      "83/56:\n",
      "simp = lut.groupby(['uri_unsh_no_clean', 'tld_unshrtn', 'status_code']).agg({'archived': np.max, 'source': get_values}).reset_index()\n",
      "simp.head()\n",
      "83/57: simp[simp.source.str.contains(',')].shape[0]\n",
      "83/58: simp[simp.source.str.contains(',')].shape[0] / simp.shape[0]\n",
      "83/59: simp.uri_unsh_no_clean.value_counts()\n",
      "83/60:\n",
      "scl = lut.groupby(['uri_unsh_no_clean', 'tld_unshrtn', 'status_code']).agg({'archived': np.max, 'source': get_values}).reset_index()\n",
      "scl[scl.source.str.contains(',')].shape[0] / simp.shape[0]\n",
      "83/61:\n",
      "scl = lut.groupby(['uri_unsh_no_clean', 'tld_unshrtn', 'status_code']).agg({'archived': np.max, 'source': get_values}).reset_index()\n",
      "scl[scl.source.str.contains(',')].shape[0] / scl.shape[0]\n",
      "83/62:\n",
      "scl = lut.groupby(['uri_cln_unshrtn', 'tld_unshrtn', 'status_code']).agg({'archived': np.max, 'source': get_values}).reset_index()\n",
      "scl[scl.source.str.contains(',')].shape[0] / scl.shape[0]\n",
      "83/63: simp.archived.value_counts()\n",
      "83/64: simp.archived.value_counts() / simp.shape[0]\n",
      "83/65: scl.archived.value_counts() / scl.shape[0]\n",
      "83/66:\n",
      "scl = lut.groupby(['uri_cln_unshrtn', 'tld_unshrtn', 'status_code', 'is_shortened']).agg({'archived': np.max, 'source': get_values}).reset_index()\n",
      "scl[scl.source.str.contains(',')].shape[0] / scl.shape[0]\n",
      "83/67:\n",
      "scl = lut.groupby(['uri_cln_unshrtn', 'tld_unshrtn', 'status_code']).agg({'archived': np.max, 'source': get_values}).reset_index()\n",
      "scl[scl.source.str.contains(',')].shape[0] / scl.shape[0]\n",
      "83/68:\n",
      "scl = lut.groupby(['uri_cln_unshrtn', 'tld_unshrtn', 'status_code']).agg({'archived': np.max, 'source': get_values, 'is_shortened': np.max}).reset_index()\n",
      "scl[scl.source.str.contains(',')].shape[0] / scl.shape[0]\n",
      "83/69:\n",
      "simp = lut.groupby(['uri_unsh_no_clean', 'tld_unshrtn', 'status_code',  'is_shortened']).agg({'archived': np.max, 'source': get_values, 'is_shortened': np.max}).reset_index()\n",
      "simp.head()\n",
      "83/70:\n",
      "simp = lut.groupby(['uri_unsh_no_clean', 'tld_unshrtn', 'status_code']).agg({'archived': np.max, 'source': get_values, 'is_shortened': np.max}).reset_index()\n",
      "simp.head()\n",
      "83/71: simp[simp.source.str.contains(',')].shape[0] / simp.shape[0]\n",
      "83/72:\n",
      "scl = lut.groupby(['uri_cln_unshrtn', 'tld_unshrtn', 'status_code']).agg({'archived': np.max, 'source': get_values, 'is_shortened': np.max}).reset_index()\n",
      "scl[scl.source.str.contains(',')].shape[0] / scl.shape[0]\n",
      "83/73: simp.archived.value_counts() / simp.shape[0]\n",
      "83/74: scl.archived.value_counts() / scl.shape[0]\n",
      "83/75: simp.archived.value_counts() / simp.shape[0]\n",
      "83/76: simp['is_shortened'].archived.value_counts() / simp.shape[0]\n",
      "83/77: simp[simp.is_shortened].archived.value_counts() / simp.shape[0]\n",
      "83/78: simp[simp.is_shortened].archived.value_counts() / simp[simp.is_shortened].shape[0]\n",
      "83/79: lut['is_unshrtn'] = lut.uri!=lut.uri_unsh_no_clean\n",
      "83/80:\n",
      "simp = lut.groupby(['uri_unsh_no_clean', 'tld_unshrtn', 'status_code']).agg({'archived': np.max, 'source': get_values, 'is_shortened': np.max,  'is_unshrtn': np.max}).reset_index()\n",
      "simp.head()\n",
      "83/81: simp[simp.source.str.contains(',')].shape[0] / simp.shape[0]\n",
      "83/82:\n",
      "scl = lut.groupby(['uri_cln_unshrtn', 'tld_unshrtn', 'status_code']).agg({'archived': np.max, 'source': get_values, 'is_shortened': np.max}).reset_index()\n",
      "scl[scl.source.str.contains(',')].shape[0] / scl.shape[0]\n",
      "83/83: simp.archived.value_counts() / simp.shape[0]\n",
      "83/84: scl.archived.value_counts() / scl.shape[0]\n",
      "83/85: simp.archived.value_counts() / simp.shape[0]\n",
      "83/86: simp[simp.is_shortened].archived.value_counts() / simp[simp.is_shortened].shape[0]\n",
      "83/87: simp[simp.is_unshrtn].archived.value_counts() / simp[simp.is_unshrtn].shape[0]\n",
      "83/88: simp[~simp.is_shortened].archived.value_counts() / simp[~simp.is_shortened].shape[0]\n",
      "83/89: simp[(simp.is_shortened) & (~simp.is_unshrtn)].archived.value_counts() / simp[~simp.is_shortened].shape[0]\n",
      "83/90: simp[(simp.is_shortened) & (~simp.is_unshrtn)].archived.value_counts() / simp[(simp.is_shortened) & (~simp.is_unshrtn)].shape[0]\n",
      "77/305: len(cached2)\n",
      "77/306:\n",
      "import pickle\n",
      "with open('savepagenow_results.pkl', 'wb') as f:\n",
      "    pickle.dump(cached2, f)\n",
      "77/307: lut.head().T\n",
      "77/308: lut.to_csv('lang_uri_time_final.csv.gz', index=False)\n",
      "77/309: cached2[:5]\n",
      "77/310: lut.dtypes\n",
      "77/311: lut.groupby(['lang', 'status_code']).uri_unsh_no_clean.nunique().unstack()\n",
      "77/312: clut.groupby(['lang', 'status_code']).uri_unsh_no_clean.nunique().unstack()\n",
      "77/313:\n",
      "clut = lut.copy()\n",
      "clut.loc[clut.lang=='en,sco', 'lang'] = 'en'\n",
      "clut = (pd.concat([clut, clut.loc[clut.lang=='id,jv,ms', :].assign(lang='id'), \n",
      "                   clut.loc[clut.lang=='id,jv,ms', :].assign(lang='jv'), \n",
      "                   clut.loc[clut.lang=='id,jv,ms', :].assign(lang='ms')], axis=0)\n",
      "       .reset_index().drop('index', axis=1))\n",
      "clut = clut[~clut.lang.isin(['id,jv,ms', 'sco', '38'])]\n",
      "77/314: clut.groupby(['lang', 'status_code']).uri_unsh_no_clean.nunique().unstack()\n",
      "77/315:\n",
      "lang_sc = clut.groupby(['lang', 'status_code']).uri_unsh_no_clean.nunique().unstack().fillna(0)\n",
      "lang_sc = lang_sc.div(lang_sc.sum(axis=1), axis=0)\n",
      "77/316:\n",
      "lang_sc = clut.groupby(['lang', 'status_code']).uri_unsh_no_clean.nunique().unstack().fillna(0)\n",
      "lang_sc = lang_sc.div(lang_sc.sum(axis=1), axis=0)\n",
      "lang_sc\n",
      "77/317: pd.set_option('display.max_colwidth', -1)\n",
      "77/318:\n",
      "lang_sc = clut.groupby(['lang', 'status_code']).uri_unsh_no_clean.nunique().unstack().fillna(0)\n",
      "lang_sc = lang_sc.div(lang_sc.sum(axis=1), axis=0)\n",
      "lang_sc\n",
      "77/319:\n",
      "pd.set_option('display.max_colwidth', -1)\n",
      "pd.set_option('display.max_columns', -1)\n",
      "77/320:\n",
      "lang_sc = clut.groupby(['lang', 'status_code']).uri_unsh_no_clean.nunique().unstack().fillna(0)\n",
      "lang_sc = lang_sc.div(lang_sc.sum(axis=1), axis=0)\n",
      "lang_sc\n",
      "77/321:\n",
      "pd.set_option('display.max_colwidth', -1)\n",
      "pd.set_option('display.max_columns', 50)\n",
      "77/322:\n",
      "lang_sc = clut.groupby(['lang', 'status_code']).uri_unsh_no_clean.nunique().unstack().fillna(0)\n",
      "lang_sc = lang_sc.div(lang_sc.sum(axis=1), axis=0)\n",
      "lang_sc\n",
      "77/323:\n",
      "source_sc = clut.groupby(['source', 'status_code']).uri_unsh_no_clean.nunique().unstack().fillna(0)\n",
      "source_sc = source_sc.div(source_sc.sum(axis=1), axis=0)\n",
      "source_sc\n",
      "77/324:\n",
      "lang_sc_short = lang_sc.copy()\n",
      "lang_sc_short['others'] = lang_sc_short.iloc[:, [2,3,5:]].sum()\n",
      "lang_sc_short.drop([2,3,5:], axis=1)\n",
      "77/325:\n",
      "lang_sc_short = lang_sc.copy()\n",
      "lang_sc_short['others'] = lang_sc_short.iloc[:, 5:].sum()  + lang_sc_short.iloc[:, [2,3]].sum()\n",
      "lang_sc_short.drop([2,3,5:], axis=1)\n",
      "77/326:\n",
      "lang_sc_short = lang_sc.copy()\n",
      "lang_sc_short['others'] = lang_sc_short.iloc[:, 5:].sum()  + lang_sc_short.iloc[:, [2,3]].sum()\n",
      "lang_sc_short.drop(, axis=1)\n",
      "77/327:\n",
      "lang_sc_short = lang_sc.copy()\n",
      "lang_sc_short['others'] = lang_sc_short.iloc[:, 5:].sum()  + lang_sc_short.iloc[:, [2,3]].sum()\n",
      "lang_sc_short.drop(, axis=1)\n",
      "77/328:\n",
      "lang_sc_short = lang_sc.copy()\n",
      "lang_sc_short['others'] = lang_sc_short.iloc[:, 5:].sum()  + lang_sc_short.iloc[:, [2,3]].sum()\n",
      "77/329:\n",
      "lang_sc_short = lang_sc.copy()\n",
      "lang_sc_short['others'] = lang_sc_short.iloc[:, 5:].sum()  + lang_sc_short.iloc[:, [2,3]].sum()\n",
      "lang_sc_short\n",
      "77/330:\n",
      "lang_sc_short = lang_sc.copy()\n",
      "lang_sc_short['others'] = lang_sc_short.iloc[:, 5:].sum(axis=1)  + lang_sc_short.iloc[:, [2,3]].sum(axis=1)\n",
      "lang_sc_short\n",
      "77/331:\n",
      "lang_sc_short = lang_sc.copy()\n",
      "lang_sc_short['others'] = lang_sc_short.iloc[:, 5:].sum(axis=1)  + lang_sc_short.iloc[:, [2,3]].sum(axis=1)\n",
      "lang_sc_short.drop[[5:]].drop[[2,3]]\n",
      "77/332:\n",
      "lang_sc_short = lang_sc.copy()\n",
      "lang_sc_short['others'] = lang_sc_short.iloc[:, 5:].sum(axis=1)  + lang_sc_short.iloc[:, [2,3]].sum(axis=1)\n",
      "lang_sc_short.drop([5:]).drop([2,3])\n",
      "77/333:\n",
      "lang_sc_short = lang_sc.copy()\n",
      "lang_sc_short['others'] = lang_sc_short.iloc[:, 5:].sum(axis=1)  + lang_sc_short.iloc[:, [2,3]].sum(axis=1)\n",
      "lang_sc_short.loc[[0, 200, 301, 'others']]\n",
      "77/334:\n",
      "lang_sc_short = lang_sc.copy()\n",
      "lang_sc_short['others'] = lang_sc_short.iloc[:, 5:].sum(axis=1)  + lang_sc_short.iloc[:, [2,3]].sum(axis=1)\n",
      "lang_sc_short.loc[:, [0, 200, 301, 'others']]\n",
      "77/335:\n",
      "source_sc_short = lang_sc.copy()\n",
      "source_sc_short['others'] = source_sc_short.iloc[:, 5:].sum(axis=1)  + source_sc_short.iloc[:, [2,3]].sum(axis=1)\n",
      "source_sc_short.loc[:, [0, 200, 301, 'others']]\n",
      "77/336:\n",
      "source_sc_short = source_sc_short.copy()\n",
      "source_sc_short['others'] = source_sc_short.iloc[:, 5:].sum(axis=1)  + source_sc_short.iloc[:, [2,3]].sum(axis=1)\n",
      "source_sc_short.loc[:, [0, 200, 301, 'others']]\n",
      "77/337:\n",
      "source_sc_short = source_sc.copy()\n",
      "source_sc_short['others'] = source_sc_short.iloc[:, 5:].sum(axis=1)  + source_sc_short.iloc[:, [2,3]].sum(axis=1)\n",
      "source_sc_short.loc[:, [0, 200, 301, 'others']]\n",
      "77/338: lut.groupby(['lang', 'source']).uri_cln_unshrtn.nunique().unstack()\n",
      "77/339:\n",
      "big_langs = lut[(lut.lang.isin(['he', 'ar', 'en', 'de', 'ru', 'fr']))&(lut.source=='twitter')]\n",
      "big_langs.groupby('lang').apply(get_top_tld).to_frame()\n",
      "77/340:\n",
      "unarch = lut[(~lut.uri_unsh_no_clean_mem_avail)]\n",
      "\n",
      "def get_top_tld(df):\n",
      "    num_uniq_uri = df.uri_unsh_no_clean.nunique()\n",
      "    return (df.groupby('tld_unshrtn').uri_unsh_no_clean.nunique().sort_values(ascending=False).head(10)/num_uniq_uri).round(2)\n",
      "unarch.groupby('source').apply(get_top_tld).to_frame()\n",
      "77/341:\n",
      "big_langs = lut[lut.lang.isin(['he', 'ar', 'en', 'de', 'ru', 'fr'])]\n",
      "big_langs.groupby('lang').apply(get_top_tld).to_frame()\n",
      "77/342:\n",
      "big_langs = lut[(lut.lang.isin(['he', 'ar', 'en', 'de', 'ru', 'fr']))&(lut.source=='twitter')]\n",
      "big_langs.groupby('lang').apply(get_top_tld).to_frame()\n",
      "77/343:\n",
      "big_langs = lut[(lut.lang.isin(['he', 'ar', 'en', 'de', 'ru', 'fr']))&(lut.source=='google')]\n",
      "big_langs.groupby('lang').apply(get_top_tld).to_frame()\n",
      "77/344:\n",
      "def get_top_tld(df):\n",
      "    num_uniq_uri = df.uri_unsh_no_clean.nunique()\n",
      "    return (df.groupby('tld_unshrtn').uri_unsh_no_clean.nunique().sort_values(ascending=False).head(5))\n",
      "77/345:\n",
      "big_langs = lut[(lut.lang.isin(['he', 'ar', 'en', 'de', 'ru', 'fr']))&(lut.source=='google')]\n",
      "big_langs.groupby('lang').apply(get_top_tld).to_frame()\n",
      "77/346:\n",
      "big_langs = lut[(lut.lang.isin(['he', 'ar', 'en', 'de', 'ru', 'fr']))&(lut.source=='google')]\n",
      "big_langs.groupby('lang').apply(get_top_tld).to_frame().unstack()\n",
      "77/347:\n",
      "big_langs = lut[(lut.lang.isin(['he', 'ar', 'en', 'de', 'ru', 'fr']))&(lut.source=='google')]\n",
      "big_langs.groupby('lang').apply(get_top_tld)\n",
      "77/348:\n",
      "big_langs = lut[(lut.lang.isin(['he', 'ar', 'en', 'de', 'ru', 'fr']))&(lut.source=='google')]\n",
      "big_langs.groupby('lang').apply(get_top_tld).to_frame()\n",
      "77/349: clut[clut.source=='google'].lang.value_counts()\n",
      "77/350:\n",
      "big_langs = clut[(clut.lang.isin(['he', 'ar', 'en', 'de', 'es', 'fr']))&(lut.source=='google')]\n",
      "big_langs.groupby('lang').apply(get_top_tld).to_frame()\n",
      "77/351:\n",
      "big_langs = clut[(clut.lang.isin(['he', 'ar', 'en', 'de', 'es', 'fr']))&(clut.source=='google')]\n",
      "big_langs.groupby('lang').apply(get_top_tld).to_frame()\n",
      "77/352: clut[(clut.source=='google')].groupby('lang').uri_unsh_no_clean.nunique()\n",
      "77/353: clut[(clut.source=='google')].groupby('lang').uri_unsh_no_clean.nunique().sort_values(ascending=False)\n",
      "77/354:\n",
      "def get_top_tld(df, field, top):\n",
      "    num_uniq_uri = df.uri_unsh_no_clean.nunique()\n",
      "    return (df.groupby(field).uri_unsh_no_clean.nunique().sort_values(ascending=False).head(top)/num_uniq_uri).round(2)\n",
      "big_langs = clut[(clut.lang.isin(['he', 'ar', 'en', 'de', 'es', 'fr']))&(clut.source=='google')]\n",
      "big_langs.groupby('source').apply(get_top_tld, 'lang', 5).to_frame()\n",
      "77/355:\n",
      "def get_top_tld(df, field, top):\n",
      "    num_uniq_uri = df.uri_unsh_no_clean.nunique()\n",
      "    return (df.groupby(field).uri_unsh_no_clean.nunique().sort_values(ascending=False).head(top)/num_uniq_uri).round(2)\n",
      "big_langs = clut[(clut.lang.isin(['he', 'ar', 'en', 'de', 'es', 'fr']))&(clut.source=='google')]\n",
      "big_langs.groupby('source').apply(get_top_tld, 'lang', 5)\n",
      "77/356:\n",
      "def get_top_tld(df, field, top):\n",
      "    num_uniq_uri = df.uri_unsh_no_clean.nunique()\n",
      "    return (df.groupby(field).uri_unsh_no_clean.nunique().sort_values(ascending=False).head(top)/num_uniq_uri).round(2)\n",
      "big_langs = clut[(clut.lang.isin(['he', 'ar', 'en', 'de', 'es', 'fr']))]\n",
      "big_langs.groupby('source').apply(get_top_tld, 'lang', 5)\n",
      "77/357:\n",
      "def get_top_tld(df, field, top):\n",
      "    num_uniq_uri = df.uri_unsh_no_clean.nunique()\n",
      "    return (df.groupby(field).uri_unsh_no_clean.nunique().sort_values(ascending=False).head(top)/num_uniq_uri).round(2)\n",
      "big_langs = clut[(clut.lang.isin(['he', 'ar', 'en', 'de', 'es', 'fr']))]\n",
      "big_langs.groupby('source').apply(get_top_tld, 'lang', 5).to_frame()\n",
      "77/358: clut.tld_unshrtn.str.split('.', expand=True)\n",
      "77/359: clut.tld_unshrtn.str.split('.')\n",
      "77/360:\n",
      "suffix = [x[-1] for x in clut.tld_unshrtn.str.split('.')]\n",
      "suffix\n",
      "77/361:\n",
      "def get_top_tld(df, field, top):\n",
      "    num_uniq_uri = df.uri_unsh_no_clean.nunique()\n",
      "    return (df.groupby(field).uri_unsh_no_clean.nunique().sort_values(ascending=False).head(top)/num_uniq_uri).round(2)\n",
      "clut.groupby('source').apply(get_top_tld, 'lang', 5).to_frame()\n",
      "77/362: clut.groupby('source').apply(get_top_tld, 'suffix', 5).to_frame()\n",
      "77/363: clut['suffix'] = [x[-1] for x in clut.tld_unshrtn.str.split('.')]\n",
      "77/364: clut.groupby('source').apply(get_top_tld, 'suffix', 5).to_frame()\n",
      "77/365:\n",
      "big_langs = clut[(clut.lang.isin(['he', 'ar', 'en', 'de', 'fr']))&(clut.source=='google')]\n",
      "big_langs.groupby(['source', 'lang']).apply(get_top_tld).to_frame()\n",
      "77/366:\n",
      "big_langs = clut[(clut.lang.isin(['he', 'ar', 'en', 'de', 'fr']))&(clut.source=='google')]\n",
      "big_langs.groupby(['source', 'lang']).apply(get_top_tld, 'tld_unshrtn', 5).to_frame()\n",
      "77/367:\n",
      "big_langs = clut[(clut.lang.isin(['he', 'ar', 'en', 'de', 'fr']))]\n",
      "big_langs.groupby(['source', 'lang']).apply(get_top_tld, 'tld_unshrtn', 5).to_frame()\n",
      "77/368:\n",
      "with pd.set_option('display.max_rows', 200):\n",
      "    big_langs = clut[(clut.lang.isin(['he', 'ar', 'en', 'de', 'fr']))]\n",
      "    big_langs.groupby(['source', 'lang']).apply(get_top_tld, 'tld_unshrtn', 5).to_frame()\n",
      "77/369:\n",
      "with pd.option_context('display.max_rows', 200):\n",
      "    big_langs = clut[(clut.lang.isin(['he', 'ar', 'en', 'de', 'fr']))]\n",
      "    big_langs.groupby(['source', 'lang']).apply(get_top_tld, 'tld_unshrtn', 5).to_frame()\n",
      "77/370:\n",
      "with pd.option_context('display.max_rows', 200):\n",
      "    big_langs = clut[(clut.lang.isin(['he', 'ar', 'en', 'de', 'fr']))]\n",
      "    big_langs.groupby(['source', 'lang']).apply(get_top_tld, 'tld_unshrtn', 5).to_frame()\n",
      "77/371:\n",
      "with pd.option_context('display.max_rows', 200)\n",
      "\n",
      "big_langs = clut[(clut.lang.isin(['he', 'ar', 'en', 'de', 'fr']))]\n",
      "big_langs.groupby(['source', 'lang']).apply(get_top_tld, 'tld_unshrtn', 5).to_frame()\n",
      "77/372:\n",
      "pd.set_option('display.max_rows', 200)\n",
      "\n",
      "big_langs = clut[(clut.lang.isin(['he', 'ar', 'en', 'de', 'fr']))]\n",
      "big_langs.groupby(['source', 'lang']).apply(get_top_tld, 'tld_unshrtn', 5).to_frame()\n",
      "77/373: clut.groupby('source').apply(get_top_tld, 'suffix', 10).to_frame()\n",
      "77/374:\n",
      "def get_top_two_levels(df, f1, top1, f2, top2):\n",
      "    keep = df.groupby(f1).uri_unsh_no_clean.nunique().sort_values(ascending=False).index[:top1]\n",
      "    df = df[df[f1].isin(keep)]\n",
      "    num_uniq_uri = df.uri_unsh_no_clean.nunique()\n",
      "    return (df.groupby(f2).uri_unsh_no_clean.nunique().sort_values(ascending=False).head(top2)/num_uniq_uri).round(2)\n",
      "77/375:\n",
      "pd.set_option('display.max_rows', 200)\n",
      "clut.groupby(['source']).apply(get_top_two_levels, 'lang', 5, 'tld_unshrtn', 5).to_frame()\n",
      "77/376:\n",
      "def get_top_two_levels(df, f1, top1, f2, top2):\n",
      "    keep = df.groupby(f1).uri_unsh_no_clean.nunique().sort_values(ascending=False).index[:top1]\n",
      "    df = df[df[f1].isin(keep)]\n",
      "    num_uniq_uri = df.uri_unsh_no_clean.nunique()\n",
      "    return (df.groupby([f1, f2]).uri_unsh_no_clean.nunique().sort_values(ascending=False).head(top2)/num_uniq_uri).round(2)\n",
      "77/377:\n",
      "pd.set_option('display.max_rows', 200)\n",
      "clut.groupby(['source']).apply(get_top_two_levels, 'lang', 5, 'tld_unshrtn', 5).to_frame()\n",
      "77/378:\n",
      "pd.set_option('display.max_rows', 200)\n",
      "\n",
      "big_langs = clut[(clut.lang.isin(['he', 'ar', 'en', 'de', 'fr']))]\n",
      "big_langs.groupby(['source', 'lang']).apply(get_top_tld, 'tld_unshrtn', 5).to_frame()\n",
      "77/379: top_lang_by_source = clut.groupby(['source', 'lang']).uri_unsh_no_clean.nunique().to_frame().sort_values(['source', 0])\n",
      "77/380: top_lang_by_source = clut.groupby(['source', 'lang']).uri_unsh_no_clean.nunique().reset_index().sort_values(['source', 'uri_unsh_no_clean'])\n",
      "77/381:\n",
      "top_lang_by_source = clut.groupby(['source', 'lang']).uri_unsh_no_clean.nunique().reset_index().sort_values(['source', 'uri_unsh_no_clean'])\n",
      "top_lang_by_source\n",
      "77/382:\n",
      "top_lang_by_source = clut.groupby(['source', 'lang']).uri_unsh_no_clean.nunique().reset_index().sort_values(['source', 'uri_unsh_no_clean'], ascending=False)\n",
      "top_lang_by_source\n",
      "77/383:\n",
      "def get_top_two_levels(df, by, f1, top1, f2, top2, keep_df):\n",
      "    keep = keep_df.loc[keep_df[by]==df[by].iat[0], f1].head[top1]\n",
      "    df = df[df[f1].isin(keep)]\n",
      "    return df.groupby(f1).apply(get_top_tld, f2, top2)\n",
      "77/384:\n",
      "pd.set_option('display.max_rows', 200)\n",
      "\n",
      "big_langs = clut[(clut.lang.isin(['he', 'ar', 'en', 'de', 'fr']))]\n",
      "big_langs.groupby(['source', 'lang']).apply(get_top_tld, 'tld_unshrtn', 5).to_frame()\n",
      "77/385:\n",
      "pd.set_option('display.max_rows', 200)\n",
      "clut.groupby(['source']).apply(get_top_two_levels, 'source', 'lang', 5, 'tld_unshrtn', 5, top_lang_by_source).to_frame()\n",
      "77/386:\n",
      "def get_top_two_levels(df, by, f1, top1, f2, top2, keep_df):\n",
      "    print (df.columns)\n",
      "    keep = keep_df.loc[keep_df[by]==df[by].iat[0], f1].head[top1]\n",
      "    df = df[df[f1].isin(keep)]\n",
      "    return df.groupby(f1).apply(get_top_tld, f2, top2)\n",
      "77/387:\n",
      "pd.set_option('display.max_rows', 200)\n",
      "clut.groupby(['source']).apply(get_top_two_levels, 'source', 'lang', 5, 'tld_unshrtn', 5, top_lang_by_source).to_frame()\n",
      "77/388:\n",
      "def get_top_two_levels(df, by, f1, top1, f2, top2, keep_df):\n",
      "    print (df.columns)\n",
      "    keep = keep_df.loc[keep_df[by]==df[by].iat[0], f1].head(top1)\n",
      "    df = df[df[f1].isin(keep)]\n",
      "    return df.groupby(f1).apply(get_top_tld, f2, top2)\n",
      "77/389:\n",
      "pd.set_option('display.max_rows', 200)\n",
      "clut.groupby(['source']).apply(get_top_two_levels, 'source', 'lang', 5, 'tld_unshrtn', 5, top_lang_by_source).to_frame()\n",
      "77/390: clut[clut.tld_unshrtn=='google.co.il']\n",
      "77/391: clut.groupby('suffix').uri_unsh_no_clean.nunique()\n",
      "77/392: clut.groupby('suffix').uri_unsh_no_clean.nunique().sort_values(ascending=False)\n",
      "77/393: clut.groupby('suffix').uri_unsh_no_clean.nunique().sort_values(ascending=False) / clut.uri_unsh_no_clean.nunique()\n",
      "77/394: clut.groupby('suffix').uri_unsh_no_clean.nunique().sort_values(ascending=False)\n",
      "77/395:\n",
      "nl = (pd.read_csv('data/collscan-daan-nlil_2014.cdx.gz', sep=' ', header=None, names=['timestamp', 'url'], usecols=[1,2])\n",
      "      .assign(timestamp=lambda x: x.timestamp.astype(int)))\n",
      "nl.head()\n",
      "77/396: clut[clut.suffix=='il'].uri_unsh_no_clean.unique()\n",
      "77/397: len(clut[clut.suffix=='il'].uri_unsh_no_clean.unique())\n",
      "77/398: nl[nl.url.isin(clut[clut.suffix=='il'].uri_unsh_no_clean.unique())\n",
      "77/399: nl[nl.url.isin(clut[clut.suffix=='il'].uri_unsh_no_clean.unique())]\n",
      "77/400: nl[nl.url.isin(clut[clut.suffix=='il'].uri_unsh_no_clean.unique())].url.nunique()/nl.url.nunique()\n",
      "77/401:\n",
      "import tldextract as tld\n",
      "def tryextract(dns):\n",
      "    try:\n",
      "        return '.'.join(tld.extract(dns)[-2:])\n",
      "    except TypeError:\n",
      "        return \"\"\n",
      "77/402: nl[~nl.url.isin(clut[clut.suffix=='il'].uri_unsh_no_clean.unique())]\n",
      "87/1: %matplotlib inline\n",
      "87/2:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "87/3: lut = pd.read_pickle('lang_uri_time_all.pkl.gz', compression='gzip')\n",
      "87/4: lut = pd.read_pickle('lang_uri_time_wb_avail_all.pkl.gz', compression='gzip')\n",
      "87/5: dict(lut.dtypes)\n",
      "87/6:\n",
      "macd_all = pd.read_pickle('data/macd_all.pkl.gz', compression='gzip')\n",
      "macd_all.head()\n",
      "87/7: lut['uri_unsh_no_clean'] = lut.uri_unshrtn_raw.combine_first(lut.uri_unshrtn_bitly.combine_first(lut.uri_unshrtn.combine_first(lut.uri)))\n",
      "87/8:\n",
      "lut['uri_mem_avail'] = lut.uri.map(~macd_all.drop_duplicates(subset='uri').set_index('uri').first_snap.isna()).fillna(False)\n",
      "lut['uri_unsh_no_clean_mem_avail'] = lut.uri_unsh_no_clean.map(~macd_all.drop_duplicates(subset='uri').set_index('uri').first_snap.isna()).fillna(False)\n",
      "87/9:\n",
      "curl = pd.read_csv('data/curl_results_parsed.csv.gz', compression='gzip')\n",
      "lut['status_code'] = lut.uri_unsh_no_clean.map(curl.drop_duplicates(subset='url').set_index('url').status_code)\n",
      "lut['content_type'] = lut.uri_unsh_no_clean.map(curl.drop_duplicates(subset='url').set_index('url').content_type)\n",
      "87/10: lut.head().T\n",
      "87/11: lut.shape\n",
      "87/12: lut.head.T\n",
      "87/13: lut.head().T\n",
      "87/14: lut[(~lut.status_code.isin([200, 300, 301, 302])) & (~lut.archived)]\n",
      "87/15: lut.head().T\n",
      "87/16:\n",
      "lut['archived'] = lut[['uri_cln_wb_avail', 'uri_cln_unshrtn_wb_avail', 'uri_mem_avail', 'uri_unsh_no_clean_mem_avail']].any(axis=1)\n",
      "\n",
      "lut.head().T\n",
      "87/17: lut.head().T\n",
      "87/18: lut[(~lut.status_code.isin([200, 300, 301, 302])) & (~lut.archived)]\n",
      "87/19: lut[(~lut.status_code.isin([200, 300, 301, 302, 303])) & (~lut.archived)]\n",
      "87/20: lut[(~lut.status_code.isin([200, 300, 301, 302, 303])) & (~lut.archived)].uri.unique()\n",
      "87/21: lut[(~lut.status_code.isin([200, 300, 301, 302, 303])) & (~lut.archived)].uri.nunique()\n",
      "87/22: lut[(~lut.status_code.isin([200, 300, 301, 302, 303])) & (~lut.archived)].uri\n",
      "87/23: lut[(~lut.status_code.isin([200, 300, 301, 302, 303])) & (~lut.archived)].uri.drop_duplicates()\n",
      "87/24: lut[(~lut.status_code.isin([200, 300, 301, 302, 303])) & (~lut.archived)].uri.drop_duplicates().to_csv('data/uri_down_not_archived.csv.gz', compression='gzip', index=False)\n",
      "87/25: lut[(~lut.status_code.isin([200, 300, 301, 302, 303])) & (~lut.archived)].uri.drop_duplicates().to_csv('data/uri_down_not_archived.csv.gz', header=True, compression='gzip', index=False)\n",
      "89/1: lut[(~lut.status_code.isin([200, 300, 301, 302, 303])) & (~lut.archived)]\n",
      "89/2: lut = pd.read_pickle('lang_uri_time_all.pkl.gz', compression='gzip')\n",
      "89/3: wb = pd.read_csv('data/wb_avail_cdx_all.csv.gz', compression='gzip')\n",
      "89/4: wb.head()\n",
      "89/5: wb['avail'] = pd.concat([wb.cdx>0, wb.available],axis=1).any(axis=1)\n",
      "89/6: %matplotlib inline\n",
      "89/7:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "89/8: lut = pd.read_pickle('lang_uri_time_all.pkl.gz', compression='gzip')\n",
      "89/9: wb = pd.read_csv('data/wb_avail_cdx_all.csv.gz', compression='gzip')\n",
      "89/10: wb.head()\n",
      "89/11: wb['avail'] = pd.concat([wb.cdx>0, wb.available],axis=1).any(axis=1)\n",
      "89/12:\n",
      "lut['uri_cln_wb_avail'] = lut.uri_cln.map(wb.set_index('uri').avail).fillna(False)\n",
      "lut['uri_cln_unshrtn_wb_avail'] = lut.uri_cln_unshrtn.map(wb.set_index('uri').avail).fillna(False)\n",
      "89/13: lut = pd.read_pickle('lang_uri_time_wb_avail_all.pkl.gz', compression='gzip')\n",
      "89/14: dict(lut.dtypes)\n",
      "89/15:\n",
      "macd_all = pd.read_pickle('data/macd_all.pkl.gz', compression='gzip')\n",
      "macd_all.head()\n",
      "89/16: lut['uri_unsh_no_clean'] = lut.uri_unshrtn_raw.combine_first(lut.uri_unshrtn_bitly.combine_first(lut.uri_unshrtn.combine_first(lut.uri)))\n",
      "89/17:\n",
      "lut['uri_mem_avail'] = lut.uri.map(~macd_all.drop_duplicates(subset='uri').set_index('uri').first_snap.isna()).fillna(False)\n",
      "lut['uri_unsh_no_clean_mem_avail'] = lut.uri_unsh_no_clean.map(~macd_all.drop_duplicates(subset='uri').set_index('uri').first_snap.isna()).fillna(False)\n",
      "89/18:\n",
      "curl = pd.read_csv('data/curl_results_parsed.csv.gz', compression='gzip')\n",
      "lut['status_code'] = lut.uri_unsh_no_clean.map(curl.drop_duplicates(subset='url').set_index('url').status_code)\n",
      "lut['content_type'] = lut.uri_unsh_no_clean.map(curl.drop_duplicates(subset='url').set_index('url').content_type)\n",
      "89/19: lut.head().T\n",
      "89/20: lut[(~lut.status_code.isin([200, 300, 301, 302, 303])) & (~lut.archived)]\n",
      "89/21:\n",
      "lut['archived'] = lut[['uri_cln_wb_avail', 'uri_cln_unshrtn_wb_avail', 'uri_mem_avail', 'uri_unsh_no_clean_mem_avail']].any(axis=1)\n",
      "\n",
      "lut.head().T\n",
      "89/22: lut[(~lut.status_code.isin([200, 300, 301, 302, 303])) & (~lut.archived)]\n",
      "89/23: lut[(~lut.status_code.isin([200, 300, 301, 302, 303])) & (~lut.archived) & (lut.uri.contains('makor'))]\n",
      "89/24: lut[(~lut.status_code.isin([200, 300, 301, 302, 303])) & (~lut.archived) & (lut.uri.str.contains('makor'))]\n",
      "89/25: lut[(~lut.status_code.isin([200, 300, 301, 302, 303, 403])) & (~lut.archived)]\n",
      "89/26: lut[(lut.status_code.isin([404, 0])) & (~lut.archived)].uri.drop_duplicates().to_csv('data/uri_down_not_archived.csv.gz', header=True, compression='gzip', index=False)\n",
      "89/27: lut[(lut.status_code.isin([404, 0])) & (~lut.archived)].shape\n",
      "89/28: lut[(lut.status_code.isin([404, 0])) & (~lut.archived)].uri.drop_duplicates().shape\n",
      "89/29: lut[(lut.status_code.isin([404, 0])) & (~lut.archived)].uri_unsh_no_clean_mem_avail.drop_duplicates().to_csv('data/uri_down_not_archived.csv.gz', header=True, compression='gzip', index=False)\n",
      "89/30:\n",
      "clut = lut.copy()\n",
      "clut.loc[clut.lang=='en,sco', 'lang'] = 'en'\n",
      "clut = (pd.concat([clut, clut.loc[clut.lang=='id,jv,ms', :].assign(lang='id'), \n",
      "                   clut.loc[clut.lang=='id,jv,ms', :].assign(lang='jv'), \n",
      "                   clut.loc[clut.lang=='id,jv,ms', :].assign(lang='ms')], axis=0)\n",
      "       .reset_index().drop('index', axis=1))\n",
      "clut = clut[~clut.lang.isin(['id,jv,ms', 'sco', '38'])]\n",
      "89/31: clut['suffix'] = [x[-1] for x in clut.tld_unshrtn.str.split('.')]\n",
      "89/32:\n",
      "def get_values(g):\n",
      "    return ','.join(list(g.unique()))\n",
      "#tlds_to_remove = ['fb.me', 'facebook.com', 'youtu.be', 'youtube.com']\n",
      "uris_down = clut[(lut.status_code.isin([404, 0])) & (~lut.archived)].groupby(['uri_unsh_no_clean', 'tld_unshrtn']).agg({'archived': np.max, 'source': get_values, 'lang': get_values}).reset_index()\n",
      "89/33:\n",
      "def get_values(g):\n",
      "    return ','.join(list(g.unique()))\n",
      "uris_down = clut[(lut.status_code.isin([404, 0])) & (~clut.archived)].groupby(['uri_unsh_no_clean', 'tld_unshrtn']).agg({'archived': np.max, 'source': get_values, 'lang': get_values}).reset_index()\n",
      "89/34:\n",
      "def get_values(g):\n",
      "    return ','.join(list(g.unique()))\n",
      "uris_down = clut[(clut.status_code.isin([404, 0])) & (~clut.archived)].groupby(['uri_unsh_no_clean', 'tld_unshrtn']).agg({'archived': np.max, 'source': get_values, 'lang': get_values}).reset_index()\n",
      "89/35: uris_down.head()\n",
      "89/36:\n",
      "def get_values(g):\n",
      "    return ','.join(list(g.unique()))\n",
      "uris_down = (clut[(clut.status_code.isin([404, 0])) & (~clut.archived)]\n",
      "             .groupby(['uri_unsh_no_clean', 'tld_unshrtn'])\n",
      "             .agg({'archived': np.max, 'source': get_values, 'lang': get_values})\n",
      "             .rename(columns={'uri_unsh_no_clean':'uri','tld_unshrtn': 'host'})\n",
      "             .reset_index()\n",
      "             )\n",
      "89/37: uris_down.head()\n",
      "89/38:\n",
      "def get_values(g):\n",
      "    return ','.join(list(g.unique()))\n",
      "uris_down = (clut[(clut.status_code.isin([404, 0])) & (~clut.archived)]\n",
      "             .groupby(['uri_unsh_no_clean', 'tld_unshrtn'])\n",
      "             .agg({'archived': np.max, 'source': get_values, 'lang': get_values})\n",
      "             .reset_index()\n",
      "             .rename(columns={'uri_unsh_no_clean':'uri','tld_unshrtn': 'host'})\n",
      "             )\n",
      "89/39: uris_down.head()\n",
      "89/40: uris_down.shape()\n",
      "89/41: uris_down.shape\n",
      "89/42: uris_down.to_csv('data/uri_down_not_archived.csv.gz', header=True, compression='gzip', index=False)\n",
      "92/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "sns.set_palette('Set2', 12)\n",
      "%matplotlib inline\n",
      "92/2: sns.__version__\n",
      "92/3:\n",
      "%%time\n",
      "df = pd.read_pickle('turf/data_df.pkl.gz', compression='gzip')\n",
      "92/4: df.dtypes\n",
      "92/5: df[df.text.contains('שואה')]\n",
      "92/6: df[df.text.str.contains('שואה')]\n",
      "92/7: df[df.text.fillna('').str.contains('שואה')]\n",
      "92/8: df[(df.text.fillna('').str.contains('אושוויץ')) | (df.text.fillna('').str.contains('אושויץ'))].shape\n",
      "92/9: df[df.text.fillna('').str.contains('שואה')].shape\n",
      "92/10: df[df.text.fillna('').str.contains('גטו')].shape\n",
      "92/11: df[(df.text.fillna('').str.contains('שואה')) & (~df.text.fillna('').str.contains('משואה'))].shape\n",
      "92/12: df[df.text.fillna('').str.contains('גטו')].shape\n",
      "92/13: df[(df.text.fillna('').str.contains('מחנות')) & ((df.text.fillna('').str.contains('ריכוז')) | (df.text.fillna('').str.contains('השמדה')))].shape\n",
      "92/14: df[df.text.fillna('').str.contains('היטלר')].shape\n",
      "92/15: df[df.text.fillna('').str.contains('אייכמן')].shape\n",
      "92/16: df[df.text.fillna('').str.contains('נאצי')].shape\n",
      "92/17: df[df.text.fillna('').str.contains('מוזלמן')].shape\n",
      "92/18: df[df.text.fillna('').str.contains('טלאי צהוב')].shape\n",
      "92/19: df[(df.text.fillna('').str.contains('מיידנק')) | (df.text.fillna('').str.contains('מאידנק'))].shape\n",
      "92/20: df[df.text.fillna('').str.contains('טרבלינקה')].shape\n",
      "92/21: df[df.text.fillna('').str.contains('יד ושם')].shape\n",
      "92/22: df[(df.text.fillna('').str.contains('גטו')) | (df.text.fillna('').str.contains('גטאות'))].shape\n",
      "92/23: df[(df.text.fillna('').str.contains('יודנראט')) | (df.text.fillna('').str.contains('יודנרט'))].shape\n",
      "92/24: df[(df.text.fillna('').str.contains('חסיד')) | (df.text.fillna('').str.contains('אומות עולם'))].shape\n",
      "92/25: df[(df.text.fillna('').str.contains('גסטפו')) | (df.text.fillna('').str.contains('גסטאפו'))].shape\n",
      "92/26:\n",
      "import os\n",
      "os.mkdir('shoa')\n",
      "92/27: df[(df.text.fillna('').str.contains('גטו')) | (df.text.fillna('').str.contains('גטאות'))].to_csv('shoa/ghetto.csv', index=False)\n",
      "92/28: df[(df.text.fillna('').str.contains('שואה')) & (~df.text.fillna('').str.contains('משואה'))].to_csv('shoa/shoa.csv', index=False)\n",
      "92/29: df[(df.text.fillna('').str.contains('גטו')) | (df.text.fillna('').str.contains('גטאות'))].to_csv('shoa/ghetto.csv', index=False)\n",
      "92/30: df[(df.text.fillna('').str.contains('אושוויץ')) | (df.text.fillna('').str.contains('אושויץ'))].to_csv('shoa/auschwitz.csv', index=False)\n",
      "92/31: df[((df.text.fillna('').str.contains('מחנות')) | (df.text.fillna('').str.contains('מחנה'))) & ((df.text.fillna('').str.contains('ריכוז')) | (df.text.fillna('').str.contains('השמדה')))].to_csv('shoa/mahane_rikuz_or_hashmada.csv', index=False)\n",
      "92/32: df[df.text.fillna('').str.contains('היטלר')].to_csv('shoa/hitler.csv', index=False)\n",
      "92/33: df[df.text.fillna('').str.contains('אייכמן')].to_csv('shoa/eichman.csv', index=False)\n",
      "92/34: df[df.text.fillna('').str.contains('נאצי')].to_csv('shoa/nazi.csv', index=False)\n",
      "92/35: df[df.text.fillna('').str.contains('מוזלמן')].to_csv('shoa/muzelman.csv', index=False)\n",
      "92/36: df[df.text.fillna('').str.contains('טלאי צהוב')].to_csv('shoa/tlay_zahov.csv', index=False)\n",
      "92/37: df[df.text.fillna('').str.contains('טרבלינקה')].to_csv('shoa/treblinka.csv', index=False)\n",
      "92/38: df[(df.text.fillna('').str.contains('מיידנק')) | (df.text.fillna('').str.contains('מאידנק'))].to_csv('shoa/maidanek.csv', index=False)\n",
      "92/39: df[df.text.fillna('').str.contains('יד ושם')].to_csv('shoa/yad_va_shem.csv', index=False)\n",
      "92/40: df[(df.text.fillna('').str.contains('יודנראט')) | (df.text.fillna('').str.contains('יודנרט'))].to_csv('shoa/judenrat.csv', index=False)\n",
      "92/41: df[(df.text.fillna('').str.contains('חסיד')) | (df.text.fillna('').str.contains('אומות עולם'))].to_csv('shoa/hasid_umot_olam.csv', index=False)\n",
      "92/42: df[(df.text.fillna('').str.contains('גסטפו')) | (df.text.fillna('').str.contains('גסטאפו'))].to_csv('shoa/gestapo.csv', index=False)\n",
      "92/43:\n",
      "df[((df.text.fillna('').str.contains('שואה')) & (~df.text.fillna('').str.contains('משואה'))) | ]\n",
      "df[(df.text.fillna('').str.contains('גטו')) | (df.text.fillna('').str.contains('גטאות'))].to_csv('shoa/ghetto.csv', index=False)\n",
      "\n",
      "df[(df.text.fillna('').str.contains('אושוויץ')) | (df.text.fillna('').str.contains('אושויץ'))].to_csv('shoa/auschwitz.csv', index=False)\n",
      "\n",
      "df[((df.text.fillna('').str.contains('מחנות')) | (df.text.fillna('').str.contains('מחנה'))) & ((df.text.fillna('').str.contains('ריכוז')) | (df.text.fillna('').str.contains('השמדה')))].to_csv('shoa/mahane_rikuz_or_hashmada.csv', index=False)\n",
      "\n",
      "df[df.text.fillna('').str.contains('היטלר')].to_csv('shoa/hitler.csv', index=False)\n",
      "\n",
      "df[df.text.fillna('').str.contains('אייכמן')].to_csv('shoa/eichman.csv', index=False)\n",
      "\n",
      "df[df.text.fillna('').str.contains('נאצי')].to_csv('shoa/nazi.csv', index=False)\n",
      "\n",
      "df[df.text.fillna('').str.contains('מוזלמן')].to_csv('shoa/muzelman.csv', index=False)\n",
      "\n",
      "df[df.text.fillna('').str.contains('טלאי צהוב')].to_csv('shoa/tlay_zahov.csv', index=False)\n",
      "\n",
      "df[df.text.fillna('').str.contains('טרבלינקה')].to_csv('shoa/treblinka.csv', index=False)\n",
      "\n",
      "df[(df.text.fillna('').str.contains('מיידנק')) | (df.text.fillna('').str.contains('מאידנק'))].to_csv('shoa/maidanek.csv', index=False)\n",
      "\n",
      "df[df.text.fillna('').str.contains('יד ושם')].to_csv('shoa/yad_va_shem.csv', index=False)\n",
      "\n",
      "df[(df.text.fillna('').str.contains('יודנראט')) | (df.text.fillna('').str.contains('יודנרט'))].to_csv('shoa/judenrat.csv', index=False)\n",
      "\n",
      "df[(df.text.fillna('').str.contains('חסיד')) | (df.text.fillna('').str.contains('אומות עולם'))].to_csv('shoa/hasid_umot_olam.csv', index=False)\n",
      "\n",
      "df[(df.text.fillna('').str.contains('גסטפו')) | (df.text.fillna('').str.contains('גסטאפו'))].to_csv('shoa/gestapo.csv', index=False)\n",
      "92/44:\n",
      "df[((df.text.fillna('').str.contains('שואה')) & (~df.text.fillna('').str.contains('משואה'))) | ]\n",
      "df[(df.text.fillna('').str.contains('גטו')) | (df.text.fillna('').str.contains('גטאות'))].to_csv('shoa/ghetto.csv', index=False)\n",
      "\n",
      "df[(df.text.fillna('').str.contains('אושוויץ')) | (df.text.fillna('').str.contains('אושויץ'))].to_csv('shoa/auschwitz.csv', index=False)\n",
      "\n",
      "df[((df.text.fillna('').str.contains('מחנות')) | (df.text.fillna('').str.contains('מחנה'))) & ((df.text.fillna('').str.contains('ריכוז')) | (df.text.fillna('').str.contains('השמדה')))].to_csv('shoa/mahane_rikuz_or_hashmada.csv', index=False)\n",
      "\n",
      "df[df.text.fillna('').str.contains('היטלר')].to_csv('shoa/hitler.csv', index=False)\n",
      "\n",
      "df[df.text.fillna('').str.contains('אייכמן')].to_csv('shoa/eichman.csv', index=False)\n",
      "\n",
      "df[df.text.fillna('').str.contains('נאצי')].to_csv('shoa/nazi.csv', index=False)\n",
      "\n",
      "df[df.text.fillna('').str.contains('מוזלמן')].to_csv('shoa/muzelman.csv', index=False)\n",
      "\n",
      "df[df.text.fillna('').str.contains('טלאי צהוב')].to_csv('shoa/tlay_zahov.csv', index=False)\n",
      "\n",
      "df[df.text.fillna('').str.contains('טרבלינקה')].to_csv('shoa/treblinka.csv', index=False)\n",
      "\n",
      "df[(df.text.fillna('').str.contains('מיידנק')) | (df.text.fillna('').str.contains('מאידנק'))].to_csv('shoa/maidanek.csv', index=False)\n",
      "\n",
      "df[df.text.fillna('').str.contains('יד ושם')].to_csv('shoa/yad_va_shem.csv', index=False)\n",
      "\n",
      "df[(df.text.fillna('').str.contains('יודנראט')) | (df.text.fillna('').str.contains('יודנרט'))].to_csv('shoa/judenrat.csv', index=False)\n",
      "\n",
      "df[(df.text.fillna('').str.contains('חסיד')) | (df.text.fillna('').str.contains('אומות עולם'))].to_csv('shoa/hasid_umot_olam.csv', index=False)\n",
      "\n",
      "df[(df.text.fillna('').str.contains('גסטפו')) | (df.text.fillna('').str.contains('גסטאפו'))].to_csv('shoa/gestapo.csv', index=False)\n",
      "92/45:\n",
      "df[((df.text.fillna('').str.contains('שואה')) & (~df.text.fillna('').str.contains('משואה')))\n",
      "  | ()]\n",
      "df[(df.text.fillna('').str.contains('גטו')) | (df.text.fillna('').str.contains('גטאות'))].to_csv('shoa/ghetto.csv', index=False)\n",
      "\n",
      "df[(df.text.fillna('').str.contains('אושוויץ')) | (df.text.fillna('').str.contains('אושויץ'))].to_csv('shoa/auschwitz.csv', index=False)\n",
      "\n",
      "df[((df.text.fillna('').str.contains('מחנות')) | (df.text.fillna('').str.contains('מחנה'))) & ((df.text.fillna('').str.contains('ריכוז')) | (df.text.fillna('').str.contains('השמדה')))].to_csv('shoa/mahane_rikuz_or_hashmada.csv', index=False)\n",
      "\n",
      "df[df.text.fillna('').str.contains('היטלר')].to_csv('shoa/hitler.csv', index=False)\n",
      "\n",
      "df[df.text.fillna('').str.contains('אייכמן')].to_csv('shoa/eichman.csv', index=False)\n",
      "\n",
      "df[df.text.fillna('').str.contains('נאצי')].to_csv('shoa/nazi.csv', index=False)\n",
      "\n",
      "df[df.text.fillna('').str.contains('מוזלמן')].to_csv('shoa/muzelman.csv', index=False)\n",
      "\n",
      "df[df.text.fillna('').str.contains('טלאי צהוב')].to_csv('shoa/tlay_zahov.csv', index=False)\n",
      "\n",
      "df[df.text.fillna('').str.contains('טרבלינקה')].to_csv('shoa/treblinka.csv', index=False)\n",
      "\n",
      "df[(df.text.fillna('').str.contains('מיידנק')) | (df.text.fillna('').str.contains('מאידנק'))].to_csv('shoa/maidanek.csv', index=False)\n",
      "\n",
      "df[df.text.fillna('').str.contains('יד ושם')].to_csv('shoa/yad_va_shem.csv', index=False)\n",
      "\n",
      "df[(df.text.fillna('').str.contains('יודנראט')) | (df.text.fillna('').str.contains('יודנרט'))].to_csv('shoa/judenrat.csv', index=False)\n",
      "\n",
      "df[(df.text.fillna('').str.contains('חסיד')) | (df.text.fillna('').str.contains('אומות עולם'))].to_csv('shoa/hasid_umot_olam.csv', index=False)\n",
      "\n",
      "df[(df.text.fillna('').str.contains('גסטפו')) | (df.text.fillna('').str.contains('גסטאפו'))].to_csv('shoa/gestapo.csv', index=False)\n",
      "92/46:\n",
      "df[((df.text.fillna('').str.contains('שואה')) & (~df.text.fillna('').str.contains('משואה')))\n",
      "   | ((df.text.fillna('').str.contains('גטו')) | (df.text.fillna('').str.contains('גטאות')))\n",
      "   | ((df.text.fillna('').str.contains('אושוויץ')) | (df.text.fillna('').str.contains('אושויץ')))\n",
      "   | (((df.text.fillna('').str.contains('מחנות')) | (df.text.fillna('').str.contains('מחנה'))) & ((df.text.fillna('').str.contains('ריכוז')) | (df.text.fillna('').str.contains('השמדה'))))\n",
      "   | (df.text.fillna('').str.contains('היטלר'))\n",
      "   | (df.text.fillna('').str.contains('אייכמן'))\n",
      "   | (df.text.fillna('').str.contains('נאצי'))\n",
      "   | (df.text.fillna('').str.contains('מוזלמן'))\n",
      "   | (df.text.fillna('').str.contains('טלאי צהוב'))\n",
      "   | (df.text.fillna('').str.contains('טרבלינקה'))\n",
      "   | ((df.text.fillna('').str.contains('מיידנק')) | (df.text.fillna('').str.contains('מאידנק')))\n",
      "   | (df.text.fillna('').str.contains('יד ושם'))\n",
      "   | ((df.text.fillna('').str.contains('יודנראט')) | (df.text.fillna('').str.contains('יודנרט')))\n",
      "   | ((df.text.fillna('').str.contains('חסיד')) | (df.text.fillna('').str.contains('אומות עולם')))\n",
      "   | ((df.text.fillna('').str.contains('גסטפו')) | (df.text.fillna('').str.contains('גסטאפו')))].groupby(pd.Grouper(created_time, freq='1d')).size()\n",
      "92/47:\n",
      "shoa = df[((df.text.fillna('').str.contains('שואה')) & (~df.text.fillna('').str.contains('משואה')))\n",
      "   | ((df.text.fillna('').str.contains('גטו')) | (df.text.fillna('').str.contains('גטאות')))\n",
      "   | ((df.text.fillna('').str.contains('אושוויץ')) | (df.text.fillna('').str.contains('אושויץ')))\n",
      "   | (((df.text.fillna('').str.contains('מחנות')) | (df.text.fillna('').str.contains('מחנה'))) & ((df.text.fillna('').str.contains('ריכוז')) | (df.text.fillna('').str.contains('השמדה'))))\n",
      "   | (df.text.fillna('').str.contains('היטלר'))\n",
      "   | (df.text.fillna('').str.contains('אייכמן'))\n",
      "   | (df.text.fillna('').str.contains('נאצי'))\n",
      "   | (df.text.fillna('').str.contains('מוזלמן'))\n",
      "   | (df.text.fillna('').str.contains('טלאי צהוב'))\n",
      "   | (df.text.fillna('').str.contains('טרבלינקה'))\n",
      "   | ((df.text.fillna('').str.contains('מיידנק')) | (df.text.fillna('').str.contains('מאידנק')))\n",
      "   | (df.text.fillna('').str.contains('יד ושם'))\n",
      "   | ((df.text.fillna('').str.contains('יודנראט')) | (df.text.fillna('').str.contains('יודנרט')))\n",
      "   | ((df.text.fillna('').str.contains('חסיד')) | (df.text.fillna('').str.contains('אומות עולם')))\n",
      "   | ((df.text.fillna('').str.contains('גסטפו')) | (df.text.fillna('').str.contains('גסטאפו')))]\n",
      "92/48: df.text = df.text.fillna('')\n",
      "92/49:\n",
      "shoa = df[((df.text.str.contains('שואה')) & (~df.text.str.contains('משואה')))\n",
      "   | ((df.text.str.contains('גטו')) | (df.text.str.contains('גטאות')))\n",
      "   | ((df.text.str.contains('אושוויץ')) | (df.text.str.contains('אושויץ')))\n",
      "   | (((df.text.str.contains('מחנות')) | (df.text.str.contains('מחנה'))) & ((df.text.str.contains('ריכוז')) | (df.text.str.contains('השמדה'))))\n",
      "   | (df.text.str.contains('היטלר'))\n",
      "   | (df.text.str.contains('אייכמן'))\n",
      "   | (df.text.str.contains('נאצי'))\n",
      "   | (df.text.str.contains('מוזלמן'))\n",
      "   | (df.text.str.contains('טלאי צהוב'))\n",
      "   | (df.text.str.contains('טרבלינקה'))\n",
      "   | ((df.text.str.contains('מיידנק')) | (df.text.str.contains('מאידנק')))\n",
      "   | (df.text.str.contains('יד ושם'))\n",
      "   | ((df.text.str.contains('יודנראט')) | (df.text.str.contains('יודנרט')))\n",
      "   | ((df.text.str.contains('חסיד')) | (df.text.str.contains('אומות עולם')))\n",
      "   | ((df.text.str.contains('גסטפו')) | (df.text.str.contains('גסטאפו')))]\n",
      "92/50: shoa.groupby(pd.Grouper(created_time, freq='1d')).size().plot()\n",
      "92/51: shoa.groupby(pd.Grouper('created_time', freq='1d')).size().plot()\n",
      "92/52: shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().plot()\n",
      "92/53: shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().plot(size=(10,7))\n",
      "92/54: shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().plot(figsize=(10,7))\n",
      "92/55: shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().plot(figsize=(15,10))\n",
      "92/56: shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size()\n",
      "92/57: shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().sort_values(asceding=False)\n",
      "92/58: shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().sort_values(ascending=False)\n",
      "92/59: fig, ax = shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().plot(figsize=(15,10))\n",
      "92/60: ax = shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().plot(figsize=(15,10))\n",
      "92/61:\n",
      "ax = shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().plot(figsize=(15,10))\n",
      "ax.annotate('local max', xy=(2, 1), xytext=(3, 1.5),\n",
      "            arrowprops=dict(facecolor='black', shrink=0.05),\n",
      "            )\n",
      "92/62:\n",
      "fig, ax = plt.subplots(figsize=(15,10))\n",
      "ax = shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().plot(ax=ax)\n",
      "for k, v in shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().sort_values(ascending=False).head(10):\n",
      "    ax.annotate(k, v)\n",
      "92/63:\n",
      "fig, ax = plt.subplots(figsize=(15,10))\n",
      "ax = shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().plot(ax=ax)\n",
      "for k, v in shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().sort_values(ascending=False).head(10).iterrows():\n",
      "    ax.annotate(k, v)\n",
      "92/64:\n",
      "fig, ax = plt.subplots(figsize=(15,10))\n",
      "ax = shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().plot(ax=ax)\n",
      "for k, v in shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().sort_values(ascending=False).head(10).itertuples():\n",
      "    ax.annotate(k, v)\n",
      "92/65:\n",
      "fig, ax = plt.subplots(figsize=(15,10))\n",
      "ax = shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().plot(ax=ax)\n",
      "for k, v in shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().sort_values(ascending=False).head(10).iteritems():\n",
      "    ax.annotate(k, v)\n",
      "92/66: shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().sort_values(ascending=False).head(10)\n",
      "92/67:\n",
      "for x in shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().sort_values(ascending=False).head(10):\n",
      "    print(x)\n",
      "92/68:\n",
      "for x in shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().sort_values(ascending=False).head(10).iteritems():\n",
      "    print(x)\n",
      "92/69:\n",
      "fig, ax = plt.subplots(figsize=(15,10))\n",
      "ax = shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().plot(ax=ax)\n",
      "for k, v in shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().sort_values(ascending=False).head(10).iteritems():\n",
      "    ax.annotate(k, v,\n",
      "                xytext=(10,-5), textcoords='offset points',\n",
      "                family='sans-serif', fontsize=18, color='darkslategrey')\n",
      "92/70:\n",
      "fig, ax = plt.subplots(figsize=(15,10))\n",
      "shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().plot(ax=ax)\n",
      "for k, v in shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().sort_values(ascending=False).head(10).iteritems():\n",
      "    ax.annotate(k, v,\n",
      "                xytext=(10,-5), textcoords='offset points',\n",
      "                family='sans-serif', fontsize=18, color='darkslategrey')\n",
      "92/71:\n",
      "fig, ax = plt.subplots(figsize=(15,10))\n",
      "shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().plot(ax=ax)\n",
      "for k, v in shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().sort_values(ascending=False).head(10).iteritems():\n",
      "    ax.annotate(k, (k, v),\n",
      "                xytext=(10,-5), textcoords='offset points',\n",
      "                family='sans-serif', fontsize=18, color='darkslategrey')\n",
      "92/72:\n",
      "fig, ax = plt.subplots(figsize=(15,10))\n",
      "shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().plot(ax=ax)\n",
      "for k, v in shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().sort_values(ascending=False).head(10).iteritems():\n",
      "    ax.annotate(k, (k, v),\n",
      "                xytext=(10,-5), textcoords='offset points',\n",
      "                family='sans-serif', fontsize=14, color='darkslategrey')\n",
      "92/73:\n",
      "fig, ax = plt.subplots(figsize=(15,10))\n",
      "shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().plot(ax=ax)\n",
      "for k, v in shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().sort_values(ascending=False).head(10).iteritems():\n",
      "    print (type(k))\n",
      "    ax.annotate(k, (k, v),\n",
      "                xytext=(10,-5), textcoords='offset points',\n",
      "                family='sans-serif', fontsize=14, color='darkslategrey')\n",
      "92/74:\n",
      "fig, ax = plt.subplots(figsize=(15,10))\n",
      "shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().plot(ax=ax)\n",
      "for k, v in shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().sort_values(ascending=False).head(10).iteritems():\n",
      "    print (dir(k))\n",
      "    ax.annotate(k, (k, v),\n",
      "                xytext=(10,-5), textcoords='offset points',\n",
      "                family='sans-serif', fontsize=14, color='darkslategrey')\n",
      "92/75:\n",
      "fig, ax = plt.subplots(figsize=(15,10))\n",
      "shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().plot(ax=ax)\n",
      "for k, v in shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().sort_values(ascending=False).head(10).iteritems():\n",
      "    ax.annotate(k.strftime('%Y-%m-%d'), (k, v),\n",
      "                xytext=(10,-5), textcoords='offset points',\n",
      "                family='sans-serif', fontsize=14, color='darkslategrey')\n",
      "92/76:\n",
      "fig, ax = plt.subplots(figsize=(15,10))\n",
      "shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().plot(ax=ax)\n",
      "for k, v in shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().sort_values(ascending=False).head(20).iteritems():\n",
      "    ax.annotate(k.strftime('%Y-%m-%d'), (k, v),\n",
      "                xytext=(10,-5), textcoords='offset points',\n",
      "                family='sans-serif', fontsize=14, color='darkslategrey')\n",
      "92/77:\n",
      "fig, ax = plt.subplots(figsize=(15,10))\n",
      "shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().plot(ax=ax)\n",
      "for k, v in shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().sort_values(ascending=False).head(20).iteritems():\n",
      "    ax.annotate(k.strftime('%Y-%m-%d'), (k, v),\n",
      "                xytext=(10,-5), textcoords='offset points',\n",
      "                family='sans-serif', fontsize=14, color='darkslategrey')\n",
      "plt.tight_layout()\n",
      "92/78:\n",
      "fig, ax = plt.subplots(figsize=(15,10))\n",
      "shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().plot(ax=ax)\n",
      "for k, v in shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().sort_values(ascending=False).head(15).iteritems():\n",
      "    ax.annotate(k.strftime('%Y-%m-%d'), (k, v),\n",
      "                xytext=(10,-5), textcoords='offset points',\n",
      "                family='sans-serif', fontsize=14, color='darkslategrey')\n",
      "plt.tight_layout()\n",
      "92/79:\n",
      "fig, ax = plt.subplots(figsize=(15,10))\n",
      "shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().plot(ax=ax, title='All shoa keyword appearances. Num. of comments by date.')\n",
      "for k, v in shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().sort_values(ascending=False).head(15).iteritems():\n",
      "    ax.annotate(k.strftime('%Y-%m-%d'), (k, v),\n",
      "                xytext=(10,-5), textcoords='offset points',\n",
      "                family='sans-serif', fontsize=14, color='darkslategrey')\n",
      "plt.tight_layout()\n",
      "92/80:\n",
      "fig, ax = plt.subplots(figsize=(15,10))\n",
      "shoa[shoa.created_time.dt>'2015-03-01'].groupby(pd.Grouper(key='created_time', freq='1d')).size().plot(ax=ax, title='All shoa keyword appearances. Num. of comments by date.')\n",
      "for k, v in shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().sort_values(ascending=False).head(15).iteritems():\n",
      "    ax.annotate(k.strftime('%Y-%m-%d'), (k, v),\n",
      "                xytext=(10,-5), textcoords='offset points',\n",
      "                family='sans-serif', fontsize=14, color='darkslategrey')\n",
      "plt.tight_layout()\n",
      "92/81:\n",
      "fig, ax = plt.subplots(figsize=(15,10))\n",
      "shoa[shoa.created_time>'2015-03-01'].groupby(pd.Grouper(key='created_time', freq='1d')).size().plot(ax=ax, title='All shoa keyword appearances. Num. of comments by date.')\n",
      "for k, v in shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().sort_values(ascending=False).head(15).iteritems():\n",
      "    ax.annotate(k.strftime('%Y-%m-%d'), (k, v),\n",
      "                xytext=(10,-5), textcoords='offset points',\n",
      "                family='sans-serif', fontsize=14, color='darkslategrey')\n",
      "plt.tight_layout()\n",
      "92/82:\n",
      "fig, ax = plt.subplots(figsize=(15,10))\n",
      "shoa[shoa.created_time>'2015-02-01'].groupby(pd.Grouper(key='created_time', freq='1d')).size().plot(ax=ax, title='All shoa keyword appearances. Num. of comments by date.')\n",
      "for k, v in shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().sort_values(ascending=False).head(15).iteritems():\n",
      "    ax.annotate(k.strftime('%Y-%m-%d'), (k, v),\n",
      "                xytext=(10,-5), textcoords='offset points',\n",
      "                family='sans-serif', fontsize=14, color='darkslategrey')\n",
      "plt.tight_layout()\n",
      "92/83:\n",
      "fig, ax = plt.subplots(figsize=(15,10))\n",
      "shoa[shoa.created_time>'2015-03-01'].groupby(pd.Grouper(key='created_time', freq='1d')).size().plot(ax=ax, title='All shoa keyword appearances. Num. of comments by date.')\n",
      "for k, v in shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().sort_values(ascending=False).head(15).iteritems():\n",
      "    ax.annotate(k.strftime('%Y-%m-%d'), (k, v),\n",
      "                xytext=(10,-5), textcoords='offset points',\n",
      "                family='sans-serif', fontsize=14, color='darkslategrey')\n",
      "plt.tight_layout()\n",
      "92/84: shoa[shoa.created_day=='2015-10-21']\n",
      "92/85: shoa.created_day.head()\n",
      "92/86: shoa[shoa.created_day=='2015-10-21']\n",
      "92/87: type(shoa.created_day.iat[0])\n",
      "92/88: shoa[shoa.created_day.dt=='2015-10-21']\n",
      "92/89: shoa[shoa.created_time=='2015-10-21']\n",
      "92/90: shoa[(shoa.created_time>='2015-10-21') & (shoa.created_time<'2015-10-22')]\n",
      "92/91: shoa[(shoa.created_time>='2015-10-21') & (shoa.created_time<'2015-10-22')].text.tolist()\n",
      "92/92: shoa[(shoa.created_time>='2017-01-27') & (shoa.created_time<'2015-10-28')].text.tolist()\n",
      "92/93: shoa[(shoa.created_time>='2017-01-27') & (shoa.created_time<'2017-01-28')].text.tolist()\n",
      "92/94: shoa[(shoa.created_time>='2016-09-23') & (shoa.created_time<'2017-09-24')].text.tolist()\n",
      "92/95: shoa[(shoa.created_time>='2016-09-23') & (shoa.created_time<'2016-09-24')].text.tolist()\n",
      "92/96:\n",
      "for x in shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().sort_values(ascending=False).head(20).iteritems():\n",
      "    print(x)\n",
      "92/97: shoa[(shoa.created_time>='2016-07-09') & (shoa.created_time<'2016-07-10')].text.tolist()\n",
      "92/98: shoa[(shoa.created_time>='2016-07-09') & (shoa.created_time<'2016-07-10')]\n",
      "92/99: shoa[(shoa.created_time>='2016-07-09') & (shoa.created_time<'2016-07-10')].post_id\n",
      "92/100: shoa[(shoa.created_time>='2016-07-09') & (shoa.created_time<'2016-07-10')].post_id.value_counts()\n",
      "92/101: shoa[(shoa.created_time>='2016-12-24') & (shoa.created_time<'2016-12-25')].post_id.value_counts()\n",
      "92/102:\n",
      "for x in shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().sort_values(ascending=False).head(30).iteritems():\n",
      "    print(x)\n",
      "92/103:\n",
      "for x in shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().sort_values(ascending=False).head(40).iteritems():\n",
      "    print(x)\n",
      "92/104: shoa[(shoa.created_time>='2015-07-31') & (shoa.created_time>='2015-07-31')].post_id.value_counts()\n",
      "92/105: shoa[(shoa.created_time>='2015-07-31') & (shoa.created_time<'2015-08-01')].post_id.value_counts()\n",
      "92/106: shoa[(shoa.created_time>='2015-12-22') & (shoa.created_time<'2015-12-23')].post_id.value_counts()\n",
      "92/107: shoa[(shoa.created_time>='2016-12-28') & (shoa.created_time<'2016-12-29')].post_id.value_counts()\n",
      "92/108: df[(df.text.fillna('').str.contains('שואה')) & (~df.text.fillna('').str.contains('משואה'))].shape\n",
      "92/109: df[(df.text.fillna('').str.contains('שואה')) & (~df.text.fillna('').str.contains('משואה'))& (~df.text.fillna('').str.contains('נשואה'))].shape\n",
      "92/110:\n",
      "shoa = df[((df.text.fillna('').str.contains('שואה')) & (~df.text.fillna('').str.contains('משואה'))& (~df.text.fillna('').str.contains('נשואה')))\n",
      "   | ((df.text.str.contains('גטו')) | (df.text.str.contains('גטאות')))\n",
      "   | ((df.text.str.contains('אושוויץ')) | (df.text.str.contains('אושויץ')))\n",
      "   | (((df.text.str.contains('מחנות')) | (df.text.str.contains('מחנה'))) & ((df.text.str.contains('ריכוז')) | (df.text.str.contains('השמדה'))))\n",
      "   | (df.text.str.contains('היטלר'))\n",
      "   | (df.text.str.contains('אייכמן'))\n",
      "   | (df.text.str.contains('נאצי'))\n",
      "   | (df.text.str.contains('מוזלמן'))\n",
      "   | (df.text.str.contains('טלאי צהוב'))\n",
      "   | (df.text.str.contains('טרבלינקה'))\n",
      "   | ((df.text.str.contains('מיידנק')) | (df.text.str.contains('מאידנק')))\n",
      "   | (df.text.str.contains('יד ושם'))\n",
      "   | ((df.text.str.contains('יודנראט')) | (df.text.str.contains('יודנרט')))\n",
      "   | ((df.text.str.contains('חסיד')) | (df.text.str.contains('אומות עולם')))\n",
      "   | ((df.text.str.contains('גסטפו')) | (df.text.str.contains('גסטאפו')))]\n",
      "92/111:\n",
      "fig, ax = plt.subplots(figsize=(15,10))\n",
      "shoa[shoa.created_time>'2015-03-01'].groupby(pd.Grouper(key='created_time', freq='1d')).size().plot(ax=ax, title='All shoa keyword appearances. Num. of comments by date.')\n",
      "for k, v in shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().sort_values(ascending=False).head(15).iteritems():\n",
      "    ax.annotate(k.strftime('%Y-%m-%d'), (k, v),\n",
      "                xytext=(10,-5), textcoords='offset points',\n",
      "                family='sans-serif', fontsize=14, color='darkslategrey')\n",
      "plt.tight_layout()\n",
      "92/112:\n",
      "for x in shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().sort_values(ascending=False).head(40).iteritems():\n",
      "    print(x)\n",
      "92/113:\n",
      "nshoa = df[((df.text.str.contains('גטו')) | (df.text.str.contains('גטאות')))\n",
      "   | ((df.text.str.contains('אושוויץ')) | (df.text.str.contains('אושויץ')))\n",
      "   | (((df.text.str.contains('מחנות')) | (df.text.str.contains('מחנה'))) & ((df.text.str.contains('ריכוז')) | (df.text.str.contains('השמדה'))))\n",
      "   | (df.text.str.contains('היטלר'))\n",
      "   | (df.text.str.contains('אייכמן'))\n",
      "   | (df.text.str.contains('נאצי'))\n",
      "   | (df.text.str.contains('מוזלמן'))\n",
      "   | (df.text.str.contains('טלאי צהוב'))\n",
      "   | (df.text.str.contains('טרבלינקה'))\n",
      "   | ((df.text.str.contains('מיידנק')) | (df.text.str.contains('מאידנק')))\n",
      "   | (df.text.str.contains('יד ושם'))\n",
      "   | ((df.text.str.contains('יודנראט')) | (df.text.str.contains('יודנרט')))\n",
      "   | ((df.text.str.contains('חסיד')) | (df.text.str.contains('אומות עולם')))\n",
      "   | ((df.text.str.contains('גסטפו')) | (df.text.str.contains('גסטאפו')))]\n",
      "92/114:\n",
      "fig, ax = plt.subplots(figsize=(15,10))\n",
      "nshoa[nshoa.created_time>'2015-03-01'].groupby(pd.Grouper(key='created_time', freq='1d')).size().plot(ax=ax, title='All shoa keyword appearances. Num. of comments by date.')\n",
      "for k, v in shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().sort_values(ascending=False).head(15).iteritems():\n",
      "    ax.annotate(k.strftime('%Y-%m-%d'), (k, v),\n",
      "                xytext=(10,-5), textcoords='offset points',\n",
      "                family='sans-serif', fontsize=14, color='darkslategrey')\n",
      "plt.tight_layout()\n",
      "92/115:\n",
      "fig, ax = plt.subplots(figsize=(15,10))\n",
      "nshoa[nshoa.created_time>'2015-03-01'].groupby(pd.Grouper(key='created_time', freq='1d')).size().plot(ax=ax, title='All shoa keyword appearances. Num. of comments by date.')\n",
      "for k, v in nshoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().sort_values(ascending=False).head(15).iteritems():\n",
      "    ax.annotate(k.strftime('%Y-%m-%d'), (k, v),\n",
      "                xytext=(10,-5), textcoords='offset points',\n",
      "                family='sans-serif', fontsize=14, color='darkslategrey')\n",
      "plt.tight_layout()\n",
      "92/116: nshoa[(nshoa.created_time>='2016-01-23') & (nshoa.created_time<'2016-01-24')].post_id.value_counts()\n",
      "92/117: nshoa[(nshoa.created_time>='2016-01-23') & (nshoa.created_time<'2016-01-24')]\n",
      "92/118: nshoa[(nshoa.created_time>='2016-01-23') & (nshoa.created_time<'2016-01-24')].name\n",
      "92/119:\n",
      "(df[(df.text.fillna('').str.contains('שואה')) & (~df.text.fillna('').str.contains('משואה'))& (~df.text.fillna('').str.contains('נשואה'))].shape,\n",
      "df[(df.text.fillna('').str.contains('גטו')) | (df.text.fillna('').str.contains('גטאות'))].shape,\n",
      "df[(df.text.fillna('').str.contains('אושוויץ')) | (df.text.fillna('').str.contains('אושויץ'))].shape,\n",
      "df[((df.text.fillna('').str.contains('מחנות')) | (df.text.fillna('').str.contains('מחנה'))) & ((df.text.fillna('').str.contains('ריכוז')) | (df.text.fillna('').str.contains('השמדה')))].shape,\n",
      "df[df.text.fillna('').str.contains('היטלר')].shape,\n",
      "df[df.text.fillna('').str.contains('אייכמן')].shape,\n",
      "df[df.text.fillna('').str.contains('נאצי')].shape,\n",
      "df[df.text.fillna('').str.contains('מוזלמן')].shape,\n",
      "df[df.text.fillna('').str.contains('טלאי צהוב')].shape,\n",
      "df[df.text.fillna('').str.contains('טרבלינקה')].shape,\n",
      "df[(df.text.fillna('').str.contains('מיידנק')) | (df.text.fillna('').str.contains('מאידנק'))].shape,\n",
      "df[df.text.fillna('').str.contains('יד ושם')].shape,\n",
      "df[(df.text.fillna('').str.contains('יודנראט')) | (df.text.fillna('').str.contains('יודנרט'))].shape,\n",
      "df[(df.text.fillna('').str.contains('חסיד')) | (df.text.fillna('').str.contains('אומות עולם'))].shape,\n",
      "df[(df.text.fillna('').str.contains('גסטפו')) | (df.text.fillna('').str.contains('גסטאפו'))].shape)\n",
      "92/120: shoa.shape\n",
      "92/121: df.shape\n",
      "92/122:\n",
      "(df[(df.text.fillna('').str.contains('שואה')) & (~df.text.fillna('').str.contains('משואה'))& (~df.text.fillna('').str.contains('נשואה'))].shape,\n",
      "df[(df.text.fillna('').str.contains('גטו')) | (df.text.fillna('').str.contains('גטאות'))].shape,\n",
      "df[(df.text.fillna('').str.contains('אושוויץ')) | (df.text.fillna('').str.contains('אושויץ'))].shape,\n",
      "df[((df.text.fillna('').str.contains('מחנות')) | (df.text.fillna('').str.contains('מחנה'))) & ((df.text.fillna('').str.contains('ריכוז')) | (df.text.fillna('').str.contains('השמדה')))].shape,\n",
      "df[df.text.fillna('').str.contains('היטלר')].shape,\n",
      "df[df.text.fillna('').str.contains('אייכמן')].shape,\n",
      "df[df.text.fillna('').str.contains('נאצי')].shape,\n",
      "df[df.text.fillna('').str.contains('מוזלמן')].shape,\n",
      "df[df.text.fillna('').str.contains('טלאי צהוב')].shape,\n",
      "df[df.text.fillna('').str.contains('טרבלינקה')].shape,\n",
      "df[(df.text.fillna('').str.contains('מיידנק')) | (df.text.fillna('').str.contains('מאידנק'))].shape,\n",
      "df[df.text.fillna('').str.contains('יד ושם')].shape,\n",
      "df[(df.text.fillna('').str.contains('יודנראט')) | (df.text.fillna('').str.contains('יודנרט'))].shape,\n",
      "df[(df.text.fillna('').str.contains('חסיד')) & (df.text.fillna('').str.contains('אומות עולם'))].shape,\n",
      "df[(df.text.fillna('').str.contains('גסטפו')) | (df.text.fillna('').str.contains('גסטאפו'))].shape)\n",
      "92/123:\n",
      "df[(df.text.fillna('').str.contains('שואה')) & (~df.text.fillna('').str.contains('משואה'))& (~df.text.fillna('').str.contains('נשואה'))].to_csv('shoa/shoa.csv', index=False)\n",
      "\n",
      "df[(df.text.fillna('').str.contains('גטו')) | (df.text.fillna('').str.contains('גטאות'))].to_csv('shoa/ghetto.csv', index=False)\n",
      "\n",
      "df[(df.text.fillna('').str.contains('אושוויץ')) | (df.text.fillna('').str.contains('אושויץ'))].to_csv('shoa/auschwitz.csv', index=False)\n",
      "\n",
      "df[((df.text.fillna('').str.contains('מחנות')) | (df.text.fillna('').str.contains('מחנה'))) & ((df.text.fillna('').str.contains('ריכוז')) | (df.text.fillna('').str.contains('השמדה')))].to_csv('shoa/mahane_rikuz_or_hashmada.csv', index=False)\n",
      "\n",
      "df[df.text.fillna('').str.contains('היטלר')].to_csv('shoa/hitler.csv', index=False)\n",
      "\n",
      "df[df.text.fillna('').str.contains('אייכמן')].to_csv('shoa/eichman.csv', index=False)\n",
      "\n",
      "df[df.text.fillna('').str.contains('נאצי')].to_csv('shoa/nazi.csv', index=False)\n",
      "\n",
      "df[df.text.fillna('').str.contains('מוזלמן')].to_csv('shoa/muzelman.csv', index=False)\n",
      "\n",
      "df[df.text.fillna('').str.contains('טלאי צהוב')].to_csv('shoa/tlay_zahov.csv', index=False)\n",
      "\n",
      "df[df.text.fillna('').str.contains('טרבלינקה')].to_csv('shoa/treblinka.csv', index=False)\n",
      "\n",
      "df[(df.text.fillna('').str.contains('מיידנק')) | (df.text.fillna('').str.contains('מאידנק'))].to_csv('shoa/maidanek.csv', index=False)\n",
      "\n",
      "df[df.text.fillna('').str.contains('יד ושם')].to_csv('shoa/yad_va_shem.csv', index=False)\n",
      "\n",
      "df[(df.text.fillna('').str.contains('יודנראט')) | (df.text.fillna('').str.contains('יודנרט'))].to_csv('shoa/judenrat.csv', index=False)\n",
      "\n",
      "df[(df.text.fillna('').str.contains('חסיד')) & (df.text.fillna('').str.contains('אומות עולם'))].to_csv('shoa/hasid_umot_olam.csv', index=False)\n",
      "\n",
      "df[(df.text.fillna('').str.contains('גסטפו')) | (df.text.fillna('').str.contains('גסטאפו'))].to_csv('shoa/gestapo.csv', index=False)\n",
      "92/124:\n",
      "shoa = df[((df.text.fillna('').str.contains('שואה')) & (~df.text.fillna('').str.contains('משואה'))& (~df.text.fillna('').str.contains('נשואה')))\n",
      "   | ((df.text.str.contains('גטו')) | (df.text.str.contains('גטאות')))\n",
      "   | ((df.text.str.contains('אושוויץ')) | (df.text.str.contains('אושויץ')))\n",
      "   | (((df.text.str.contains('מחנות')) | (df.text.str.contains('מחנה'))) & ((df.text.str.contains('ריכוז')) | (df.text.str.contains('השמדה'))))\n",
      "   | (df.text.str.contains('היטלר'))\n",
      "   | (df.text.str.contains('אייכמן'))\n",
      "   | (df.text.str.contains('נאצי'))\n",
      "   | (df.text.str.contains('מוזלמן'))\n",
      "   | (df.text.str.contains('טלאי צהוב'))\n",
      "   | (df.text.str.contains('טרבלינקה'))\n",
      "   | ((df.text.str.contains('מיידנק')) | (df.text.str.contains('מאידנק')))\n",
      "   | (df.text.str.contains('יד ושם'))\n",
      "   | ((df.text.str.contains('יודנראט')) | (df.text.str.contains('יודנרט')))\n",
      "   | ((df.text.str.contains('חסיד')) & (df.text.str.contains('אומות עולם')))\n",
      "   | ((df.text.str.contains('גסטפו')) | (df.text.str.contains('גסטאפו')))]\n",
      "92/125: shoa.shape\n",
      "92/126: df.shape\n",
      "92/127:\n",
      "fig, ax = plt.subplots(figsize=(15,10))\n",
      "shoa[shoa.created_time>'2015-03-01'].groupby(pd.Grouper(key='created_time', freq='1d')).size().plot(ax=ax, title='All shoa keyword appearances. Num. of comments by date.')\n",
      "for k, v in shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().sort_values(ascending=False).head(15).iteritems():\n",
      "    ax.annotate(k.strftime('%Y-%m-%d'), (k, v),\n",
      "                xytext=(10,-5), textcoords='offset points',\n",
      "                family='sans-serif', fontsize=14, color='darkslategrey')\n",
      "plt.tight_layout()\n",
      "92/128:\n",
      "for x in shoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().sort_values(ascending=False).head(40).iteritems():\n",
      "    print(x)\n",
      "92/129: shoa[(shoa.created_time>='2016-12-28') & (shoa.created_time<'2016-12-29')].post_id.value_counts()\n",
      "92/130:\n",
      "nshoa = df[((df.text.str.contains('גטו')) | (df.text.str.contains('גטאות')))\n",
      "   | ((df.text.str.contains('אושוויץ')) | (df.text.str.contains('אושויץ')))\n",
      "   | (((df.text.str.contains('מחנות')) | (df.text.str.contains('מחנה'))) & ((df.text.str.contains('ריכוז')) | (df.text.str.contains('השמדה'))))\n",
      "   | (df.text.str.contains('היטלר'))\n",
      "   | (df.text.str.contains('אייכמן'))\n",
      "   | (df.text.str.contains('נאצי'))\n",
      "   | (df.text.str.contains('מוזלמן'))\n",
      "   | (df.text.str.contains('טלאי צהוב'))\n",
      "   | (df.text.str.contains('טרבלינקה'))\n",
      "   | ((df.text.str.contains('מיידנק')) | (df.text.str.contains('מאידנק')))\n",
      "   | (df.text.str.contains('יד ושם'))\n",
      "   | ((df.text.str.contains('יודנראט')) | (df.text.str.contains('יודנרט')))\n",
      "   | ((df.text.str.contains('חסיד')) & (df.text.str.contains('אומות עולם')))\n",
      "   | ((df.text.str.contains('גסטפו')) | (df.text.str.contains('גסטאפו')))]\n",
      "92/131:\n",
      "fig, ax = plt.subplots(figsize=(15,10))\n",
      "nshoa[nshoa.created_time>'2015-03-01'].groupby(pd.Grouper(key='created_time', freq='1d')).size().plot(ax=ax, title='All shoa keyword appearances. Num. of comments by date.')\n",
      "for k, v in nshoa.groupby(pd.Grouper(key='created_time', freq='1d')).size().sort_values(ascending=False).head(15).iteritems():\n",
      "    ax.annotate(k.strftime('%Y-%m-%d'), (k, v),\n",
      "                xytext=(10,-5), textcoords='offset points',\n",
      "                family='sans-serif', fontsize=14, color='darkslategrey')\n",
      "plt.tight_layout()\n",
      "92/132: nshoa[(nshoa.created_time>='2016-01-23') & (nshoa.created_time<'2016-01-24')].name\n",
      "91/1: %matplotlib inline\n",
      "91/2:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_context('talk')\n",
      "sns.set_style('white')\n",
      "91/3: lut = pd.read_pickle('lang_uri_time_all.pkl.gz', compression='gzip')\n",
      "91/4: wb = pd.read_csv('data/wb_avail_cdx_all.csv.gz', compression='gzip')\n",
      "91/5: wb.head()\n",
      "91/6: wb['avail'] = pd.concat([wb.cdx>0, wb.available],axis=1).any(axis=1)\n",
      "91/7:\n",
      "lut['uri_cln_wb_avail'] = lut.uri_cln.map(wb.set_index('uri').avail).fillna(False)\n",
      "lut['uri_cln_unshrtn_wb_avail'] = lut.uri_cln_unshrtn.map(wb.set_index('uri').avail).fillna(False)\n",
      "91/8: lut = pd.read_pickle('lang_uri_time_wb_avail_all.pkl.gz', compression='gzip')\n",
      "91/9: dict(lut.dtypes)\n",
      "91/10:\n",
      "macd_all = pd.read_pickle('data/macd_all.pkl.gz', compression='gzip')\n",
      "macd_all.head()\n",
      "91/11: lut['uri_unsh_no_clean'] = lut.uri_unshrtn_raw.combine_first(lut.uri_unshrtn_bitly.combine_first(lut.uri_unshrtn.combine_first(lut.uri)))\n",
      "91/12:\n",
      "lut['uri_mem_avail'] = lut.uri.map(~macd_all.drop_duplicates(subset='uri').set_index('uri').first_snap.isna()).fillna(False)\n",
      "lut['uri_unsh_no_clean_mem_avail'] = lut.uri_unsh_no_clean.map(~macd_all.drop_duplicates(subset='uri').set_index('uri').first_snap.isna()).fillna(False)\n",
      "91/13:\n",
      "curl = pd.read_csv('data/curl_results_parsed.csv.gz', compression='gzip')\n",
      "lut['status_code'] = lut.uri_unsh_no_clean.map(curl.drop_duplicates(subset='url').set_index('url').status_code)\n",
      "lut['content_type'] = lut.uri_unsh_no_clean.map(curl.drop_duplicates(subset='url').set_index('url').content_type)\n",
      "91/14: lut.head().T\n",
      "91/15:\n",
      "clut = lut.copy()\n",
      "clut.loc[clut.lang=='en,sco', 'lang'] = 'en'\n",
      "clut = (pd.concat([clut, clut.loc[clut.lang=='id,jv,ms', :].assign(lang='id'), \n",
      "                   clut.loc[clut.lang=='id,jv,ms', :].assign(lang='jv'), \n",
      "                   clut.loc[clut.lang=='id,jv,ms', :].assign(lang='ms')], axis=0)\n",
      "       .reset_index().drop('index', axis=1))\n",
      "clut = clut[~clut.lang.isin(['id,jv,ms', 'sco', '38'])]\n",
      "91/16:\n",
      "lut['archived'] = lut[['uri_cln_wb_avail', 'uri_cln_unshrtn_wb_avail', 'uri_mem_avail', 'uri_unsh_no_clean_mem_avail']].any(axis=1)\n",
      "\n",
      "lut.head().T\n",
      "91/17:\n",
      "lut['archived'] = lut[['uri_cln_wb_avail', 'uri_cln_unshrtn_wb_avail', 'uri_mem_avail', 'uri_unsh_no_clean_mem_avail']].any(axis=1)\n",
      "\n",
      "lut.head().T\n",
      "91/18: pd.set_option('display.max_colwidth', -1)\n",
      "91/19:\n",
      "def get_values(g):\n",
      "    return ','.join(list(g.unique()))\n",
      "tlds_to_remove = ['fb.me', 'facebook.com', 'youtu.be', 'youtube.com']\n",
      "uris_for_appr_200 = lut[(lut.status_code==200) & (~lut.tld_unshrtn.isin(tlds_to_remove))].groupby(['uri_unsh_no_clean', 'tld_unshrtn']).agg({'archived': np.max, 'source': get_values}).reset_index()\n",
      "91/20: uris_for_appr_200.shape\n",
      "91/21: uris_for_appr_200.archived.value_counts()\n",
      "91/22: uris_for_appr_200.source.value_counts()\n",
      "91/23: uris_for_appr_200.tld_unshrtn.value_counts().head()\n",
      "91/24: uris_for_appr_300 = lut[(lut.status_code.isin([301,302])) & (~lut.tld_unshrtn.isin(tlds_to_remove))].groupby(['uri_unsh_no_clean', 'tld_unshrtn']).agg({'archived': np.max, 'source': get_values}).reset_index()\n",
      "91/25: uris_for_appr_300.archived.value_counts()\n",
      "91/26: uris_for_appr_300.source.value_counts()\n",
      "91/27: uris_for_appr_300.tld_unshrtn.value_counts().head()\n",
      "91/28:\n",
      "uris_for_appr_200['appraisal'] = ''\n",
      "uris_for_appr_300['appraisal'] = ''\n",
      "91/29:\n",
      "n_samp = 5\n",
      "n_rows = 50\n",
      "samples = {}\n",
      "\n",
      "def get_samples(df, arch, n_rows=n_rows, n_samp=n_samp):\n",
      "    all_samples = df[(df.archived==arch) & (~df.source.str.contains(','))].sample(n=n_rows*n_samp, random_state=42)\n",
      "    return [all_samples.iloc[n_rows*i:n_rows*i+n_rows] for i in range(n_samp)]\n",
      "\n",
      "samples['archived_200'] = get_samples(uris_for_appr_200, True)\n",
      "samples['not_archived_200'] = get_samples(uris_for_appr_200, False)\n",
      "samples['archived_30X'] = get_samples(uris_for_appr_300, True)\n",
      "samples['not_archived_30X'] = get_samples(uris_for_appr_300, False)\n",
      "91/30:\n",
      "from collections import defaultdict\n",
      "srcs = []\n",
      "ks = ['a', 'b', 'c', 'd']\n",
      "annot = defaultdict(list)\n",
      "for k, (key, g) in zip(ks, samples.items()):\n",
      "    s = []\n",
      "    for i, samp in enumerate(g):\n",
      "        print (key, i, samp.shape)\n",
      "        srcs.append(samp.source.value_counts().reset_index())\n",
      "        annot[i].append(samp[['uri_unsh_no_clean', 'appraisal']])\n",
      "91/31: len(annot)\n",
      "91/32: pd.concat(srcs).groupby('index').sum()\n",
      "91/33:\n",
      "cross2 = uris_for_appr_200.loc[uris_for_appr_200.source.str.contains(','), ['uri_unsh_no_clean', 'appraisal']].sample(frac=1, random_state=42)\n",
      "cross3 = uris_for_appr_300.loc[uris_for_appr_300.source.str.contains(','), ['uri_unsh_no_clean', 'appraisal']].sample(frac=1, random_state=42)\n",
      "91/34:\n",
      "for s, (g, df) in zip(annot, cross2.groupby(np.arange(len(cross2)) // (len(cross2)//5 +1))):\n",
      "    print(df.shape)\n",
      "    annot[g].append(df)\n",
      "\n",
      "for s, (g, df) in zip(annot, cross3.groupby(np.arange(len(cross3)) // (len(cross3)//5 +1))):\n",
      "    print(df.shape)\n",
      "    annot[g].append(df)\n",
      "91/35:\n",
      "names = ['Anat', 'Dan', 'Nehoray', 'Nurit', 'Tzipy']\n",
      "for name, (key, dfs) in zip(names, annot.items()):\n",
      "    print (i, len(dfs))\n",
      "    pd.concat(dfs).sample(frac=1, random_state=42).to_csv(f'data/appraisal/{name}.csv', index=False)\n",
      "91/36:\n",
      "n_samp = [1, 7]\n",
      "n_rows = [30, 50]\n",
      "samples = {}\n",
      "\n",
      "def get_samples(df, arch, n_rows=n_rows, n_samp=n_samp):\n",
      "    all_samples = df[(df.archived==arch) & (~df.source.str.contains(','))].sample(n=[a*b for a,b in zip(n_rows,n_samp)], random_state=42)\n",
      "    return [[all_samples.iloc[nr*i:nr*i+nr] for i in range(ns)] for nr, ns in zip(n_rows, n_samp)]\n",
      "\n",
      "samples2['archived_200'] = get_samples(uris_for_appr_200, True)\n",
      "samples2['not_archived_200'] = get_samples(uris_for_appr_200, False)\n",
      "samples2['archived_30X'] = get_samples(uris_for_appr_300, True)\n",
      "samples2['not_archived_30X'] = get_samples(uris_for_appr_300, False)\n",
      "91/37:\n",
      "n_samp = [1, 7]\n",
      "n_rows = [30, 50]\n",
      "samples = {}\n",
      "\n",
      "def get_samples(df, arch, n_rows=n_rows, n_samp=n_samp):\n",
      "    all_samples = df[(df.archived==arch) & (~df.source.str.contains(','))].sample(n=sum([a*b for a,b in zip(n_rows,n_samp)]), random_state=42)\n",
      "    return [[all_samples.iloc[nr*i:nr*i+nr] for i in range(ns)] for nr, ns in zip(n_rows, n_samp)]\n",
      "\n",
      "samples2['archived_200'] = get_samples(uris_for_appr_200, True)\n",
      "samples2['not_archived_200'] = get_samples(uris_for_appr_200, False)\n",
      "samples2['archived_30X'] = get_samples(uris_for_appr_300, True)\n",
      "samples2['not_archived_30X'] = get_samples(uris_for_appr_300, False)\n",
      "91/38:\n",
      "n_samp = [1, 7]\n",
      "n_rows = [30, 50]\n",
      "samples2 = {}\n",
      "\n",
      "def get_samples(df, arch, n_rows=n_rows, n_samp=n_samp):\n",
      "    all_samples = df[(df.archived==arch) & (~df.source.str.contains(','))].sample(n=sum([a*b for a,b in zip(n_rows,n_samp)]), random_state=42)\n",
      "    return [[all_samples.iloc[nr*i:nr*i+nr] for i in range(ns)] for nr, ns in zip(n_rows, n_samp)]\n",
      "\n",
      "samples2['archived_200'] = get_samples(uris_for_appr_200, True)\n",
      "samples2['not_archived_200'] = get_samples(uris_for_appr_200, False)\n",
      "samples2['archived_30X'] = get_samples(uris_for_appr_300, True)\n",
      "samples2['not_archived_30X'] = get_samples(uris_for_appr_300, False)\n",
      "91/39: samples2\n",
      "91/40: samples2['archived_200'][0]\n",
      "91/41: samples2['archived_200'][0].shape\n",
      "91/42: samples2['archived_200'][0]\n",
      "91/43: uris_for_appr_200[(uris_for_appr_200.source.str.contains(','))]\n",
      "91/44: uris_for_appr_200[(uris_for_appr_200.source.str.contains(','))].shape\n",
      "91/45: uris_for_appr_300[(uris_for_appr_300.source.str.contains(','))].shape\n",
      "91/46:\n",
      "n_samp = [1, 7]\n",
      "n_rows = [30, 30]\n",
      "samples2 = {}\n",
      "\n",
      "def get_samples(df, arch, n_rows=n_rows, n_samp=n_samp):\n",
      "    all_samples = df[(df.archived==arch) & (~df.source.str.contains(','))].sample(n=sum([a*b for a,b in zip(n_rows,n_samp)]), random_state=42)\n",
      "    return [[all_samples.iloc[nr*i:nr*i+nr] for i in range(ns)] for nr, ns in zip(n_rows, n_samp)]\n",
      "\n",
      "samples2['archived_200'] = get_samples(uris_for_appr_200, True)\n",
      "samples2['not_archived_200'] = get_samples(uris_for_appr_200, False)\n",
      "samples2['archived_30X'] = get_samples(uris_for_appr_300, True)\n",
      "samples2['not_archived_30X'] = get_samples(uris_for_appr_300, False)\n",
      "91/47:\n",
      "n_samp = [1, len(names)]\n",
      "n_rows = [30, 30]\n",
      "samples2 = {}\n",
      "\n",
      "def get_samples(df, arch, n_rows=n_rows, n_samp=n_samp):\n",
      "    all_samples = df[(df.archived==arch) & (~df.source.str.contains(','))].sample(n=sum([a*b for a,b in zip(n_rows,n_samp)]), random_state=42)\n",
      "    return [[all_samples.iloc[nr*i:nr*i+nr] for i in range(ns)] for nr, ns in zip(n_rows, n_samp)]\n",
      "\n",
      "samples2['archived_200'] = get_samples(uris_for_appr_200, True)\n",
      "samples2['not_archived_200'] = get_samples(uris_for_appr_200, False)\n",
      "samples2['archived_30X'] = get_samples(uris_for_appr_300, True)\n",
      "samples2['not_archived_30X'] = get_samples(uris_for_appr_300, False)\n",
      "91/48:\n",
      "cross2 = uris_for_appr_200.loc[uris_for_appr_200.source.str.contains(','), ['uri_unsh_no_clean', 'appraisal']].sample(frac=1, random_state=42)\n",
      "cross3 = uris_for_appr_300.loc[uris_for_appr_300.source.str.contains(','), ['uri_unsh_no_clean', 'appraisal']].sample(frac=1, random_state=42)\n",
      "91/49:\n",
      "joint = pd.concat([cross2.iloc[:30,:]cross3.iloc[:30,:]]).sample(frac=1, random_state=42)\n",
      "joint.head()\n",
      "91/50:\n",
      "joint = pd.concat([cross2.iloc[:30,:], cross3.iloc[:30,:]]).sample(frac=1, random_state=42)\n",
      "joint.head()\n",
      "91/51:\n",
      "n_samp = 5\n",
      "n_rows = 50\n",
      "samples = {}\n",
      "\n",
      "def get_samples(df, arch, n_rows=n_rows, n_samp=n_samp):\n",
      "    all_samples = df[(df.archived==arch) & (~df.source.str.contains(','))].sample(n=n_rows*n_samp, random_state=42)\n",
      "    return [all_samples.iloc[n_rows*i:n_rows*i+n_rows] for i in range(n_samp)]\n",
      "\n",
      "samples['archived_200'] = get_samples(uris_for_appr_200, True)\n",
      "samples['not_archived_200'] = get_samples(uris_for_appr_200, False)\n",
      "samples['archived_30X'] = get_samples(uris_for_appr_300, True)\n",
      "samples['not_archived_30X'] = get_samples(uris_for_appr_300, False)\n",
      "91/52:\n",
      "joint = {}\n",
      "joint['archived_200'] = uris_for_appr_200.loc[(uris_for_appr_200.archived), :].iloc[:30, ['uri_unsh_no_clean', 'appraisal']]\n",
      "joint['not_archived_200'] = uris_for_appr_200.loc[(~uris_for_appr_200.archived), :].iloc[:30, ['uri_unsh_no_clean', 'appraisal']]\n",
      "joint['archived_30X'] = uris_for_appr_300.loc[(~uris_for_appr_300.archived), :].iloc[:30, ['uri_unsh_no_clean', 'appraisal']]\n",
      "joint['not_archived_30X'] = uris_for_appr_300.loc[(~uris_for_appr_300.archived), :].iloc[:30, ['uri_unsh_no_clean', 'appraisal']]\n",
      "joint['cross_source_200'] = cross2.iloc[:30,:]\n",
      "joint['cross_source_30X'] = cross3.iloc[:30,:]\n",
      "\n",
      "joint = pd.concat[[d for d in joint.values()]].sample(frac=1, random_state=42)\n",
      "joint.head()\n",
      "91/53:\n",
      "joint = {}\n",
      "joint['archived_200'] = uris_for_appr_200.loc[(uris_for_appr_200.archived), ['uri_unsh_no_clean', 'appraisal']].iloc[:30, :]\n",
      "joint['not_archived_200'] = uris_for_appr_200.loc[(~uris_for_appr_200.archived), ['uri_unsh_no_clean', 'appraisal']].iloc[:30, :]\n",
      "joint['archived_30X'] = uris_for_appr_300.loc[(~uris_for_appr_300.archived), ['uri_unsh_no_clean', 'appraisal']].iloc[:30, :]\n",
      "joint['not_archived_30X'] = uris_for_appr_300.loc[(~uris_for_appr_300.archived), ['uri_unsh_no_clean', 'appraisal']].iloc[:30, :]\n",
      "joint['cross_source_200'] = cross2.iloc[:30,:]\n",
      "joint['cross_source_30X'] = cross3.iloc[:30,:]\n",
      "\n",
      "joint = pd.concat[[d for d in joint.values()]].sample(frac=1, random_state=42)\n",
      "joint.head()\n",
      "91/54:\n",
      "joint = {}\n",
      "joint['archived_200'] = uris_for_appr_200.loc[(uris_for_appr_200.archived), ['uri_unsh_no_clean', 'appraisal']].iloc[:30, :]\n",
      "joint['not_archived_200'] = uris_for_appr_200.loc[(~uris_for_appr_200.archived), ['uri_unsh_no_clean', 'appraisal']].iloc[:30, :]\n",
      "joint['archived_30X'] = uris_for_appr_300.loc[(~uris_for_appr_300.archived), ['uri_unsh_no_clean', 'appraisal']].iloc[:30, :]\n",
      "joint['not_archived_30X'] = uris_for_appr_300.loc[(~uris_for_appr_300.archived), ['uri_unsh_no_clean', 'appraisal']].iloc[:30, :]\n",
      "joint['cross_source_200'] = cross2.iloc[:30,:]\n",
      "joint['cross_source_30X'] = cross3.iloc[:30,:]\n",
      "\n",
      "joint = pd.concat([d for d in joint.values()]).sample(frac=1, random_state=42)\n",
      "joint.head()\n",
      "91/55:\n",
      "joint = {}\n",
      "joint['archived_200'] = uris_for_appr_200.loc[(uris_for_appr_200.archived), ['uri_unsh_no_clean', 'appraisal']].iloc[:30, :]\n",
      "joint['not_archived_200'] = uris_for_appr_200.loc[(~uris_for_appr_200.archived), ['uri_unsh_no_clean', 'appraisal']].iloc[:30, :]\n",
      "joint['archived_30X'] = uris_for_appr_300.loc[(~uris_for_appr_300.archived), ['uri_unsh_no_clean', 'appraisal']].iloc[:30, :]\n",
      "joint['not_archived_30X'] = uris_for_appr_300.loc[(~uris_for_appr_300.archived), ['uri_unsh_no_clean', 'appraisal']].iloc[:30, :]\n",
      "joint['cross_source_200'] = cross2.iloc[:30,:]\n",
      "joint['cross_source_30X'] = cross3.iloc[:30,:]\n",
      "\n",
      "joint = pd.concat([d for d in joint.values()]).sample(frac=1, random_state=42)\n",
      "joint.shape\n",
      "91/56:\n",
      "def get_sample(start, n=30):\n",
      "    samp = {}\n",
      "    samp['archived_200'] = uris_for_appr_200.loc[(uris_for_appr_200.archived), ['uri_unsh_no_clean', 'appraisal']].iloc[start:start+n, :]\n",
      "    samp['not_archived_200'] = uris_for_appr_200.loc[(~uris_for_appr_200.archived), ['uri_unsh_no_clean', 'appraisal']].iloc[start:start+n, :]\n",
      "    samp['archived_30X'] = uris_for_appr_300.loc[(~uris_for_appr_300.archived), ['uri_unsh_no_clean', 'appraisal']].iloc[start:start+n, :]\n",
      "    samp['not_archived_30X'] = uris_for_appr_300.loc[(~uris_for_appr_300.archived), ['uri_unsh_no_clean', 'appraisal']].iloc[start:start+n, :]\n",
      "    samp['cross_source_200'] = cross2.iloc[start:start+n,:]\n",
      "    samp['cross_source_30X'] = cross3.iloc[start:start+n,:]\n",
      "\n",
      "    samp = pd.concat([d for d in samp.values()]).sample(frac=1, random_state=42)\n",
      "    return samp\n",
      "\n",
      "joint = samp(0)\n",
      "joint.shape\n",
      "91/57:\n",
      "def get_sample(start, n=30):\n",
      "    samp = {}\n",
      "    samp['archived_200'] = uris_for_appr_200.loc[(uris_for_appr_200.archived), ['uri_unsh_no_clean', 'appraisal']].iloc[start:start+n, :]\n",
      "    samp['not_archived_200'] = uris_for_appr_200.loc[(~uris_for_appr_200.archived), ['uri_unsh_no_clean', 'appraisal']].iloc[start:start+n, :]\n",
      "    samp['archived_30X'] = uris_for_appr_300.loc[(~uris_for_appr_300.archived), ['uri_unsh_no_clean', 'appraisal']].iloc[start:start+n, :]\n",
      "    samp['not_archived_30X'] = uris_for_appr_300.loc[(~uris_for_appr_300.archived), ['uri_unsh_no_clean', 'appraisal']].iloc[start:start+n, :]\n",
      "    samp['cross_source_200'] = cross2.iloc[start:start+n,:]\n",
      "    samp['cross_source_30X'] = cross3.iloc[start:start+n,:]\n",
      "\n",
      "    samp = pd.concat([d for d in samp.values()]).sample(frac=1, random_state=42)\n",
      "    return samp\n",
      "\n",
      "joint = get_sample(0)\n",
      "joint.shape\n",
      "91/58:\n",
      "distinct = {name: get_sample((i+1)*30) for i, name in enumerate(names)}\n",
      "[d.shape for d in distinct.values()]\n",
      "91/59:\n",
      "lut['archived'] = lut[['uri_cln_wb_avail', 'uri_cln_unshrtn_wb_avail', 'uri_mem_avail', 'uri_unsh_no_clean_mem_avail']].any(axis=1)\n",
      "\n",
      "lut.head().T\n",
      "91/60: pd.set_option('display.max_colwidth', -1)\n",
      "91/61:\n",
      "def get_values(g):\n",
      "    return ','.join(list(g.unique()))\n",
      "tlds_to_remove = ['fb.me', 'facebook.com', 'youtu.be', 'youtube.com']\n",
      "uris_for_appr_200 = lut[(lut.status_code==200) & (~lut.tld_unshrtn.isin(tlds_to_remove))].groupby(['uri_unsh_no_clean', 'tld_unshrtn']).agg({'archived': np.max, 'source': get_values}).reset_index()\n",
      "91/62: uris_for_appr_200.shape\n",
      "91/63: uris_for_appr_200.archived.value_counts()\n",
      "91/64: uris_for_appr_200.source.value_counts()\n",
      "91/65: uris_for_appr_200.tld_unshrtn.value_counts().head()\n",
      "91/66: uris_for_appr_200[(uris_for_appr_200.source.str.contains(','))].shape\n",
      "91/67: uris_for_appr_300 = lut[(lut.status_code.isin([301,302])) & (~lut.tld_unshrtn.isin(tlds_to_remove))].groupby(['uri_unsh_no_clean', 'tld_unshrtn']).agg({'archived': np.max, 'source': get_values}).reset_index()\n",
      "91/68: uris_for_appr_300.archived.value_counts()\n",
      "91/69: uris_for_appr_300.source.value_counts()\n",
      "91/70: uris_for_appr_300.tld_unshrtn.value_counts().head()\n",
      "91/71: uris_for_appr_300[(uris_for_appr_300.source.str.contains(','))].shape\n",
      "91/72:\n",
      "uris_for_appr_200['appraisal'] = ''\n",
      "uris_for_appr_300['appraisal'] = ''\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91/73:\n",
      "names = ['Anat', 'Dan', 'Nehoray', 'Nurit', 'Tzipy', 'Yonatan', 'Sveta']\n",
      "uris_for_appr_200 = uris_for_appr_200.sample(frac=1, random_state=42)\n",
      "uris_for_appr_300 = uris_for_appr_300.sample(frac=1, random_state=42)\n",
      "91/74:\n",
      "def get_sample(start, n=30):\n",
      "    samp = {}\n",
      "    samp['archived_200'] = uris_for_appr_200.loc[(uris_for_appr_200.archived), ['uri_unsh_no_clean', 'appraisal']].iloc[start:start+n, :]\n",
      "    samp['not_archived_200'] = uris_for_appr_200.loc[(~uris_for_appr_200.archived), ['uri_unsh_no_clean', 'appraisal']].iloc[start:start+n, :]\n",
      "    samp['archived_30X'] = uris_for_appr_300.loc[(~uris_for_appr_300.archived), ['uri_unsh_no_clean', 'appraisal']].iloc[start:start+n, :]\n",
      "    samp['not_archived_30X'] = uris_for_appr_300.loc[(~uris_for_appr_300.archived), ['uri_unsh_no_clean', 'appraisal']].iloc[start:start+n, :]\n",
      "    samp['cross_source_200'] = cross2.iloc[start:start+n,:]\n",
      "    samp['cross_source_30X'] = cross3.iloc[start:start+n,:]\n",
      "\n",
      "    samp = pd.concat([d for d in samp.values()]).sample(frac=1, random_state=42)\n",
      "    return samp\n",
      "\n",
      "joint = get_sample(0)\n",
      "joint.shape\n",
      "91/75:\n",
      "distinct = {name: get_sample((i+1)*30) for i, name in enumerate(names)}\n",
      "[d.shape for d in distinct.values()]\n",
      "91/76:\n",
      "import os \n",
      "os.mkdir('data/appraisal2')\n",
      "91/77: joint.to_csv(f'data/appraisal2/joint.csv', index=False)\n",
      "91/78:\n",
      "for name, samp in distinct.items():\n",
      "    samp.to_csv(f'data/appraisal2/{name}.csv', index=False)\n",
      "91/79:\n",
      "x = [\"2019-06-03\", \"2019-06-04\", \"2019-06-04\", \"2019-06-06\", \"2019-06-06\", \"2019-06-06\", \"2019-06-06\", \"2019-06-11\", \"2019-06-11\", \"2019-06-11\", \"2019-06-11\", \"2019-06-11\", \"2019-06-11\", \"2019-06-12\", \"2019-06-12\", \"2019-06-12\", \"2019-06-12\", \"2019-06-12\", \"2019-06-12\", \"2019-06-13\", \"2019-06-13\", \"2019-06-13\", \"2019-06-13\", \"2019-06-13\", \"2019-06-13\", \"2019-06-13\", \"2019-06-16\", \"2019-06-16\", \"2019-06-16\", \"2019-06-17\", \"2019-06-18\", \"2019-06-19\", \"2019-06-19\", \"2019-06-19\", \"2019-06-19\", \"2019-06-19\", \"2019-06-19\", \"2019-06-20\", \"2019-06-20\", \"2019-06-20\", \"2019-06-20\", \"2019-06-20\", \"2019-06-24\", \"2019-06-25\", \"2019-06-25\", \"2019-06-25\", \"2019-06-25\", \"2019-06-26\", \"2019-06-26\", \"2019-06-26\", \"2019-06-26\", \"2019-06-27\", \"2019-06-27\", \"2019-06-27\", \"2019-06-27\", \"2019-06-27\"]\n",
      "from collections import Counter\n",
      "Counter(x)\n",
      "91/80:\n",
      "cdx_line = \"rs,vecto)/70/vector-of-a-cartoon-plaid-easter-bunny-crashed-on-the-ground-with-a-broken-egg-during-delivery-by-ron-leishman-25609.jpg 20140908223912 http://vecto.rs/70/vector-of-a-cartoon-plaid-easter-bunny-crashed-on-the-ground-with-a-broken-egg-during-delivery-by-ron-leishman-25609.jpg image/jpeg 200 4SRBFHZCBHUJW3RXSOXH33HA4JTBT52N - - 3411 11588417 NLIL-CRAWL-02-20140908181759012-01967-01978-wbgrp-crawl004/NLIL-CRAWL-02-20140908223528814-01971-28130~wbgrp-crawl004.us.archive.org~8443.warc.gz\"\n",
      "\n",
      "cdx_line.split(' ')\n",
      "91/81:\n",
      "import os\n",
      "for file in os.listdir('C:\\dev\\omilab\\2014\\cdx'):\n",
      "    print(file)\n",
      "91/82:\n",
      "import os\n",
      "for file in os.listdir(r'C:\\dev\\omilab\\2014\\cdx'):\n",
      "    print(file)\n",
      "91/83:\n",
      "def get_values(g):\n",
      "    return ','.join(list(g.unique()))\n",
      "tlds_to_remove = ['fb.me', 'facebook.com', 'youtu.be', 'youtube.com']\n",
      "uris_for_appr_200 = lut[(lut.status_code==200) & (~lut.tld_unshrtn.isin(tlds_to_remove))].groupby(['uri_unsh_no_clean', 'tld_unshrtn']).agg({'archived': np.max, 'source': get_values}).reset_index()\n",
      "91/84: uris_for_appr_200.shape\n",
      "91/85: uris_for_appr_200.archived.value_counts()\n",
      "91/86: uris_for_appr_200.source.value_counts()\n",
      "91/87: uris_for_appr_200.tld_unshrtn.value_counts().head()\n",
      "91/88: uris_for_appr_200[(uris_for_appr_200.source.str.contains(','))].shape\n",
      "91/89: uris_for_appr_300 = lut[(lut.status_code.isin([301,302])) & (~lut.tld_unshrtn.isin(tlds_to_remove))].groupby(['uri_unsh_no_clean', 'tld_unshrtn']).agg({'archived': np.max, 'source': get_values}).reset_index()\n",
      "91/90: uris_for_appr_300.archived.value_counts()\n",
      "91/91: uris_for_appr_300.source.value_counts()\n",
      "91/92: uris_for_appr_300.tld_unshrtn.value_counts().head()\n",
      "91/93: uris_for_appr_300[(uris_for_appr_300.source.str.contains(','))].shape\n",
      "91/94:\n",
      "uris_for_appr_200['appraisal'] = ''\n",
      "uris_for_appr_300['appraisal'] = ''\n",
      "91/95:\n",
      "n_samp = 5\n",
      "n_rows = 50\n",
      "samples = {}\n",
      "\n",
      "def get_samples(df, arch, n_rows=n_rows, n_samp=n_samp):\n",
      "    all_samples = df[(df.archived==arch) & (~df.source.str.contains(','))].sample(n=n_rows*n_samp, random_state=42)\n",
      "    return [all_samples.iloc[n_rows*i:n_rows*i+n_rows] for i in range(n_samp)]\n",
      "\n",
      "samples['archived_200'] = get_samples(uris_for_appr_200, True)\n",
      "samples['not_archived_200'] = get_samples(uris_for_appr_200, False)\n",
      "samples['archived_30X'] = get_samples(uris_for_appr_300, True)\n",
      "samples['not_archived_30X'] = get_samples(uris_for_appr_300, False)\n",
      "91/96:\n",
      "from collections import defaultdict\n",
      "srcs = []\n",
      "ks = ['a', 'b', 'c', 'd']\n",
      "annot = defaultdict(list)\n",
      "for k, (key, g) in zip(ks, samples.items()):\n",
      "    s = []\n",
      "    for i, samp in enumerate(g):\n",
      "        print (key, i, samp.shape)\n",
      "        srcs.append(samp.source.value_counts().reset_index())\n",
      "        annot[i].append(samp[['uri_unsh_no_clean', 'appraisal']])\n",
      "91/97: len(annot)\n",
      "91/98: pd.concat(srcs).groupby('index').sum()\n",
      "91/99:\n",
      "cross2 = uris_for_appr_200.loc[uris_for_appr_200.source.str.contains(','), ['uri_unsh_no_clean', 'appraisal']].sample(frac=1, random_state=42)\n",
      "cross3 = uris_for_appr_300.loc[uris_for_appr_300.source.str.contains(','), ['uri_unsh_no_clean', 'appraisal']].sample(frac=1, random_state=42)\n",
      "91/100:\n",
      "names = ['Anat', 'Dan', 'Nehoray', 'Nurit', 'Tzipy']\n",
      "for name, (key, dfs) in zip(names, annot.items()):\n",
      "    print (i, len(dfs))\n",
      "    pd.concat(dfs).sample(frac=1, random_state=42).to_csv(f'data/appraisal/{name}.csv', index=False)\n",
      "91/101:\n",
      "names = ['Anat', 'Dan', 'Nehoray', 'Nurit', 'Tzipy', 'Yonatan', 'Sveta']\n",
      "uris_for_appr_200 = uris_for_appr_200.sample(frac=1, random_state=42)\n",
      "uris_for_appr_300 = uris_for_appr_300.sample(frac=1, random_state=42)\n",
      "91/102:\n",
      "def get_sample(start, n=30):\n",
      "    samp = {}\n",
      "    samp['archived_200'] = uris_for_appr_200.loc[(uris_for_appr_200.archived) & (~df.source.str.contains(',')), ['uri_unsh_no_clean', 'appraisal']].iloc[start:start+n, :]\n",
      "    samp['not_archived_200'] = uris_for_appr_200.loc[(~uris_for_appr_200.archived) & (~df.source.str.contains(',')), ['uri_unsh_no_clean', 'appraisal']].iloc[start:start+n, :]\n",
      "    samp['archived_30X'] = uris_for_appr_300.loc[(uris_for_appr_300.archived) & (~df.source.str.contains(',')), ['uri_unsh_no_clean', 'appraisal']].iloc[start:start+n, :]\n",
      "    samp['not_archived_30X'] = uris_for_appr_300.loc[(~uris_for_appr_300.archived) & (~df.source.str.contains(',')), ['uri_unsh_no_clean', 'appraisal']].iloc[start:start+n, :]\n",
      "    samp['cross_source_200'] = cross2.iloc[start:start+n,:]\n",
      "    samp['cross_source_30X'] = cross3.iloc[start:start+n,:]\n",
      "\n",
      "    samp = pd.concat([d for d in samp.values()]).sample(frac=1, random_state=42)\n",
      "    return samp\n",
      "\n",
      "joint = get_sample(0)\n",
      "joint.shape\n",
      "91/103:\n",
      "def get_sample(start, n=30):\n",
      "    samp = {}\n",
      "    samp['archived_200'] = uris_for_appr_200.loc[(uris_for_appr_200.archived) & (~uris_for_appr_200.source.str.contains(',')), ['uri_unsh_no_clean', 'appraisal']].iloc[start:start+n, :]\n",
      "    samp['not_archived_200'] = uris_for_appr_200.loc[(~uris_for_appr_200.archived) & (~uris_for_appr_200.source.str.contains(',')), ['uri_unsh_no_clean', 'appraisal']].iloc[start:start+n, :]\n",
      "    samp['archived_30X'] = uris_for_appr_300.loc[(uris_for_appr_300.archived) & (~uris_for_appr_300.source.str.contains(',')), ['uri_unsh_no_clean', 'appraisal']].iloc[start:start+n, :]\n",
      "    samp['not_archived_30X'] = uris_for_appr_300.loc[(~uris_for_appr_300.archived) & (~uris_for_appr_300.source.str.contains(',')), ['uri_unsh_no_clean', 'appraisal']].iloc[start:start+n, :]\n",
      "    samp['cross_source_200'] = cross2.iloc[start:start+n,:]\n",
      "    samp['cross_source_30X'] = cross3.iloc[start:start+n,:]\n",
      "\n",
      "    samp = pd.concat([d for d in samp.values()]).sample(frac=1, random_state=42)\n",
      "    return samp\n",
      "\n",
      "joint = get_sample(0)\n",
      "joint.shape\n",
      "91/104:\n",
      "distinct = {name: get_sample((i+1)*30) for i, name in enumerate(names)}\n",
      "[d.shape for d in distinct.values()]\n",
      "91/105:\n",
      "import os \n",
      "os.mkdir('data/appraisal2')\n",
      "91/106: joint.to_csv(f'data/appraisal2/joint.csv', index=False)\n",
      "91/107:\n",
      "for name, samp in distinct.items():\n",
      "    samp.to_csv(f'data/appraisal2/{name}.csv', index=False)\n",
      "91/108:\n",
      "x = [\"2019-06-03\", \"2019-06-04\", \"2019-06-04\", \"2019-06-06\", \"2019-06-06\", \"2019-06-06\", \"2019-06-06\", \"2019-06-11\", \"2019-06-11\", \"2019-06-11\", \"2019-06-11\", \"2019-06-11\", \"2019-06-11\", \"2019-06-12\", \"2019-06-12\", \"2019-06-12\", \"2019-06-12\", \"2019-06-12\", \"2019-06-12\", \"2019-06-13\", \"2019-06-13\", \"2019-06-13\", \"2019-06-13\", \"2019-06-13\", \"2019-06-13\", \"2019-06-13\", \"2019-06-16\", \"2019-06-16\", \"2019-06-16\", \"2019-06-17\", \"2019-06-18\", \"2019-06-19\", \"2019-06-19\", \"2019-06-19\", \"2019-06-19\", \"2019-06-19\", \"2019-06-19\", \"2019-06-20\", \"2019-06-20\", \"2019-06-20\", \"2019-06-20\", \"2019-06-20\", \"2019-06-24\", \"2019-06-25\", \"2019-06-25\", \"2019-06-25\", \"2019-06-25\", \"2019-06-26\", \"2019-06-26\", \"2019-06-26\", \"2019-06-26\", \"2019-06-27\", \"2019-06-27\", \"2019-06-27\", \"2019-06-27\", \"2019-06-27\"]\n",
      "from collections import Counter\n",
      "Counter(x)\n",
      "91/109: pd.concat(distinct.values()).uri_unsh_no_clean.nunique()\n",
      "91/110: pd.concat(distinct.values()+[joint]).uri_unsh_no_clean.nunique()\n",
      "91/111: pd.concat(list(distinct.values())+[joint]).uri_unsh_no_clean.nunique()\n",
      "91/112:\n",
      "for name, samp in distinct.items():\n",
      "    pd.concat([joint, samp]).to_csv(f'data/appraisal2/{name}.csv', index=False)\n",
      "91/113: curl.shape\n",
      "91/114: clut.timestamp.isna()\n",
      "91/115: clut.timestamp.isna().sum()\n",
      "91/116: clut.uri.nunique()\n",
      "91/117: clut.uri_unsh_no_clean.nunique()\n",
      "   1: import pandas as pd\n",
      "   2:\n",
      "import os \n",
      "os.listdir('data/appraisal2/filled/')\n",
      "   3:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "   4:\n",
      "import os \n",
      "os.listdir('data/appraisal2/filled/')\n",
      "   5:\n",
      "nd = pd.read_csv('data/appraisal2/filled/ALL_NO_DUPLICATES.csv')\n",
      "nd.head()\n",
      "91/118: clut.to_csv('data/clut.csv.gz', compression='gzip', index=False)\n",
      "   6: clut = pd.read_csv('data/clut.csv.gz', compression='gzip')\n",
      "   7:\n",
      "def get_values(g):\n",
      "    return ','.join(list(g.unique()))\n",
      "uris = clut.groupby(['uri_unsh_no_clean', 'tld_unshrtn']).agg({'archived': np.max, 'source': get_values}).reset_index()\n",
      "   8: clut['archived'] = clut[['uri_cln_wb_avail', 'uri_cln_unshrtn_wb_avail', 'uri_mem_avail', 'uri_unsh_no_clean_mem_avail']].any(axis=1)\n",
      "   9:\n",
      "def get_values(g):\n",
      "    return ','.join(list(g.unique()))\n",
      "uris = clut.groupby(['uri_unsh_no_clean', 'tld_unshrtn']).agg({'archived': np.max, 'source': get_values}).reset_index()\n",
      "  10: uris.head()\n",
      "  11:\n",
      "nd = nd.merge(uris, how='left', left_on='URL', right_on='uri_unsh_no_clean')\n",
      "\n",
      "nd.head()\n",
      "  12:\n",
      "def get_values(g):\n",
      "    return ','.join(list(g.unique()))\n",
      "uris = clut.groupby(['uri_unsh_no_clean', 'tld_unshrtn']).agg({'archived': np.max, 'source': get_values}).reset_index().rename(columns={'uri_unsh_no_clean': 'URL'})\n",
      "  13: uris.head()\n",
      "  14:\n",
      "nd = pd.read_csv('data/appraisal2/filled/ALL_NO_DUPLICATES.csv')\n",
      "nd.head()\n",
      "  15:\n",
      "nd = nd.merge(uris, how='left').\n",
      "\n",
      "nd.head()\n",
      "  16:\n",
      "nd = nd.merge(uris, how='left')\n",
      "\n",
      "nd.head()\n",
      "  17: nd.tld_unshrtn.isna().sum(), nd.shape[0], nd.tld_unshrtn.isna().sum()/nd.shape[0]\n",
      "  18: nd.to_csv('data/appraisal2/filled/ALL_NO_DUPLICATES_with_extra_columns.csv')\n",
      "  19: nd['URL'] = nd.URL.str.strip()\n",
      "  20:\n",
      "nd = pd.read_csv('data/appraisal2/filled/ALL_NO_DUPLICATES.csv')\n",
      "nd.head()\n",
      "  21: nd['URL'] = nd.URL.str.strip()\n",
      "  22:\n",
      "nd = nd.merge(uris, how='left')\n",
      "\n",
      "nd.head()\n",
      "  23: nd.tld_unshrtn.isna().sum(), nd.shape[0], nd.tld_unshrtn.isna().sum()/nd.shape[0]\n",
      "  24:\n",
      "df = pd.read_csv('data/appraisal2/filled/2014-URLs-Responses-xlsxALL.csv')\n",
      "df.head()\n",
      "  25:\n",
      "df = pd.read_csv('data/appraisal2/filled/2014-URLs-Responses-xlsxALL.csv')\n",
      "df.head()\n",
      "  26:\n",
      "df = pd.read_csv('data/appraisal2/filled/2014-URLs-Responses-xlsxALL.csv', encoding='utf16')\n",
      "df.head()\n",
      "  27:\n",
      "df = pd.read_csv('data/appraisal2/filled/2014-URLs-Responses-xlsxALL.csv')\n",
      "df.head()\n",
      "  28:\n",
      "df = pd.read_csv('data/appraisal2/filled/2014-URLs-Responses-xlsxALL.csv')\n",
      "df.head()\n",
      "  29:\n",
      "from sklearn.metrics import cohen_kappa_score\n",
      "#cohen_kappa_score(rater1, rater2)\n",
      "  30:\n",
      "header = [\"timestamp\",\"url\",\"is_page_active\",\"is_relevant\",\"date\",\"title\",\"content\",\"language\",\"name\"]\n",
      "df = pd.read_csv('data/appraisal2/filled/2014-URLs-Responses-xlsxALL.csv', header=header)\n",
      "df.head()\n",
      "  31:\n",
      "header = [\"timestamp\",\"url\",\"is_page_active\",\"is_relevant\",\"date\",\"title\",\"content\",\"language\",\"name\"]\n",
      "df = pd.read_csv('data/appraisal2/filled/2014-URLs-Responses-xlsxALL.csv', columns=header)\n",
      "df.head()\n",
      "  32:\n",
      "header = [\"timestamp\",\"url\",\"is_page_active\",\"is_relevant\",\"date\",\"title\",\"content\",\"language\",\"name\"]\n",
      "df = pd.read_csv('data/appraisal2/filled/2014-URLs-Responses-xlsxALL.csv', names=header)\n",
      "df.head()\n",
      "  33:\n",
      "header = [\"timestamp\",\"url\",\"is_page_active\",\"is_relevant\",\"date\",\"title\",\"content\",\"language\",\"name\"]\n",
      "df = pd.read_csv('data/appraisal2/filled/2014-URLs-Responses-xlsxALL.csv', names=header, header=None)\n",
      "df.head()\n",
      "  34:\n",
      "header = [\"timestamp\",\"url\",\"is_page_active\",\"is_relevant\",\"date\",\"title\",\"content\",\"language\",\"name\"]\n",
      "df = pd.read_csv('data/appraisal2/filled/2014-URLs-Responses-xlsxALL.csv', names=header, skiprows=0)\n",
      "df.head()\n",
      "  35:\n",
      "header = [\"timestamp\",\"url\",\"is_page_active\",\"is_relevant\",\"date\",\"title\",\"content\",\"language\",\"name\"]\n",
      "df = pd.read_csv('data/appraisal2/filled/2014-URLs-Responses-xlsxALL.csv', names=header, skiprows=1)\n",
      "df.head()\n",
      "  36: df.groupby('url').name.nuniuqe()\n",
      "  37: df.groupby('url').name.nunique()\n",
      "  38:\n",
      "url_counts = df.groupby('url').name.nunique()\n",
      "url_counts[url_counts].shape\n",
      "  39:\n",
      "url_counts = df.groupby('url').name.nunique()\n",
      "url_counts[url_counts==5].shape\n",
      "  40:\n",
      "url_counts = df.groupby('url').name.nunique()\n",
      "url_counts[url_counts==4].shape\n",
      "  41:\n",
      "url_counts = df.groupby('url').name.nunique()\n",
      "url_counts[url_counts==3].shape\n",
      "  42:\n",
      "url_counts = df.groupby('url').name.nunique()\n",
      "url_counts[url_counts==2].shape\n",
      "  43:\n",
      "url_counts = df.groupby('url').name.nunique()\n",
      "url_counts[url_counts>1].shape\n",
      "  44:\n",
      "url_counts = df[df.name.isin(['Dan', 'Anat']).groupby('url').name.nunique()\n",
      "url_counts[url_counts>1].shape\n",
      "  45:\n",
      "url_counts = df[df.name.isin(['Dan', 'Anat'])].groupby('url').name.nunique()\n",
      "url_counts[url_counts>1].shape\n",
      "  46:\n",
      "url_counts = df[df.name.isin(['Dan', 'Tzipy'])].groupby('url').name.nunique()\n",
      "url_counts[url_counts>1].shape\n",
      "  47:\n",
      "url_counts = df[df.name.isin(['Dan', 'Nurit'])].groupby('url').name.nunique()\n",
      "url_counts[url_counts>1].shape\n",
      "  48:\n",
      "url_counts = df[df.name.isin(['Dan', 'Nehoray'])].groupby('url').name.nunique()\n",
      "url_counts[url_counts>1].shape\n",
      "  49:\n",
      "url_counts = df[df.name.isin(['Dan', 'Nehoray'])].groupby('url').name.nunique()\n",
      "url_counts[url_counts>1].shape\n",
      "  50:\n",
      "url_counts = df[df.name.isin(['Dan', 'Tzipy'])].groupby('url').name.nunique()\n",
      "url_counts[url_counts>1].shape\n",
      "  51:\n",
      "url_counts = df[df.name.isin(['Dan', 'Nurit'])].groupby('url').name.nunique()\n",
      "url_counts[url_counts>1].shape\n",
      "  52:\n",
      "url_counts = df[df.name.isin(['Dan', 'Nurit'])].groupby('url').name.nunique()\n",
      "df[url.isin(url_counts[url_counts>1])\n",
      "  53: df.groupby(['url', 'name'])\n",
      "  54: df.groupby(['url', 'name']).size()\n",
      "  55: df.groupby(['url', 'name']).size().unstack()\n",
      "  56:\n",
      "df_asint = df.groupby(['url', 'name']).size().unstack().fillna(0).astype(int)\n",
      "coocc = df_asint.T.dot(df_asint)\n",
      "coocc\n",
      "  57:\n",
      "df_asint = df.groupby(['url', 'name']).size().unstack().fillna(0).astype(int)\n",
      "coocc = df_asint.T.dot(df_asint)\n",
      "coocc = coocc.where(np.triu(np.ones(coocc.shape)).astype(np.bool))\n",
      "  58: from sklearn.metrics import cohen_kappa_score\n",
      "  59:\n",
      "df_asint = df.groupby(['url', 'name']).size().unstack().fillna(0).astype(int)\n",
      "coocc = df_asint.T.dot(df_asint)\n",
      "coocc = coocc.where(np.triu(np.ones(coocc.shape)).astype(np.bool))\n",
      "coocc\n",
      "  60:\n",
      "df_asint = df.groupby(['url', 'name']).size().unstack().fillna(0).astype(int)\n",
      "coocc = df_asint.T.dot(df_asint)\n",
      "coocc = coocc.where(np.triu(np.ones(coocc.shape)).astype(np.bool)).astype(int)\n",
      "coocc\n",
      "  61:\n",
      "df_asint = df.groupby(['url', 'name']).size().unstack().fillna(0).astype(int)\n",
      "coocc = df_asint.T.dot(df_asint)\n",
      "coocc = coocc.where(np.triu(np.ones(coocc.shape)).astype(np.bool)).fillna(0).astype(int)\n",
      "coocc\n",
      "  62:\n",
      "df_asint = df.groupby(['url', 'name']).size().unstack().fillna(0).astype(int)\n",
      "coocc = df_asint.T.dot(df_asint)\n",
      "coocc = coocc.where(np.triu(np.ones(coocc.shape)).astype(np.bool)).fillna(0).astype(int).replace(0, '')\n",
      "coocc\n",
      "  63:\n",
      "df_asint = df.groupby(['url', 'name']).size().unstack().fillna(0).astype(int)\n",
      "coocc = df_asint.T.dot(df_asint)\n",
      "#coocc = coocc.where(np.triu(np.ones(coocc.shape)).astype(np.bool)).fillna(0).astype(int).replace(0, '')\n",
      "coocc\n",
      "  64:\n",
      "url_counts = df.groupby('url').name.nunique()\n",
      "df[url.isin(url_counts[url_counts==5])\n",
      "  65:\n",
      "url_counts = df.groupby('url').name.nunique()\n",
      "df[url.isin(url_counts[url_counts==5])[]\n",
      "  66:\n",
      "url_counts = df.groupby('url').name.nunique()\n",
      "df[url.isin(url_counts[url_counts==5])[]\n",
      "  67:\n",
      "url_counts = df.groupby('url').name.nunique()\n",
      "df[url.isin(url_counts[url_counts==5])]\n",
      "  68:\n",
      "url_counts = df.groupby('url').name.nunique()\n",
      "url_counts[url_counts==5]\n",
      "  69:\n",
      "url_counts = df.groupby('url').name.nunique()\n",
      "url_counts[url_counts==5].shape[0]\n",
      "  70:\n",
      "def get_agreement_array(df, name, col):\n",
      "    url_counts = df.groupby('url').name.nunique()\n",
      "    co = url_counts[url_counts==5].index\n",
      "    return df.loc[(df.name==name) & (df.url.isin(co)), [url, col]].sort_values('url')[col].tolist()\n",
      "  71:\n",
      "from sklearn.metrics import cohen_kappa_score\n",
      "cohen_kappa_score(get_agreement_array(df, 'Anat', 'is_relevant'), get_agreement_array(df, 'Tzipy', 'is_relevant'))\n",
      "  72:\n",
      "def get_agreement_array(df, name, col):\n",
      "    url_counts = df.groupby('url').name.nunique()\n",
      "    co = url_counts[url_counts==5].index\n",
      "    return df.loc[(df.name==name) & (df.url.isin(co)), ['url', col]].sort_values('url')[col].tolist()\n",
      "  73:\n",
      "from sklearn.metrics import cohen_kappa_score\n",
      "cohen_kappa_score(get_agreement_array(df, 'Anat', 'is_relevant'), get_agreement_array(df, 'Tzipy', 'is_relevant'))\n",
      "  74:\n",
      "from sklearn.metrics import cohen_kappa_score\n",
      "def get_kappa_score(df, name1, name2, col):\n",
      "    cohen_kappa_score(get_agreement_array(df, name1, col), get_agreement_array(df, name2, col))\n",
      "get_kappa_score(df, 'Anat', 'Tzipy', 'is_relevant')\n",
      "  75:\n",
      "from sklearn.metrics import cohen_kappa_score\n",
      "def get_kappa_score(df, name1, name2, col):\n",
      "    return cohen_kappa_score(get_agreement_array(df, name1, col), get_agreement_array(df, name2, col))\n",
      "get_kappa_score(df, 'Anat', 'Tzipy', 'is_relevant')\n",
      "  76:\n",
      "from itertools import combinations\n",
      "for name1, name2 in combinations(df.name.unique(), df.name.unique())\n",
      "  77:\n",
      "from itertools import combinations\n",
      "res = {}\n",
      "for name1, name2 in combinations(df.name.unique(), df.name.unique()):\n",
      "    res[(name1, name2)] = get_kappa_score(df, name1, name2, 'is_relevant')\n",
      "    \n",
      "res\n",
      "  78:\n",
      "from itertools import combinations\n",
      "res = {}\n",
      "for name1, name2 in combinations(list(df.name.unique()), list(df.name.unique())):\n",
      "    res[(name1, name2)] = get_kappa_score(df, name1, name2, 'is_relevant')\n",
      "    \n",
      "res\n",
      "  79:\n",
      "from itertools import combinations\n",
      "res = {}\n",
      "for name1, name2 in combinations(list(df.name.unique()), 2):\n",
      "    res[(name1, name2)] = get_kappa_score(df, name1, name2, 'is_relevant')\n",
      "    \n",
      "res\n",
      "  80:\n",
      "from itertools import combinations\n",
      "\n",
      "def get_all_scores(df, col):\n",
      "    res = []\n",
      "    for name1, name2 in combinations(list(df.name.unique()), 2):\n",
      "        res.append([(name1, name2, col, get_kappa_score(df, name1, name2, 'is_relevant'))]\n",
      "    return res\n",
      "get_all_scores('is_relevant')\n",
      "  81:\n",
      "from itertools import combinations\n",
      "\n",
      "def get_all_scores(df, col):\n",
      "    res = []\n",
      "    for name1, name2 in combinations(list(df.name.unique()), 2):\n",
      "        res.append((name1, name2, col, get_kappa_score(df, name1, name2, 'is_relevant')))\n",
      "    return res\n",
      "                   \n",
      "get_all_scores('is_relevant')\n",
      "  82:\n",
      "from itertools import combinations\n",
      "\n",
      "def get_all_scores(df, col):\n",
      "    res = []\n",
      "    for name1, name2 in combinations(list(df.name.unique()), 2):\n",
      "        res.append((name1, name2, col, get_kappa_score(df, name1, name2, 'is_relevant')))\n",
      "    return res\n",
      "                   \n",
      "get_all_scores(df, 'is_relevant')\n",
      "  83:\n",
      "from itertools import combinations\n",
      "\n",
      "def get_all_scores(df, cols):\n",
      "    res = []\n",
      "    for col in cols:\n",
      "        for name1, name2 in combinations(list(df.name.unique()), 2):\n",
      "            res.append((name1, name2, col, get_kappa_score(df, name1, name2, 'is_relevant')))\n",
      "    return res\n",
      "                   \n",
      "get_all_scores(df, ['is_active', 'is_relevant', 'content'])\n",
      "  84:\n",
      "from itertools import combinations\n",
      "\n",
      "def get_all_scores(df, cols):\n",
      "    res = []\n",
      "    for col in cols:\n",
      "        for name1, name2 in combinations(list(df.name.unique()), 2):\n",
      "            res.append((name1, name2, col, get_kappa_score(df, name1, name2, col)))\n",
      "    return res\n",
      "                   \n",
      "get_all_scores(df, ['is_active', 'is_relevant', 'content'])\n",
      "  85:\n",
      "from itertools import combinations\n",
      "\n",
      "def get_all_scores(df, cols):\n",
      "    res = []\n",
      "    for col in cols:\n",
      "        for name1, name2 in combinations(list(df.name.unique()), 2):\n",
      "            res.append((name1, name2, col, get_kappa_score(df, name1, name2, col)))\n",
      "    return res\n",
      "                   \n",
      "get_all_scores(df, ['is_active', 'is_relevant'])\n",
      "  86:\n",
      "from itertools import combinations\n",
      "\n",
      "def get_all_scores(df, cols):\n",
      "    res = []\n",
      "    for col in cols:\n",
      "        for name1, name2 in combinations(list(df.name.unique()), 2):\n",
      "            res.append((name1, name2, col, get_kappa_score(df, name1, name2, col)))\n",
      "    return res\n",
      "                   \n",
      "get_all_scores(df, ['is_relevant'])\n",
      "  87: df.is_relevant.value_counts()\n",
      "  88: df.is_active.value_counts()\n",
      "  89:\n",
      "from itertools import combinations\n",
      "\n",
      "def get_all_scores(df, cols):\n",
      "    res = []\n",
      "    for col in cols:\n",
      "        for name1, name2 in combinations(list(df.name.unique()), 2):\n",
      "            res.append((name1, name2, col, get_kappa_score(df, name1, name2, col)))\n",
      "    return res\n",
      "                   \n",
      "get_all_scores(df, ['is_page_active', 'is_relevant', 'content'])\n",
      "  90:\n",
      "from itertools import combinations\n",
      "\n",
      "def get_all_scores(df, cols):\n",
      "    res = []\n",
      "    for col in cols:\n",
      "        for name1, name2 in combinations(list(df.name.unique()), 2):\n",
      "            res.append((name1, name2, col, get_kappa_score(df, name1, name2, col)))\n",
      "    return pd.DataFrame(res)\n",
      "                   \n",
      "get_all_scores(df, ['is_page_active', 'is_relevant', 'content'])\n",
      "  91:\n",
      "from itertools import combinations\n",
      "\n",
      "def get_all_scores(df, cols):\n",
      "    res = []\n",
      "    for col in cols:\n",
      "        for name1, name2 in combinations(list(df.name.unique()), 2):\n",
      "            res.append((name1, name2, col, get_kappa_score(df, name1, name2, col)))\n",
      "    return pd.DataFrame(res, columns=['name1', 'name2', 'col', 'kappa']))\n",
      "                   \n",
      "get_all_scores(df, ['is_page_active', 'is_relevant', 'content'])\n",
      "  92:\n",
      "from itertools import combinations\n",
      "\n",
      "def get_all_scores(df, cols):\n",
      "    res = []\n",
      "    for col in cols:\n",
      "        for name1, name2 in combinations(list(df.name.unique()), 2):\n",
      "            res.append((name1, name2, col, get_kappa_score(df, name1, name2, col)))\n",
      "    return pd.DataFrame(res, columns=['name1', 'name2', 'col', 'kappa'])\n",
      "                   \n",
      "get_all_scores(df, ['is_page_active', 'is_relevant', 'content'])\n",
      "  93:\n",
      "from itertools import combinations\n",
      "\n",
      "def get_all_scores(df, cols):\n",
      "    res = []\n",
      "    for col in cols:\n",
      "        for name1, name2 in combinations(list(df.name.unique()), 2):\n",
      "            res.append((name1, name2, col, get_kappa_score(df, name1, name2, col)))\n",
      "    return pd.DataFrame(res, columns=['name1', 'name2', 'col', 'kappa'])\n",
      "                   \n",
      "get_all_scores(df, ['is_page_active', 'is_relevant', 'content']).groupby('col').kappa.mean()\n",
      "  94: df.is_page_active.value_counts()\n",
      "  95: df.content.value_counts()\n",
      "  96:\n",
      "header = [\"timestamp\",\"url\",\"is_page_active\",\"is_relevant\",\"date\",\"title\",\"content\",\"language\",\"name\"]\n",
      "df = pd.read_csv('data/appraisal2/filled/2014-URLs-Responses-all.csv', names=header, skiprows=1)\n",
      "df.head()\n",
      "  97:\n",
      "header = [\"timestamp\",\"url\",\"is_page_active\",\"is_relevant\",\"date\",\"title\",\"content\",\"language\",\"name\"]\n",
      "df = pd.read_csv('data/appraisal2/filled/2014-URLs-Responses-all.csv', names=header, skiprows=1)\n",
      "df.head()\n",
      "  98:\n",
      "header = [\"timestamp\",\"url\",\"is_page_active\",\"is_relevant\",\"date\",\"title\",\"content\",\"language\",\"name\"]\n",
      "df = pd.read_csv('data/appraisal2/filled/2014-URLs-Responses-all.csv', names=header, skiprows=1)\n",
      "df.head()\n",
      "  99:\n",
      "header = [\"timestamp\",\"url\",\"is_page_active\",\"is_relevant\",\"date\",\"title\",\"content\",\"language\",\"email\", \"name\"]\n",
      "df = pd.read_csv('data/appraisal2/filled/2014-URLs-Responses-all.csv', names=header, skiprows=1)\n",
      "df.head()\n",
      " 100:\n",
      "header = [\"timestamp\",\"url\",\"is_page_active\",\"is_relevant\",\"date\",\"title\",\"content\",\"language\",\"email\", \"name\"]\n",
      "df = pd.read_csv('data/appraisal2/filled/2014-URLs-Responses-all.csv', names=header, skiprows=1).drop('email', axis=1)\n",
      "df.head()\n",
      " 101:\n",
      "df = df.merge(uris.rename(columns={'URL', 'url'}), how='left')\n",
      "df.head()\n",
      " 102:\n",
      "df = df.merge(uris.rename(columns={'URL': 'url'}), how='left')\n",
      "df.head()\n",
      " 103:\n",
      "header = [\"timestamp\",\"url\",\"is_page_active\",\"is_relevant\",\"date\",\"title\",\"content\",\"language\",\"email\", \"name\"]\n",
      "df = (pd.read_csv('data/appraisal2/filled/2014-URLs-Responses-all.csv', names=header, skiprows=1,\n",
      "                  parse_dates=['timestamp', 'date'])\n",
      "      .drop('email', axis=1)\n",
      "      #.assign(timestamp=lambda x: pd.to_datetime)\n",
      "     )\n",
      "df.head()\n",
      " 104:\n",
      "header = [\"timestamp\",\"url\",\"is_page_active\",\"is_relevant\",\"date\",\"title\",\"content\",\"language\",\"email\", \"name\"]\n",
      "df = (pd.read_csv('data/appraisal2/filled/2014-URLs-Responses-all.csv', names=header, skiprows=1,\n",
      "                  parse_dates=['timestamp'])\n",
      "      .drop('email', axis=1)\n",
      "      #.assign(timestamp=lambda x: pd.to_datetime)\n",
      "     )\n",
      "df.head()\n",
      " 105:\n",
      "df = df.merge(uris.rename(columns={'URL': 'url'}), how='left')\n",
      "df.head()\n",
      " 106: df.groupby(['url', 'name']).size().hist()\n",
      " 107: %matplotlib inline\n",
      " 108: import matplotlib.pyplot as plt\n",
      " 109: df.groupby(['url', 'name']).size().hist()\n",
      " 110: df.groupby(['url', 'name']).size().value_counts()\n",
      " 111: df.sort_values('timestamp').drop_duplicates(subset=['url', 'name'], keep='last')\n",
      " 112: df = df.sort_values('timestamp').drop_duplicates(subset=['url', 'name'], keep='last')\n",
      " 113: df.groupby(['url', 'name']).size().value_counts()\n",
      " 114: df.groupby('url').size().value_counts()\n",
      " 115: (df.groupby('url').size()>1).sum()\n",
      " 116: df.groupby('url').size().value_counts()\n",
      " 117: dedup_df = df = df.drop_duplicates(subset='url', keep=False)\n",
      " 118: dedup_df.groupby('url').size().value_counts()\n",
      " 119: df.to_csv('2014-URLs-Responses-all-NO-USER-DUPLICATES.csv')\n",
      " 120: df.to_csv('2014-URLs-Responses-all-NO-SAME-USER-DUPLICATES.csv')\n",
      " 121: df.to_csv('data/appraisal2/filled/2014-URLs-Responses-all-NO-SAME-USER-DUPLICATES.csv')\n",
      " 122: dedup_df.to_csv('data/appraisal2/filled/2014-URLs-Responses-all-ONLY_ONE_PER_URL.csv')\n",
      " 123:\n",
      "header = [\"timestamp\",\"url\",\"is_page_active\",\"is_relevant\",\"date\",\"title\",\"content\",\"language\",\"email\", \"name\"]\n",
      "df = (pd.read_csv('data/appraisal2/filled/2014-URLs-Responses-all.csv', names=header, skiprows=1,\n",
      "                  parse_dates=['timestamp'])\n",
      "      .drop('email', axis=1)\n",
      "      #.assign(timestamp=lambda x: pd.to_datetime)\n",
      "     )\n",
      "df.head()\n",
      " 124:\n",
      "df = df.merge(uris.rename(columns={'URL': 'url'}), how='left')\n",
      "df.head()\n",
      " 125: df.groupby(['url', 'name']).size().value_counts()\n",
      " 126: df = df.sort_values('timestamp').drop_duplicates(subset=['url', 'name'], keep='last')\n",
      " 127: df.groupby(['url', 'name']).size().value_counts()\n",
      " 128: df.to_csv('data/appraisal2/filled/2014-URLs-Responses-all-NO-SAME-USER-DUPLICATES.csv')\n",
      " 129: df.groupby('url').size().value_counts()\n",
      " 130: dedup_df = df.drop_duplicates(subset='url', keep=False)\n",
      " 131: dedup_df.groupby('url').size().value_counts()\n",
      " 132: dedup_df.to_csv('data/appraisal2/filled/2014-URLs-Responses-all-ONLY_ONE_PER_URL.csv')\n",
      " 133: get_all_scores(df, ['is_page_active', 'is_relevant', 'content']).groupby('col').kappa.mean()\n",
      " 134:\n",
      "url_counts = df.groupby('url').name.nunique()\n",
      "url_counts[url_counts==7]\n",
      " 135:\n",
      "url_counts = df.groupby('url').name.nunique()\n",
      "url_counts[url_counts==7].size()\n",
      " 136:\n",
      "url_counts = df.groupby('url').name.nunique()\n",
      "url_counts[url_counts==7].shae()\n",
      " 137:\n",
      "url_counts = df.groupby('url').name.nunique()\n",
      "url_counts[url_counts==7].shape()\n",
      " 138:\n",
      "url_counts = df.groupby('url').name.nunique()\n",
      "url_counts[url_counts==7].shape\n",
      " 139:\n",
      "\n",
      "get_all_scores(df[df.url.isin(url_counts[url_counts==7])], ['is_page_active', 'is_relevant', 'content']).groupby('col').kappa.mean()\n",
      " 140: df[df.url.isin(url_counts[url_counts==7])]\n",
      " 141: df[df.url.isin(url_counts[url_counts==7].index)]\n",
      " 142:\n",
      "\n",
      "get_all_scores(df[df.url.isin(url_counts[url_counts==7].index)], ['is_page_active', 'is_relevant', 'content']).groupby('col').kappa.mean()\n",
      " 143: df[df.url.isin(url_counts[url_counts==7].index)]\n",
      " 144: df[df.url.isin(url_counts[url_counts==7].index)]\n",
      " 145: df.is_page_active.value_counts()\n",
      " 146:\n",
      "def get_agreement_array(df, name, col):\n",
      "    url_counts = df.groupby('url').name.nunique()\n",
      "    co = url_counts[url_counts==url_counts.max()].index\n",
      "    return df.loc[(df.name==name) & (df.url.isin(co)), ['url', col]].sort_values('url')[col].tolist()\n",
      " 147:\n",
      "from sklearn.metrics import cohen_kappa_score\n",
      "def get_kappa_score(df, name1, name2, col):\n",
      "    return cohen_kappa_score(get_agreement_array(df, name1, col), get_agreement_array(df, name2, col))\n",
      "get_kappa_score(df, 'Anat', 'Tzipy', 'is_relevant')\n",
      " 148:\n",
      "\n",
      "get_all_scores(df[df.url.isin(url_counts[url_counts==7].index)], ['is_page_active', 'is_relevant', 'content']).groupby('col').kappa.mean()\n",
      " 149:\n",
      "\n",
      "get_all_scores(df, ['is_page_active', 'is_relevant', 'content']).groupby('col').kappa.mean()\n",
      " 150:\n",
      "def get_agreement_array(df, name, col):\n",
      "    url_counts = df.groupby('url').name.nunique()\n",
      "    m = url_counts.max()\n",
      "    co = url_counts[url_counts==m].index\n",
      "    return df.loc[(df.name==name) & (df.url.isin(co)), ['url', col]].sort_values('url')[col].tolist()\n",
      " 151:\n",
      "from sklearn.metrics import cohen_kappa_score\n",
      "def get_kappa_score(df, name1, name2, col):\n",
      "    return cohen_kappa_score(get_agreement_array(df, name1, col), get_agreement_array(df, name2, col))\n",
      "get_kappa_score(df, 'Anat', 'Tzipy', 'is_relevant')\n",
      " 152:\n",
      "\n",
      "get_all_scores(df, ['is_page_active', 'is_relevant', 'content']).groupby('col').kappa.mean()\n",
      " 153:\n",
      "from itertools import combinations\n",
      "\n",
      "def get_all_scores(df, cols):\n",
      "    res = []\n",
      "    for col in cols:\n",
      "        for name1, name2 in combinations(list(df.name.unique()), 2):\n",
      "            print (name1, name2)\n",
      "            res.append((name1, name2, col, get_kappa_score(df, name1, name2, col)))\n",
      "    return pd.DataFrame(res, columns=['name1', 'name2', 'col', 'kappa'])\n",
      "                   \n",
      "get_all_scores(df, ['is_page_active', 'is_relevant', 'content']).groupby('col').kappa.mean()\n",
      " 154: df.name.isna().sum()\n",
      " 155: df.name.isna()\n",
      " 156: df = df.dropna(subset=['name'])\n",
      " 157:\n",
      "url_counts = df.groupby('url').name.nunique()\n",
      "url_counts[url_counts==7].shape\n",
      " 158: df[df.url.isin(url_counts[url_counts==7].index)]\n",
      " 159:\n",
      "\n",
      "get_all_scores(df, ['is_page_active', 'is_relevant', 'content']).groupby('col').kappa.mean()\n",
      " 160:\n",
      "from itertools import combinations\n",
      "\n",
      "def get_all_scores(df, cols):\n",
      "    res = []\n",
      "    for col in cols:\n",
      "        for name1, name2 in combinations(list(df.name.unique()), 2):\n",
      "            res.append((name1, name2, col, get_kappa_score(df, name1, name2, col)))\n",
      "    return pd.DataFrame(res, columns=['name1', 'name2', 'col', 'kappa'])\n",
      " 161:\n",
      "              \n",
      "get_all_scores(df, ['is_page_active', 'is_relevant', 'content']).groupby('col').kappa.mean()\n",
      " 162:\n",
      "\n",
      "get_all_scores(df, ['is_page_active', 'is_relevant', 'content']).groupby('col').kappa.mean()\n",
      " 163:\n",
      "\n",
      "get_all_scores(df, ['is_page_active', 'is_relevant', 'content']).groupby('col').kappa\n",
      " 164:\n",
      "\n",
      "cohen_kappa = get_all_scores(df, ['is_page_active', 'is_relevant', 'content'])\n",
      "cohen_kappa\n",
      " 165:\n",
      "\n",
      "cohen_kappa = get_all_scores(df, ['is_page_active', 'is_relevant', 'content'])\n",
      "cohen_kappa.head(10)\n",
      " 166: cohen_kappa.groupby('col').kappa.mean()\n",
      " 167: cohen_kappa.groupby('name1').kappa.mean()\n",
      " 168: cohen_kappa.groupby('name2').kappa.mean()\n",
      " 169: from statsmodels.stats.inter_rater import fleiss_kappa\n",
      " 170: !pip install krippendorff\n",
      " 171:\n",
      "kdf = df[df.url.isin(url_counts[url_counts>1].index)]\n",
      "kdf.shape\n",
      " 172: url_counts[url_counts>1].index\n",
      " 173: url_counts[url_counts>1]\n",
      " 174: url_counts.value_counts()\n",
      " 175: url_counts[url_counts>1].shape\n",
      " 176: url_counts[url_counts>1].index.shape\n",
      " 177: url_counts[url_counts>1].index\n",
      " 178:\n",
      "kdf = df[df.url.isin(url_counts[url_counts>1].index)]\n",
      "kdf.shape\n",
      " 179:\n",
      "kdf = df[df.url.isin(url_counts[url_counts>1].index)]\n",
      "kdf.shape\n",
      " 180: kdf.url.nunique()\n",
      " 181: kdf.pivot_table(index=['url'], values='is_relevant', columns='name')\n",
      " 182: kdf.pivot_table(index=['url'], values='is_relevant', columns='name', aggfunc='first')\n",
      " 183: kdf.pivot_table(index=['url'], values='is_relevant', columns='name', aggfunc='first').values()\n",
      " 184: kdf.pivot_table(index=['url'], values='is_relevant', columns='name', aggfunc='first').values\n",
      " 185: krippendorff.alpha(kdf.pivot_table(index=['url'], values='is_relevant', columns='name', aggfunc='first').values)\n",
      " 186: import krippendorff\n",
      " 187: krippendorff.alpha(kdf.pivot_table(index=['url'], values='is_relevant', columns='name', aggfunc='first').values)\n",
      " 188:\n",
      "from sklearn import preprocessing\n",
      "le = preprocessing.LabelEncoder()\n",
      "le.fit(kdf.pivot_table(index=['url'], values='is_relevant', columns='name', aggfunc='first').values)\n",
      " 189:\n",
      "kdf = df[df.url.isin(url_counts[url_counts>1].index)]\n",
      "df['is_relevant'] = pd.Categorical(df.zipcode)\n",
      "df['is_page_active'] = pd.Categorical(df.zipcode)\n",
      "df['content'] = pd.Categorical(df.zipcode)\n",
      "\n",
      "kdf.shape\n",
      " 190:\n",
      "kdf = df[df.url.isin(url_counts[url_counts>1].index)]\n",
      "kdf['is_relevant'] = pd.Categorical(kdf.is_relevant)\n",
      "kdf['is_page_active'] = pd.Categorical(kdf.is_page_active)\n",
      "kdf['content'] = pd.Categorical(kdf.content)\n",
      "\n",
      "kdf.shape\n",
      " 191: kdf.url.nunique()\n",
      " 192: kdf.pivot_table(index=['url'], values='is_relevant', columns='name', aggfunc='first').values\n",
      " 193:\n",
      "from sklearn import preprocessing\n",
      "le = preprocessing.LabelEncoder()\n",
      "le.fit(kdf.pivot_table(index=['url'], values='is_relevant', columns='name', aggfunc='first').values)\n",
      " 194: import krippendorff\n",
      " 195: krippendorff.alpha(kdf.pivot_table(index=['url'], values='is_relevant', columns='name', aggfunc='first').values)\n",
      " 196: kdf.pivot_table(index=['url'], values='is_relevant', columns='name', aggfunc='first').cat.codes\n",
      " 197: kdf.pivot_table(index=['url'], values='is_relevant', columns='name', aggfunc='first').values\n",
      " 198:\n",
      "from sklearn.preprocessing import OrdinalEncoder\n",
      "enc = OrdinalEncoder()\n",
      "enc.fit(kdf.pivot_table(index=['url'], values='is_relevant', columns='name', aggfunc='first').values)\n",
      " 199:\n",
      "from sklearn.preprocessing import OrdinalEncoder\n",
      "enc = OrdinalEncoder()\n",
      "enc.fit(kdf.pivot_table(index=['url'], values='is_relevant', columns='name', aggfunc='first').values)\n",
      " 200:\n",
      "from sklearn import preprocessing\n",
      "le = preprocessing.LabelEncoder()\n",
      "le.fit(kdf.fillna('---').pivot_table(index=['url'], values='is_relevant', columns='name', aggfunc='first').values)\n",
      " 201:\n",
      "kdf = df[df.url.isin(url_counts[url_counts>1].index)].fillna('---')\n",
      "kdf['is_relevant'] = pd.Categorical(kdf.is_relevant)\n",
      "kdf['is_page_active'] = pd.Categorical(kdf.is_page_active)\n",
      "kdf['content'] = pd.Categorical(kdf.content)\n",
      "\n",
      "kdf.shape\n",
      " 202: kdf.url.nunique()\n",
      " 203: kdf.pivot_table(index=['url'], values='is_relevant', columns='name', aggfunc='first').values\n",
      " 204:\n",
      "from sklearn import preprocessing\n",
      "le = preprocessing.LabelEncoder()\n",
      "le.fit(kdf.pivot_table(index=['url'], values='is_relevant', columns='name', aggfunc='first').values)\n",
      " 205: krippendorff.alpha(kdf.pivot_table(index=['url'], values='is_relevant', columns='name', aggfunc='first').values)\n",
      " 206:\n",
      "kdf = df[df.url.isin(url_counts[url_counts>1].index)].fillna('---')\n",
      "kdf['is_relevant']    = pd.Categorical(kdf.is_relevant)\n",
      "kdf['is_page_active'] = pd.Categorical(kdf.is_page_active)\n",
      "kdf['content']        = pd.Categorical(kdf.content)\n",
      "kdf['is_relevant_cat'] = kdf.is_relevant.cat.codes \n",
      "kdf['is_page_active_cat'] = kdf.is_page_active.cat.codes\n",
      "kdf['content_cat'] = kdf.content.cat.codes\n",
      "kdf.shape\n",
      " 207: kdf.url.nunique()\n",
      " 208: kdf.pivot_table(index=['url'], values='is_relevant_cat', columns='name', aggfunc='first').values\n",
      " 209: krippendorff.alpha(kdf.pivot_table(index=['url'], values='is_relevant_cat', columns='name', aggfunc='first').values)\n",
      " 210: krippendorff.alpha(kdf.pivot_table(index='name', values='is_relevant_cat', columns='url', aggfunc='first').values)\n",
      " 211: krippendorff.alpha(kdf.pivot_table(index='name', values='is_page_active_cat', columns='url', aggfunc='first').values)\n",
      " 212: krippendorff.alpha(kdf.pivot_table(index='name', values='content_cat', columns='url', aggfunc='first').values)\n",
      " 213: krippendorff.alpha(kdf.pivot_table(index='name', values='is_relevant_cat', columns='url', aggfunc='first').values)\n",
      " 214: krippendorff.alpha(kdf.pivot_table(index='name', values='is_page_active_cat', columns='url', aggfunc='first').values)\n",
      " 215: krippendorff.alpha(kdf.pivot_table(index='name', values='content_cat', columns='url', aggfunc='first').values)\n",
      " 216:\n",
      "kdf = df[df.url.isin(url_counts[url_counts>1].index)]\n",
      "kdf['is_relevant']    = pd.Categorical(kdf.is_relevant)\n",
      "kdf['is_page_active'] = pd.Categorical(kdf.is_page_active)\n",
      "kdf['content']        = pd.Categorical(kdf.content)\n",
      "kdf['is_relevant_cat'] = kdf.is_relevant.cat.codes \n",
      "kdf['is_page_active_cat'] = kdf.is_page_active.cat.codes\n",
      "kdf['content_cat'] = kdf.content.cat.codes\n",
      "kdf.shape\n",
      " 217: kdf.url.nunique()\n",
      " 218: kdf.pivot_table(index='name', values='is_relevant_cat', columns='url', aggfunc='first').values\n",
      " 219: import krippendorff\n",
      " 220: krippendorff.alpha(kdf.pivot_table(index='name', values='is_relevant_cat', columns='url', aggfunc='first').values)\n",
      " 221: krippendorff.alpha(kdf.pivot_table(index='name', values='is_page_active_cat', columns='url', aggfunc='first').values)\n",
      " 222: krippendorff.alpha(kdf.pivot_table(index='name', values='content_cat', columns='url', aggfunc='first').values)\n",
      " 223:\n",
      "kdf = df[df.url.isin(url_counts[url_counts>6].index)]\n",
      "kdf['is_relevant']    = pd.Categorical(kdf.is_relevant)\n",
      "kdf['is_page_active'] = pd.Categorical(kdf.is_page_active)\n",
      "kdf['content']        = pd.Categorical(kdf.content)\n",
      "kdf['is_relevant_cat'] = kdf.is_relevant.cat.codes \n",
      "kdf['is_page_active_cat'] = kdf.is_page_active.cat.codes\n",
      "kdf['content_cat'] = kdf.content.cat.codes\n",
      "kdf.shape\n",
      " 224: kdf.url.nunique()\n",
      " 225: kdf.pivot_table(index='name', values='is_relevant_cat', columns='url', aggfunc='first').values\n",
      " 226: import krippendorff\n",
      " 227: krippendorff.alpha(kdf.pivot_table(index='name', values='is_relevant_cat', columns='url', aggfunc='first').values)\n",
      " 228: krippendorff.alpha(kdf.pivot_table(index='name', values='is_page_active_cat', columns='url', aggfunc='first').values)\n",
      " 229: krippendorff.alpha(kdf.pivot_table(index='name', values='content_cat', columns='url', aggfunc='first').values)\n",
      " 230: kdf.pivot_table(index='name', values='is_relevant_cat', columns='url', aggfunc='first').values.shape\n",
      " 231: kdf.pivot_table(index='url', values='is_page_active', columns='name', aggfunc='first')\n",
      " 232: df.shape\n",
      " 233:\n",
      "header = [\"timestamp\",\"url\",\"is_page_active\",\"is_relevant\",\"date\",\"title\",\"content\",\"language\",\"email\", \"name\"]\n",
      "df = (pd.read_csv('data/appraisal2/filled/2014-URLs-Responses-all.csv', names=header, skiprows=1,\n",
      "                  parse_dates=['timestamp'])\n",
      "      .drop('email', axis=1)\n",
      "      #.assign(timestamp=lambda x: pd.to_datetime)\n",
      "     )\n",
      "df.head()\n",
      " 234: df.shape\n",
      " 235: df.name.value_counts()\n",
      " 236: %history -g\n"
     ]
    }
   ],
   "source": [
    "%history -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omilab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/omilab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/omilab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/home/omilab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/home/omilab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/omilab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1071, 12)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdf = df[df.url.isin(url_counts[url_counts>6].index)]\n",
    "kdf['is_relevant']    = pd.Categorical(kdf.is_relevant)\n",
    "kdf['is_page_active'] = pd.Categorical(kdf.is_page_active)\n",
    "kdf['content']        = pd.Categorical(kdf.content)\n",
    "kdf['is_relevant_cat'] = kdf.is_relevant.cat.codes \n",
    "kdf['is_page_active_cat'] = kdf.is_page_active.cat.codes\n",
    "kdf['content_cat'] = kdf.content.cat.codes\n",
    "kdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdf.url.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 2, ..., 2, 2, 2],\n",
       "       [0, 2, 2, ..., 2, 2, 2],\n",
       "       [0, 2, 2, ..., 2, 2, 2],\n",
       "       ...,\n",
       "       [1, 2, 2, ..., 2, 2, 2],\n",
       "       [1, 2, 2, ..., 1, 2, 2],\n",
       "       [1, 2, 2, ..., 2, 2, 2]], dtype=int8)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdf.pivot_table(index='name', values='is_relevant_cat', columns='url', aggfunc='first').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7843723411925878"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import krippendorff\n",
    "krippendorff.alpha(kdf.pivot_table(index='name', values='is_relevant_cat', columns='url', aggfunc='first').values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6538947104883386"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "krippendorff.alpha(kdf.pivot_table(index='name', values='is_page_active_cat', columns='url', aggfunc='first').values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.536110681053003"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "krippendorff.alpha(kdf.pivot_table(index='name', values='content_cat', columns='url', aggfunc='first').values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appraisal2 analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>source</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>uri</th>\n",
       "      <th>uri_cln</th>\n",
       "      <th>tld</th>\n",
       "      <th>is_shortened</th>\n",
       "      <th>uri_unshrtn</th>\n",
       "      <th>uri_cln_unshrtn</th>\n",
       "      <th>uri_unshrtn_bitly</th>\n",
       "      <th>...</th>\n",
       "      <th>tld_unshrtn</th>\n",
       "      <th>uri_cln_wb_avail</th>\n",
       "      <th>uri_cln_unshrtn_wb_avail</th>\n",
       "      <th>uri_unsh_no_clean</th>\n",
       "      <th>uri_mem_avail</th>\n",
       "      <th>uri_unsh_no_clean_mem_avail</th>\n",
       "      <th>status_code</th>\n",
       "      <th>content_type</th>\n",
       "      <th>archived</th>\n",
       "      <th>is_unshrtn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zh</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>2018-08-22 13:21:14</td>\n",
       "      <td>https://web.archive.org/web/20150626164255/htt...</td>\n",
       "      <td>web.archive.org/web/20150626164255/http://www....</td>\n",
       "      <td>archive.org</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>web.archive.org/web/20150626164255/http://www....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>archive.org</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://web.archive.org/web/20150626164255/htt...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>200.0</td>\n",
       "      <td>text/html; charset=utf-8</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>zh</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>2018-06-25 15:15:42</td>\n",
       "      <td>https://web.archive.org/web/20140727005451/htt...</td>\n",
       "      <td>web.archive.org/web/20140727005451/http://trav...</td>\n",
       "      <td>archive.org</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>web.archive.org/web/20140727005451/http://trav...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>archive.org</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://web.archive.org/web/20140727005451/htt...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>200.0</td>\n",
       "      <td>text/html;charset=utf-8</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>zh</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>2017-11-10 21:53:43</td>\n",
       "      <td>https://archive.is/20140828023420/http://www.w...</td>\n",
       "      <td>archive.is/20140828023420/http://www.worldjour...</td>\n",
       "      <td>archive.is</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>archive.is/20140828023420/http://www.worldjour...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>archive.is</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://archive.is/20140828023420/http://www.w...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>200.0</td>\n",
       "      <td>text/html;charset=utf-8</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zh</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>2017-11-10 21:53:43</td>\n",
       "      <td>https://web.archive.org/web/20140803200140/htt...</td>\n",
       "      <td>web.archive.org/web/20140803200140/http://www....</td>\n",
       "      <td>archive.org</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>web.archive.org/web/20140803200140/http://www....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>archive.org</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://web.archive.org/web/20140803200140/htt...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>200.0</td>\n",
       "      <td>text/html; charset=utf-8-sig</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zh</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>2017-11-10 21:53:43</td>\n",
       "      <td>https://web.archive.org/web/20140808044248/htt...</td>\n",
       "      <td>web.archive.org/web/20140808044248/http://m.al...</td>\n",
       "      <td>archive.org</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>web.archive.org/web/20140808044248/http://m.al...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>archive.org</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://web.archive.org/web/20140808044248/htt...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>200.0</td>\n",
       "      <td>text/html; charset=utf-8</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang     source            timestamp  \\\n",
       "0   zh  wikipedia  2018-08-22 13:21:14   \n",
       "1   zh  wikipedia  2018-06-25 15:15:42   \n",
       "2   zh  wikipedia  2017-11-10 21:53:43   \n",
       "3   zh  wikipedia  2017-11-10 21:53:43   \n",
       "4   zh  wikipedia  2017-11-10 21:53:43   \n",
       "\n",
       "                                                 uri  \\\n",
       "0  https://web.archive.org/web/20150626164255/htt...   \n",
       "1  https://web.archive.org/web/20140727005451/htt...   \n",
       "2  https://archive.is/20140828023420/http://www.w...   \n",
       "3  https://web.archive.org/web/20140803200140/htt...   \n",
       "4  https://web.archive.org/web/20140808044248/htt...   \n",
       "\n",
       "                                             uri_cln          tld  \\\n",
       "0  web.archive.org/web/20150626164255/http://www....  archive.org   \n",
       "1  web.archive.org/web/20140727005451/http://trav...  archive.org   \n",
       "2  archive.is/20140828023420/http://www.worldjour...   archive.is   \n",
       "3  web.archive.org/web/20140803200140/http://www....  archive.org   \n",
       "4  web.archive.org/web/20140808044248/http://m.al...  archive.org   \n",
       "\n",
       "   is_shortened uri_unshrtn  \\\n",
       "0         False         NaN   \n",
       "1         False         NaN   \n",
       "2         False         NaN   \n",
       "3         False         NaN   \n",
       "4         False         NaN   \n",
       "\n",
       "                                     uri_cln_unshrtn uri_unshrtn_bitly  ...  \\\n",
       "0  web.archive.org/web/20150626164255/http://www....               NaN  ...   \n",
       "1  web.archive.org/web/20140727005451/http://trav...               NaN  ...   \n",
       "2  archive.is/20140828023420/http://www.worldjour...               NaN  ...   \n",
       "3  web.archive.org/web/20140803200140/http://www....               NaN  ...   \n",
       "4  web.archive.org/web/20140808044248/http://m.al...               NaN  ...   \n",
       "\n",
       "   tld_unshrtn uri_cln_wb_avail  uri_cln_unshrtn_wb_avail  \\\n",
       "0  archive.org            False                     False   \n",
       "1  archive.org            False                     False   \n",
       "2   archive.is            False                     False   \n",
       "3  archive.org            False                     False   \n",
       "4  archive.org            False                     False   \n",
       "\n",
       "                                   uri_unsh_no_clean uri_mem_avail  \\\n",
       "0  https://web.archive.org/web/20150626164255/htt...         False   \n",
       "1  https://web.archive.org/web/20140727005451/htt...         False   \n",
       "2  https://archive.is/20140828023420/http://www.w...         False   \n",
       "3  https://web.archive.org/web/20140803200140/htt...         False   \n",
       "4  https://web.archive.org/web/20140808044248/htt...         False   \n",
       "\n",
       "   uri_unsh_no_clean_mem_avail  status_code                  content_type  \\\n",
       "0                        False        200.0      text/html; charset=utf-8   \n",
       "1                        False        200.0       text/html;charset=utf-8   \n",
       "2                        False        200.0       text/html;charset=utf-8   \n",
       "3                        False        200.0  text/html; charset=utf-8-sig   \n",
       "4                        False        200.0      text/html; charset=utf-8   \n",
       "\n",
       "  archived  is_unshrtn  \n",
       "0    False       False  \n",
       "1    False       False  \n",
       "2    False       False  \n",
       "3    False       False  \n",
       "4    False       False  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clut.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "clut['is_unshrtn'] = clut.uri_cln_unshrtn!=clut.uri_cln\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    271386\n",
       "True      56706\n",
       "Name: is_shortened, dtype: int64"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clut.is_shortened.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name\n",
       "Anat       394\n",
       "Dan        292\n",
       "Nehoray    204\n",
       "Nurit      289\n",
       "Sveta      292\n",
       "Tzipy      296\n",
       "Yonatan    286\n",
       "Name: uri, dtype: int64"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca = (clut.merge((dedup_df[['url', 'is_page_active', 'is_relevant', 'date', 'title', 'content', 'language', 'name']]\n",
    "            .rename(columns={'url': 'uri_unsh_no_clean'})), how='inner'))\n",
    "\n",
    "ca.groupby('name').uri.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    15545\n",
       "True      1486\n",
       "Name: is_shortened, dtype: int64"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca.is_shortened.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_page_active</th>\n",
       "      <th>No</th>\n",
       "      <th>Yes</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <th>is_shortened</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>google</th>\n",
       "      <th>False</th>\n",
       "      <td>34</td>\n",
       "      <td>329</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">twitter</th>\n",
       "      <th>False</th>\n",
       "      <td>85</td>\n",
       "      <td>243</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>103</td>\n",
       "      <td>1140</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wikipedia</th>\n",
       "      <th>False</th>\n",
       "      <td>32</td>\n",
       "      <td>225</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">youtube</th>\n",
       "      <th>False</th>\n",
       "      <td>15</td>\n",
       "      <td>31</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "is_page_active           No   Yes  ratio\n",
       "source    is_shortened                  \n",
       "google    False          34   329   0.91\n",
       "twitter   False          85   243   0.74\n",
       "          True          103  1140   0.92\n",
       "wikipedia False          32   225   0.88\n",
       "youtube   False          15    31   0.67\n",
       "          True            0     1   1.00"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ca\n",
    " .assign(is_page_active = lambda x: x.is_page_active.replace('No.*', 'No',regex=True))\n",
    " .groupby(['source', 'is_shortened', 'is_page_active']).uri.nunique()\n",
    " .unstack().fillna(0).astype(int)\n",
    " .assign(ratio = lambda x: np.round((x.Yes /(x.No+x.Yes)), 2))\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Yes                    13967\n",
       "No                      1840\n",
       "Content Unavailable     1214\n",
       "Name: is_relevant, dtype: int64"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca.is_relevant.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_relevant</th>\n",
       "      <th>Content Unavailable</th>\n",
       "      <th>No</th>\n",
       "      <th>Yes</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <th>is_shortened</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>google</th>\n",
       "      <th>False</th>\n",
       "      <td>31</td>\n",
       "      <td>115</td>\n",
       "      <td>217</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">twitter</th>\n",
       "      <th>False</th>\n",
       "      <td>78</td>\n",
       "      <td>30</td>\n",
       "      <td>220</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>99</td>\n",
       "      <td>216</td>\n",
       "      <td>928</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wikipedia</th>\n",
       "      <th>False</th>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>217</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">youtube</th>\n",
       "      <th>False</th>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "is_relevant             Content Unavailable   No  Yes  ratio\n",
       "source    is_shortened                                      \n",
       "google    False                          31  115  217   0.60\n",
       "twitter   False                          78   30  220   0.67\n",
       "          True                           99  216  928   0.75\n",
       "wikipedia False                          30   10  217   0.84\n",
       "youtube   False                          14   17   15   0.33\n",
       "          True                            0    1    0   0.00"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ca\n",
    " .assign(is_relevant = lambda x: x.is_relevant.replace('No.*', 'No',regex=True))\n",
    " .groupby(['source', 'is_shortened', 'is_relevant']).uri.nunique()\n",
    " .unstack().fillna(0).astype(int)\n",
    " .assign(ratio = lambda x: np.round((x.Yes /(x.No+x.Yes+x['Content Unavailable'])), 2))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
